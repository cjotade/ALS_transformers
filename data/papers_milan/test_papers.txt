The first equality from the continuity of the function with respect to as . Then from the fact that , we have that , 
 , a .. Finally , proving that a .. an equivalent symmetric argument and we omit it . 
 and , we have that from and . Let us fix an arbitrary . Considering that 
 , then eventually in , and therefore eventually , which from definition of that . Finally , the fact the event almost surely the result . 
 V . OF THE RATE APPROXIMATION ERROR CURVE FOR NON COMPRESSIBLE 
 For the family of non compressible ergodic , in this section we study two tail that characterize the achievable rate distortion region in . Let be stationary and ergodic with its invariant probability measure . Here we focus on the case where , then the measure in is well defined and by construction , where the Radon derivative or density of with respect to is given by for all . Furthermore , from the strict positivity of 
 all the in , and any distortion is in with a given rate . 
 In summary , the rate distortion approximation function is continuous , and all the , in the sense that for any there is only one such that . In addition , it is strictly decreasing and the following boundary : and . Furthermore , it is simple to verify that and 
 should be a non increasing function as progress to considering that , and consequently , 
 should present a convex behavior eventually as to . The next section the convexity of more formally . 
 First in this section we show that is a convex function of . Then , we provide a necessary and sufficient condition for to be a convex function of . 
 Theorem : Let by a non compressible stationary and ergodic sequence with . Then 
 Remark : The condition in is implicit and may be difficult to verify . A simple to check sufficient condition for the convexity of is the following : 
 where . In particular , a non increasing i . e ., almost everywhere in a convex rate approximation error function . 
 We present few of rate approximation error of non compressible i . i . In particular following , we consider the exponentially distribution , which is non compressible for any , from Corollary , and the family of Student distribution with parameter 
 , whose goes to cero as . From Corollary , the i . i .. process with a Student distribution is compressible for any and non compressible for 
 , for all . For this we consider an estimation approach . Considering a sufficiently large set of i . i .. of let say , the law of large us that for any and , 
 The of a Student distribution with of freedom is given by , where is the gamma function . 
 Fig . . Numerically rate approximation error for several non compressible i . i .. and one compressible i . i .. process . 
 the use of . Then , by sampling the space of and considering a sufficiently large , we can estimate with an arbitrary good precision the rate distortion region 
 . Following this path , Fig . the rate approximation error for the , and several Student distribution for . We verify that some Student distribution are compressible and and are non compressible , and 
 as the Theorem . More interesting is to validate in all the of non compressible , that the have a convex behavior , which is from . Furthermore , the density with the exponentially tail is less compressible than any prior with a power law decay , in the sense that for a distortion the i . i .. process needs a higher rate . From these , as goes to infinity the i . i .. process with a heavy tail distribution the approximation error behavior of the law . 
 Fig . the rate approximation error for the prior for different of . It is interesting to observe the increasing monotonic behavior of as , for any fixed value of . Again all have a convex behavior . To contrast , Fig . a set of for the distribution i . e ., Student distribution with 
 We conclude this work compressible stationary and ergodic , as in Theorem , in of their ability to be with an arbitrary small proportion of linear for that the classical compressed measurement and reconstruction setting . In particular , the focus is on compressible , as the standard i . i .. linear acquisition and 
 minimization sparsity of , offer a well known instance guarantee stated below in Lemma that the modeling assumption of compressible . 
 is a linear operator , that given a signal a measurement vector . The case of interest is on the under regime , i . e . where under sparse or compressible on , can offer perfect or near optimal reconstruction by the solution of the following linear problem : 
 Notably , the theory , based on the restricted isometry property RIP , sufficient over and implicitly over the number of in order that 
 , when for some . The next result , in its original form stated in , that random offer a solution to that problem with a near optimal relationship between and . 
 Lemma : , Th . . and , Th . . Let be a random matrix whose are driven by i . i . of a distribution or a binary variable with uniform distribution over . For any arbitrary and , we have that : 
 with a probability , over the sampling space , at least equal to . Here is the solution of 
 The proof of this result directly from , Th . . and , Th . . 
 Here we formalize the reconstruction of infinite . For this , we consider a finite length or fixed rate approach , where the idea is to analyze consecutive finite block of the sequence , i . e ., to sense and reconstruct for any , and study reconstruction in the limit when the block length to infinity . 
 More precisely , let be a sequence of positive such that . From this sequence , we consider the family of where for any , is the random matrix of by i . i .. as in Lemma , and is the function from to that the minimization problem in . Given a sequence and any finite block length , we can apply the approach over to recover , which is a random reconstruction function of the matrix . In relation with the relative approximation error in , we consider as a fidelity indicator the noise to signal ratio given by : 
 More generally , if we have a process with distribution and a sequence of with its associated finite block scheme , we can also analyze the finite block performance of the scheme by the object 
 of two independent random : the vector and the random matrix . Therefore , it is important to consider the average with respect to the statistics of the source i . e ., 
 Then the question we focus here is : for an compressible process that , what is the minimum rate of i . e . or more generally 
 with probability one with respect to the statistics of the sequence of random matrices 
 From Theorem and the RIP based instance result of the setting in Lemma , we can state the following result : 
 Theorem : Let be a stationary ergodic process . If is compressible , then for any sequence such 
 : This result that in order to achieve zero distortion in the reconstruction for an compressible process , almost surely in the sense of , the scheme needs an arbitrary small number of per sample . In other , under the compressibility model assumption for the process , the minimum rate to achieve zero distortion is zero for the scheme . Then , it is remarkable to validate that is able to achieve the same zero critical rate that it is by the analysis of the pure oracle best term approximation error of compressible process see the result in Lemma . 
 : This result that the lucid notion of compressibility by Amini al . really in a meaningful performance result for the classical setting in the asymptotic regime when the goes to infinity . In other , we can say that 
 compressibility , meaning a sort of zero rate of innovation in the process , zero rate of per signal dimension for perfect recovery in the sense of distortion for the scheme . This result a gap not in between their notion of compressibility and performance guarantee in the asymptotic regime . 
 : Concerning compressibility of random and performance guarantee , we want to highlight the work of al . for the case of i . i . They show in , Theorem that if then 
 of Theorem for the i . i .. case , Remark . Then , we want to give credit to this contribution to be the first result that a connection between of compressibility for i . i .. based on relative approximation and the performance in the asymptotic regime of the classical scheme . In this context , Theorem can be seen as an extension of these to the case of stationary and ergodic and , in the technical side , an extension on the use of the instance property of the 
 minimization . On the other hand , on the i . i .. context , and offer a way to verify that Amini al . compressible notion variable rate in nature a connection with the in al . , Th . and . in of what can for the case of compressible . 
 The main result of this work Theorem a connection between al . , Prop . almost sure convergence result of relative approximation , and Amini al . notion of compressibility for random . More importantly , Theorem new to extend that connection and , consequently , a dichotomy between being and non being compressible random to the family of stationary and ergodic . This extension is over the almost sure convergence of the empirical of and , respectively see in the statement of Theorem to the true on the family of tail in Section . 
 The idea of looking at specific empirical as the basic object of interest , in b and c , instead of the statistics of the sum of the ordered sequence as considered in , Prop . and , was essential to extend the analysis from the i . i .. case to the case of stationary ergodic . 
 Finally , one can notice from the proof of Theorem that this result does not rely on a stationary property , as it is essentially over the family of indicator of the tail in . Then , we conjecture that the analysis of compressible can be extended over a family of random with a specific ergodic property over the tail , which is an interesting direction for future work . This observation us to put the attention on the general theory of non stationary with ergodic , , . 
 Point : of all and in : Proposition and and the hypothesis that 
 in . Then , Proposition e , we have that all the and in the range . 
 First , it is important to verify that the implicit characterization of in and a well defined function . By contradiction , let assume that such characterization is not a function in the sense that for a given distortion 
 there are two solution of associated to two different . This last condition that from , which the fact that and are of . 
 Moving to the continuity , let us fix an arbitrary and . Then by the of the , 
 such that . On the other hand , by the of the , there , where 
 and without loss of generality we assume that . Then from of we have that for any , 
 we have by the of the function that there such that , and consequently , 
 First , it is clear that both and are differentiable by construction . In fact from 
 Furthermore , we can introduce the auxiliary function that is differentiable , and 
 Proof : Considering that , let be its . In view of Theorem , for a fixed , there such that can be expressed as : 
 , the middle term in is negative and with , proving the convexity for that case . For the case , from the right hand side of convexity will hold , if and only if , 
 Proof : Let fix and . Assuming that is compressible , from Theorem we have that and , consequently , it that : 
 this inequality is valid with probability over sampling space of the random object . There 
 , which is valid for any arbitrary small and . In other , if we define the set 
 , we need a slightly different argument . Under the assumption that , it is simple to verify that there a sequence of positive such that and , more importantly , it that 
 from the Lemma and the fact that as by hypothesis . 
 Following similar than before , there such that we have that , and therefore , from Lemma , 
 with probability in . It is important to define the set where Lemma us that . Furthermore , from 
 with probability one with respect to the process distribution of . In other , if we define the set 
 where it is simple to show that , by and the of and . Hence , the problem to evaluate , 
 the last equality from the fact that . Then from the additivity and monotony of the measure , which the result by the definition of . 
 and . Consequently eventually in which that eventually . This the result . 
 For the other inequality , we consider the case when . We prove it by contradiction assuming that . Then from , there and a sequence such that 
 . Under the fact that , there such that this and the definition of in 
 This approach can be a finite number of times independent of the length 
 Proof : the to prove Theorem Point , if we consider , then for any there such that . This last auxiliary function is with respect to and we have that : 
 with , it that is negative and increasing with , i . e ., is a convex function of . 
 The family of stable is the class of non degenerate that are limit in distribution of of random of the form : 
 where are i . i . of a random variable , and and are a of real . For the scenario when , the Central Limit Theorem us that the limit is a normal law . For the case , we have the less known family of stable , whose characteristic function is given by , Th . . : 
 Definition : . The distribution is said to be in the domain of attraction of an stable law with 
 , which we denote by , if there and such that : in distribution and the stable distribution in . 
 Proof : Let and . Without loss of generality let us assume that is slowly . Then it is simple to verify that , and such 
 ﻿ This paper novel on scalar feedback quantization with uniform . We focus on general where reconstruction is via a linear combination of frame . a deterministic approach , we derive two necessary and sufficient for to be optimal , i . e ., to produce , for every input , a sequence that is a global minimizer of the norm of the reconstruction error . The first condition is related to the design of the feedback , and can always be . The second condition only on the reconstruction , and is given explicitly in of the Gram matrix of the reconstruction frame . As a by product , we also show that the the first condition alone scalar feedback that yield the , when one quantization noise as uncorrelated , identically distributed random . 
 In many signal , have to be by a series of , so that they can be , or in digital form . This paradigm sampling , quantization and reconstruction . 
 The quantization of the , namely the sequence ,, a sequence whose are constrained to belong to a discrete set of . We focus our attention on uniform quantization , and thus require that 
 The and most common paradigm to recover the signal from the is linear reconstruction . Here , one is able to recover the original signal , say a , via 
 In , is a set of a frame in the reconstruction space typically a subspace of or of L . Thus , the are the frame expansion of a . of linear reconstruction are the reconstruction formula , the reconstruction stage in filter , and the inverse wavelet transform . 
 Throughout this work , we will be concerned with the squared norm of the reconstruction error , i . e ., 
 for every . Unfortunately , minimization of subject to is a non convex optimization problem . Moreover , the complexity of this problem exponentially with the number of to be . In addition , unless an orthogonal set , one would need to preview the entire input sequence before being able to calculate any optimal value for . This is incompatible with delay sensitive . 
 For the above , in practice quantization is often accomplished via simpler sub optimal that operate sequentially . The of these correspond to scalar feedback . At the i th iteration , these A obtain the output sample by simple scalar quantization of an auxiliary sequence , which is a linear combination of input and output , i . e ., 
 In , the real ai ,, i , , ,..., are design , and is the nearest scalar quantization function 
 The above can be used to describe many scalar quantization and bit Sigma Delta . The latter have been well studied in the context of shift invariant reconstruction wherein reconstruction is done by , and recently also for frame see , e .., , . 
 Not surprisingly , for a given reconstruction frame , and in return for the above , optimal vector quantization generally . However , it is not known under what this performance gap . In this paper we derive those . More precisely , we state necessary and sufficient for to be optimal , i . e ., to yield , for any input , the output sequence that , in . Our extend the work in , to more general . 
 Notation We use bold , e .., to denote both the sequence and the column vector x , where the meaning is clear from the context . We also use bold to represent matrices and their corresponding column . For example , a matrix , we use to refer to the i th column of , and , to refer to the th element of . The null space and the pseudo inverse of a respectively via and †. The notation to the sub matrix by removing the first i and i from . Similarly , the its first . The symbol N an length column vector of . We use the short hand the quadratic form . We write as an abbreviation for if and only if . to the , and we use to denote the set of all length with integer . We say a matrix or vector is integral all its are . 
 Here we will first present some regarding that will be used in our subsequent analysis . 
 A finite frame for a an ordered set of such that , for every , 
 where the set of square summable . The Gram matrix of the frame is defined element wise via 
 It is easy to show from that an cannot yield for all unless ai , di ,, i ,, where di , is the delta function . If the latter , then can be written as : 
 where v , , is the feedback matrix the vector of quantization . In order for the above to be well defined , needs to be lower strictly triangular , i . e ., lower triangular with all main diagonal equal to zero . Notice also the only degree of freedom in the design of an . 
 In order to determine , u for , it is convenient to define the noise shaping matrix 
 where IN matrix . Clearly , is constrained to be lower unit triangular , i . e ., lower triangular with all its main diagonal equal to . 
 Substituting into u . this , and substituting and into , the distortion by can be written as 
 G , the distortion , u of an for all the following two hold : 
 Notice that i a matching condition between the feedback matrix and the reconstruction frame . Thus , i can always be satisfied by a proper choice of which is given explicitly by . On the other hand , condition only on the reconstruction frame , or more precisely , on its Gram matrix . 
 The proof of Theorem will be given in Section , based on preliminary given in and . The latter provide valuable insight into the problem , and stem from two alternative : lattice quantization and dynamic . 
 In this section we use the fact that minimization of subject to is equivalent to a lattice quantization problem . To show this , we first note that any symmetric positive semidefinite matrix can be decomposed as 
 where is lower triangular see ., e .., . It then directly from and that 
 . Thus , one can analyze the relationship between the of any group of by looking at their in through . In particular , 
 Since the quantization alphabet U is uniform see , the all the UN constitute the reconstruction lattice 
 Accordingly , we say that is the generating matrix for . Every lattice a basic cell , V , associated with it , i . e ., the region of closer to the origin than to any other point in the lattice . More precisely , 
 . Similarly , we define the quantization cell around of an converter as 
 where the hyper cube : , , is the set all possible quantization noise . Thus , is the set of all target for which an the sequence . 
 With the above , we can now prove the necessity of condition i of Theorem . 
 Lemma For a reconstruction frame with gram matrix , the distortion , u of an can equal for all only if condition i in Theorem . 
 is that it the minimum second moment among all the whose form a tessellation , see . Thus , an is a candidate to be optimal only if its second moment N . This second moment can be readily shown to be given by trace I I 
 f , lower triangular lower strictly triangular . The fact that each trace term only on its corresponding column the trace is each i i i . 
 where i is an arbitrary vector in Hi and , thus , in as well . Substitution of the identity A † † AT into , thus the proof . 
 Remark If a vector of uncorrelated , uniformly distributed u . u ., random , one trace trace I I . On the other hand , an whose feedback matrix to characterize one of the noise shaping for frame in . More precisely , condition i is satisfied by the variant in which the error associated with each coefficient is onto all the ahead of the current iteration coefficient . Thus , Lemma also that , an u . u . model for quantization , the latter scheme the minimum among all . 
 Sequential quantization , such as , decide upon the value of each output coefficient sequentially . Insight can be by them from a dynamic point of view . The key point is that each of the additively to the cost defined in , leaving , after each step , a sub problem similar in form to the original one . In turn , each of these sub is determined by the already made . The following result us to formalize these 
 Proof The result from direct algebraic manipulation , the identity A † AA † A † and from the fact that † , i ,..., which positive semidefinite . 
 Recursive application of Lemma to one to split the total cost , as : 
 The summation on the right hand side of the irreducible reconstruction error stemming from the first i . The cost to go after decision i is the last term in . It the same form as the original cost , but it the target vector . The latter can be as a state vector which the effect of , and of previous , on the go . 
 Joint Sufficiency of i and It is well known in lattice theory that any two L and L , with M and 
 M non singular , are equal there an integral such that M see , e .., . On the other hand , if i and hold , then there a lower matrix such that is integral . lower unit triangular , we have that , and thus . It then that . On the other hand , if condition i , then it directly from that the product an orthogonal , lower triangular matrix . This in turn a rectangular lattice . Moreover , it is easy to verify that the associated cell V is given by the hyper rectangle , which is precisely the quantization cell of the , N see . Therefore i and guarantee that the corresponding is optimal . 
 Necessity of i and The necessity of i was shown in Lemma . Thus , it to prove the necessity of assuming that i . If i , then the target given in can be written in of the feedback 
 Now let us consider a that the target vector , at iteration i , 
 for some UN i and some e ,. With the above target , an would choose ti , i , and thus . Then , from , the cost to go for the after i can be split as 
 the minimum difference between the cost to go achievable by and that of the choice is 
 that . As a consequence , the first term on the right hand side of is strictly positive . It then that , u is strictly than , for sufficiently small of e , the proof . 
 Lattice Quantization Interpretation It been shown in the proof of Theorem that is a sufficient condition be rectangular and have a hyper rectangular cell . It is important to note that this can happen for a non diagonal reconstruction Gram matrix , i . e ., reconstruction that are non orthogonal , and even linearly dependent , not to be non singular . It is also important to note that the converse does not necessarily hold , that is , a not ensure that condition is satisfied . More precisely , the fact that a lattice is rectangular the existence of an integral such that is orthogonal . It be also lower unit triangular , as by . On the other hand , i alone is orthogonal , and thus N is . For a uniformly distributed , the gap between such an and a lattice vector is given by the difference between the second of N and V . Although no closed from are known for the second moment of V of arbitrary , preliminary suggest that it is possible to derive lower for this gap from the non integer part of the mi defined in b . 
 Reconstruction by a Single Filter By and considering the distortion per sample , as the cost function , our can be applied to where reconstruction is a discrete time filter , say . Without loss of generality , we assume that . In this case , the reconstruction frame take the form , where is the impulse response of . This setup infinite dimensional matrices , the first column . In turn , f can be seen as the impulse response of a filter . It then that the orthogonality of the of stemming from i is equivalent to . This to a whitening , which minimum , in the alternative white quantization noise paradigm . Similarly , an satisfying i also the , see Remark . On the other hand , for this case , all the mi see b are equal to the impulse response first sample removed of . Thus , into the impulse response of being integer valued . Hence , the standard th order bit converter is optimal for . This the in , for U , . 
 We derived necessary and sufficient that make scalar feedback quantization optimal , in the sense of generating , for any input , the sequence that the norm of the reconstruction error . The first condition , which only on the design of the scalar feedback , to characterize the best of this class , when a stochastic framework is adopted . The second condition only on the Gram matrix of the reconstruction frame , and can be satisfied for non orthogonal , and even linearly dependent , reconstruction . 
 ﻿ We obtain the maximum average data achievable over block fading when the receiver perfect channel state information , and only an entropy constrained approximation of this is available at the transmitter . We assume channel gains in consecutive are independent and identically distributed and consider a short term power constraint . Our analysis is valid for a wide variety of channel fading statistics , and fading . For this situation , the problem into designing an optimal entropy constrained to convey to the transmitter and to define a rate adaptation policy for the latter so as to maximize average data rate . A numerical procedure is which the and reconstruction of the optimal , together with the associated maximum average , by finding the of a small set of scalar of two scalar . this procedure , it is found that the maximum average capacity , in some , time between two . In addition , it is found that , for an entropy constraint log , a with more a small capacity increase , especially at high . 
 Index Channel state information feedback , Information , fading , quantization , radio communication . 
 T is well known that the achievable data for reliable communication over a fading wireless channel depend on the availability of channel state information at the transmitter and end , . For single input flat fading , the of channel gain and phase . If perfect is available at the transmitter perfect and at the receiver perfect , the channel is slowly fading and the transmission is subject to a long term average power constraint , then the average capacity is by rate and power to the channel gain in a time fashion , . By contrast , if an instantaneous per block maximum power constraint is , the are ergodic and the transmission are long enough so that the fade statistics over each block converge to their ensemble statistics , then the ergodic channel capacity is achievable without , . Else , if the fading is so slow that channel gain can be as constant within each block which to a block fading scenario then is beneficial . In this case , with perfect and per block power constraint , the capacity is by at maximum power , with only the data rate being to the channel gain in each transmission block . 
 If perfect is available and the receiver back this via an with limited information throughput , then only imperfect will be available at the transmitter . In a block fading situation , the uncertainty at the transmitter about the true channel gain in each block a trade off between throughput and reliability : the the data rate chosen by the transmitter , the higher the probability of exceeding the channel capacity during the transmission block . This the problem of the at the receiver and it at the transmitter i . e ., choosing rate and power in a rate distortion optimal fashion , where the distortion is some measure of the decrease in throughput , as in , or the increase in error probability , as in . 
 The capacity of memory less block fading with long term power constrained transmission and fixed rate constrained feedback was studied in . A similar situation was considered in , assuming a scheme in which data are perfectly or totally lost if transmission data rate is , respectively , below or above the channel capacity during the block . The idea in was to design a with a fixed number of quantization so as to maximize the rate , i . e ., the number or long term average of successfully . Also for a constraint in the number of quantization , studied the maximization of throughput considering a noisy feedback channel . There exist also numerous related to throughput maximization for multiple output wireless see , e .. , and the therein . Although not directly related to the problem which is the focus of this work , it is worth that , in all the in , and the therein , the only constraint on the where there is a is its . In , the maximum average throughput under a long term power constraint and for a fixed number of quantization is . The performance of to as MASA was against that of average reliable throughput to as ART , which allow for to occur . It is shown in that , in some , when the additional feedback load of the ART associated with the transmitter of a previous outage is in , MASA outperform ART . In that context , the feedback load to the entropy of the plus and that are sent to the transmitter . However , in this entropy is a , i . e ., after the have been without considering entropy as a constraint . 
 Thus , in all these , the design of optimal been only considering a constraint on the number of quantization or . However , if one the question what is the maximum throughput that can be if there is a constraint on the amount of information that can be sent to the transmitter for the , then it is more appropriate to consider an entropy constraint instead of a constraint for the . On the other hand , the entropy of the output , say , is a lower bound to the average number of to represent this output . At the same time , by , it is possible to find prefix free for each outcome with an average length not greater than per realization . Moreover , in a situation in . i .. are at a time which would happen , for example , in an system fading , joint entropy would yield with an average length , at most , bit . Since , in general , information to feed back for each realization less average power , or time , the latter can be directly associated with a low entropy . This a practical motivation for considering entropy , instead of , as a constraint for the . However , to the best of the knowledge , there are no available on average throughput maximization in which the to encode for the transmitter is to be designed subject to a constraint on the entropy of its output . 
 With the stated in the previous paragraph , in this paper we study the problem of finding entropy constrained with any given number of quantization , for block channel gains for the transmitter , that yield the average data rate . In our setup , the channel is assumed to experience i . i .. block fading , with associated gains and phases perfectly known to the receiver . We consider a wide family of fading statistics , general enough to include and fading with one or more of freedom . As in , the over which is fed back is an , zero delay channel . To solve this problem , we propose a numerical method which the optimal quantization and reconstruction for any given number of quantization and average channel signal to noise ratio . The optimization problem is partly similar to the design in because of the common entropy constraint . However , as we shall see , since the distortion measure in this case is the decrease in average rate not mean squared error , the resulting situation is vastly different from the one in standard entropy constrained quantization . The problem turns out to be non convex , and our analysis that it , in general , several local . Its formulation , for quantization , to a system of N non linear in N , each of which taking over the non negative real . Since each of these must be numerically , and due to the high dimensionality of the search space , direct solution of this system of a high numerical complexity task . The numerical procedure in this paper greatly this complexity by turning the problem into finding the of a small set of scalar of two scalar , only one of which unbounded support . The evaluation of each of search with respect to monotonic . By this procedure , it is found that , in general , the maximum average a given entropy is a non concave function . Since in our formulation time between two an average capacity and entropy equal to the weighted of the capacity entropy of each regime , the region of all achievable , is given by the convex hull of curve . On the other hand , it is found that if is log of the number of available quantization , then the so as to obtain is nearly optimal . Our also allow one to find the gain in average throughput of an optimal entropy constrained instead of a constrained optimal . For instance , when the average is , then an entropy with and an average rate of bit per realization an increase in average throughput over an optimal fixed rate with two i . e ., the same average rate . The performance of the latter fixed rate with the one found in . It is also found that for any given maximum entropy constraint log , the increase in maximum capacity by a with more relatively small . Moreover , our analysis also that , for any given , the maximum average capacity is a with a finite number of . This with what is also for an exponentially distributed source but with as the distortion measure , wherein the optimal turns out to be uniform with infinitely many . 
 In the following section we present a precise model description , introduce some notation , and formally state the problem of interest . To illustrate some of the of this problem and its , we first analyze the case two quantization , which can be explicitly , in Section . Then we extend the analysis to the case in Section , where we introduce the numerical procedure to solve the problem in its generality . the with this procedure for the case and under fading . Finally , Section . 
 We consider a block fading additive white noise channel , a transmitter , a receiver and an error free , zero delay channel , as in Fig . . In the transmitter , the binary message into consecutive of . During each block , a real valued sequence is over the channel . The random block channel gain magnitude for the th block is assumed constant within each block . Channel gains in consecutive are i . i .. according to a probability density function satisfying the following : 
 Assumption : The of the the channel gain squared magnitude the form 
 for appropriate K , K , where the differentiable function is such that the ratio u u is non increasing with respect to u over , . 
 The structure of the is fairly general . For example , if channel gain magnitude is distributed , then the form 
 where I is the function of the first kind of order zero . From direct comparison with , we obtain , for this case ,, K s and u . It can be numerically that the latter form of the by Assumption . Likewise , if channel gain are by a distribution , then the form 
 and we have K , K and u um . If , it is easy to verify that u also the 
 to Fig . , the real valued random process , ,...,, is with sample variance N . Thus , if were the , taken at frequency , of continuous time band limited to , then the 
 The necessity of the condition upon in Assumption will become evident in Lemma Section , in which it us to prove the convexity of a function a key role in the problem under study . 
 two sided of the latter would be N . On the other hand , the information bearing signal is subject to a per block power constraint of the form 
 where . With this constraint , if the block large , then the maximum achievable data rate during any given be well by capacity formula as , where 
 is the mean at the receiver for a channel power unit mean value . 
 At the other end of the channel , the receiver is assumed to acquire a perfect estimation of prior to or at the beginning of the th transmission block . This channel power gain is instantaneously and entropy , with the resulting being sent over a zero delay , error free channel . These about the feedback channel have been considered before in , . The zero delay condition can be to be a good approximation when the time spent to feed the back to the transmitter is much shorter than the duration of a frame . In turn , it is possible to have an almost error free feedback channel if the feedback is sufficiently large and or strong forward error correction is employed for the . And naturally , if in a given situation these latter are not present , then our would provide upper to achievable performance . 
 As in the Introduction , it is possible to translate a small entropy of the into less average power , or time to convey this to the transmitter . At this point , it is perhaps worth that if only a single realization is and fed back at the beginning of each block , then these may require one to match the channel and modulation scheme in the feedback link to the variable coming out of the entropy coder . For instance , an off the shelf channel coder and modulator in the feedback channel would yield an that only of fixed length data . Such choice which would entail significant when variable length , in comparison to sending . However , in this scenario wherein a single realization is and fed back at a time , the 
 feedback channel and modulation can be chosen so as to handle variable length bit or the associated unequal probability of the as efficiently as it is possible for fixed rate . This can be done , e .., by variable length error correcting or joint source channel see , e .., and the therein . Although the design of such and is beyond the scope of this work , we illustrate this fact with an example a simple scheme similar in spirit to , which can be found in Section A in the Appendix . 
 Upon the , the transmitter a transmission data rate from a discrete set of data . To define the and its reconstruction , let 
 be the number of quantization or , and let denote the set , where 
 Define also the quantization , , i ,...,. As in , whenever the channel power gain within cell , the transmitter a satisfying , belonging to the i th amongst , one for each cell . This is capacity for some nominal channel power gain associated with the cell , i . e ., 
 Thus , the power gain can be seen as the set of reconstruction or of the , as in Fig . . 
 Since in each block the transmitter information over the a capacity code for a nominal channel gain , all the are correctly if . Else , if , then is not by the channel , and the receiver an outage , all the information received during the th block . From this , it is clear that a necessary condition for a set of reconstruction to be optimal is 
 Let the random variable , taking in with , denote the output of the for the th block . As already in Section I , we focus on that satisfy an entropy constraint , which we now formally state as 
 with the maximum entropy for the output . 
 We are interested on finding the i . e ., the and reconstruction satisfying the entropy constraint and the average in the channel , defined as the average number of correctly . The average number of correctly sometimes to as average or average reliable throughput is also the the objective function in , , . It is a reasonable figure of merit if one forward error correction by been applied to the data being sent over the , so that , with high probability , lost due to outage do not cause irrecoverable . Otherwise , and if all data is to be correctly , and would have to be sent back over the to request retransmission of lost data . As we shall see from the in Section , at least for fading and for equal and , the optimal entropy constrained are such that only the first cell V for outage . The latter that if sending and is necessary , then it would mean at most V per realization to the . The extra bit rate is upper bounded by V block because only when V it becomes necessary to send an or during block , which at most extra bit every time V . 
 Since in consecutive are i . i .. and ergodic , over a large number of converge to ensemble . Thus , for notation simplicity , in the following we drop the frame and channel . With this , the design problem can be stated as finding the and satisfying and , that maximize the average data rate in the channel without exceeding the entropy constraint in the . More precisely , combining , , and , we state the optimization problem in canonical form as minimize : 
 with and , and where is the cumulative distribution function of . 
 This optimization problem is difficult to solve primarily because the entropy constraint c is non convex . As we shall see , this to the existence of several local , which , in principle , one to run an optimization program several times with a potentially large number of different starting . In the following we solve this optimization problem , first explicitly for the case , and then numerically by of optimization and a novel procedure which greatly the overall complexity of the task . 
 We now address the optimization problem stated in for the case , corresponding to two quantization . In this case , to : minimize : , i 
 This problem can be explicitly without by that the entropy associated with the two only on the threshold . Supposing is given , the optimal value of u is found by the objective respect to it and to zero : 
 We see from this equation that for every u there a unique u for which u . On the other hand , in order to determine the optimal value of u given , we notice that this value to minimize the term u u in a . Although , in general , such value cannot be found explicitly , the following lemma that it is unique and that it can easily be found numerically : Lemma : a satisfying Assumption . Define u 
 The proof of this lemma , which will play a key role later in Section , can be found in the Appendix , at the end of this document . 
 It turns out that the value of u which u , u u with . More precisely , define the function U 
 for any , with the inequality being a consequence of the fact that is non decreasing . from Lemma that u , is convex in u , we conclude that becomes zero at a single value of u greater than or equal to . This that for all , . 
 The latter result that the optimal value for u must belong to the interval ,, where . 
 Also , by Lemma with , it is readily found that the unique value of u a also . The convexity by Lemma that the latter function can be easily numerically by line search . Moreover , Lemma that u u , which to the conclusion that the optimal value of u given is 
 It then that , if u is part of an optimal for some entropy constraint , then the optimal can be explicitly from u by , and then the optimal u can be directly derived from . In this manner , by increasing u from to and and u with the latter , one a family of the optimal for every value of . 
 Fig . top the curve of capacity versus entropy by the method in the 
 paragraph , for two quantization , under an average , left and , right . More precisely , for every , the corresponding was calculated . Then , for this value of , the optimal u was determined . In this way , for each value of u , a different was . The pair capacity , output entropy associated with each a single point in the top two in Fig . . In this example , channel gain distribute , so exponential , chosen to yield , i . e ., u e u , u , . Notice that , for every value of , there exist two to and , corresponding to different local constrained . There is one associated with each of these two . One of these the upper section of the curve in each of the shown in top , and the other one the lower section . Of course , the optimal are those responsible for the upper part of their respective , i . e ., those which yield the maximum capacity for a given entropy constraint . As , in general , capacity when the entropy of the available to the transmitter entropy is . 
 Fig . . to and two quantization for fading with unit mean channel power gain . Top : under an average , left and , right . Bottom : and code for the optimal solution as a function of entropy under an average , left and , right . 
 Note that the maximum capacity for , . , at an entropy of . block , not with maximum entropy , which , for a two cell is bit block . This is expectable for a with a fixed number of , since there is no reason why all being equally likely the only situation in which the entropy is should yield the capacity . Also , the maximum for a two cell at and coincide with what was in , where average capacity was without an entropy constraint on the output . At an average of , Fig . top right that the maximum capacity closer to maximum entropy , and that , corresponding to this maximum entropy , are near optimal . Interestingly , the maximum capacity curve at this a at an entropy of about . block where the curve itself in Fig . top right , which it non concave . For this case , this that better performance for below block can be by doing time between two : one with a single cell and zero entropy , and another with two and an entropy of about . block . By choosing one regime more frequently than the other , it is possible to achieve a capacity and an entropy equal , respectively , to the weighted of the and of both . In this manner , all capacity entropy within the convex hull of the can be . 
 Fig . bottom the evolution of and for the optimal solution , as the entropy of the output . For , on the left , u at all . We see also that higher are by and code closer together . For the case , shown in Fig . bottom right , the in with a change in the arrangement of and code in the . Except in a neighborhood to the right of this , we find that the u with its left boundary threshold . 
 The straightforward approach in the previous section cannot be extended directly to the case in which there are more than two quantization . optimization can be instead , which , as we shall see , to a system of non linear that must be numerically . The non convexity of the optimization problem and the existence of several local satisfying the the need to solve this system of non linear possibly many times with different initial . However , we will introduce an algorithm , in the same spirit as the strategy in the previous section , that one to simplify the optimization problem to a sequence of simple line search , each with a single solution . 
 Before the associated with , we note from a that , at the optimum , the inequality stated in e are not active . Similarly , constraint d for the case i is not active since increasing u above would raise the average data rate without increasing the entropy of the output . 
 Taking the above into account , the associated with the following form 
 . with respect to u and to zero 
 Notice that a is identical to , which that u is a solution to a only if 
 L with respect to the other and to zero 
 On the other hand , the with respect to the we obtain 
 Finally , the Tucker , provide another set of , 
 Although it is possible to solve this system of N non linear by standard numerical , the existence of numerous local minima one to apply these repeatedly , each time with different initial . This shortcoming is by the fact that the vector of initial plus threshold and code point in a N dimensional space , which a large number of initial is to obtain a reasonably good coverage of the search space . In the following section we will show how these can be by an approach similar to the one in Section . More precisely , we derive a method which , for this problem , one to find all local minima in a systematic , sequential manner , greatly reducing the number of . 
 In this section we exploit the recursive nature of to reduce its induced system of non linear into a sequence of line search over bounded , in a spirit similar to the one behind the approach in Section . 
 To begin with , recall that the imply that , for every constrained local minimizer of a , the multiplier only if i . e ., only if the associated constraint is active . The next corollary of Lemma an verify condition for to hold in such a minimizer : 
 Suppose are a solution to optimization problem . Then for some ,..., if and only if 
 Proof : The result directly from Lemma , since it the convexity of the function u u , and from a , upon that the entropy of the output does not depend on the choice of . 
 Corollary will allow us to find and from , , and in a simple manner . For this purpose , define , for ,..., where 
 is the complementary . With these , the combined c , d , g and i can be written in the following equivalent form 
 , , ,..., a , ,...,. b 
 Fig . . of , and defined in , as of . Left : a case in which . Right : a case in which 
 Figure a qualitative description of , and as of . It can be seen that both are monotonically increasing , the first one being affine , the second one convex . 
 A look at Fig . immediately that , for any given , and depending on the of the , and , there will be , in general , more than one pair of of and satisfying . Let us find out which actually by first considering the under which the constraint is active or inactive : 
 • Inactive Constraint : In this case which see b . In view of , this is equivalent to 
 The first equality of is satisfied by a unique value of the argument of , say , shown in Fig . . 
 From the definition of , this quantity must be . On the other hand , Corollary that if and only if 
 situation and in Fig . right . In addition , Lemma that , if this inequality is satisfied , then there a unique satisfying the second equality of . Thus , there a solution to for which the constraint is inactive if and only if and . In this case , we say solution for . 
 • Active Constraint : In this case , , and can be positive . By looking at , a solution satisfying this condition if and only if there is for which 
 which correspond to of the of , and on the first quadrant in Fig . . Since , with respect to , the function , is affine and increasing , and the function is convex and monotonically increasing , it that is satisfied for either none , one or two of . Indeed , these imply that two to equality a in will exist if and only if 
 is the unique value which , , see Fig . . It is also easy to show that if , these two , say and , will lie at opposite sides of . Also , it is straightforward to verify that both are not than . Therefore , 
 problem , each solution to equality a in will also be a solution to if and only if it is non negative and it a non negative value for . This one to discard solution or if 
 . On the converse , if these two do not hold , solution will be non negative and yield 
 , i . e ., it will be a valid solution to . In this case , it is easy to verify that 
 , with i , , , be the threshold value associated with , we can devise the following procedure to find the to for a single , given : 
 Procedure : Suppose , and that , and are given , with . Then , in order to find the to for , 
 a Find by equality a in with respect to by line search over , 
 , find equality a in by line search over unless , in which case . Set , and 
 The last step , where yielding are whenever , to the fact that is a complementary value . This requirement is only if one is calculating the last threshold i . e ., when , to allow a higher level routine to iteratively adjust so that . Such a routine is by Procedure below . Of course , unless , solution can be before doing the corresponding line search if . The same for solution if . 
 In each of the line in Procedure , there a single solution over the corresponding search interval , since , in all , the involved function is monotonic within it . This each step straightforward to execute . Notice also that for every , and depending on the of , and , there are between zero and three of for , , namely , and , which satisfy for that . 
 The above procedure can be applied sequentially to find all , , for , ,...,, that satisfy . More precisely , one can first choose for u , and , with which one can calculate explicitly from a . Then , setting , Procedure can be carried out to find at most three of for u , satisfying for . Each solution can be considered a branch in a tree structure . one and the procedure for each branch , until , the complete tree of valid , each of which is associated with a path in the tree . Thus , for each choice of u and , there can be at most N different , each associated with a sequence of and satisfying for all , ,...,. However , we shall see in the following that the number of valid in practice is much smaller than N . 
 Following our notation for adopted in Procedure , we label each solution path in the tree the of the solution associated with its , thus to a sequence . For a given choice 
 It is important to note that there is a one to one relationship between every solution satisfying for a given choice of u , and every path in O u ,. Indeed , the path associated with any such solution can be easily determined by each of its code point threshold the the reasoning that led to Procedure . 
 Now , suppose one to know whether a given path is associated with a valid solution to and then find this solution . Instead of Procedure to find the entire tree of valid and then if is one of them , one can apply the following algorithm , derived directly from Procedure , for this purpose : 
 Procedure : Let u ,, and the corresponding be given by a . Let be a path . 
 Then the following can be taken to determine whether and find its associated and code 
 With the above procedure , the resulting of all and are a function of . For convenience , we denote this function by , defined 
 where we have chosen † as a special symbol to indicate that the path does not yield a valid solution . For future use , we also define 
 Although being a solution to is a necessary condition for code and to be a solution to , two additional must be satisfied for sufficiency . The first one is the entropy constraint , expressed in the equation f . The second is the construction condition or , equivalently , . For a given path , u and , this condition can be expressed as . 
 is the set of all , for the first u and multiplier associated with to for some entropy constraint , the set of all 
 , respectively , as the entropy and capacity associated with , the original optimization problem can be stated as 
 Thus , we have reduced the problem from a set of N non linear , over a N dimensional space , into a moderate number of one per valid path , over two , the evaluation of a scalar function to compute . 
 Define a grid of of u over ,, say U , and a grid of for say , for some . 
 Detect of consecutive of between which a sign change of for some valid path . In each of the formed by such , find the value of for which by line search , running Procedure for the corresponding path . 
 Calculate and for each of the found in the previous step . Select the combination that the capacity with an entropy not greater than . 
 In the following section we present an example in which Procedure was to find the solution to for . 
 In this section we apply Procedure to find the set see , for the case , i . e ., for four , assuming is fading . The latter set the , for u and that characterize a solution to , i . e ., and code that yield a local maximum or minimum capacity for some fixed maximum entropy constraint . The are in Fig . , for , on the left , and for , on the right . 
 The two top graphics in this figure correspond to for all to . It can be seen that , although for each there exist several such , for each entropy constraint the number of these is significantly smaller than N . For the case , the absolute maximum capacity is . , with an entropy of . block , by a solution with associated path . The curve that this maximum is ended at that point and , to the eye , it as if there was a missing segment which would connect it to the curve that the right boundary of the plot , at an entropy of block . The absence of this segment can be to the fact that in the formulation of the problem , the entropy is an inequality and not an equality constraint . 
 The solution yielding the maximum capacity for a given entropy is given by the highest curve in each of the . Interestingly , for somewhat below log and log block , the curve with the optimal solution with and quantization , respectively . This can be seen by from the bottom in Fig . that a few block below these entropy , the optimal solution is such that one , two or three , respectively , tend to infinity , effectively leaving three , two and then one cell , as the entropy is . Such behaviour that for a finite entropy constraint , the maximum capacity over all is with a a finite number of . 
 As already for the same and two , the region of achievable average is given by the convex hull formed by all the in the . Unlike what is for , since the composite maximum capacity curve for Fig . top right is not concave , the maximum capacity for some entropy constraint between two . 
 The point of maximum capacity for each number of quantization to the solution when a with that number of is for maximum average throughput without an entropy constraint . Therefore , those peak are the found in for single layer , where the was under a constraint only . From this fact , we can conclude from Fig . top left that at , and for the same average rate of an optimal fixed rate from with two that is , bit per realization , which . , an optimal entropy with quantization approximately . . This an increase of roughly in average throughput for the same average rate . The corresponding increase with respect to a fixed rate with goes from . at a fixed 
 Fig . . entropy for the to , for fading with unit mean channel power gain , up to quantization . Left : Average . Right : Average . 
 . an optimal entropy with . For an of , Fig . top right that an optimal entropy constrained smaller gains over fixed rate optimal . Notice also that can only operate at a limited set of given by log realization , Thus , entropy quantization is the only scheme which one to send other average for example , below bit realization . 
 We have a numerical procedure to find the maximum average capacity over block fading , under a fixed per block power constraint , when the receiver perfect and an error less , delay free , channel is available to convey to the transmitter . This procedure , which a smaller numerical complexity than trying to directly solve the associated with the problem , also the and that achieve the optimal solution . Our are valid for a broad class of channel fade , and fading . We have applied the procedure here to find optimal and quantization . The revealed that , for a given number of quantization , say , maximum capacity is at an entropy slightly below log block . Furthermore , our show that for any entropy below log block , there is little to be in average capacity by more or . This that for any finite entropy , the optimal a finite number of quantization . Our analysis also revealed that for high average , the maximum average capacity time between two with different and associated . 
 As a final remark , we would like to mention that , after several , the have found that the and here for a short term power constraint do not seem to be applicable for the long term power constrained version of the problem . Indeed , the latter problem to be significantly harder to solve than both the one in this paper and the long term problem without the entropy constraint . 
 Here we provide an example to illustrate how a discrete random source with small entropy which can be associated with the variable length coming out of an entropy coder can be efficiently a channel coder . The latter coder is able to transmit the low entropy source less power and with a smaller message error probability than what is when modulation and maximum likelihood . For this purpose , suppose we have two , each with four quantization . The first is not entropy , and each of its , two , equal probability . For simplicity , suppose that this is by a rate error correcting channel coder and that is . Assume each symbol in the constellation unit energy and that there is a memoryless channel in the , with complex circularly symmetric white noise with variance . Therefore , each outcome or message is into of length two that is , two consecutive are sent for each message , which an average energy of . At the other end , the is done by by the most likely symbol sequence given the received signal . For this scheme , it can be found either analytically or via that at an of , the message error probability is approximately . . 
 For the entropy , suppose that its four possible , say m , m , m , m , have , , and , respectively . An entropy coder for this for example , a coder would output , , and , respectively , for each of its . In order to send these four or over the feedback channel , consider a time digital modulator generating from the shown in Fig top . Each of these except symbol o unit energy . 
 An outcome from the entropy is fed back to the transmitter by sending a sequence of three channel , each coming from the first , second and third constellation , as in the table of Fig . Notice that with higher probability are to symbol with smaller total energy , the latter being proportional to the length of the by a entropy coder i . e ., proportional to log , where is the probability of the i th message or outcome . This to the sequence length or energy distribution that average energy , the latter energy being in this case equal to . Therefore , the average energy by the channel scheme for the entropy is just of the mean energy associated with the fixed rate . On top of this power reduction gain , if this variable rate scheme to the entropy is combined with a maximum likelihood sequence at an of , then each message is with a message error probability not greater than . . This that if the of the have uneven which to a smaller entropy than with probable , then it is possible to transmit the less power and with a smaller probability of error than when the of a fixed rate . 
 where we a short hand notation for . For every u such that u , it readily from the structure of see that u and therefore immediately da u . The same for u . Thus , it is only left to consider of u and such that u . If da u , then , from , 
 Since we are only considering the in which u , u . This one to 
 Since we are considering of u such that u , we can substitute into , 
 It then that also if da u . We therefore conclude that , irrespective of the sign of da u , 
 Fig . . Top : Three consecutive of a digital modulator for the of an entropy with quantization . a ,,, have unit energy . Bottom : A between quantization m to m and channel symbol . The 
 Since is a non increasing function , it that the of is positive . Therefore , we obtain from that a u is also convex for all u , such that u . This the proof . 
 ﻿ This paper novel on perfect reconstruction feedback , i . e ., noise shaping , predictive and sigma delta A whose signal transfer function is unity . Our analysis of this class of is based upon an additive white noise model of quantization . Our key result is a formula that the minimum achievable of such to the signal to noise ratio of the scalar in the feedback loop . This result us to obtain analytical that characterize the corresponding optimal . We also show that , for a fixed of the scalar , the end to end of an optimal which the optimal which for this case turn out to be increasing ratio . Key from work include the fact that fed back quantization noise is explicitly taken into account and that the order of the converter is not apriori restricted . 
 Index Differential pulse code modulation , optimization , quantization , sigma delta modulation , source . 
 HE term feedback to a class of to digital converter wherein a scalar is within a linear feedback loop . Well known of include , and sigma delta . The latter have been very successfully applied in a number of , audio compression , , A conversion , , subband , digital image half , power conversion , and control over . 
 Fig . a general configuration . In this scheme , may take the form of a nonuniform or a uniform , the latter being either or . 
 The in an system allow one to exploit the predictability of the input signal so as to reduce the variance of . When with simple 
 smaller quantization step . The error feedback filter the possibility of spectrally shaping the effect of quantization noise in the frequency where it is less harmful from a user point of view . Accordingly , it is convenient to use a frequency weighted error criterion , via an error frequency weighting filter , and to focus on the frequency weighted see discussion in and . 
 For the sake of generality , we consider the possible use of a clipper before . This device the value of the input signal so that if , and if 
 , where is the saturation threshold of the clipper which is helpful in reducing limit cycle idle in an with high order , as in . On the other hand , if we chose to be sufficiently large , then , and the clipper no effect on the system . 
 If the of and the spectral of the input signal are known , then the design of an converter that the variance of to choosing the 
 It is often desirable that a converter is transparent to the system in which it is inserted . This to the widespread paradigm in which the scheme to the application that it , without need to modify the latter . A transparent converter is one whose signal transfer function i . e ., the transfer function from input to output is unity at the of interest . The design of such perfect reconstruction feedback the main topic of the present work . are by the property that , in the absence of quantization effects , there is no frequency weighted reconstruction error , i . e .,. If we denote the power spectral density of , then it can be seen from Fig . that the latter if and only 
 Thus , in the design of an optimal converter , only two of freedom are available : the or , alternatively 
 To the best of our knowledge , on optimal filter design for either consider finite order , , , assume or require that the variance of the signal is much smaller than that of , , 
 , , or have a heuristic component in the optimization , , , . available for the optimal performance and corresponding filter frequency of a converter are those given in . However , the assumption of negligible fed back quantization in these suboptimal . Indeed , as we will show in the sequel , there exist where the in yield large fed back quantization error , even when a fine step scalar is used . In these , not only is the main assumption in , but also an much than can result due to excessive overload see , e .., and . 
 In the present paper , we will show how to design optimal . For this purpose , as in , , and , we model the scalar as a linear device that additive white noise whose variance is proportional to that of the signal being . A key departure from , however , is that we explicitly take into account fed back quantization noise in the feedback loop . Our main are : 
 i We derive one parameter that relate the minimum achievable frequency weighted to the signal to noise ratio of ; We show , within our model , that the frequency weighted in an optimal where the of is fixed exponentially with ratio ; and We derive that characterize the optimal for a . Our can be applied to any given number of quantization , and to almost arbitrary input spectra and frequency weighting criteria . 
 The remainder of this paper is organized as : In Section , we present our analysis model for . In Section , we formulate the associated optimization problem . Section a one parameter characterization of the solution . In discuss the main of an . The case of is in Section . Section the relationship to previous and the importance of taking account of fed back quantization noise . Section simulation . Section . For ease of exposition , all of our are included in the Appendix . 
 We write as a short hand expression for if and only if . The of all complex valued square integrable and absolutely integrable on are by and , respectively . Given we adopt the standard inner product , where complex conjugation . We denote the corresponding 
 transform . If is a transfer function , then we use the short hand notation to refer to the associated frequency response 
 . If is a set , then we write a . e . on almost everywhere on for everywhere on , except on a zero measure subset of . We use to denote the variance of a given wide sense stationary ... random process We recall that if zero mean , then 
 where is any given function and any arbitrary and positive bounded value . For later use , we also recall the following definition . 
 In this section , we discuss some of the main of feedback quantization . We also describe the analysis model and the to be considered later in the search for the optimal . 
 We begin by the that describe the behavior of the shown in Fig . . 
 Quantization and Clipping : From Fig . , the quantization error is given by 
 such that , if , then is said to be . When the is not , then is only granular quantization error , namely , which can be bounded as for some 
 see , e .., . For example , if is a symmetric , uniform , with and quantization interval , then one needs in order to obtain . 
 As outlined in the introduction , the clipper in Fig . can be used to keep from . For simplicity , we will only consider here two , namely , that , or else 
 . The former choice that does not overload , since clipping error , defined as 
 clipping is that , unlike overload , clipping are not fed back into through . This to avoid large limit cycle from the overload of , see . Since such are not part of the analysis model we will use , their occurrence could increase the significantly above the value by the model . 
 which that from by the sum of the quantization and clipping . 
 Notice are require no on the involved . From b one can see that to the signal transfer function , from to , of the converter . Similarly , the product is the transfer function for quantization , usually to as the noise transfer function of the converter . The term will play a crucial role in the derivation of the optimal 
 Output stable for any input sequence satisfying . 
 , and , thus , all the other in the converter are bounded . On the other hand , if , then can be written as 
 If the a finite number of quantization , then is bounded . If is stable and is minimum phase , then it from that is bounded . This , in turn , that and all the other in the converter are bounded see and if all the in Fig . are stable , and if no on or outside the unit circle , then the resulting is stable . 
 In addition , if and are stable , then the norm of their impulse , namely and , are bounded . 
 Therefore , for a uniform with quantization interval , it to have or more quantization in order to avoid clipping or overload . 
 Input Spectrum and Frequency Weighting : The error weighting filter in Fig . the impact that reconstruction have at each frequency . This performance assessment filter is application dependent , and is assumed to be stable and given . The input signal is a zero mean ... stochastic process with known and finite power , i . e . In 
 order to simplify our subsequent analysis , we shall further restrict and to satisfy the following : 
 Assumption : The product is a piece wise differentiable function at most a finite number of and satisfying In addition , is such that one of the following . 
 such that . Furthermore , if the set of noncontiguous and nonoverlapping in such that 
 We note that the above is a rather weak constraint , since i and include almost any product of practical or theoretical interest . In particular , condition i all the where the product no on the unit circle . In turn , condition is satisfied if is zero over any interval on nonzero measure , or if is rational and on the unit circle . 
 The : We shall focus our analysis on the effect that granular quantization have on the . For this effect to closely represent the actual , we need to assume the following : 
 Assumption : The of overload and clipping are negligible , i . e . 
 In addition , and as stated in the introduction , we will adopt an additive white noise model for . This model is widely used for the analysis and design of data see , e .., , , , and , being usually as . 
 Assumption : The sequence of quantization noise is a zero mean ... random process , 
 The above additive white noise model , although not exact , is , in general a good approximation when a signal with a smooth probability density function is with many and negligible overload in the sense of Assumption , see , e .., . The model can be made exact , even for few quantization , by a uniform scalar with either subtractive or dither , provided overload does not occur , see . As before , one way to achieve this is to use a with a sufficiently large number of quantization , so as to satisfy . In this case , if the quantization interval is and the dither sequence 
 , uncorrelated to when is not and is bounded as , then any number of greater than or equal to will make Assumption hold exactly . If a smaller number of quantization are employed so that , then the use of dither with the same as before , together with clipping i . e ., setting , will also make satisfy Assumption exactly . 
 . . on through the feedback path . However , if the scalar a finite and fixed number of quantization , then another link between these two needs to be considered . In order to model this relationship , we will use the fixed model employed in , e .., , , , , and . 
 Assumption : For a fixed number of quantization , the variance of quantization is proportional to the variance of the signal being , i . e ., there such that 
 If no clipping is used i . e ., if , then exactly to the of . If , then is a good a approximation of the of when b in Assumption . In our model , is assumed fixed and given . Strictly speaking , on the of , on the number of quantization of , and on how quantization and are distributed along the dynamic range of . In practice , for a given number of quantization , should be chosen such that the dynamic range of is used efficiently , whilst en 
 Here and in the sequel , we assume the dither is such white and uncorrelated not . 
 a low probability of overload or clipping . For example , for the often uniform with loading factor equal to we obtain assuming that a uniform and overload . We note that for large , and provided overload are negligible , a quadratic relationship between and for most of scalar see , e .., . This is indeed the well known rule of reduction of quantization noise variance per additional bit of resolution . 
 In the sequel , we refer to the model of determined by , , and as The Linear Model the Linear Model is exact if the a enough quantization to avoid overload . If not enough quantization are available and dither is used jointly with clipping , then the model is exact in the effects of granular quantization , and is a good approximation in the total if Assumption also . If the scalar is , a small quantization interval relative to and enough quantization to avoid overload , then the Linear Model can be to yield a good approximation of the total . Perhaps surprisingly , the Linear Model turns out to predict with remarkable accuracy the of an optimal when few quantization and clipping are used with a loading factor big enough to satisfy Assumption , even without dither , and even for a bit . This can be from the simulation in Section . 
 We shall restrict the search for the optimal to those satisfying the following constraint . 
 As in Section I , the first constraint perfect reconstruction . As in Section A , the stability on are a necessary condition for the converter to be stable . The additional requirement on , namely strict causality , is for the feedback loop in Fig . to be well defined see , e .., . Notice that we will not a require to have only inside the open unit disk . Instead , we will show that the latter property naturally from the solution of the design optimization problem . 
 An additional constraint on from the value of , as next . The ratio between the of and 
 One can see from the above that if , then any filter or scaling of the quantization of will yield , thus , making large overload or clipping inevitable . This would and , if no clipping is used , may lead to large limit cycle . We , thus , conclude that the use of feedback the following constraint . Constraint : 
 If the above constraint is met , then can be found by substituting into . This 
 Given the model in the previous section , we can now evaluate the quantity that we aim to minimize , namely , the frequency weighted mean squared error . From c , and and , it that the is given by 
 of the in the Linear Model can be stated as . Optimization Problem : For given , and for given and satisfying Assumption , find the frequency , and satisfying and that minimize 
 The following proposition us to further reduce the number of in by the optimal 
 the following change of : 
 proof of Proposition in the Appendix , Constraint is satisfied . In addition , a stable and strictly causal 
 i . e ., one satisfying Constraint always to a function , see , which 
 This result directly from formula see also the Bode Integral Theorem in , e .., . 
 On the other hand , as we shall see in Section , if Assumption , then the optimal within the set of by and the requirement turns out to be piece wise differentiable on , at most a finite number of discontinuity , and 
 Under these , it is always possible to find a stable and strictly causal filter such that arbitrarily well on , as stated in the following lemma . 
 , that it at most a finite number of discontinuity and that it . Then , for every , there 
 Optimization Problem : For given and known and for satisfying Assumption , find 
 the optimal feedback filter , say , via see also Lemma . In the following section , we will show how to solve this optimization problem . 
 It would be desirable to provide an explicit analytical solution to Optimization Problem . Unfortunately , and as will become apparent in the discussion later , a closed form solution , for arbitrary , infeasible . Nevertheless , we can provide a one parameter characterization of the optimal function in as . 
 Theorem : For any given satisfying Assumption , and for any , the function in to the one parameter family of , where 
 Here is the lower bound of feasible , and , if it , is the unique scalar such that 
 Note that the above result an explicit analytic expression for , once the optimal , defined as 
 Theorem can be used to develop an efficient algorithm to solve Optimization Problem . The key point is that substitution of a into the search space from the infinite dimensional set to the real interval . More precisely , Optimization Problem is turned into the simpler problem of finding the minimizer of the single variable scalar function 
 , and hence the solution of Optimization Problem is unique . Furthermore , can be by finding the root of a scalar , convex , and monotonically decreasing function . 
 Moreover , it from and that , for any satisfying Assumption , and for any , the global minimizer of and is unique . In addition , these guarantee that can be easily found by , via , for example , the bisection algorithm , or any other convex optimization method . 
 We can now express and the minimum achievable , namely , in of , and . Indeed , combining and a with after some algebraic simplification that 
 It can be seen from a that is a monotonically increasing function of . In view of Theorem , this that , as , is monotonically decreasing with increasing . As a consequence , the converse of Optimization Problem , namely , finding the optimal and minimum of for a given target distortion , can be by and . Moreover , since the of a is a concave , monotonically increasing function of , this parameter can be easily found by standard iterative , as in the original optimization problem . 
 It is also interesting to note that and a , which relate and via the parameter , have a structure akin to the well known reverse water filling see , e .., , and . The latter characterize the rate distortion function for . 
 To summarize , we have given an explicit analytic expression for the optimal and , once been determined . Furthermore , we have shown that the parameter always , is unique , and can be easily found simple numerical . 
 In the following , we will provide additional insight into the of , as well as into some of optimal , 
 In the sequel , we say that a is optimal or if its , satisfy for negligibly small of and , and is such that , a . e . on , with as defined by . 
 It from and that , for any given satisfying Assumption , in a the family of all noise shaping that are optimal for some . 
 As we will show , from to equivalently , from to one to undergo a smooth progression from full noise shaping to no noise shaping , in an optimal manner . An example of this progression is shown in Fig . . Note in this figure how solid a unit transfer function as the for which in the figure , becomes smaller and . It can also be that the inverse of is . 
 Such asymptotic convergence does indeed take place in general , as the following theorem : 
 Theorem : For any satisfying Assumption , the defined in a converge uniformly to 
 as . Similarly , for any function satisfying condition i in Assumption , the defined in a converge uniformly to 
 , which the to a converter . In view of , this no noise shaping scenario is asymptotically optimal as . In turn , defined in to the full whitening feedback in , , . From and , is optimal . See also the discussion in Section . 
 The Output of the : By looking at Fig . and Assumption , we find that the of in an is given by 
 and , we conclude that the variance of the quantization noise in an is given by 
 a been used . Substitution of a and into this expression to 
 , the output of the in an is white . This that near optimal of the output can be with a memory less entropy coder . 
 The Frequency Weighted Reconstruction Error : The of the frequency weighted reconstruction error is given by of into the above . 
 Thus , we conclude that the frequency weighted quantization error in an is not white . This fact in stark contrast to the when the are without the perfect reconstruction constraint , see , e .., . It also from the result when the feedback filter is fed back quantization error , as in and . Note that , as is made , not only becomes smaller , but its asymptotically a constant function over the 
 It is well known that i . e ., sampling a continuous time signal at a frequency above its rate one to smaller error for a given , fixed number of quantization . For instance , the of simple scalar quantization without feedback is known to decrease as , see , where is the ratio , given by the order of the feedback filter see also recent work in . From a rate distortion viewpoint , the inversely polynomial error decay of this error estimate is too slow to compensate for the increase in the overall bit rate due to which is proportional to . To be more precise , let us consider a scalar with quantization , where the quantization resolution in per sample . If the additional by was instead to increase , then the would decay as , i . e ., exponentially . 
 A faster decay of the of with can be by a different feedback filter with possibly different order for each ratio . An example of such a family of bit was given in . Here , the continuous time reconstruction error can be uniformly bounded by is independent of . This bound an that with as , which is faster than any inverse polynomial , but still far from 
 Strictly speaking , this only for whose have finite support . Indeed , it been shown that for several infinite support , the of uniform quantization asymptotically faster than , where a is a constant independent of , see . 
 exponential . Based on this result , the family of bit in achieve an that is , i . e ., increasing . Notably , the in and were an exact , deterministic model of quantization . 
 We will next show that , within the Linear Model , if the optimal infinite order in Section are used for each value of , then one can achieve an exponential decay of with the ratio , provided is kept constant . If the input sequence is from sampling a band limited signal , would cause defined in to vary with . To capture this effect , we replace 
 In , the square root of the of the frequency weighted input without , and . Notice that , that is , the total power of in of variance per sample , remains constant for all . This a uniform comparison basis for the distortion . 
 to the output of . Interestingly , it is possible to establish a precise exchange formula for and . Indeed , in of minimal achievable distortion , the effect of increasing equivalent to an exponential increase in the output of . This is shown in the next theorem : 
 Theorem : Under the Linear Model in Section , for any function , and for any , the minimum achievable : 
 If we assume that exponentially on the number of per sample , then Theorem an that exponentially with , provided the Linear Model and that optimal , and by , a and are employed for each . The following simple example this idea : 
 is constant , without . For this setup , the optimal for our model of is 
 , i . e ., a converter . From , the minimum without i . e ., with becomes 
 where . To analyze behavior of in this case , we apply Theorem to the above expression . 
 for all . Note that , to achieve , needs to be according to b and . Therefore , for this example , the of an with fixed an exponential decay with the ratio since , by definition 
 being a uniform with many and operating with a loading factor of , then becomes 
 posing that and hold , we obtain from that is lower and upper bounded by proportional to 
 . For loading factor of , , and , the exponent in the latter expression to , respectively . 
 The next theorem that the exponential decay of the in the example above can be extended to arbitrary band limited input and frequency weighting criteria . 
 Thus , under the Linear Model , we have that the of an exponentially with . 
 Remark : Model in Section . Here it is convenient to present some further regarding the validity of that model when the ratio to infinity , for different of a . 
 As already in Section , if is bounded and a sufficiently large number of quantization to avoid overload is used together with dither , then the Linear Model is exact . Nevertheless , there is no guarantee that the number of necessary quantization to avoid overload remains constant as . If such number with , then can only be kept constant by increasing the number of quantization in the . 
 If the number of quantization is insufficient to avoid clipping overload , and if dither and clipping are used with a fixed loading factor , then there a certain finite value of beyond which Assumption is . This from the fact that , for any fixed loading factor , the effect of clipping in the output does not decay with , thus , becoming the dominant component in the for sufficiently high . Further reduction of the would then require one to balance clipping and granular quantization by increasing the loading factor . If the number of quantization is fixed , this would necessarily reduce the value of , clearly increasing the component of the due to granular quantization . Nevertheless , if clipping and dither are used with , then the Linear Model and Theorem is exact in the due to granular quantization . 
 If one tried to optimize the of a fed back quantization noise , i . e ., by trying to minimize 
 Theorem . This to the result in 
 , the noise transfer function magnitude is also equivalent to that derived in . The latter is optimal in the sense of the ratio , but not in the sense of for a fixed . 
 As shown in Theorem in general , does approach as . One can then expect to be near optimal in where , see . The latter is often satisfied at high bit i . e ., when many quantization are available . However , for any given number of quantization , it is easy to find practical where is such that is comparable to or greater than . More precisely , from , and that see Appendix , one can show that , if over a set of in with measure , where is some positive 
 whose magnitude becomes significantly small in relative over certain frequency . An example is included in Section . A direct consequence is that , for these , and in view of , trying to match to will yield a performance far from optimal , also increasing the risk of large limit cycle if no clipping is employed see , e .., and . 
 was already in . Several heuristic have been since then see , e .., , , , , , and . In contrast to these , the method derived in the present paper one to characterize the true optimal , by explicitly taking into account in the cost functional to be see . Our method not only that , but also the actual optimal . Our proposal also the advantage of being applicable to arbitrary input spectra and frequency weighting , regardless of how small the 
 To illustrate our , we have designed the of a at digitally audio in a as well as the of both the and the numerical are given later . 
 The of audio was as unit variance zero mean white noise through 
 of the frequency response of is in Fig . solid line . The frequency weighting filter considered had a frequency response magnitude which the curve derived in , Table , thus , modeling the sensitivity of human hearing to noise . The corresponding frequency response is plotted with dotted line in Fig . the sampling frequency is . . The resulting 
 for these is also shown in the same figure dashed line . For this choice of , and in view of , one could expect the norm of a full whitening feedback filter to be very large . This is indeed the case : . Thus , the suboptimal feedback filter by the use of a scalar with at least in order to become feasible see Constraint . 
 In the , was chosen to be a uniform mid rise with quantization interval . Several were considered for the , calculated as 
 the loading factor . Two different loading were considered : and . The latter choice a slightly lower than the usual loading factor of . However , this regime the benefit of making overload smaller and more infrequent . As the simulation will show , for our of and , this more conservative loading factor lower 
 Fig . . Frequency response for solid line , dotted line and e e dashed line . 
 For each and corresponding two for , one for each loading factor , the of the converter were designed according to the following : 
 These were then with rational transfer , of order and 
 An appropriate value for the parameter in was chosen via , see , assuming 
 For each combination of and , the resulting converter was two different . 
 , with virtually infinitely many . Thus , for all neither clipping nor overload er 
 and Clipped : Here which a scalar with a finite input dynamic range . As a consequence , any value would overload if or produce clipping error if . To avoid large limit cycle , this variant was clipping i . e ., 
 Each simulation with the comprised , . For the converter , five , were for each combination of 
 The of the numerical and the are next . 
 Comparison Between and the Rate Distortion Function : The information theoretic lower bound see for the associated with the given source and filter is plotted in Fig . solid line . This to quadratic frequency weighted Distortion Rate function when . As the bit rate is , the gap between and this absolute lower bound to approximately . for and for 
 , at . This difference can be to the rate distortion inefficiency of the uniform scalar . On the other hand , the performance gap at lower bit can be to the perfect reconstruction constraint . Recall that , at low bit , the achievement of rate distortion function the suppression of relatively less significant of the of the input signal see , e .., and . This linear distortion , which a cannot achieve , is more severe at lower bit . Thus , the performance gap as is reduced . 
 : The of this converter variant is in four of the in Fig . , with beginning with opt . These differ in the loading factor , and in the meaning of in each case . For the whose do not have the ending E .. entropy , is simply the number to generate the value for which the were . The whose end in E .. correspond to the same , but for each point the value of is the scalar entropy of the output of the converter . It can be seen in Fig . that the for the without entropy is remarkably close to the theoretical value by a . More importantly , even for bit as small as , each ratio from its nominal value of by less than . For the extreme situation , the was slightly lower than , while was higher than due to the highly nonuniform of the resulting sequence . It can also be seen that the scalar entropy of the output of the in these is very close to function for a given distortion . This with the observation that the output of in an is white , see the comment at the end of Section . The difference between these is bigger for lower of , for the same reason in Section . 
 : For the an of , the along with the corresponding 
 that range of bit . This performance degradation can be to clipping . The fact that overload become noticeable only for high bit many quantization might seem , at first , surprising . However , this phenomenon can be easily by that the size of the of the of that fall outside the dynamic range of remains approximately constant in relation to for all . This is a direct consequence of the loading factor rule . In contrast , granular quantization error is proportional to 
 which is constant in the . Therefore , the ratio between clipping and granular quantization approximately as and clipping become dominant for sufficiently high bit . 
 Because of the reduced occurrence and magnitude of clipping , the with 
 and an smaller than that of its counterpart with . Furthermore , this more conservative loading factor the converter to perform almost exactly as by our analytical expression for 
 . For the chosen input and frequency weighting filter , and calculating as , the value of with as shown in Fig . dotted line . As seen in this figure , the gap between and , for each value of , smaller as the bit rate . This with the fact that the optimal a converter as , see Section A . It can also be seen in Fig . that the with and an improvement of over at . Equivalently , in order to obtain the same as that of at , the converter with less than . At lower bit , the improvement of the optimal is also significant . For example , the with a lower than the converter with , thus , a data rate compression of see Fig . . 
 This paper studied perfect reconstruction feedback based on an additive white noise model for quantization . We have derived that relate the minimum frequency weighted and the of the scalar in the converter . We have also provided closed form for the optimal frequency of the in the converter and have derived several of optimal . In particular , we have shown that the optimal frequency response of the are unique , that the frequency weighted of an optimal are nonwhite , and that consecutive of the output sequence of the scalar are uncorrelated . We have also shown that , within our model , exponentially with ratio . 
 The following preliminary are necessary to prove the stated in the previous . We begin by the following definition . 
 We say that two are similarly functionally related there a monotonically increasing function such that , for all , and write . Similarly , if there a monotonically decreasing function such that , for all 
 If and are oppositely functionally related , then the inequality in is reversed . In either case , equality is and therefore is almost constant . 
 Proof : We will examine the difference between the and in . We obtain 
 We now proceed to upper bound the last term in the above inequality . From and , we have that 
 where the last inequality from and . Substitution of into 
 Since is bounded , and from b , it from that for any , one can always choose sufficiently large 
 Denote the squared norm of see via , and define the set of all the the same norm as 
 It is easy to show that must belong to . From this , and since , it that 
 The problem by within the category of isoperimetrical , well known in variational calculus see , e .., and . The standard solution of these is based upon the fact that any that see needs to satisfy 
 We note that for the trivial case in which is almost constant see Definition , is also almost constant . this to constraint i in that , for this case , is such that 
 . Thus , the remainder of the proof only the in which is not almost constant . 
 In order to find , we will next discard the possible of which do not correspond to global of in . The unique function , which is with and in , will characterize the solution of Optimization Problem . 
 The Case : Fore this case , substitution of into that needs to satisfy 
 so that can be explicitly from . Note that cannot be zero in the above expression , otherwise would be undefined . From this , the feasible sign for , the sign before the square root , and in are 
 Option a : We show next that any solution by option a in , say , a greater 
 by Theorem to the numerator of , together with and the fact that . Both are strict since is not almost constant see Theorem and Definition . 
 On the other hand . From the above , it that for all non 
 Option : The candidate are now by and only . a to and , these take the form 
 Combining this result with , and considering to be not almost constant , we obtain 
 response magnitude in the absence of fed back quantization noise recall . This is not surprising , since taking to removing constraint i which the 
 Since the , are continuously differentiable , so is . We , therefore , have that if 
 , then we obtain . This would imply for all such that . Thus , belong to , the integral of over the needs to be infinite . Since ; x , this that infeasible and . 
 We will first elaborate upon to derive . Then we will prove that . 
 where and are as defined in . Application of the identity , which from and a to the numerator on the of 
 The Sign of : Since , this limit needs to be for two possible , depending on whether or not is positive . 
 must necessarily hold in order to obtain . Thus , and its first are continuous . Therefore , in view of , we get 
 , it is clear from that there a value for greater than under which is small enough to render negative . Therefore 
 : In order to show that the and in Theorem hold , we write as 
 We will first prove the validity of . Clearly , if for all condition i of Assumption , then the of the above equation to as 
 . If this the case , then the second condition of Assumption must be satisfied , and therefore the of 
 In order to show that i . e ., , we first note from that for all . On the 
 H . Proof of Theorem c to one can write Since is monotonically decreasing see Theorem 
 to the minimum for a constant , by virtue of we have that . Substitution of this into the last inequality . 
 ﻿ We improve the achievable rate for causal and for zero delay source of stationary under an average mean squared error distortion measure . To begin with , we find a closed form expression for the information theoretic causal rate distortion function under such distortion measure , by , for first order Gauss . is a lower bound to the optimal performance theoretically attainable by any causal source code , namely . We show that , for , the latter can also be upper bounded as sample . In order to 
 analyze for arbitrary zero mean stationary , we introduce , the information theoretic causal when the reconstruction error is jointly stationary with the source . Based upon , we derive three closed form upper to the additive rate loss defined as , where . Two of these are strictly smaller than . sample at all . These differ from one another in their tightness and ease of evaluation ; the the bound , the more involved its evaluation . We then show that , for any source spectral density and any positive distortion , can be by an additive white noise channel surrounded by a unique set of causal , post , and feedback . We show that finding such a convex optimization problem . In order to solve the latter , we propose an iterative optimization procedure that the optimal and is to converge to . Finally , by a connection to feedback quantization , we design a causal and a zero delay scheme which , for , an 
 sample , respectively . This that the among all zero delay source , by , is upper bounded as sample . 
 Causality , convex optimization , differential modulation , entropy quantization , noise shaping , rate distortion theory , sequential . 
 N zero delay source , the reconstruction of each input sample must take place at the same time instant the corresponding input sample been . Zero delay source is desirable in many , e .., in real time where one cannot afford to have large , or in feedback , in which the current input on the previous . A notion closely related to the principle behind zero delay is that of causal source , wherein the reproduction of the present source sample only on the present and past source but not on the future source , . This notion does not preclude the use of entropy , and thus , it does not guarantee zero delay reconstruction . Nevertheless , any zero delay source code must also be causal . 
 It is known that , in general , causal cannot achieve the rate distortion function of the source , which is the optimal performance theoretically attainable in the absence of causality . However , it is in general not known how close to one can get when attention to the class of causal or zero delay source , except , for causal , when dealing with memory less , stationary at high resolution , or first order Gauss under a per sample mean squared error distortion metric . 
 For the case of memory less , it was shown by and Gilbert that the optimum rate distortion performance of causal source , say , is by time at most two memory less scalar by entropy . In this case , the rate loss due to causality was shown to be given by the space filling loss of the , i . e ., the loss is at most sample . For the case of stationary with memory and distortion , and that the information theoretic causal , here by 
 to be defined formally in Section and which , to as the distortion goes to zero , . The possible gap between the of causal source and this information theoretic causal was not assessed . Since operational data are lower bounded by the mutual information between the source and its reconstruction , we also have that . 
 On the other hand , for arbitrary stationary with finite differential entropy and under high resolution , it was shown in that the rate loss of causal i . e ., the difference between their and is at most the space filling loss of a uniform scalar . With the exception of memory less and first order Gauss , the price of causality at general rate for other stationary remains an open problem . However , it is known that for any source , the mutual information across an additive white noise channel and across a scalar entropy quantization channel do not exceed by more than . and . sample , respectively , . This immediately the and . 
 In causal source , it is generally difficult to provide a constructive proof of since random construction , which upon jointly long of source , is not directly applicable even in the case of memory less . Thus , even if one could obtain an outer bound for the achievable region based on an information theoretic , finding the inner bound , i . e ., the , would still remain being a challenge . 
 There exist other related to the information theoretic causal , in which is not . The minimum sum rate necessary to sequentially block encode and block decode two scalar correlated random under a coupled fidelity criterion was studied in . A closed form expression for this minimum rate is given in , Th . for the special case of a squared error distortion measure and a per variable as opposed to a sum or average distortion constraint . In , the minimum rate for causally and source under either a per sample or average distortion was given the name sequential rate distortion function . Under a per sample distortion constraint , it was also shown in ,. that for a first order Gauss source , a zero mean white process with variance , the information theoretic the form 
 for all . No are known for for higher order Gauss . Also , with the exception of memory less with its average distortion constraint than a per sample constraint , not been . 
 In this paper , we improve the inner and outer rate distortion for causal and for zero delay source of zero mean stationary and average distortion . We start by showing that , for any zero mean source with bounded differential entropy rate , the causal by less than approximately . sample . Then , we revisit the problem for first order Gauss under a per sample distortion constraint schedule and find the explicit expression for the corresponding by of an alternative , constructive derivation . This expression , which turns out to differ from the one found in , bottom of . , us to show that for first order 
 Gauss , the information theoretic causal for an average as opposed to per sample distortion 
 measure with . In order to upper bound for general stationary , we introduce the information theoretic causal when the distortion is jointly stationary with the source and denote it by . We then derive three closed form upper bounding to the rate loss 
 , which can be applied to any stationary random process . Two of these are , at all , strictly than the best previously known general bound of . sample . Since , by definition we have that 
 . As we shall see , equality would hold in if could be by a test channel with distortion jointly stationary with the source , which a reasonable conjecture for stationary . 
 We do not provide a closed form expression for except for first order Gauss , and thus the upper bound on the right hand side of the bound in this paper is not analytically for the general case . However , we propose an iterative procedure that can be numerically and which one to evaluate , for any source power spectral density and 
 , with any desired accuracy . This procedure is based upon the iterative optimization of causal , post , and feedback around an channel . A key result in this paper and its second main contribution is showing that such filter optimization problem is convex in the frequency of all the . This that the mutual information rate between source and reconstruction by our iterative procedure monotonically to as the number of and the order of the tend to infinity . This equivalence between the solution to a convex filter design optimization problem and the troublesome minimization over mutual , thus making it possible to actually compute in practice , for general stationary . We then make the link between and the of causal and zero delay . More precisely , when the channel is by a uniform scalar by memory less entropy , the with the iterative procedure yield a causal source system whose operational rate is below sample . If the entropy coder in this system is restricted to encode individually as opposed to long of them , then this system zero delay operation with an operational rate sample . This directly into an upper bound to the of zero delay source , namely . To illustrate our , we present an example for a zero mean AR and a zero mean AR source , for which we evaluate the closed form and obtain an approximation of numerically by the iterative procedure herein . 
 This paper is organized as . In Section , we review some preliminary . We prove in Section that the for does not exceed the information theoretic by more than approximately . sample . Section the derivation of a closed form expression for for first order Gauss . In formally introduce and derive the three closed form upper bounding for the information theoretic rate loss of causality . Section the iterative procedure to calculate , after the proof of convexity that its convergence . The two are provided in Section . Finally , Section . Most of the of our are given in . 
 denote , respectively , the set of real and the set of real . and denote , respectively , the of and positive . We use lower case , such as , to denote scalar random , and and to denote and matrices , respectively . We use and to denote the , the column span , and the null space of the matrix , respectively . The expectation operator is by . The notation to the variance of . The notation a one sided random process , which may also be written simply as . We write to refer to the sequence . The of a wide sense stationary process is by 
