 We obtain the maximum average data achievable over block fading when the receiver perfect channel state information , and only an entropy constrained approximation of this is available at the transmitter . We assume channel gains in consecutive are independent and identically distributed and consider a short term power constraint . Our analysis is valid for a wide variety of channel fading statistics , and fading . For this situation , the problem into designing an optimal entropy constrained to convey to the transmitter and to define a rate adaptation policy for the latter so as to maximize average data rate . A numerical procedure is which the and reconstruction of the optimal , together with the associated maximum average , by finding the of a small set of scalar of two scalar . this procedure , it is found that the maximum average capacity , in some , time between two . In addition , it is found that , for an entropy constraint log , a with more a small capacity increase , especially at high . Index Channel state information feedback , Information , fading , quantization , radio communication . is well known that the achievable data for reliable communication over a fading wireless channel depend on the availability of channel state information at the transmitter and end , . For single input flat fading , the of channel gain and phase . If perfect is available at the transmitter perfect and at the receiver perfect , the channel is slowly fading and the transmission is subject to a long term average power constraint , then the average capacity is by rate and power to the channel gain in a time fashion , . By contrast , if an instantaneous per block maximum power constraint is , the are ergodic and the transmission are long enough so that the fade statistics over each block converge to their ensemble statistics , then the ergodic channel capacity is achievable without , . Else , if the fading is so slow that channel gain can be as constant within each block which to a block fading scenario then is beneficial . In this case , with perfect and per block power constraint , the capacity is by at maximum power , with only the data rate being to the channel gain in each transmission block . If perfect is available and the receiver back this via an with limited information throughput , then only imperfect will be available at the transmitter . In a block fading situation , the uncertainty at the transmitter about the true channel gain in each block a trade off between throughput and reliability : the the data rate chosen by the transmitter , the higher the probability of exceeding the channel capacity during the transmission block . This the problem of the at the receiver and it at the transmitter i . e ., choosing rate and power in a rate distortion optimal fashion , where the distortion is some measure of the decrease in throughput , as in , or the increase in error probability , as in . The capacity of memory less block fading with long term power constrained transmission and fixed rate constrained feedback was studied in . A similar situation was considered in , assuming a scheme in which data are perfectly or totally lost if transmission data rate is , respectively , below or above the channel capacity during the block . The idea in was to design a with a fixed number of quantization so as to maximize the rate , i . e ., the number or long term average of successfully . Also for a constraint in the number of quantization , studied the maximization of throughput considering a noisy feedback channel . There exist also numerous related to throughput maximization for multiple output wireless see , e .. , and the therein . Although not directly related to the problem which is the focus of this work , it is worth that , in all the in , and the therein , the only constraint on the where there is a is its . In , the maximum average throughput under a long term power constraint and for a fixed number of quantization is . The performance of to as MASA was against that of average reliable throughput to as ART , which allow for to occur . It is shown in that , in some , when the additional feedback load of the ART associated with the transmitter of a previous outage is in , MASA outperform ART . In that context , the feedback load to the entropy of the plus and that are sent to the transmitter . However , in this entropy is a , i . e ., after the have been without considering entropy as a constraint . Thus , in all these , the design of optimal been only considering a constraint on the number of quantization or . However , if one the question what is the maximum throughput that can be if there is a constraint on the amount of information that can be sent to the transmitter for the , then it is more appropriate to consider an entropy constraint instead of a constraint for the . On the other hand , the entropy of the output , say , is a lower bound to the average number of to represent this output . At the same time , by , it is possible to find prefix free for each outcome with an average length not greater than per realization . Moreover , in a situation in . i .. are at a time which would happen , for example , in an system fading , joint entropy would yield with an average length , at most , bit . Since , in general , information to feed back for each realization less average power , or time , the latter can be directly associated with a low entropy . This a practical motivation for considering entropy , instead of , as a constraint for the . However , to the best of the knowledge , there are no available on average throughput maximization in which the to encode for the transmitter is to be designed subject to a constraint on the entropy of its output . With the stated in the previous paragraph , in this paper we study the problem of finding entropy constrained with any given number of quantization , for block channel gains for the transmitter , that yield the average data rate . In our setup , the channel is assumed to experience i . i .. block fading , with associated gains and phases perfectly known to the receiver . We consider a wide family of fading statistics , general enough to include and fading with one or more of freedom . As in , the over which is fed back is an , zero delay channel . To solve this problem , we propose a numerical method which the optimal quantization and reconstruction for any given number of quantization and average channel signal to noise ratio . The optimization problem is partly similar to the design in because of the common entropy constraint . However , as we shall see , since the distortion measure in this case is the decrease in average rate not mean squared error , the resulting situation is vastly different from the one in standard entropy constrained quantization . The problem turns out to be non convex , and our analysis that it , in general , several local . Its formulation , for quantization , to a system of N non linear in N , each of which taking over the non negative real . Since each of these must be numerically , and due to the high dimensionality of the search space , direct solution of this system of a high numerical complexity task . The numerical procedure in this paper greatly this complexity by turning the problem into finding the of a small set of scalar of two scalar , only one of which unbounded support . The evaluation of each of search with respect to monotonic . By this procedure , it is found that , in general , the maximum average a given entropy is a non concave function . Since in our formulation time between two an average capacity and entropy equal to the weighted of the capacity entropy of each regime , the region of all achievable , is given by the convex hull of curve . On the other hand , it is found that if is log of the number of available quantization , then the so as to obtain is nearly optimal . Our also allow one to find the gain in average throughput of an optimal entropy constrained instead of a constrained optimal . For instance , when the average is , then an entropy with and an average rate of bit per realization an increase in average throughput over an optimal fixed rate with two i . e ., the same average rate . The performance of the latter fixed rate with the one found in . It is also found that for any given maximum entropy constraint log , the increase in maximum capacity by a with more relatively small . Moreover , our analysis also that , for any given , the maximum average capacity is a with a finite number of . This with what is also for an exponentially distributed source but with as the distortion measure , wherein the optimal turns out to be uniform with infinitely many . In the following section we present a precise model description , introduce some notation , and formally state the problem of interest . To illustrate some of the of this problem and its , we first analyze the case two quantization , which can be explicitly , in Section . Then we extend the analysis to the case in Section , where we introduce the numerical procedure to solve the problem in its generality . the with this procedure for the case and under fading . Finally , Section . We consider a block fading additive white noise channel , a transmitter , a receiver and an error free , zero delay channel , as in Fig . . In the transmitter , the binary message into consecutive of . During each block , a real valued sequence is over the channel . The random block channel gain magnitude for the th block is assumed constant within each block . Channel gains in consecutive are i . i .. according to a probability density function satisfying the following : Assumption : The of the the channel gain squared magnitude the form for appropriate K , K , where the differentiable function is such that the ratio u u is non increasing with respect to u over , . The structure of the is fairly general . For example , if channel gain magnitude is distributed , then the form where I is the function of the first kind of order zero . From direct comparison with , we obtain , for this case ,, K s and u . It can be numerically that the latter form of the by Assumption . Likewise , if channel gain are by a distribution , then the form and we have K , K and u um . If , it is easy to verify that u also the to Fig . , the real valued random process , ,...,, is with sample variance N . Thus , if were the , taken at frequency , of continuous time band limited to , then the The necessity of the condition upon in Assumption will become evident in Lemma Section , in which it us to prove the convexity of a function a key role in the problem under study . two sided of the latter would be N . On the other hand , the information bearing signal is subject to a per block power constraint of the form where . With this constraint , if the block large , then the maximum achievable data rate during any given be well by capacity formula as , where is the mean at the receiver for a channel power unit mean value . At the other end of the channel , the receiver is assumed to acquire a perfect estimation of prior to or at the beginning of the th transmission block . This channel power gain is instantaneously and entropy , with the resulting being sent over a zero delay , error free channel . These about the feedback channel have been considered before in , . The zero delay condition can be to be a good approximation when the time spent to feed the back to the transmitter is much shorter than the duration of a frame . In turn , it is possible to have an almost error free feedback channel if the feedback is sufficiently large and or strong forward error correction is employed for the . And naturally , if in a given situation these latter are not present , then our would provide upper to achievable performance . As in the Introduction , it is possible to translate a small entropy of the into less average power , or time to convey this to the transmitter . At this point , it is perhaps worth that if only a single realization is and fed back at the beginning of each block , then these may require one to match the channel and modulation scheme in the feedback link to the variable coming out of the entropy coder . For instance , an off the shelf channel coder and modulator in the feedback channel would yield an that only of fixed length data . Such choice which would entail significant when variable length , in comparison to sending . However , in this scenario wherein a single realization is and fed back at a time , the feedback channel and modulation can be chosen so as to handle variable length bit or the associated unequal probability of the as efficiently as it is possible for fixed rate . This can be done , e .., by variable length error correcting or joint source channel see , e .., and the therein . Although the design of such and is beyond the scope of this work , we illustrate this fact with an example a simple scheme similar in spirit to , which can be found in Section A in the Appendix . Upon the , the transmitter a transmission data rate from a discrete set of data . To define the and its reconstruction , let be the number of quantization or , and let denote the set , where Define also the quantization , , i ,...,. As in , whenever the channel power gain within cell , the transmitter a satisfying , belonging to the i th amongst , one for each cell . This is capacity for some nominal channel power gain associated with the cell , i . e ., Thus , the power gain can be seen as the set of reconstruction or of the , as in Fig . . Since in each block the transmitter information over the a capacity code for a nominal channel gain , all the are correctly if . Else , if , then is not by the channel , and the receiver an outage , all the information received during the th block . From this , it is clear that a necessary condition for a set of reconstruction to be optimal is Let the random variable , taking in with , denote the output of the for the th block . As already in Section I , we focus on that satisfy an entropy constraint , which we now formally state as with the maximum entropy for the output . We are interested on finding the i . e ., the and reconstruction satisfying the entropy constraint and the average in the channel , defined as the average number of correctly . The average number of correctly sometimes to as average or average reliable throughput is also the the objective function in , , . It is a reasonable figure of merit if one forward error correction by been applied to the data being sent over the , so that , with high probability , lost due to outage do not cause irrecoverable . Otherwise , and if all data is to be correctly , and would have to be sent back over the to request retransmission of lost data . As we shall see from the in Section , at least for fading and for equal and , the optimal entropy constrained are such that only the first cell V for outage . The latter that if sending and is necessary , then it would mean at most V per realization to the . The extra bit rate is upper bounded by V block because only when V it becomes necessary to send an or during block , which at most extra bit every time V . Since in consecutive are i . i .. and ergodic , over a large number of converge to ensemble . Thus , for notation simplicity , in the following we drop the frame and channel . With this , the design problem can be stated as finding the and satisfying and , that maximize the average data rate in the channel without exceeding the entropy constraint in the . More precisely , combining , , and , we state the optimization problem in canonical form as minimize : with and , and where is the cumulative distribution function of . This optimization problem is difficult to solve primarily because the entropy constraint c is non convex . As we shall see , this to the existence of several local , which , in principle , one to run an optimization program several times with a potentially large number of different starting . In the following we solve this optimization problem , first explicitly for the case , and then numerically by of optimization and a novel procedure which greatly the overall complexity of the task . We now address the optimization problem stated in for the case , corresponding to two quantization . In this case , to : minimize : , i This problem can be explicitly without by that the entropy associated with the two only on the threshold . Supposing is given , the optimal value of u is found by the objective respect to it and to zero : We see from this equation that for every u there a unique u for which u . On the other hand , in order to determine the optimal value of u given , we notice that this value to minimize the term u u in a . Although , in general , such value cannot be found explicitly , the following lemma that it is unique and that it can easily be found numerically : Lemma : a satisfying Assumption . Define u The proof of this lemma , which will play a key role later in Section , can be found in the Appendix , at the end of this document . It turns out that the value of u which u , u u with . More precisely , define the function U for any , with the inequality being a consequence of the fact that is non decreasing . from Lemma that u , is convex in u , we conclude that becomes zero at a single value of u greater than or equal to . This that for all , . The latter result that the optimal value for u must belong to the interval ,, where . Also , by Lemma with , it is readily found that the unique value of u a also . The convexity by Lemma that the latter function can be easily numerically by line search . Moreover , Lemma that u u , which to the conclusion that the optimal value of u given is It then that , if u is part of an optimal for some entropy constraint , then the optimal can be explicitly from u by , and then the optimal u can be directly derived from . In this manner , by increasing u from to and and u with the latter , one a family of the optimal for every value of . Fig . top the curve of capacity versus entropy by the method in the paragraph , for two quantization , under an average , left and , right . More precisely , for every , the corresponding was calculated . Then , for this value of , the optimal u was determined . In this way , for each value of u , a different was . The pair capacity , output entropy associated with each a single point in the top two in Fig . . In this example , channel gain distribute , so exponential , chosen to yield , i . e ., u e u , u , . Notice that , for every value of , there exist two to and , corresponding to different local constrained . There is one associated with each of these two . One of these the upper section of the curve in each of the shown in top , and the other one the lower section . Of course , the optimal are those responsible for the upper part of their respective , i . e ., those which yield the maximum capacity for a given entropy constraint . As , in general , capacity when the entropy of the available to the transmitter entropy is . Fig . . to and two quantization for fading with unit mean channel power gain . Top : under an average , left and , right . Bottom : and code for the optimal solution as a function of entropy under an average , left and , right . Note that the maximum capacity for , . , at an entropy of . block , not with maximum entropy , which , for a two cell is bit block . This is expectable for a with a fixed number of , since there is no reason why all being equally likely the only situation in which the entropy is should yield the capacity . Also , the maximum for a two cell at and coincide with what was in , where average capacity was without an entropy constraint on the output . At an average of , Fig . top right that the maximum capacity closer to maximum entropy , and that , corresponding to this maximum entropy , are near optimal . Interestingly , the maximum capacity curve at this a at an entropy of about . block where the curve itself in Fig . top right , which it non concave . For this case , this that better performance for below block can be by doing time between two : one with a single cell and zero entropy , and another with two and an entropy of about . block . By choosing one regime more frequently than the other , it is possible to achieve a capacity and an entropy equal , respectively , to the weighted of the and of both . In this manner , all capacity entropy within the convex hull of the can be . Fig . bottom the evolution of and for the optimal solution , as the entropy of the output . For , on the left , u at all . We see also that higher are by and code closer together . For the case , shown in Fig . bottom right , the in with a change in the arrangement of and code in the . Except in a neighborhood to the right of this , we find that the u with its left boundary threshold . The straightforward approach in the previous section cannot be extended directly to the case in which there are more than two quantization . optimization can be instead , which , as we shall see , to a system of non linear that must be numerically . The non convexity of the optimization problem and the existence of several local satisfying the the need to solve this system of non linear possibly many times with different initial . However , we will introduce an algorithm , in the same spirit as the strategy in the previous section , that one to simplify the optimization problem to a sequence of simple line search , each with a single solution . Before the associated with , we note from a that , at the optimum , the inequality stated in e are not active . Similarly , constraint d for the case i is not active since increasing u above would raise the average data rate without increasing the entropy of the output . Taking the above into account , the associated with the following form . with respect to u and to zero Notice that a is identical to , which that u is a solution to a only respect to the other and to zero On the other hand , the with respect to the we obtain Finally , the Tucker , provide another set of , Although it is possible to solve this system of N non linear by standard numerical , the existence of numerous local minima one to apply these repeatedly , each time with different initial . This shortcoming is by the fact that the vector of initial plus threshold and code point in a N dimensional space , which a large number of initial is to obtain a reasonably good coverage of the search space . In the following section we will show how these can be by an approach similar to the one in Section . More precisely , we derive a method which , for this problem , one to find all local minima in a systematic , sequential manner , greatly reducing the number of . In this section we exploit the recursive nature of to reduce its induced system of non linear into a sequence of line search over bounded , in a spirit similar to the one behind the approach in Section . To begin with , recall that the imply that , for every constrained local minimizer of a , the multiplier only if i . e ., only if the associated constraint is active . The next corollary of Lemma an verify condition for to hold in such a minimizer : Suppose are a solution to optimization problem . Then for some ,..., if and only if Proof : The result directly from Lemma , since it the convexity of the function u u , and from a , upon that the entropy of the output does not depend on the choice of . Corollary will allow us to find and from , , and in a simple manner . For this purpose , define , for ,..., where is the complementary . With these , the combined c , d , g and i can be written in the following equivalent form , , ,..., a , ,...,. b Fig . . of , and defined in , as of . Left : a case in which . Right : a case in which Figure a qualitative description of , and as of . It can be seen that both are monotonically increasing , the first one being affine , the second one convex . A look at Fig . immediately that , for any given , and depending on the of the , and , there will be , in general , more than one pair of of and satisfying . Let us find out which actually by first considering the under which the constraint is active or inactive : • Inactive Constraint : In this case which see b . In view of , this is equivalent to The first equality of is satisfied by a unique value of the argument of , say , shown in Fig . . From the definition of , this quantity must be . On the other hand , Corollary that if and only if situation and in Fig . right . In addition , Lemma that , if this inequality is satisfied , then there a unique satisfying the second equality of . Thus , there a solution to for which the constraint is inactive if and only if and . In this case , we say solution for . • Active Constraint : In this case , , and can be positive . By looking at , a solution satisfying this condition if and only if there is for which which correspond to of the of , and on the first quadrant in Fig . . Since , with respect to , the function , is affine and increasing , and the function is convex and monotonically increasing , it that is satisfied for either none , one or two of . Indeed , these imply that two to equality a in will exist if and only if is the unique value which , , see Fig . . It is also easy to show that if , these two , say and , will lie at opposite sides of . Also , it is straightforward to verify that both are not than . Therefore , problem , each solution to equality a in will also be a solution to if and only if it is non negative and it a non negative value for . This one to discard solution or if . On the converse , if these two do not hold , solution will be non negative and yield , i . e ., it will be a valid solution to . In this case , it is easy to verify that , with i , , , be the threshold value associated with , we can devise the following procedure to find the to for a single , given : Procedure : Suppose , and that , and are given , with . Then , in order to find the to for , a Find by equality a in with respect to by line search over find equality a in by line search over unless , in which case . Set , and The last step , where yielding are whenever , to the fact that is a complementary value . This requirement is only if one is calculating the last threshold i . e ., when , to allow a higher level routine to iteratively adjust so that . Such a routine is by Procedure below . Of course , unless , solution can be before doing the corresponding line search if . The same for solution if . In each of the line in Procedure , there a single solution over the corresponding search interval , since , in all , the involved function is monotonic within it . This each step straightforward to execute . Notice also that for every , and depending on the of , and , there are between zero and three of for , , namely , and , which satisfy for that . The above procedure can be applied sequentially to find all , , for , ,...,, that satisfy . More precisely , one can first choose for u , and , with which one can calculate explicitly from a . Then , setting , Procedure can be carried out to find at most three of for u , satisfying for . Each solution can be considered a branch in a tree structure . one and the procedure for each branch , until , the complete tree of valid , each of which is associated with a path in the tree . Thus , for each choice of u and , there can be at most N different , each associated with a sequence of and satisfying for all , ,...,. However , we shall see in the following that the number of valid in practice is much smaller than N . Following our notation for adopted in Procedure , we label each solution path in the tree the of the solution associated with its , thus to a sequence . For a given choice It is important to note that there is a one to one relationship between every solution satisfying for a given choice of u , and every path in O u ,. Indeed , the path associated with any such solution can be easily determined by each of its code point threshold the the reasoning that led to Procedure . Now , suppose one to know whether a given path is associated with a valid solution to and then find this solution . Instead of Procedure to find the entire tree of valid and then if is one of them , one can apply the following algorithm , derived directly from Procedure , for this purpose : Procedure : Let u ,, and the corresponding be given by a . Let be a path . Then the following can be taken to determine whether and find its associated and code With the above procedure , the resulting of all and are a function of . For convenience , we denote this function by , defined where we have chosen † as a special symbol to indicate that the path does not yield a valid solution . For future use , we also define Although being a solution to is a necessary condition for code and to be a solution to , two additional must be satisfied for sufficiency . The first one is the entropy constraint , expressed in the equation f . The second is the construction condition or , equivalently , . For a given path , u and , this condition can be expressed as . is the set of all , for the first u and multiplier associated with to for some entropy constraint , the set of all , respectively , as the entropy and capacity associated with , the original optimization problem can be stated as Thus , we have reduced the problem from a set of N non linear , over a N dimensional space , into a moderate number of one per valid path , over two , the evaluation of a scalar function to compute . Define a grid of of u over ,, say U , and a grid of for say , for some . Detect of consecutive of between which a sign change of for some valid path . In each of the formed by such , find the value of for which by line search , running Procedure for the corresponding path . Calculate and for each of the found in the previous step . Select the combination that the capacity with an entropy not greater than . In the following section we present an example in which Procedure was to find the solution to for . In this section we apply Procedure to find the set see , for the case , i . e ., for four , assuming is fading . The latter set the , for u and that characterize a solution to , i . e ., and code that yield a local maximum or minimum capacity for some fixed maximum entropy constraint . The are in Fig . , for , on the left , and for , on the right . The two top graphics in this figure correspond to for all to . It can be seen that , although for each there exist several such , for each entropy constraint the number of these is significantly smaller than N . For the case , the absolute maximum capacity is . , with an entropy of . block , by a solution with associated path . The curve that this maximum is ended at that point and , to the eye , it as if there was a missing segment which would connect it to the curve that the right boundary of the plot , at an entropy of block . The absence of this segment can be to the fact that in the formulation of the problem , the entropy is an inequality and not an equality constraint . The solution yielding the maximum capacity for a given entropy is given by the highest curve in each of the . Interestingly , for somewhat below log and log block , the curve with the optimal solution with and quantization , respectively . This can be seen by from the bottom in Fig . that a few block below these entropy , the optimal solution is such that one , two or three , respectively , tend to infinity , effectively leaving three , two and then one cell , as the entropy is . Such behaviour that for a finite entropy constraint , the maximum capacity over all is with a a finite number of . As already for the same and two , the region of achievable average is given by the convex hull formed by all the in the . Unlike what is for , since the composite maximum capacity curve for Fig . top right is not concave , the maximum capacity for some entropy constraint between two . The point of maximum capacity for each number of quantization to the solution when a with that number of is for maximum average throughput without an entropy constraint . Therefore , those peak are the found in for single layer , where the was under a constraint only . From this fact , we can conclude from Fig . top left that at , and for the same average rate of an optimal fixed rate from with two that is , bit per realization , which . , an optimal entropy with quantization approximately . . This an increase of roughly in average throughput for the same average rate . The corresponding increase with respect to a fixed rate with goes from . at a fixed Fig . . entropy for the to , for fading with unit mean channel power gain , up to quantization . Left : Average . Right : Average . . an optimal entropy with . For an of , Fig . top right that an optimal entropy constrained smaller gains over fixed rate optimal . Notice also that can only operate at a limited set of given by log realization , Thus , entropy quantization is the only scheme which one to send other average for example , below bit realization . We have a numerical procedure to find the maximum average capacity over block fading , under a fixed per block power constraint , when the receiver perfect and an error less , delay free , channel is available to convey to the transmitter . This procedure , which a smaller numerical complexity than trying to directly solve the associated with the problem , also the and that achieve the optimal solution . Our are valid for a broad class of channel fade , and fading . We have applied the procedure here to find optimal and quantization . The revealed that , for a given number of quantization , say , maximum capacity is at an entropy slightly below log block . Furthermore , our show that for any entropy below log block , there is little to be in average capacity by more or . This that for any finite entropy , the optimal a finite number of quantization . Our analysis also revealed that for high average , the maximum average capacity time between two with different and associated . As a final remark , we would like to mention that , after several , the have found that the and here for a short term power constraint do not seem to be applicable for the long term power constrained version of the problem . Indeed , the latter problem to be significantly harder to solve than both the one in this paper and the long term problem without the entropy constraint . Here we provide an example to illustrate how a discrete random source with small entropy which can be associated with the variable length coming out of an entropy coder can be efficiently a channel coder . The latter coder is able to transmit the low entropy source less power and with a smaller message error probability than what is when modulation and maximum likelihood . For this purpose , suppose we have two , each with four quantization . The first is not entropy , and each of its , two , equal probability . For simplicity , suppose that this is by a rate error correcting channel coder and that is . Assume each symbol in the constellation unit energy and that there is a memoryless channel in the , with complex circularly symmetric white noise with variance . Therefore , each outcome or message is into of length two that is , two consecutive are sent for each message , which an average energy of . At the other end , the is done by by the most likely symbol sequence given the received signal . For this scheme , it can be found either analytically or via that at an of , the message error probability is approximately . . For the entropy , suppose that its four possible , say m , m , m , m , have , , and , respectively . An entropy coder for this for example , a coder would output , , and , respectively , for each of its . In order to send these four or over the feedback channel , consider a time digital modulator generating from the shown in Fig top . Each of these except symbol o unit energy . An outcome from the entropy is fed back to the transmitter by sending a sequence of three channel , each coming from the first , second and third constellation , as in the table of Fig . Notice that with higher probability are to symbol with smaller total energy , the latter being proportional to the length of the by a entropy coder i . e ., proportional to log , where is the probability of the i th message or outcome . This to the sequence length or energy distribution that average energy , the latter energy being in this case equal to . Therefore , the average energy by the channel scheme for the entropy is just of the mean energy associated with the fixed rate . On top of this power reduction gain , if this variable rate scheme to the entropy is combined with a maximum likelihood sequence at an of , then each message is with a message error probability not greater than . . This that if the of the have uneven which to a smaller entropy than with probable , then it is possible to transmit the less power and with a smaller probability of error than when the of a fixed rate . where we a short hand notation for . For every u such that u , it readily from the structure of see that u and therefore immediately da u . The same for u . Thus , it is only left to consider of u and such that u . If da u , then , from , Since we are only considering the in which u , u . This one to Since we are considering of u such that u , we can substitute into , It then that also if da u . We therefore conclude that , irrespective of the sign of da u , Fig . . Top : Three consecutive of a digital modulator for the of an entropy with quantization . a ,,, have unit energy . Bottom : A between quantization m to m and channel symbol . The Since is a non increasing function , it that the of is positive . Therefore , we obtain from that a u is also convex for all u , such that u . This the proof . ﻿ This paper novel on perfect reconstruction feedback , i . e ., noise shaping , predictive and sigma delta A whose signal transfer function is unity . Our analysis of this class of is based upon an additive white noise model of quantization . Our key result is a formula that the minimum achievable of such to the signal to noise ratio of the scalar in the feedback loop . This result us to obtain analytical that characterize the corresponding optimal . We also show that , for a fixed of the scalar , the end to end of an optimal which the optimal which for this case turn out to be increasing ratio . Key from work include the fact that fed back quantization noise is explicitly taken into account and that the order of the converter is not apriori restricted . Index Differential pulse code modulation , optimization , quantization , sigma delta modulation , source . HE term feedback to a class of to digital converter wherein a scalar is within a linear feedback loop . Well known of include , and sigma delta . The latter have been very successfully applied in a number of , audio compression , , A conversion , , subband , digital image half , power conversion , and control over . Fig . a general configuration . In this scheme , may take the form of a nonuniform or a uniform , the latter being either or . The in an system allow one to exploit the predictability of the input signal so as to reduce the variance of . When with simple smaller quantization step . The error feedback filter the possibility of spectrally shaping the effect of quantization noise in the frequency where it is less harmful from a user point of view . Accordingly , it is convenient to use a frequency weighted error criterion , via an error frequency weighting filter , and to focus on the frequency weighted see discussion in and . For the sake of generality , we consider the possible use of a clipper before . This device the value of the input signal so that if , and if , where is the saturation threshold of the clipper which is helpful in reducing limit cycle idle in an with high order , as in . On the other hand , if we chose to be sufficiently large , then , and the clipper no effect on the system . If the of and the spectral of the input signal are known , then the design of an converter that the variance of to choosing the It is often desirable that a converter is transparent to the system in which it is inserted . This to the widespread paradigm in which the scheme to the application that it , without need to modify the latter . A transparent converter is one whose signal transfer function i . e ., the transfer function from input to output is unity at the of interest . The design of such perfect reconstruction feedback the main topic of the present work . are by the property that , in the absence of quantization effects , there is no frequency weighted reconstruction error , i . e .,. If we denote the power spectral density of , then it can be seen from Fig . that the latter if and only Thus , in the design of an optimal converter , only two of freedom are available : the or , alternatively To the best of our knowledge , on optimal filter design for either consider finite order , , , assume or require that the variance of the signal is much smaller than that of , , , , or have a heuristic component in the optimization , , , . available for the optimal performance and corresponding filter frequency of a converter are those given in . However , the assumption of negligible fed back quantization in these suboptimal . Indeed , as we will show in the sequel , there exist where the in yield large fed back quantization error , even when a fine step scalar is used . In these , not only is the main assumption in , but also an much than can result due to excessive overload see , e .., and . In the present paper , we will show how to design optimal . For this purpose , as in , , and , we model the scalar as a linear device that additive white noise whose variance is proportional to that of the signal being . A key departure from , however , is that we explicitly take into account fed back quantization noise in the feedback loop . Our main are : i We derive one parameter that relate the minimum achievable frequency weighted to the signal to noise ratio of ; We show , within our model , that the frequency weighted in an optimal where the of is fixed exponentially with ratio ; and We derive that characterize the optimal for a . Our can be applied to any given number of quantization , and to almost arbitrary input spectra and frequency weighting criteria . The remainder of this paper is organized as : In Section , we present our analysis model for . In Section , we formulate the associated optimization problem . Section a one parameter characterization of the solution . In discuss the main of an . The case of is in Section . Section the relationship to previous and the importance of taking account of fed back quantization noise . Section simulation . Section . For ease of exposition , all of our are included in the Appendix . We write as a short hand expression for if and only if . The of all complex valued square integrable and absolutely integrable on are by and , respectively . Given we adopt the standard inner product , where complex conjugation . We denote the corresponding transform . If is a transfer function , then we use the short hand notation to refer to the associated frequency response . If is a set , then we write a . e . on almost everywhere on for everywhere on , except on a zero measure subset of . We use to denote the variance of a given wide sense stationary ... random process We recall that if zero mean , then where is any given function and any arbitrary and positive bounded value . For later use , we also recall the following definition . In this section , we discuss some of the main of feedback quantization . We also describe the analysis model and the to be considered later in the search for the optimal . We begin by the that describe the behavior of the shown in Fig . . Quantization and Clipping : From Fig . , the quantization error is given by such that , if , then is said to be . When the is not , then is only granular quantization error , namely , which can be bounded as for some see , e .., . For example , if is a symmetric , uniform , with and quantization interval , then one needs in order to obtain . As outlined in the introduction , the clipper in Fig . can be used to keep from . For simplicity , we will only consider here two , namely , that , or else . The former choice that does not overload , since clipping error , defined as clipping is that , unlike overload , clipping are not fed back into through . This to avoid large limit cycle from the overload of , see . Since such are not part of the analysis model we will use , their occurrence could increase the significantly above the value by the model . which that from by the sum of the quantization and clipping . Notice are require no on the involved . From b one can see that to the signal transfer function , from to , of the converter . Similarly , the product is the transfer function for quantization , usually to as the noise transfer function of the converter . The term will play a crucial role in the derivation of the optimal Output stable for any input sequence satisfying and , thus , all the other in the converter are bounded . On the other hand , if , then can be written as If the a finite number of quantization , then is bounded . If is stable and is minimum phase , then it from that is bounded . This , in turn , that and all the other in the converter are bounded see and if all the in Fig . are stable , and if no on or outside the unit circle , then the resulting is stable . In addition , if and are stable , then the norm of their impulse , namely and , are bounded . Therefore , for a uniform with quantization interval , it to have or more quantization in order to avoid clipping or overload . Input Spectrum and Frequency Weighting : The error weighting filter in Fig . the impact that reconstruction have at each frequency . This performance assessment filter is application dependent , and is assumed to be stable and given . The input signal is a zero mean ... stochastic process with known and finite power , i . e . In order to simplify our subsequent analysis , we shall further restrict and to satisfy the following : Assumption : The product is a piece wise differentiable function at most a finite number of and satisfying In addition , is such that one of the following . such that . Furthermore , if the set of noncontiguous and nonoverlapping in such that We note that the above is a rather weak constraint , since i and include almost any product of practical or theoretical interest . In particular , condition i all the where the product no on the unit circle . In turn , condition is satisfied if is zero over any interval on nonzero measure , or if is rational and on the unit circle . The : We shall focus our analysis on the effect that granular quantization have on the . For this effect to closely represent the actual , we need to assume the following : Assumption : The of overload and clipping are negligible , i . e . In addition , and as stated in the introduction , we will adopt an additive white noise model for . This model is widely used for the analysis and design of data see , e .., , , , and , being usually as . Assumption : The sequence of quantization noise is a zero mean ... random process , The above additive white noise model , although not exact , is , in general a good approximation when a signal with a smooth probability density function is with many and negligible overload in the sense of Assumption , see , e .., . The model can be made exact , even for few quantization , by a uniform scalar with either subtractive or dither , provided overload does not occur , see . As before , one way to achieve this is to use a with a sufficiently large number of quantization , so as to satisfy . In this case , if the quantization interval is and the dither sequence , uncorrelated to when is not and is bounded as , then any number of greater than or equal to will make Assumption hold exactly . If a smaller number of quantization are employed so that , then the use of dither with the same as before , together with clipping i . e ., setting , will also make satisfy Assumption exactly . on through the feedback path . However , if the scalar a finite and fixed number of quantization , then another link between these two needs to be considered . In order to model this relationship , we will use the fixed model employed in , e .., , , , , and . Assumption : For a fixed number of quantization , the variance of quantization is proportional to the variance of the signal being , i . e ., there such that If no clipping is used i . e ., if , then exactly to the of . If , then is a good a approximation of the of when b in Assumption . In our model , is assumed fixed and given . Strictly speaking , on the of , on the number of quantization of , and on how quantization and are distributed along the dynamic range of . In practice , for a given number of quantization , should be chosen such that the dynamic range of is used efficiently , whilst en Here and in the sequel , we assume the dither is such white and uncorrelated not . a low probability of overload or clipping . For example , for the often uniform with loading factor equal to we obtain assuming that a uniform and overload . We note that for large , and provided overload are negligible , a quadratic relationship between and for most of scalar see , e .., . This is indeed the well known rule of reduction of quantization noise variance per additional bit of resolution . In the sequel , we refer to the model of determined by , , and as The Linear Model the Linear Model is exact if the a enough quantization to avoid overload . If not enough quantization are available and dither is used jointly with clipping , then the model is exact in the effects of granular quantization , and is a good approximation in the total if Assumption also . If the scalar is , a small quantization interval relative to and enough quantization to avoid overload , then the Linear Model can be to yield a good approximation of the total . Perhaps surprisingly , the Linear Model turns out to predict with remarkable accuracy the of an optimal when few quantization and clipping are used with a loading factor big enough to satisfy Assumption , even without dither , and even for a bit . This can be from the simulation in Section . We shall restrict the search for the optimal to those satisfying the following constraint . As in Section I , the first constraint perfect reconstruction . As in Section A , the stability on are a necessary condition for the converter to be stable . The additional requirement on , namely strict causality , is for the feedback loop in Fig . to be well defined see , e .., . Notice that we will not a require to have only inside the open unit disk . Instead , we will show that the latter property naturally from the solution of the design optimization problem . An additional constraint on from the value of , as next . The ratio between the of and One can see from the above that if , then any filter or scaling of the quantization of will yield , thus , making large overload or clipping inevitable . This would and , if no clipping is used , may lead to large limit cycle . We , thus , conclude that the use of feedback the following constraint . Constraint : If the above constraint is met , then can be found by substituting into . This Given the model in the previous section , we can now evaluate the quantity that we aim to minimize , namely , the frequency weighted mean squared error . From c , and and , it that the is given by of the in the Linear Model can be stated as . Optimization Problem : For given , and for given and satisfying Assumption , find the frequency , and satisfying and that minimize The following proposition us to further reduce the number of in by the optimal the following change of : proof of Proposition in the Appendix , Constraint is satisfied . In addition , a stable and strictly causal i . e ., one satisfying Constraint always to a function , see , which This result directly from formula see also the Bode Integral Theorem in , e .., . On the other hand , as we shall see in Section , if Assumption , then the optimal within the set of by and the requirement turns out to be piece wise differentiable on , at most a finite number of discontinuity , and Under these , it is always possible to find a stable and strictly causal filter such that arbitrarily well on , as stated in the following lemma that it at most a finite number of discontinuity and that it . Then , for every , there Optimization Problem : For given and known and for satisfying Assumption , find the optimal feedback filter , say , via see also Lemma . In the following section , we will show how to solve this optimization problem . It would be desirable to provide an explicit analytical solution to Optimization Problem . Unfortunately , and as will become apparent in the discussion later , a closed form solution , for arbitrary , infeasible . Nevertheless , we can provide a one parameter characterization of the optimal function in as . Theorem : For any given satisfying Assumption , and for any , the function in to the one parameter family of , where Here is the lower bound of feasible , and , if it , is the unique scalar such that Note that the above result an explicit analytic expression for , once the optimal , defined as Theorem can be used to develop an efficient algorithm to solve Optimization Problem . The key point is that substitution of a into the search space from the infinite dimensional set to the real interval . More precisely , Optimization Problem is turned into the simpler problem of finding the minimizer of the single variable scalar function , and hence the solution of Optimization Problem is unique . Furthermore , can be by finding the root of a scalar , convex , and monotonically decreasing function . Moreover , it from and that , for any satisfying Assumption , and for any , the global minimizer of and is unique . In addition , these guarantee that can be easily found by , via , for example , the bisection algorithm , or any other convex optimization method . We can now express and the minimum achievable , namely , in of , and . Indeed , combining and a with after some algebraic simplification that It can be seen from a that is a monotonically increasing function of . In view of Theorem , this that , as , is monotonically decreasing with increasing . As a consequence , the converse of Optimization Problem , namely , finding the optimal and minimum of for a given target distortion , can be by and . Moreover , since the of a is a concave , monotonically increasing function of , this parameter can be easily found by standard iterative , as in the original optimization problem . It is also interesting to note that and a , which relate and via the parameter , have a structure akin to the well known reverse water filling see , e .., , and . The latter characterize the rate distortion function for . To summarize , we have given an explicit analytic expression for the optimal and , once been determined . Furthermore , we have shown that the parameter always , is unique , and can be easily found simple numerical . In the following , we will provide additional insight into the of , as well as into some of optimal , In the sequel , we say that a is optimal or if its , satisfy for negligibly small of and , and is such that , a . e . on , with as defined by . It from and that , for any given satisfying Assumption , in a the family of all noise shaping that are optimal for some . As we will show , from to equivalently , from to one to undergo a smooth progression from full noise shaping to no noise shaping , in an optimal manner . An example of this progression is shown in Fig . . Note in this figure how solid a unit transfer function as the for which in the figure , becomes smaller and . It can also be that the inverse of is . Such asymptotic convergence does indeed take place in general , as the following theorem : Theorem : For any satisfying Assumption , the defined in a converge uniformly to as . Similarly , for any function satisfying condition i in Assumption , the defined in a converge uniformly to , which the to a converter . In view of , this no noise shaping scenario is asymptotically optimal as . In turn , defined in to the full whitening feedback in , , . From and , is optimal . See also the discussion in Section . The Output of the : By looking at Fig . and Assumption , we find that the of in an is given by and , we conclude that the variance of the quantization noise in an is given by a been used . Substitution of a and into this expression to , the output of the in an is white . This that near optimal of the output can be with a memory less entropy coder . The Frequency Weighted Reconstruction Error : The of the frequency weighted reconstruction error is given by of into the above . Thus , we conclude that the frequency weighted quantization error in an is not white . This fact in stark contrast to the when the are without the perfect reconstruction constraint , see , e .., . It also from the result when the feedback filter is fed back quantization error , as in and . Note that , as is made , not only becomes smaller , but its asymptotically a constant function over the It is well known that i . e ., sampling a continuous time signal at a frequency above its rate one to smaller error for a given , fixed number of quantization . For instance , the of simple scalar quantization without feedback is known to decrease as , see , where is the ratio , given by the order of the feedback filter see also recent work in . From a rate distortion viewpoint , the inversely polynomial error decay of this error estimate is too slow to compensate for the increase in the overall bit rate due to which is proportional to . To be more precise , let us consider a scalar with quantization , where the quantization resolution in per sample . If the additional by was instead to increase , then the would decay as , i . e ., exponentially . A faster decay of the of with can be by a different feedback filter with possibly different order for each ratio . An example of such a family of bit was given in . Here , the continuous time reconstruction error can be uniformly bounded by is independent of . This bound an that with as , which is faster than any inverse polynomial , but still far from Strictly speaking , this only for whose have finite support . Indeed , it been shown that for several infinite support , the of uniform quantization asymptotically faster than , where a is a constant independent of , see . exponential . Based on this result , the family of bit in achieve an that is , i . e ., increasing . Notably , the in and were an exact , deterministic model of quantization . We will next show that , within the Linear Model , if the optimal infinite order in Section are used for each value of , then one can achieve an exponential decay of with the ratio , provided is kept constant . If the input sequence is from sampling a band limited signal , would cause defined in to vary with . To capture this effect , we replace In , the square root of the of the frequency weighted input without , and . Notice that , that is , the total power of in of variance per sample , remains constant for all . This a uniform comparison basis for the distortion . to the output of . Interestingly , it is possible to establish a precise exchange formula for and . Indeed , in of minimal achievable distortion , the effect of increasing equivalent to an exponential increase in the output of . This is shown in the next theorem : Theorem : Under the Linear Model in Section , for any function , and for any , the minimum achievable : If we assume that exponentially on the number of per sample , then Theorem an that exponentially with , provided the Linear Model and that optimal , and by , a and are employed for each . The following simple example this idea : is constant , without . For this setup , the optimal for our model of is , i . e ., a converter . From , the minimum without i . e ., with becomes where . To analyze behavior of in this case , we apply Theorem to the above expression . for all . Note that , to achieve , needs to be according to b and . Therefore , for this example , the of an with fixed an exponential decay with the ratio since , by definition being a uniform with many and operating with a loading factor of , then becomes posing that and hold , we obtain from that is lower and upper bounded by proportional to . For loading factor of , , and , the exponent in the latter expression to , respectively . The next theorem that the exponential decay of the in the example above can be extended to arbitrary band limited input and frequency weighting criteria . Thus , under the Linear Model , we have that the of an exponentially with . Remark : Model in Section . Here it is convenient to present some further regarding the validity of that model when the ratio to infinity , for different of a . As already in Section , if is bounded and a sufficiently large number of quantization to avoid overload is used together with dither , then the Linear Model is exact . Nevertheless , there is no guarantee that the number of necessary quantization to avoid overload remains constant as . If such number with , then can only be kept constant by increasing the number of quantization in the . If the number of quantization is insufficient to avoid clipping overload , and if dither and clipping are used with a fixed loading factor , then there a certain finite value of beyond which Assumption is . This from the fact that , for any fixed loading factor , the effect of clipping in the output does not decay with , thus , becoming the dominant component in the for sufficiently high . Further reduction of the would then require one to balance clipping and granular quantization by increasing the loading factor . If the number of quantization is fixed , this would necessarily reduce the value of , clearly increasing the component of the due to granular quantization . Nevertheless , if clipping and dither are used with , then the Linear Model and Theorem is exact in the due to granular quantization . If one tried to optimize the of a fed back quantization noise , i . e ., by trying to minimize Theorem . This to the result in , the noise transfer function magnitude is also equivalent to that derived in . The latter is optimal in the sense of the ratio , but not in the sense of for a fixed . As shown in Theorem in general , does approach as . One can then expect to be near optimal in where , see . The latter is often satisfied at high bit i . e ., when many quantization are available . However , for any given number of quantization , it is easy to find practical where is such that is comparable to or greater than . More precisely , from , and that see Appendix , one can show that , if over a set of in with measure , where is some positive whose magnitude becomes significantly small in relative over certain frequency . An example is included in Section . A direct consequence is that , for these , and in view of , trying to match to will yield a performance far from optimal , also increasing the risk of large limit cycle if no clipping is employed see , e .., and . was already in . Several heuristic have been since then see , e .., , , , , , and . In contrast to these , the method derived in the present paper one to characterize the true optimal , by explicitly taking into account in the cost functional to be see . Our method not only that , but also the actual optimal . Our proposal also the advantage of being applicable to arbitrary input spectra and frequency weighting , regardless of how small the To illustrate our , we have designed the of a at digitally audio in a as well as the of both the and the numerical are given later . The of audio was as unit variance zero mean white noise through of the frequency response of is in Fig . solid line . The frequency weighting filter considered had a frequency response magnitude which the curve derived in , Table , thus , modeling the sensitivity of human hearing to noise . The corresponding frequency response is plotted with dotted line in Fig . the sampling frequency is . . The resulting for these is also shown in the same figure dashed line . For this choice of , and in view of , one could expect the norm of a full whitening feedback filter to be very large . This is indeed the case : . Thus , the suboptimal feedback filter by the use of a scalar with at least in order to become feasible see Constraint . In the , was chosen to be a uniform mid rise with quantization interval . Several were considered for the , calculated as the loading factor . Two different loading were considered : and . The latter choice a slightly lower than the usual loading factor of . However , this regime the benefit of making overload smaller and more infrequent . As the simulation will show , for our of and , this more conservative loading factor lower Fig . . Frequency response for solid line , dotted line and e e dashed line . For each and corresponding two for , one for each loading factor , the of the converter were designed according to the following : These were then with rational transfer , of order and An appropriate value for the parameter in was chosen via , see , assuming For each combination of and , the resulting converter was two different . , with virtually infinitely many . Thus , for all neither clipping nor overload er and Clipped : Here which a scalar with a finite input dynamic range . As a consequence , any value would overload if or produce clipping error if . To avoid large limit cycle , this variant was clipping i . e ., Each simulation with the comprised , . For the converter , five , were for each combination of The of the numerical and the are next . Comparison Between and the Rate Distortion Function : The information theoretic lower bound see for the associated with the given source and filter is plotted in Fig . solid line . This to quadratic frequency weighted Distortion Rate function when . As the bit rate is , the gap between and this absolute lower bound to approximately . for and for , at . This difference can be to the rate distortion inefficiency of the uniform scalar . On the other hand , the performance gap at lower bit can be to the perfect reconstruction constraint . Recall that , at low bit , the achievement of rate distortion function the suppression of relatively less significant of the of the input signal see , e .., and . This linear distortion , which a cannot achieve , is more severe at lower bit . Thus , the performance gap as is reduced . : The of this converter variant is in four of the in Fig . , with beginning with opt . These differ in the loading factor , and in the meaning of in each case . For the whose do not have the ending E .. entropy , is simply the number to generate the value for which the were . The whose end in E .. correspond to the same , but for each point the value of is the scalar entropy of the output of the converter . It can be seen in Fig . that the for the without entropy is remarkably close to the theoretical value by a . More importantly , even for bit as small as , each ratio from its nominal value of by less than . For the extreme situation , the was slightly lower than , while was higher than due to the highly nonuniform of the resulting sequence . It can also be seen that the scalar entropy of the output of the in these is very close to function for a given distortion . This with the observation that the output of in an is white , see the comment at the end of Section . The difference between these is bigger for lower of , for the same reason in Section . : For the an of , the along with the corresponding that range of bit . This performance degradation can be to clipping . The fact that overload become noticeable only for high bit many quantization might seem , at first , surprising . However , this phenomenon can be easily by that the size of the of the of that fall outside the dynamic range of remains approximately constant in relation to for all . This is a direct consequence of the loading factor rule . In contrast , granular quantization error is proportional to which is constant in the . Therefore , the ratio between clipping and granular quantization approximately as and clipping become dominant for sufficiently high bit . Because of the reduced occurrence and magnitude of clipping , the with and an smaller than that of its counterpart with . Furthermore , this more conservative loading factor the converter to perform almost exactly as by our analytical expression for . For the chosen input and frequency weighting filter , and calculating as , the value of with as shown in Fig . dotted line . As seen in this figure , the gap between and , for each value of , smaller as the bit rate . This with the fact that the optimal a converter as , see Section A . It can also be seen in Fig . that the with and an improvement of over at . Equivalently , in order to obtain the same as that of at , the converter with less than . At lower bit , the improvement of the optimal is also significant . For example , the with a lower than the converter with , thus , a data rate compression of see Fig . . This paper studied perfect reconstruction feedback based on an additive white noise model for quantization . We have derived that relate the minimum frequency weighted and the of the scalar in the converter . We have also provided closed form for the optimal frequency of the in the converter and have derived several of optimal . In particular , we have shown that the optimal frequency response of the are unique , that the frequency weighted of an optimal are nonwhite , and that consecutive of the output sequence of the scalar are uncorrelated . We have also shown that , within our model , exponentially with ratio . The following preliminary are necessary to prove the stated in the previous . We begin by the following definition . We say that two are similarly functionally related there a monotonically increasing function such that , for all , and write . Similarly , if there a monotonically decreasing function such that , for all If and are oppositely functionally related , then the inequality in is reversed . In either case , equality is and therefore is almost constant . Proof : We will examine the difference between the and in . We obtain We now proceed to upper bound the last term in the above inequality . From and , we have that where the last inequality from and . Substitution of into Since is bounded , and from b , it from that for any , one can always choose sufficiently large Denote the squared norm of see via , and define the set of all the the same norm as It is easy to show that must belong to . From this , and since , it that The problem by within the category of isoperimetrical , well known in variational calculus see , e .., and . The standard solution of these is based upon the fact that any that see needs to satisfy We note that for the trivial case in which is almost constant see Definition , is also almost constant . this to constraint i in that , for this case , is such that . Thus , the remainder of the proof only the in which is not almost constant . In order to find , we will next discard the possible of which do not correspond to global of in . The unique function , which is with and in , will characterize the solution of Optimization Problem . The Case : Fore this case , substitution of into that needs to satisfy so that can be explicitly from . Note that cannot be zero in the above expression , otherwise would be undefined . From this , the feasible sign for , the sign before the square root , and in are Option a : We show next that any solution by option a in , say , a greater by Theorem to the numerator of , together with and the fact that . Both are strict since is not almost constant see Theorem and Definition . On the other hand . From the above , it that for all non Option : The candidate are now by and only . a to and , these take the form Combining this result with , and considering to be not almost constant , we obtain response magnitude in the absence of fed back quantization noise recall . This is not surprising , since taking to removing constraint i which the Since the , are continuously differentiable , so is . We , therefore , have that if , then we obtain . This would imply for all such that . Thus , belong to , the integral of over the needs to be infinite . Since ; x , this that infeasible and . We will first elaborate upon to derive . Then we will prove that . where and are as defined in . Application of the identity , which from and a to the numerator on the of The Sign of : Since , this limit needs to be for two possible , depending on whether or not is positive . must necessarily hold in order to obtain . Thus , and its first are continuous . Therefore , in view of , we get , it is clear from that there a value for greater than under which is small enough to render negative . Therefore : In order to show that the and in Theorem hold , we write as We will first prove the validity of . Clearly , if for all condition i of Assumption , then the of the above equation to as . If this the case , then the second condition of Assumption must be satisfied , and therefore the of In order to show that i . e ., , we first note from that for all . On the . Proof of Theorem c to one can write Since is monotonically decreasing see Theorem to the minimum for a constant , by virtue of we have that . Substitution of this into the last inequality . ﻿ We improve the achievable rate for causal and for zero delay source of stationary under an average mean squared error distortion measure . To begin with , we find a closed form expression for the information theoretic causal rate distortion function under such distortion measure , by , for first order Gauss . is a lower bound to the optimal performance theoretically attainable by any causal source code , namely . We show that , for , the latter can also be upper bounded as sample . In order to analyze for arbitrary zero mean stationary , we introduce , the information theoretic causal when the reconstruction error is jointly stationary with the source . Based upon , we derive three closed form upper to the additive rate loss defined as , where . Two of these are strictly smaller than . sample at all . These differ from one another in their tightness and ease of evaluation ; the the bound , the more involved its evaluation . We then show that , for any source spectral density and any positive distortion , can be by an additive white noise channel surrounded by a unique set of causal , post , and feedback . We show that finding such a convex optimization problem . In order to solve the latter , we propose an iterative optimization procedure that the optimal and is to converge to . Finally , by a connection to feedback quantization , we design a causal and a zero delay scheme which , for , an sample , respectively . This that the among all zero delay source , by , is upper bounded as sample . Causality , convex optimization , differential modulation , entropy quantization , noise shaping , rate distortion theory , sequential . zero delay source , the reconstruction of each input sample must take place at the same time instant the corresponding input sample been . Zero delay source is desirable in many , e .., in real time where one cannot afford to have large , or in feedback , in which the current input on the previous . A notion closely related to the principle behind zero delay is that of causal source , wherein the reproduction of the present source sample only on the present and past source but not on the future source , . This notion does not preclude the use of entropy , and thus , it does not guarantee zero delay reconstruction . Nevertheless , any zero delay source code must also be causal . It is known that , in general , causal cannot achieve the rate distortion function of the source , which is the optimal performance theoretically attainable in the absence of causality . However , it is in general not known how close to one can get when attention to the class of causal or zero delay source , except , for causal , when dealing with memory less , stationary at high resolution , or first order Gauss under a per sample mean squared error distortion metric . For the case of memory less , it was shown by and Gilbert that the optimum rate distortion performance of causal source , say , is by time at most two memory less scalar by entropy . In this case , the rate loss due to causality was shown to be given by the space filling loss of the , i . e ., the loss is at most sample . For the case of stationary with memory and distortion , and that the information theoretic causal , here by to be defined formally in Section and which , to as the distortion goes to zero , . The possible gap between the of causal source and this information theoretic causal was not assessed . Since operational data are lower bounded by the mutual information between the source and its reconstruction , we also have that . On the other hand , for arbitrary stationary with finite differential entropy and under high resolution , it was shown in that the rate loss of causal i . e ., the difference between their and is at most the space filling loss of a uniform scalar . With the exception of memory less and first order Gauss , the price of causality at general rate for other stationary remains an open problem . However , it is known that for any source , the mutual information across an additive white noise channel and across a scalar entropy quantization channel do not exceed by more than . and . sample , respectively , . This immediately the and . In causal source , it is generally difficult to provide a constructive proof of since random construction , which upon jointly long of source , is not directly applicable even in the case of memory less . Thus , even if one could obtain an outer bound for the achievable region based on an information theoretic , finding the inner bound , i . e ., the , would still remain being a challenge . There exist other related to the information theoretic causal , in which is not . The minimum sum rate necessary to sequentially block encode and block decode two scalar correlated random under a coupled fidelity criterion was studied in . A closed form expression for this minimum rate is given in , Th . for the special case of a squared error distortion measure and a per variable as opposed to a sum or average distortion constraint . In , the minimum rate for causally and source under either a per sample or average distortion was given the name sequential rate distortion function . Under a per sample distortion constraint , it was also shown in ,. that for a first order Gauss source , a zero mean white process with variance , the information theoretic the form for all . No are known for for higher order Gauss . Also , with the exception of memory less with its average distortion constraint than a per sample constraint , not been . In this paper , we improve the inner and outer rate distortion for causal and for zero delay source of zero mean stationary and average distortion . We start by showing that , for any zero mean source with bounded differential entropy rate , the causal by less than approximately . sample . Then , we revisit the problem for first order Gauss under a per sample distortion constraint schedule and find the explicit expression for the corresponding by of an alternative , constructive derivation . This expression , which turns out to differ from the one found in , bottom of . , us to show that for first order Gauss , the information theoretic causal for an average as opposed to per sample distortion measure with . In order to upper bound for general stationary , we introduce the information theoretic causal when the distortion is jointly stationary with the source and denote it by . We then derive three closed form upper bounding to the rate loss , which can be applied to any stationary random process . Two of these are , at all , strictly than the best previously known general bound of . sample . Since , by definition we have that . As we shall see , equality would hold in if could be by a test channel with distortion jointly stationary with the source , which a reasonable conjecture for stationary . We do not provide a closed form expression for except for first order Gauss , and thus the upper bound on the right hand side of the bound in this paper is not analytically for the general case . However , we propose an iterative procedure that can be numerically and which one to evaluate , for any source power spectral density and , with any desired accuracy . This procedure is based upon the iterative optimization of causal , post , and feedback around an channel . A key result in this paper and its second main contribution is showing that such filter optimization problem is convex in the frequency of all the . This that the mutual information rate between source and reconstruction by our iterative procedure monotonically to as the number of and the order of the tend to infinity . This equivalence between the solution to a convex filter design optimization problem and the troublesome minimization over mutual , thus making it possible to actually compute in practice , for general stationary . We then make the link between and the of causal and zero delay . More precisely , when the channel is by a uniform scalar by memory less entropy , the with the iterative procedure yield a causal source system whose operational rate is below sample . If the entropy coder in this system is restricted to encode individually as opposed to long of them , then this system zero delay operation with an operational rate sample . This directly into an upper bound to the of zero delay source , namely . To illustrate our , we present an example for a zero mean AR and a zero mean AR source , for which we evaluate the closed form and obtain an approximation of numerically by the iterative procedure herein . This paper is organized as . In Section , we review some preliminary . We prove in Section that the for does not exceed the information theoretic by more than approximately . sample . Section the derivation of a closed form expression for for first order Gauss . In formally introduce and derive the three closed form upper bounding for the information theoretic rate loss of causality . Section the iterative procedure to calculate , after the proof of convexity that its convergence . The two are provided in Section . Finally , Section . Most of the of our are given in . denote , respectively , the set of real and the set of real . and denote , respectively , the of and positive . We use lower case , such as , to denote scalar random , and and to denote and matrices , respectively . We use and to denote the , the column span , and the null space of the matrix , respectively . The expectation operator is by . The notation to the variance of . The notation a one sided random process , which may also be written simply as . We write to refer to the sequence . The of a wide sense stationary process is by . Notice that . For any two we write the standard squared norm and inner product as and , respectively , where complex conjugation . For one sided random and , the term the mutual information rate between and , provided the limit . Similarly , for a stationary random process A source pair a source into binary , from which a reconstruction is . The end to end effect of any pair can be by a series of reproduction , such that , for every where we write as a short notation for . Following , we say that an pair is causal if and only if it the following definition . Definition Causal Source Coder : An pair is said to be causal if and only if its reproduction are such that It also from Definition that an pair is causal if and only if the following chain for every possible random input process : It is worth that if the are random , then this equivalent causality constraint must require that is satisfied for each realization of the be the total number of that the received when it the output subsequence . Define as the random binary sequence that the that the received when is . Notice that is , in general , a function of all source , since the binary may be , i . e ., may be only after the received enough to reproduce , with . We highlight the fact that even though may contain which depend on with , the random may still satisfy , i . e ., the pair can still be causal . Notice also a random variable , which on , the , and on the manner in which the source is into the binary sequence sent to the . For further analysis , we define the average operational rate of an pair as In the sequel , we focus only on the as the distortion measure . Accordingly , we define the average distortion associated with an pair as The allow us to define the operational causal as : We note that the operational causal defined previously to the of all causal . where the last inequality turns into equality for a causal pair , since in that case . Thus , combining , , and This lower bound the study of an information theoretic causal , as defined in the following . Definition : The information theoretic causal for a source , with respect to the average distortion measure , is defined as The definition is a special case of the nonanticipative epsilon entropy by and , which was shown to converge to , for stationary and in the limit as the rate goes to infinity , In the case , it is known that for any source and for any single letter distortion measure , the the information theoretic . Unfortunately , such a strong equivalence between the and the information theoretic . One exception is if one is to jointly and causally encode an asymptotically large number of parallel , of causal . Nevertheless , as outlined in Section I , it is possible to obtain lower and upper to the of causal from . Indeed , and to begin with , since , it directly from and that The previous inequality in is strict , in general , and becomes equality when the source is white or when the rate to infinity . Also , as it will be shown in Section , for , does not exceed by more than For completeness , and for future reference , we recall that for any distortion , the for a stationary source with is equal to the associated , given by the reverse water filling a Although , in general , it is not known by how much , for stationary one can readily More generally , it is known that , for any source , the mutual information across an channel which noise with variance , Until now it been an open question whether a bound than can be for with memory and at general rate . In , we show that for , this is indeed the case . But before on upper for , its operational importance will be established by showing in the following section that , for , the does not exceed by more than approximately . sample . and , an upper bound to can be readily from by approximately . per sample to . This result is first formally stated and proved for finite of any source . Then , it is extended to stationary . We start with two . Definition : The causal information theoretic for a zero mean random vector of length is defined as where the is taken over all output satisfying the causality constraint Definition : The operational causal for a zero mean random vector of length is defined as Lemma : Let be two random with zero mean and the same covariance matrix , i . e . and the same cross covariance matrix with respect to , that is If If furthermore , then equality is in if and only if with and being jointly . Notice that if one Lemma to a reconstruction error with which the output sequence the causality constraint , then the version of the same reconstruction error will also produce an output causally related with the input . To see this , let be the factorization of the covariance matrix of the random vector where is a lower triangular matrix . This one to write as where satisfy . Then , there a set of reproduction satisfying the of Definition which generate each partial vector . Specifically , for any given , there a function , such that . From and given that and thus , for some function . From this and the fact that is independent of , we have that where probabilistic independence . On the other hand , for each let be the minimum mean square error linear estimator of given . Then , the notation for the left corner of a matrix , we have that where from and all the subsequent stem from and from the fact that is lower triangular . Therefore , since the residual is uncorrelated to , it that The result stated in Lemma for random vector is extended to stationary in the following theorem the second main result of this section . The fact that for one to find upper to the of causal by explicitly finding or upper bounding . This is accomplished in the following . In this section , we will find when the source is a Gauss process . More precisely , we will show that the information theoretic causal , which is associated with an average distortion constraint , with the expression for the on the of in for a per sample distortion constraint . To do so , and to provide also a constructive method of realizing the as well as , we will start by an alternative derivation of the for scalar source of length . In this case , from its definition in . . ,. , the the following form : where the is over all conditional of given satisfying the causality constraint and the distortion schedule Before proceeding , it will be convenient to introduce some . , to denote the random column vector and adopt the shorter notation . For any two random , It was already stated in Lemma that the reconstruction vector which mutual information between a source vector and for any given distortion constraint , must be jointly with the source . This , in particular , for a realization of the with distortion schedule . In the next theorem , we will obtain an explicit expression for this and prove that in its realization , the sample Fig . . Recursive Procedure at its th iteration . Starting from known covariance matrices , their next partial and are found . The indicate the step in the algorithm which the corresponding part of the matrix . step is responsible of revealing the partial and by number in the figure . The are formally stated in the following theorem , which also an exact expression for the of first order Gauss . the . Under the latter interpretation , nothing one from choosing an arbitrarily large value for , say , yielding an arbitrarily large value for the second term in the summation on the of , which is , of course , inadequate . We are now in a position to find the expression for for first order Gauss . This is done in the following theorem , whose proof is in Section . The technique applied to prove and does not seem to be extensible to Gauss of order greater than . In the sequel , we will find upper to for arbitrary any order stationary . In order to upper bound the difference between and for arbitrary stationary , we will start this section by an upper bounding function for , by . then derive three closed form upper bounding to the rate loss , applicable to any stationary process . Two of these are Definition Causal Stationary : For a stationary , the information theoretic causal Next , we derive three closed form upper bounding to that are applicable to arbitrary zero mean stationary with finite differential entropy rate . This result is stated in the following theorem , proved in Section : Theorem : Let be a zero mean stationary source with with bounded differential entropy rate and variance . Let denote for given by , and let denote the quadratic for source uncorrelated for the source defined in . Let denote the information theoretic causal see Definition . Then , for all Notice that is independent of , being therefore numerically simpler to evaluate than the other bounding in Theorem . However , as is away from and , becomes very loose . In fact , it can be seen from a that for , the gap between and is actually upper bounded by , which is of course than , but one to evaluate . It is easy to see that time between two causal an output process which causality with a rate distortion pair corresponding to the linear combination of Thus , in some , one could get a bound than by considering the boundary of the convex hull of the region above and then . However , such bound would be much more involved to compute , since it to evaluate not only , but also the already convex hull . It is also worth that the first term within the on the of becomes smaller when is reduced . This difference , which from inequality is Fig . . channel within a perfect reconstruction system by the causal filter . always , could be taken as a measure of the of the of especially when . Indeed , as a white process , to zero . It can be seen from that the upper bound for the information theoretic among all so far . Although it does not seem to be feasible to obtain a closed form expression for , we show in the next section how to get arbitrarily close to it . In this section , we present an iterative procedure that one to calculate with arbitrary accuracy , for any . In addition , we will see that this procedure a characterization of the in a feedback that which sample . To derive the previously , we will work on a scheme of an channel and a set of causal , as in Fig . . In this scheme , the source is and stationary , with , and is assumed to is a zero mean process with i . i . independent of . Thus , between and the channel . The filter is stable and strictly causal , i . e ., it at least a one sample delay . The are causal and stable . The idea , to be in the remainder of this section , is to first show that with the that minimize the variance of the reconstruction error for a fixed ratio , the system of Fig . a mutual information rate between source and reconstruction equal to , with a reconstruction equal to . We will then show that finding such is a convex optimization problem , which naturally an iterative procedure to solve it . In order to analyze the system in Fig . , and for notational convenience , we define The perfect reconstruction condition a division , convenient of the optimization problem associated with it . On the one hand , because of , the net effect of the channel and the and is to introduce colored stationary additive noise , namely , independent of the source . The of this noise is given by The diagram in Fig . how the signal transfer function and the noise transfer function act upon and to yield the output process . On the other hand , by looking at Fig . , one can see that also the role of a filter , which can be to reduce additive noise at the expense of linear upon the stationary corrupted by additive stationary noise On the of , the first term is the variance of the additive , source independent , noise . The second term to the error due to linear distortion , that is , from the deviation of from a unit gain . Since we will be interested in , for any given and , the and in Fig . are chosen so as to minimize in , while still satisfying . From the viewpoint of the subsystem comprised of the and and the channel , as an error frequency weighting filter , see . Thus , for any and , the that minimize are those in , Prop . , by setting in b equal to . With the minimizer in , the variance of the source independent error term is given by On the other hand , the filter needs to be strictly causal and stable . As a consequence , it that which from formula see also the Bode Integral Theorem in . Thus , from and , if one to minimize the reconstruction by choosing appropriate causal in the where the space of all frequency that can be with causal . From the lemma , whose proof can be found in Section , one can find either by the minimization in Definition or by Optimization Problem . In the following , we will pursue the latter approach . As we shall see , our formulation of Optimization Problem a convenient of its decision . In fact , it defined in a with respect to the set of all causal frequency involved . That result can be directly from the following key lemma , proved in Section : Proof : With the change of in , we obtain , see . With this , Optimization Problem to finding the and that Clearly , the space of frequency associated with causal transfer is a convex set . This that is a convex set . In addition , is also a convex set , and from Lemma , is a convex functional . Therefore , the optimization problem stated in , and thus Lemma and the in Optimization Problem allow one to define an iterative algorithm that , as will be shown later , the information theoretic causal . Such algorithm is in iterative Procedure . Notice that after Step in the first iteration of Procedure , the is comprised of only additive noise independent of the source . Step then the by source independent noise at the expense of linear distortion . Each step the until a local or global minimum of the is . Based upon the Problem , the following theorem , which is the main technical result in this section , convergence to the global minimum of the , say , for a given end to end mutual information . Since all the in Optimization Problem are causal , the mutual information at this global minimum is equal to . Theorem Convergence of Iterative Procedure : Iterative Procedure monotonically to the unique and that realize . More precisely , denote the Indeed , after Step for the first time , the resulting rate is the quadratic for source uncorrelated in see also . Fig . . Uniform scalar and dither forming an , the channel of the system from Fig . . Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . after the th iteration of Iterative Procedure at a target rate , we have that Proof : The result directly from the fact that Optimization Problem is strictly convex in and , which was shown in Lemma , and from Lemma . The theorem that the stationary information theoretic causal can be by Iterative Procedure . In practice , this that an approximation arbitrarily for a given can be if sufficient of the procedure are carried out . The feasibility of running Iterative Procedure on being able to solve each of the minimization sub involved in and . We next show how these can be . If is given , the minimization problem in Step of Iterative Procedure is equivalent to a feedback design problem with the constraint and with error weighting filter . Therefore , the solution to Step is given in closed form by , , and b , where in b is by . The latter Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . in characterize the frequency response of the optimal , and given . The existence of rational transfer and arbitrarily close in an sense to such frequency response is also shown in . Finding the causal frequency response that for a given is equivalent to for a given , where is as defined in . Since are convex , is a convex optimization problem . As such , its global solution can always be found iteratively . In particular , if is constrained to be an th order finite impulse response FIR filter with impulse response , such that the discrete time is a convex functional . The latter directly from the convexity of and the linearity of . As a consequence , one can solve the minimization problem in Step , to any degree of accuracy , by over the of the impulse response of , standard convex optimization see , e .., . This approach also the benefit of being amenable to numerical computation . It is interesting to note that if the order of the filter were not a restricted , then , after Iterative Procedure Wiener filter i . e ., the causal estimator for the noisy signal that comes out of the perfect reconstruction system that . Notice also that one can get the system in Fig . to yield a realization of Iterative Procedure by simply to be . This would yield a system equivalent to the one that was analytically in . An important observation is that one could not obtain a realization such a system in one step by simply a Wiener filter by the causal estimator that is , a causal Wiener filter . To see , and would no longer be optimal for . One would then have to change , and then again , and so on , thus to carry out infinitely many recursive optimization . However , a causally truncated version of the non causal Wiener filter that could be used as an alternative starting guess in Step of the iterative procedure . If the channel in the system of Fig . is by an , as shown in Fig . , then instead of the noise , we will have an i . i .. process independent of , whose are uniformly distributed over the quantization interval . The dither signal , by , is an i . i .. sequence of uniformly distributed random , independent of the source . Let be the output of the . Denote the resulting input and the output to the , before and after the dither , respectively , as and , and let be the quantization noise by the . Notice that the of are independent , both mutually and from the source . However , unlike and , the and are not , since they contain of the uniformly distributed process . We then have the following . Theorem : If the scheme shown in Fig . the by Iterative Procedure , and if long of the output of this system are entropy conditioned to the dither in a memoryless fashion , then an operational rate satisfying , and linear time invariant a reconstruction error jointly stationary with the source , it that the operational rate distortion performance of the feedback thus is within Remark : When the rate goes to infinity , so does . In that limiting case , the transfer function to unity , and it from that the optimal asymptotically satisfy Moreover , when , the system of Fig . which , in this asymptotic regime , with If the requirement of zero delay , which is than that of causality , was to be satisfied , then it would not be possible to apply entropy to long of . This would entail an excess bit rate not greater than bit per sample , see , e .., , Sec . . . Consequently , we have the following result . Theorem : The of zero delay , say , can be upper bounded by the operational rate of the scheme of Fig . when each output value is entropy independently , conditioned to the current dither value . Thus The . sample in , commonly to as the space filling loss of scalar quantization , can be reduced by vector quantization , . Vector quantization could be applied while causality and without delay if the of the source were dimensional . This would also allow for the use of entropy over dimensional of , which the extra bit sample at the end of to sample see , Th . . . . It is worth that Lemma and the an interesting fact : the rate loss due to causality for with memory , i . e ., the difference between the of causal and , is upper bounded by the sum of two . The first term is . sample , and from the space filling loss associated with scalar quantization , as was also pointed out in for the high resolution situation . This term is associated only with the . For a scalar stationary source , such excess rate can only be by jointly of consecutive source vector quantization , i . e ., by for or by several parallel . The second term can be to the reduced of causal , to those of or smoothing . The contribution of the causal filtering aspect to the total rate loss is indeed . This latter gap can also be associated with the performance loss of causal . As a final remark , we note that the architecture of Fig . , which us to pose the search of as a convex optimization problem , is by no the only scheme capable of the upper and . For instance , it can be shown that the same performance can be removing either or in the system of Fig . , provided an entropy coder with infinite memory is used . Indeed , the theoretical among causal of the differential pulse code , with estimation at the end , been shown in a different setting . To illustrate the upper in the previous , we here evaluate and , and calculate an approximation of via Iterative Procedure , for two zero mean AR and AR . These were by the recursion where the of the process are i . i .. zero mean unit variance random . Iterative Procedure was carried out by to be an eight tap FIR filter . For each of the target considered , the procedure was stopped after four complete . The first order source Source was chosen by setting the of the in to be . This to zero mean , unit variance white noise through the coloring transfer function . The second order source Source of zero mean , unit variance white noise through the coloring transfer function . The resulting upper for Source and Source are shown in . and , respectively . As by and , all the upper for derived in to in the limit of both large and small i . e ., when and , respectively . For both , the gap between and is significantly smaller than . sample , for all at which was . Indeed , this gap is smaller than . For the first order source , the magnitude of the of the FIR filter rapidly with coefficient index . For example , when running five of Iterative Procedure , a tenth order FIR filter for , for Source at sample , the was Such fast decay of the impulse response of that , at least for AR , there is little to be by be an FIR filter of order . It is worth that , in the iterative procedure , the initial guess for is a unit scalar gain . The frequency response magnitude of is plotted in Fig . , together with and the resulting frequency after for a target rate of . Notice that for Source , after four of Iterative Procedure , the for are almost identical to , according to . This that Iterative Procedure fast convergence . For example , when four of Iterative Procedure to Source with a target rate of . sample , the after each iteration were . , . , . , and . , respectively . For the same source with a target rate of . sample , the distortion took the . , . , . , and . as the . A similar behavior is for other target , and for other of in as well . Thus , at least for AR , one close to the global optimum after just three . In this paper , we have and upper to the causal and zero delay rate distortion function for stationary and as the distortion measure . We first that for with bounded differential entropy rate , the causal does not exceed the information theoretic by more than approximately . sample . After that , we derived an explicit expression for the information theoretic under per sample distortion a constructive method . This result was then for a closed form formula for the causal information theoretic of first order Gauss under an average distortion constraint . We then derived three closed form upper bounding to the difference between and . Two of these bounding are than the previously best known bound of . sample , at all . We also provided a fourth upper bound to that is constructive . More precisely , we provide a practical scheme that this bound , based on a noise shaped predictive coder of an channel surrounded by , post , and feedback . For a given source spectral density and desired distortion , the design of the is convex in their frequency . We an iterative algorithm , which is to converge to the optimal set of unique . Moreover , the mutual information across the channel , monotonically to . Thus , one to solve the more complicated minimization of the mutual information over all possible conditional satisfying the distortion constraint . To achieve the upper on the operational , one may simply replace the channel by a scalar and memoryless entropy conditioned to the dither . We will first show that can be by a vector channel between two square matrices . It was already established in Lemma that an output to a realization of only if it is jointly with the source where the inverse of from the fact that bounded differential entropy . It is clear from and the joint between and that the causality condition is satisfied if and only if the matrix We will now describe a simple scheme which is capable of the joint statistics between and any given jointly with satisfying . Suppose is first by a matrix yielding the random vector . Then a vector with i . i .. with unit variance , independent from , say , is added to , to yield the random vector . Finally , this result is by a matrix to yield the output On the other hand , the joint second order statistics between and are fully by the matrices It can be seen from these that all that is for the system previously to reproduce any given pair of covariance matrices is that the matrices and Thus , can be chosen , for example , as the lower triangular matrix in a factorization of . With this , a tentative solution for could be as , which would satisfy if and only if . The latter if and only if recall that is nonsingular since bounded differential entropy . We will now show that this condition actually by a contradiction argument . Suppose . Since , the former supposition is equivalent to . If this were the case , then there would exist such that and . The latter , combined with , would imply . One could then construct the scalar random variable , which would have nonzero variance . The of from is given by From this , and in view of the fact that is with nonzero variance , we conclude that would be unbounded . However , by construction , the chain , and therefore by the Data Inequality we would have that , that is unbounded too . This the assumption that is a realization of , leading to the conclusion that . There is to satisfy , and thus for every , there exist matrices and which yield an output vector satisfying . The first equality from the data inequality and the fact that is from . To prove that the second equality in , we will prove that . We first have , from , that , which combined with the identity immediately that where the orthogonal projection operator onto a given subspace . Since and is orthogonal to the other two on the of , we have that where the last equality from the fact that is i . i .., which that is independent of the other two in the expression . On the other hand , from Thus , we have , shown at the bottom of the page , where comes from the data inequality , and from the fact that and from . To complete the proof of the second equality in , we note that the data inequality also . Finally , and and replace the noise by the vector of noise with unit variance by independently operating , with their being jointly conditioned to the dither , then the operational data rate would be upper bounded by where is the output of the channel . Since the distortion by the is the same as that with the original channel , we conclude that First , following exactly the same proof as in Lemma in the Appendix , it is straightforward to show that Now , consider the following family of . For some positive integer , the entire source sequence is in of contiguous . and of each block is independent of the and of any other block . As in the scheme in the second part of the proof of Lemma , each source block is by the optimal matrix , the resulting block being and parallel and independent , with their jointly entropy conditioned to the dither . When , the result is then by the optimal post matrix in the proof of Lemma . For such an pair , and from , the operational rate after have been reconstructed is where rounding to the nearest integer since the th sample is reconstructed only after of length are . On the other hand , since the variance of each reconstruction error sample cannot be than the variance source , we with the first is upper bounded as where rounding to the nearest smaller integer . Therefore , for any finite , the average distortion of this scheme when i . e ., when we consider the entire source process . Also , from and we conclude that for every finite . Our aim is to use this result to show that . Since is valid only for finite of , we must resort to the convergence of as . First of all , since is bounded , From Lemma , for any given reconstruction error covariance matrix , the mutual information is if and only if the output is jointly with the source . In addition , for any information between and a jointly , the variance of every reconstruction error sample is if and only if is the estimation error resulting from from , that is , if and only if Thus , hereafter we restrict the analysis to output jointly with and causally related also satisfy . For any such output process , say the following : and thus equality in if and only if the following chain is satisfied : is lower bounded by the of , which in turn only on the error associated with . We shall now see that this lower bound is by a unique set of error , and then show that the resulting bound is achievable while these error . , we have that With this , and since the of when any error variance , the minimum value of the of subject to the see . Therefore , for all causally related to and jointly with satisfying the distortion con Now , we will show that for any distortion schedule , the output by the recursive algorithm of Procedure is such that the lower bound , thus being a realization of . and the , and Source Past Independence which are necessary and sufficient to attain equality in . pose causality . Then , since , it from that the top left square is lower triangular , being This that the top in the th column of depend only on the of above its th row . that , we conclude that is also lower triangular , and thus also causality . Notice that for any given causality up to sample , the by Step is the only vector consistent with satisfying causality up to the th sample . : for . , and mean all . Therefore , the reconstruction vector by the above algorithm for all . Source Past Independence : Since all are jointly , condition is equivalent to Since the above algorithm an output which , , and , for all , this output equality in , thus being a realization of . Notice that once the are given , each step in the recursive algorithm the only and that satisfy , and . Therefore , for any given distortion schedule , the latter algorithm the unique output that . This the proof . Consider the first of input and output . The average distortion constraint here the form Then , we have , shown at the bottom of the page , where the last inequality from inequality and the fact that is a convex function of . Equality is if and . Given that the of is when constraint is active i . e ., by making , we can attain equality in and minimize its by The first inequality in directly from and . For a plain channel with noise variance , the mutual information between source and reconstruction is In both the end to end distortion can be reduced by a scalar gain after the test channel . The optimal minimum gain is . The mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it . However , now the resulting end to end distortion is . Therefore , for a given end to end distortion , the distortion between the source and the signal before the optimal scalar gain is where from , , and and by that , from , and from inequality . Notice that the of the first term on the of . The middle term on the of directly from . Finally , for close to , a bound than can be from a as : which is precisely the third term on the of . In the above , a trivially since , and b from inequality . Therefore , equality in b if and only if is white . The validity of the chain of in directly from and . This the proof . Immediately afterward we prove that , despite the distortion and causality , the scheme in Fig . enough of freedom to turn all the above into . That that if we are able to globally over the of the system while satisfying the distortion and causality , then that , say , must satisfy We now proceed to demonstrate the validity of and to state the under which are . The first equality in from the fact that is a i . i .. process . Inequality from the following : where is the signal at the output of , see Fig . . In the above , from the fact that and are independent and from the fact that is strictly causal . As a consequence , is independent of , for all . Inequality from the property , with equality if and only if and are independent , i . e ., if and only if is white . Similarly , since the of are independent . By that is a linear combination of and , it immediately that is independent from upon knowledge of , which to . On the other hand , from the fact that . Equality in from the fact that , if is known , then can be from , and vice , see Fig . . Equality from the fact that there no feedback from to , and thus the chain . On the other hand with equality if and only if is invertible for all for which . Finally , directly from the Since is by definition an , it that , for every , there an output process jointly with , satisfying the causality and distortion and such that . Such output can be by its noise , say , and its signal transfer function , say , by the model in Fig . . chosen in a for simplicity and because , as we shall see next , we have enough of freedom to do so without compromising rate distortion performance . the system of formed by a , c , and b , we obtain that their squared equal their in . To do so , we will make use of the Wiener theorem Theorem in the Appendix . Substitution of the of the second equation of c into the above , together with the Wiener theorem , that there a causal , stable and minimum phase transfer Since can be chosen to be arbitrarily small , it can always be chosen so that , which . Therefore from Theorem that if we construct an output process by the recursive algorithm of that theorem , with the choice , for all , then this output process is such that . Proposition Column Correspondence : Let be a random vector source with covariance matrix . A reconstruction random vector , with , be a random source vector with covariance matrix . A reconstruction random vector Proof : Let us first introduce the notation , the top left of any given square matrix Lemma that , if the reconstruction is the output of a causal Wiener filter applied to the noisy source for some noise vector a condition equivalent to , then and have identical on and above their main . Theorem see ,. : Let be a function defined on . There a unique stable , causal and minimum phase transfer function such that if and only if In ,. , it is stated that is a sufficient condition for such a to exist . However , from , Note ,. and the discrete continuous equivalence in ,. , it that is also necessary . We study the increase in per sample differential entropy rate of random and after being through a non minimum phase discrete time , linear time invariant filter . For discrete time and random , it long been established that this entropy gain ,, the integral of . It is also known that , if the first sample of the impulse response magnitude , then the latter integral the sum of the logarithm of the of the non minimum phase of i . e ., its outside the unit circle , say . These have been derived in the frequency domain as well as in the time domain . In this note , we begin by showing that time domain , which consider finite length and then to infinity , have significant mathematical and , therefore , are inaccurate . We discuss some of the of this oversight when considering random . We then present a rigorous time domain analysis of the entropy gain of for random . In particular , we show that the entropy gain between equal length input and output is upper bounded by and if and only if there an output additive disturbance with finite differential entropy no matter how small or a random initial state . Unlike what with linear , the entropy gain in this case on the distribution of all the involved . Instead , when the input differential entropy to that of the entire longer output of , the entropy gain irrespective of the and without the need for additional exogenous random . We illustrate some of the of these by their in three different . Specifically : a simple derivation of the rate distortion function for non stationary , for In his seminal paper , gave a formula for the increase in differential entropy per degree of freedom that a continuous time , band limited random process u after passing through a linear time invariant continuous time filter . In this formula , if the input process is to a frequency range ,, differential entropy rate per degree of freedom u , and the filter frequency response , then the resulting differential entropy rate of the output process is given by , Theorem The last term on the right hand side of can be understood as the entropy gain entropy amplification or entropy boost by the filter . proved this result by that an filter can be seen as a linear operator that selectively scales its input signal along infinitely many , each of them an orthogonal component of the source . The result is then by writing down the determinant of the of this operator as the product of the frequency response of the filter overfrequency , logarithm and then taking the limit as the number of frequency to infinity . An analogous result can be for discrete time input u and output , and an discrete time filter by them to their continuous time , which is the differential entropy rate of the process u . Of course the same formula can also be by the frequency domain proof technique that in his derivation of . The rightmost term in , which to the entropy gain of , can be related to the structure of this filter . It is well known that causal with a rational transfer function such that i . e ., such that the first sample of its impulse response unit magnitude , then where i are the of and , : is the open unit disk on the complex plane . This a straightforward way to evaluate the entropy gain of a given filter with rational transfer function . In addition , that , if , then such gain is greater than one if and only if outside . A filter with the latter property is said to be non minimum phase ; conversely , a filter with all its said to be minimum phase . appear naturally in various . For instance , any unstable system via linear feedback control will yield transfer which are , . Additionally , also appear when a discrete time with zero order hold equivalent system is from a plant whose number of its number of by at least , as the sampling rate , Lemma . . On the other hand , all linear phase , which are specially for audio and , are , . The same is true for any all pass filter , which is an important building block in signal , . An alternative approach for the entropy gain of is to work in the time do where yn , y y yn and the random vector un is defined likewise . From this , it is clear that regardless of whether i . e ., the polynomial g with magnitude greater than one , which clearly and . Perhaps surprisingly , the above contradiction not only been in previous works such as , , but the time domain formulation in the form of been as a to prove or disprove see , for example , the reasoning in ,. . A reason for why the contradiction between , and can be from the analysis in for an a noisy feedback loop , as the one in Fig . . In Figure . Left : a noisy feedback loop . Right : equivalent system when the feedback channel is noiseless and unit gain . this scheme , a causal feedback channel which the output an exogenous noise random process c to generate its output . The process c is assumed independent of the initial state of , by the random vector x , which finite differential entropy . For this system , it is shown in , Theorem . that with equality a deterministic function of . Furthermore , it is shown in , Lemma . that if x and the steady state variance of asymptotically bounded as , then where pi are the of . Thus , for the case in which , the output y is the result of filtering u by a filter as shown in Fig . right , and the resulting entropy rate of will exceed that of u only if there is a random initial state with bounded differential entropy see a . Moreover , under the latter , , Lemma . that if is stable and x , then this entropy gain will be lower bounded by the right hand side of , which is greater than zero if and only . However , the result in b does not provide under which the equality in the latter equation . Additional and intuition related to this problem can be from in . There it is shown that if is a two sided stationary random process by a state space recursion of the form for some A , , , with unit variance i . i .. u , then its entropy rate will be exactly i . e ., the differential entropy rate of u plus the of with i now being the of A outside the unit circle . However , as noted in , if the same system with zero or deterministic initial state is excited by a one sided infinite i . i .. process u with unit sample variance , then the asymptotic entropy rate of the output process y is just i . e ., there is no entropy gain . Moreover , it is also shown that if is a random sequence with positive definite covariance matrix and , then the entropy rate that of u by the of . This that for an system which a representation of the form , the entropy gain for a single sided i . i .. input is zero , and that the entropy gain from the input to the output plus disturbance is , for any disturbance of positive definite covariance matrix no matter how small this covariance matrix may The previous analysis that it is the absence of a random initial state or a random additive output disturbance that the time domain formulation yield a zero entropy gain . But , how would the addition of such finite energy exogenous random to actually produce an increase in the differential entropy rate which asymptotically the of In a sense , it is not clear from the above what the necessary and sufficient are under which an entropy gain equal to the of the analysis in only a set of sufficient and on second order statistics and to derive the previously . Another important observation to be made is the following : it is well known that the entropy gain by a linear is independent of the input statistics . However , there is no reason to assume such independence when this entropy gain as the result of a random signal to the input of the , i . e ., when the by itself does not produce the entropy gain . Hence , it remains to characterize the set of input statistics which yield an entropy gain , and the magnitude of this gain . The first part of this paper to these . In particular , in Section explain how and when the entropy gain in the above , starting with input and output of finite length , in a time domain analysis similar to , and then taking the limit as the length to infinity . In Section it is shown that , in the output plus disturbance scenario , the entropy gain is at most the of . We show that , for a broad class of input not necessarily or stationary , this maximum entropy gain is only when the disturbance bounded differential entropy and its length is at least equal to the number of non minimum phase of the filter . We provide upper and lower on the entropy gain if the latter condition is not met . A similar result is shown to hold when there is a random initial state in the system with finite differential entropy . In addition , in Section we study the entropy gain between the entire output sequence that a filter as response to a shorter input sequence in Section . In this case , however , it is necessary to consider a new definition for differential entropy , effective differential entropy . Here we show that an effective entropy gain equal to the of is provided the input finite differential entropy rate , even when there is no random initial state or output disturbance . In the second part of this paper we apply the in the first part to three , namely , control , the rate distortion function for non stationary , and the channel capacity with feedback . In particular , we show that equality in b for the feedback system in Fig . left under very general even when the noisy . For the problem of finding the quadratic rate distortion function for non stationary auto regressive , previously in , we provide a simpler proof based upon the we derive in the first part . This proof the result stated in , to a class of non stationary . For the feedback capacity problem , we show that capacity based on a short random sequence as channel input and on a feedback filter which the entropy rate of the end to end channel noise such as the one in , crucially depend upon the complete absence of any additional disturbance anywhere in the system . Specifically , we show that the information rate of such capacity to zero in the presence of any such additional disturbance . As a consequence , the relevance of the robust i . e ., in the presence of feedback capacity of , which to be a fairly unexplored problem , becomes evident . For any system , the transfer function to the transform of the impulse response g , g ,..., i . e . For a transfer function , we denote by the lower triangular matrix g as its first column . We write as a shorthand for the sequence x ,..., and , when convenient , we write in vector form as , x x , where transposition . Random are non , such as non and , such as . For matrices we use upper case , such as A . We write i A to the note the i th magnitude eigenvalue of A . If An , then Figure . Linear , causal , stable and time invariant input and output , initial state and output disturbance . Ai , the entry in the intersection between the i th row and the th column . We write , with i i , to refer to the matrix formed by the i to i of A . The expression m A m to the square sub matrix along the main diagonal of A , with its top left and bottom right on Am , m and Am , m , respectively . A diagonal matrix whose are the as Consider the discrete time system in Fig . . In this setup , the input u is a random process and the a causal , linear and time invariant system with random initial state vector x and random output disturbance z . In vector notation , where n is the natural response the initial state x . We make the following further the around it : Assumption . is a causal , stable and rational transfer function of finite order , whose impulse It is worth that there is no loss of generality in considering g , since otherwise one can write as g g , and thus the entropy gain by would be plus the entropy gain due to g , which an impulse response where the first sample . Assumption . The disturbance z is independent of u and to a dimensional linear subspace , for some finite . This subspace is by the of a matrix where for the countably infinite size of , such that z . Equivalently , z , where the random vector s , z finite differential entropy and is independent of u . As in the Introduction , we are interested in the entropy the presence or absence of the random u , by In the next section we provide geometrical insight into the behaviour of for the situation where there is a random output disturbance and no random initial state . A formal and precise treatment of this scenario is then in Section . The other are considered in the subsequent In this section we provide an intuitive geometric interpretation of how and when the entropy gain defined in . This understanding will justify the introduction of the notion of an random process in Definition below , which will be shown to play a key role in this and in related . Suppose for the moment Fig . is an FIR filter with impulse response g , g , , i . Notice that this choice , and thus one non minimum phase zero , at . The associated matrix for is whose determinant is clearly one indeed , all its are . Hence , as in the introduction , u , and thus G and in general does not introduce an entropy gain by itself . However , an interesting phenomenon becomes evident by looking at the singular value decomposition of G , given by , where Q and R are unitary matrices and D , d , d , d . In this case , D . , . , . , and thus one of the singular of G is much smaller than the although the product of all singular , as . As will be shown in Section , for a stable such uneven distribution of singular only when non minimum phase . The effect of this can be by looking at the image of the cube , through G shown in Fig . . If the input u were uniformly distributed over this cube of unit Figure . Image of the cube , through the square matrix with , and . volume , then would distribute uniformly over the unit volume parallelepiped in Fig . , and hence u . Now , if we add to a disturbance z , with distributed over . , . independent of u , and with R , the effect would be to thicken the support over which the resulting random vector y z is distributed , along the direction pointed by . with the direction along which the support of is given by q , , the first row of Q , then the resulting support would have its volume significantly , which can be associated with a large increase in the differential entropy of y with respect to u . Indeed , a relatively small variance an still produce a significant entropy gain . The above example that the entropy gain from un to yn as a combination of two . The first of these is the uneven way in which the random vector is distributed over . The second factor is the alignment of the disturbance vector with respect to the span of the subset , i i of of , associated with singular of , indexed by the in the set . As we shall discuss in the next section , non minimum phase , then , as , there will of going to zero exponentially . Since the product of the singular of for all , it that , i must grow exponentially with , where , i is the i th diagonal entry of . This that the span of , i i , compensating its shrinkage along the span of , i i , thus keeping un for all . Thus , as , any small disturbance distributed over the span of , i i , added to , will keep the support of the resulting distribution from shrinking along this subspace . Consequently , the expansion of the span of , i i is no longer , yielding an entropy increase proportional to log , i . The above analysis one to anticipate a situation in which no entropy gain would take place even when some singular of tend to zero as . Since the increase in entropy is made possible by the fact that , as , the support of the distribution of along the span of , i i , no such entropy gain should arise if the support of the distribution of the input un accordingly along the pointed by the , i i of . An example of such situation can be easily as : Let in Fig . have phase and suppose that u is as , where u is an i . i .. random process with bounded entropy rate . Since the determinant of n for all , we have that un u n , for all . On the other hand , yn nu n u n . Since for some finite The preceding discussion that the entropy gain produced the situation shown in Fig . on the distribution of the input and on the support and distribution of the disturbance . This in stark contrast with the well known fact that the increase in differential entropy produced by an invertible linear operator only on its , and not on the statistics of the input . We have also seen that the distribution of a random process along the different within the space which it a key role as well . This the need to specify a class of random which distribute more or less evenly over all . The following section a rigorous definition of this class and a large family of belonging to it . We begin by formally the notion of an entropy balanced process u , being one in which , for every finite , the differential entropy rate of the orthogonal projection of un into any subspace of dimension the entropy rate of u . This idea is precisely in the following Definition . A random process is said to be entropy balanced if , for every , for every sequence of matrices , with . Equivalently , a random process is entropy balanced if every unitary transformation on sequence yn that one cannot predict its last with arbitrary accuracy by its previous , even to infinity . We now characterize a large family of entropy balanced random and establish some of their . Although intuition may suggest that most random such as i . i .. or stationary should be entropy balanced , that statement rather difficult to prove . In the following , we show that the entropy balanced condition is met by i . i .. with per sample probability density function being uniform , piece wise constant or . It is also shown that to an entropy balanced process an independent random independent of the former another entropy balanced process , and that filtering an entropy balanced process by a stable and minimum phase filter an entropy balanced process as well . Proposition . Let u be a i . i .. random process with positive and bounded per sample Lemma . Let u be an i . i .. process with finite differential entropy rate , in which each is distributed according to a piece wise constant in which each interval where this is constant measure greater than o , for some bounded away from zero constant o . Then u is entropy balanced . Lemma . Let u and v be mutually independent random . If u is entropy balanced , then The working behind this lemma can be intuitively by that to a random process another independent random process can only increase the spread of the distribution of the former , which to balance the entropy of the resulting process along all in space . In addition , it from Lemma that all i . i .. a per sample which can be by uniform , piece wise constant or as many times as are entropy balanced . It also that one can have non stationary which are entropy balanced , since Lemma no for the process v . Our last lemma related to the of entropy balanced that filtering by a stable and minimum phase filter the entropy balanced condition of its input . Lemma . Let u be an entropy balanced process stable and minimum phase filter . Then the also an entropy balanced process . This result that any stable moving average auto regressive process from is also entropy balanced , provided the of the and regression correspond to a stable filter . We finish this section by pointing out two of which are non entropy balanced , namely , the output of a filter to an entropy balanced input and the output of an unstable filter to an entropy balanced input . The first of these a central role in the next section . In this section we formalize the which were qualitatively outlined in the previous section . Specifically , for the system shown in Fig . we will characterize the entropy gain defined in for the case in which the initial state x is zero or deterministic and there an output random disturbance of possibly infinite length z which Assumption . The following will be instrumental for that purpose . Lemma . Let A be a causal , finite order , stable and minimum phase rational transfer function with Lemma . Consider the system in Fig . , and suppose z Assumption , and that the input process u is entropy balanced . Let the of , where , ,... are the singular of , with , , ,, such that . the number of these singular which tend to zero exponentially as . Then The previous lemma precisely the geometric idea outlined in Section . To see this , notice that no entropy gain is if the output disturbance vector is orthogonal to the space by the . If this were the case , then the disturbance would not be able fill the subspace along which is shrinking exponentially . Indeed , if for all , then , and the latter sum out the one on the of , while limn n nun since u is entropy balanced . On the contrary and loosely speaking , if the projection of the support of onto the subspace by the is of dimension , then remains bounded for all , and the entropy limit of the sum on the of the possible entropy gain . Notice that because , and thus this entropy gain from the uncompensated expansion of along the space by the Lemma also the following corollary , which that only a filter with outside the unit circle i . e ., an transfer function can introduce entropy gain . Corollary Minimum Phase do not Introduce Entropy Gain . Consider the system shown in Fig . and let u be an entropy balanced random process with bounded entropy rate . Besides Assumption , suppose that is minimum phase . Then Proof : Since is minimum phase and stable , it from Lemma that the number of singular of which go to zero exponentially , as , is zero . Indeed , all the singular vary with . Thus and Lemma directly that the entropy gain is zero since the of is zero . In this section we show that random satisfying Assumption , when added to the input u i . e ., before , do not introduce entropy gain . This result can be from Lemma , as stated in the following theorem : Theorem Input do not Introduce Entropy Gain . Assumption . Suppose that u is entropy balanced and consider the output where b a , with a being a random vector satisfying a , and where . Then , Proof : In this case , the effect of the input disturbance in the output is the forced response it . This response can be as an output disturbance . Thus , the argument of the differential entropy on the of is The proof is by substituting this result into the of and that Remark . An alternative proof for this result can be given based upon the of an sequence , as . Since , , we have that un un . Let and be a matrices with which satisfy and such that is a unitary matrix . Then which is upper bounded for an and , the latter due to u being entropy balanced . On the other hand , since is independent of un , it that un un , We show here that the entropy gain of a transfer function with outside the unit circle is at most the sum of the logarithm of the magnitude of these . To be more precise , the following assumption is . Assumption . The and its transfer , of which are . the number of distinct , given by , i . e ., such that , with li being the multiplicity of the i th distinct zero . We denote by i , where : ,.. ,...,, the distinct zero of associated with the i th non distinct zero of , i . e ., As can be from the previous in this section , we will need to characterize the asymptotic behaviour of the singular of . This is accomplished in the following lemma , which these singular to the of . This result is a generalization of the unnumbered lemma in the proof of , Theorem in the appendix as Lemma , which for FIR transfer , to the case of infinite impulse response transfer i . e ., transfer . where the in the sequence an , are positive and increase or decrease at most Theorem . In the system of Fig . , suppose that u is entropy balanced and that and z satisfy and , respectively . Then where , min , and is as defined in Assumption . Both are tight . The upper bound is if limn n n , where the unitary matrices the Theorem . In the system of Fig . , suppose that u is entropy balanced and that Assumption . Let z be a random output disturbance , such that i , i , and that . Then Here we analyze the case in which there a random initial state x independent of the input u , and zero or deterministic output disturbance . The effect of a random initial state in the output as the natural response it , namely the sequence n . Thus , yn can be written in vector form as This that the effect of a random initial state can be as a random output disturbance , which us to apply the from the previous . Recall from Assumption that is a stable and rational transfer function with . As such , it can be as where is a filter only all the of , and is a FIR filter , all the of . We have already established recall Theorem that the entropy gain by the minimum phase system is zero . It then that the entropy gain can be only by the of and an appropriate output disturbance . Notice that , in this case , the input process w to i . e ., the output sequence to a random input u is independent of since we have the natural response after , hose initial state is now zero . This condition us to directly use Lemma in order to analyze the entropy gain that u after being Theorem . Consider a stable th order filter , and with a random initial state x , such that x . Then , the entropy gain due to the existence of a random initial Proof : Being a and stable rational transfer function , can be as where is a stable transfer function only all the of and with all its at the origin , while is stable and FIR filter , all the of . Let and be the natural of their common random initial state x , respectively , where , . Then we can write Since is stable and , it from Corollary that un for all , and therefore Therefore , we only need to consider the entropy gain by the possibly non minimum to a random output disturbance n , which is independent of the input . Thus , the of Lemma are met considering , where the for , and , , ,. Consequently , it to consider the differential entropy on the of , whose argument is where , un bounded entropy rate and is entropy balanced since is the natural response of a stable system and because of Lemma . We remark that , in , is not independent of x , which one from the proof of Theorem directly . On the other hand , since is FIR of order at most , we have that , where is a non singular upper triangular matrix independent of . Hence , can be written as , where and , . According to , the entropy gain in as long as is lower bounded by a finite constant or if it sub linearly as . Then , we need m n to be a full row ranked matrix in the limit as . However , where m the the . We will now show that these do not go to zero as . Define the matrix such that m . Then , it that , Hence , the minimum singular value of m is lower bounded by the singular value of , for all . But it was shown in the proof of Theorem see page that limn min . this result in and taking the limit , we arrive to is upper and lower bounded by a constant independent v is entropy balanced , m , and , which that the entropy rate in the of to zero . The proof is finished by Lemma . Theorem us to formalize the effect that the presence or absence of a random initial state on the entropy gain similar to those in Section . Indeed , if the random initial state x finite differential entropy , then the entropy gain , since the alignment between x and the is . This us to characterize the behavior of the entropy gain due only to a random initial state , when the initial state x can be written as pst , with , which that x an undefined or differential entropy . Corollary . Consider an FIR , order filter , such that its random initial state can be written as x , where and . Then , where , min ,. The upper bound in is when is a non singular matrix , with defined by n as in Theorem . Proof : The effect of the random initial state to the output sequence y can be written as yn , remains bounded , for , if and only if limn . Define the rank of as ,... If m , then the lower bound is by in . Otherwise , enough such that , We then proceed as the proof of Theorem , by considering a unitary matrix , and a matrix An such that that the lower limit in the latter sum when is a full row rank matrix . the latter into the proof . Remark . If the random initial state x is with , then the entropy gain by an FIR minimum phase at least log . Otherwise , the entropy gain could be identically zero , as long as the of fill only the orthogonal space to the span of the row in m , where En , and m are defined as in the proof of Theorem . Both , Theorem and Corollary , reveal that the entropy gain as long as the effect of the random initial state with the first of , just as in the of the previous section . If there are no and the initial state is zero , then the to an input un is given by . Therefore , the entropy gain in this case , as defined in , is zero , regardless of whether Despite the above , there is an interesting question which , to the best of the knowledge , not been before : Since in any filter the entire output is longer than the input , what would happen if one the differential of the complete output sequence to that of the shorter input sequence As we show next , a proper definition of this question recasting the problem in of a new definition of differential entropy . After providing a geometrical interpretation of this problem , we prove that the new entropy gain in this case is exactly . Suppose u is uniformly distributed over , , . the conventional definition of differential entropy of a random sequence , we would have that In other , the problem in that although the output is a three dimensional vector , it only two of freedom , i . e ., it is restricted to a dimensional subspace of R . This is in Fig . , where the set , , is shown with the u plane , together with its image through as defined in . As can be seen in this figure , the image of the square , through is a dimensional rhombus over which y , y , y uniformly . Since the intuitive notion of differential entropy of an ensemble of random such as how difficult it is to compress it in a fashion to the size of the region by the associated random vector , one could argue that the differential entropy of y , y , y , far from being , should be somewhat than that of u , u since the rhombus , a area than , . So , what does it mean that and why should y , y , y Simply put , the differential entropy to the volume by the support of the probability density function . our example , the latter three dimensional volume is clearly zero . Figure . Support of u laying in the u plane to that of u the rhombus in R . From the above discussion , the comparison between the differential of R and u R of our previous example should take into account in a two dimensional subspace of R . Indeed , since the multiplication by a unitary matrix does not alter differential , we could consider the differential entropy of the matrix with in the singular value decomposition of and is a unit norm vector orthogonal to the of and thus orthogonal well . We are now able to compute the differential entropy in R for , corresponding to the rotated version that its support is now with R . The preceding discussion the use of a version of the notion of differential entropy for a random vector which the number of actually Definition The Effective Differential Entropy . Let be a random vector . be written as a linear transformation , for some u , , then the effective differential entropy defined as It is worth that differential entropy of a vector , whose support is greater than zero , from considering it as the difference between its absolute entropy and that of a random variable uniformly distributed over an dimensional , unit volume region of . More precisely , if in this case the probability density function of y y is integrable , then . . , where is the discrete valued random vector resulting an dimensional uniform with cubic quantization with volume . However , if we consider a support to an dimensional subspace of , i . e ., AT , as in Definition , then the entropy of its version in , say , is distinct from Ay , the entropy of Ay in . Moreover , it turns out that , in general , despite the fact that A . Thus , the definition given by does not yield consistent for the case wherein a random vector a support dimension i . e ., its number of of freedom smaller that its length If this were not the case , then we could redefine by , in a spirit similar to the one behind dimensional entropy . To see this , consider the case in which u uniformly over , and . Clearly , uniformly over the unit length segment the origin with the point . Then The latter example further why the notion of effective entropy is appropriate in the setup considered in this section , where the effective dimension of the random does not coincide with their length it is easy to verify that the effective entropy not change if . Indeed , we will need to consider only which can be by multiplying some random vector u , with bounded differential entropy , by a tall matrix , with as in , which are precisely the by Definition . Theorem . Let the entropy balanced random sequence u be the input of an filter , and let y be its output . Assume that is the transform of the length sequence . Then Theorem that , when considering the full length output of a filter , the effective entropy gain is by the filter itself , without the presence of external random or initial . This may seem a surprising result , in view of the made in the previous , where the entropy gain only when such random exogenous were present . In other , when observing the full length output and the input , the maximum entropy gain of a filter can be in of the volume expansion by the filter as a linear operator , provided we measure effective differential instead of differential entropy . Proof of Theorem : The total length of the output , will grow with the the input , FIR , and will be infinite , . Thus , we define the output length function It is also convenient to define the sequence of matrices , with ni , i , ni , i . This one to write the entire output of a causal Let the , where , is where the first equality from the fact that un can be written as , which that un un . But . The product , is a symmetric matrix , with its first column , h h , given by Thus , the sequence to the to of those resulting from the complete convolution , even when the , where the time reversed perhaps infinitely large response . Consequently , the and ¨ theorem , it that In order to finish the proof , we divide by , take the limit as , and replace in the In this section we obtain a simpler proof of a result by Gray , and , which the rate distortion function of a non stationary auto regressive process x of Figure . Block diagram representation of how the non stationary source x is built and then reconstructed as u . a certain class to that of a corresponding stationary version , under distortion . Our proof is based upon the in the previous , and the class of non stationary for which the in are valid . To be more precise , let and be the impulse of two linear time invariant A and A with rational transfer where pi , i ,...,. From these it is clear that A is unstable , A is stable , and A A , ,. Notice also that lim A and lim A pi , and thus Consider the non stationary random source x and the asymptotically stationary source by passing a stationary process w through A and A , respectively , which can be written as A block diagram associated with the construction in Fig . . Define the rate distortion for these two as where , for each , the are taken over all the conditional probability density The above rate distortion have been in for the case in which w is an i . i .. process . In particular , it is explicitly stated in , that , for that case , We will next provide an alternative and simpler proof of this result , and extend its validity for general not necessarily stationary w , the entropy gain of non minimum phase established in Section . Indeed , the approach in is based upon asymptotically equivalent matrices in of the covariance matrices . This w to be and i . i .. and A to be an all pole unstable transfer function , and then , the only non stationary is that from unstable . For instance , a innovation by an unstable filter A would yield a source which cannot be Gray and approach . By contrast , the reasoning behind our proof w be any process , and then let the source be Aw , with A unstable and possibly and stable as well . Theorem . Let w be any stationary process with bounded differential entropy rate , and let Thanks to the in the previous , it is possible to give an intuitive outline of the proof of this theorem given in the appendix , page by a sequence of block . More precisely , consider the shown in Fig . . In the top diagram in this figure , suppose the for the non stationary source . The sequence u is independent of , and the linear filter is such that the error a necessary condition for minimum . The filter is the product of A see in the appendix a stable , filter with unit frequency response magnitude such that . If one now the filter towards the source , then the middle diagram in Fig . is . By doing this , the stationary source with an additive error signal u that the same asymptotic variance as u , reconstructed as u . From the invertibility of , it also that the mutual information rate between and that . Thus , the channel u the same rate and distortion as the . However , if one now a short the error signal u as in the bottom diagram Figure . Block diagram representation of the of in the proof of Theorem . of Fig . , then the resulting additive error term u u will be independent of and will have the same asymptotic variance as u . However , the differential entropy rate of u will exceed that of u by the of . This will make the mutual information rate between and to be less than that between and by the same amount . Hence , be at most . A similar reasoning can Here we revisit the setup shown in Fig . and in Section I . Recall from b that , for this general class of control , it was shown in , Lemma . that By the in show next that equality in b provided the feedback channel the following assumption : Figure . Top : The class of feedback by Assumption . Bottom : an equivalent form . A stable rational transfer such that is , the same unstable as , and the feedback the plant . is any possibly non linear operator such that , , for all , and An illustration of the class of feedback satisfying this assumption is on top of Fig . . Trivial of satisfying Assumption are a additive channel and by linear . Indeed , an system with a strictly causal transfer function , the feedback channel that Assumption is widely known as a noise shaper with input and post filter , used in , e .. . Theorem . In the control system of Fig . , suppose that the feedback channel Assumption and that the input u is entropy balanced . If the random initial state of the plant , with , x , then Proof : Let and , A . Then , from Lemma in the appendix , the output yn can be written as where the initial state s to yn , the initial state x to the output of , and the initial state x of to yn . Since u is entropy balanced and c finite entropy rate , it from Lemma that u is entropy balanced as well . Thus , we can proceed as in the proof of Theorem to conclude that The feedback information capacity of this channel is by a input , and is given by Figure . Block diagram representation a non white the scheme considered in . where is the covariance matrix of and , for every , the input is to depend upon the channel since there a causal , noise less feedback channel with one step delay . In , it was shown that an auto regressive moving average process of th order , then can be by the scheme shown in Fig . . In this system , is a strictly causal and stable finite order filter and v is with for all such that is with a positive definite covariance matrix . Here we use the in Section to show that the information rate by the capacity scheme in to zero if there any additive disturbance of length at finite differential entropy affecting the output , no matter how small . To see this , notice that , in this case , and for all n , since In . From Theorem , this gap between differential is precisely the entropy gain by In to an input when the output is affected by the disturbance . Thus , from Theorem , the capacity of this scheme will correspond to , where are the of , which is precisely the result stated in , Theorem . . However , if the output is now affected by an additive disturbance d not passing through such that , , with , then we will have But limn n In In , which directly from Theorem to each of the differential . Notice that this result irrespective of how small the power of the disturbance may be . Thus , the capacity scheme in and further studied in , although of theoretical importance , would yield zero rate in any practical situation , since every real signal is unavoidably affected by some amount of noise . This paper provided a geometrical insight and rigorous for the increase in differential entropy rate to as entropy gain by passing an input random sequence through a discrete time linear time invariant filter such that the first sample of its impulse response unit magnitude . Our time domain analysis us to explain and establish under what the entropy gain with what was by , who a approach to a related problem in his seminal paper . In particular , we that the entropy gain only if outside the unit circle i . e ., it is non minimum phase , . This is not sufficient , nonetheless , since the input and output be u and , the difference is zero for all , yielding no entropy gain . However , if the distribution of the input process u a certain regularity condition defined as being entropy balanced and the output the form , an output disturbance with bounded differential entropy , we have shown that the entropy gain can range from zero to the sum of the logarithm of the of the of , depending on distributed . A similar result is if , instead of an output disturbance , we let have a random initial state . We also considered the difference between the differential entropy rate of the entire and longer output of and that of its input , i . e .,, where is the length of the impulse response of . For this purpose , we the notion of effective differential entropy , which can be applied to a random sequence whose support dimensionality smaller than its dimension . Interestingly , the effective differential entropy gain in this case , which is intrinsic to , is also the sum of the logarithm of the of the of , without the need to add or a random initial state . We have some of the of these in three . Specifically , we used the fundamental here to provide a simpler and more general proof to characterize the rate distortion function for non stationary and distortion . Then , we applied our to provide sufficient for equality in an information inequality of significant importance in control . Finally , we that the information rate of the capacity scheme in for the channel with feedback to zero in the presence of any additive disturbance in the channel input or output of sufficient finite length , no matter how small it may be . yn , . Then , where In is the identity matrix . Proof of Lemma : Let be the the sample is constant . Let be the of these . Define the discrete random process c , where i if and where the inequality is due to the fact that un and yn are deterministic of un , and hence un yn . un from we obtain where the last equality from Lemma see Appendix whose are met because , given , the sequence un independent each of them distributed uniformly over a possibly different interval with bounded and positive measure . The opposite inequality is by following the same as in the proof of Lemma , from onwards , which the proof . Proof of Lemma : Let yn , , where is a unitary matrix and where and have . Then Substituting this result into , dividing taking the limit as , and that , since where n is a jointly sequence with the same second order moment as . Therefore , that a bounded second moment at each entry i , and the latter inequality in , Proof of Lemma : Let yn , where is a unitary matrix and where and have . Since , we have that Let be the of , where An is an orthogonal matrix , and is a diagonal matrix with the singular of . Hence It is straightforward to show that the diagonal in are lower and upper bounded by the and singular of , say and , respectively , which where the last equality is due to the fact that u is entropy balanced . This the proof . Proof of Lemma : The fact that limn is upper bounded directly from the fact that A is a stable transfer function . On the other hand , An is positive definite with all its equal to , and so is positive definite as well , with limn . Suppose that limn . If this were true , then it would hold that limn A nA . But A n is the lower triangular matrix associated with A , which is stable since A is minimum phase , that limn A nA , thus leading to a contradiction . This the proof . Notice that . Thus , it only remains to determine the limit of as . We will do this by a lower and an upper bound for this differential entropy and show that these converge to the same expression as . where a from as well to the set , while and stem from the independence between u and z . Inequality is a consequence of , and e from to the set in the second term , and that is not reduced upon the knowledge of . then , by and in , dividing by , and taking the limit , we obtain n un Xi , i m un where the last equality is a consequence of the fact that u is entropy balanced . Notice that by Assumption and thus is restricted to the span of of dimension , for all . Then , for , one can construct a unitary matrix , such that the of An span the space by the of and such that . Therefore , from , z where and KAn z are the covariance matrices of A and A , respectively , and where the last inequality from . The fact that and are bounded and remain bounded away from zero for all , and the fact that min either sub exponentially since singular decay exponentially to zero , with , imply in that Proof of Lemma : The transfer function can be as , where is stable and minimum phase and is stable with all the non minimum phase of , both being rational . From Lemma , in the limit as , the of are lower and upper bounded by min and T , respectively , where min . Let and the , respectively , with , , , and , , , being the diagonal of the diagonal matrices respectively . Then the i th row of by , i be , we have that , from the Courant theorem that Notice that the of the matrix m n span a space of dimension , ,.., which that one can have m n if . In this case i . e ., if limn m n then the lower bound is by the latter expression into and Lemma . We now consider the case in which limn m n . This condition that there large such that for all . Then , for exist unitary matrices The first differential entropy on the of the latter expression is uniformly upper bounded because u is entropy balanced , m , and . For the last differential entropy , notice that m . Consider the An m , being unitary , being diagonal , . We can then conclude that that A and that is unitary , it is easy to show by the with equality if and only if A . Substituting this into and then the latter into we arrive to Substituting this into , the fact that u is entropy balanced and Lemma the upper bound in . Clearly , this upper bound is if , for example , n n is non singular for large , since , in that case , and we can choose An I Proof of Theorem : As in , the transfer function can be as , where is stable and minimum phase and is a stable FIR transfer function with all the phase of in total . u n , nun , we have that yn nu n , u n un , and that is entropy balanced from Lemma . Thus , This that the entropy gain of due to the output disturbance z to the entropy gain due to the same output disturbance . One can then evaluate the entropy gain of by Theorem to the filter instead of , which we do next . Since only the z are non zero , it that in this case see Assumption . Therefore , m n m n and the sufficient condition given in Theorem will be satisfied for if limn , where now is the left unitary matrix in the . We will prove that this is the case by a Then , there a sequence of unit norm , with for all , such that such that an and . Then , from this definition and from , we have that where the last equality from the fact that , by construction , is in the span of the , together with the fact that is unitary which that . Since the we have applied and the fact that m is bounded and does not depend on . Now , notice that is a matrix with the convolution the impulse response its time reversed version , respectively on its first row and column . It then from , Lemma . that the inequality is strict because all the of are strictly outside the unit disk . Substituting this into we conclude that which . Therefore , to a contradiction , the proof . where b is the first sample in the impulse response of . Notice that that limn n limn n kunk for every sequence of random u with uniformly bounded variance . Since only stable and its coincide exactly with the of A , it that A is a stable transfer function . Thus , the asymptotically stationary process defined in can be as where is a lower triangular matrix with its main diagonal equal to b . The fact that is with b as in that for any un with finite differential entropy For any given , suppose that is chosen and and un are distributed so as to minimize I ; un subject to the constraint E E I E kunk i . e ., , un is a realization of ,, yielding the reconstruction Since we are considering mean squared error distortion , it that , for rate distortion , un must be jointly with . From these , define n , x n u n , n , n n u n . where is a zero mean vector independent of u n , n with finite differential entropy such that , k . Then , we have that , I ; yn a I ; I n ; n where a from being invertible , is due to the fact that n n u n , because un . The equality from u n un see . Equality in e because n u n , and in because of . The last inequality because n n and n . But from Theorem , limn n u n un , and thus , limn n n ; n . At the same time , the distortion for the source n when reconstructed as n is where a because is bounded , and is due to the fact that , in the limit , is a unitary operator . the of and , we conclude that limn n n ; n ,, and therefore purpose , consider now the asymptotically stationary source n , and suppose that n un ,. Again , n and un will be jointly , satisfying un the latter condition is for minimum . From this , one can propose an alternative realization in which the error The change of and the in this chain of is by the block shown in Fig . . sequence is u , , yielding an output n n u n with n u n . Then where a by that n un and because un , from , is a consequence of n u n , from the fact that n n u n . Finally , e because is invertible for all . Since , asymptotically as , the distortion by yn for the non stationary source is the same which is when n is reconstructed as recall , we conclude that Lemma . Let u be a random process with independent , and where each element is uniformly distributed over possible different , such that ai amin , i , Proof : Without loss of generality , we can assume that ai , for all i otherwise , we could scale the input by amin , which would scale the output by the same proportion , increasing the input entropy by amin and the output entropy by log amin , without the result . The input vector un is confined to an box Un the support of un of volume and entropy these is determined by un to ai , and freely over . Thus , the volume of each box is the product of sizes ai of the associated selected free sweeping . But that ai for all i , the volume of each box can be upper bounded by . With this , the added volume of all the in the original box can be upper bounded as have . From this definition , will distribute over a finite region , corresponding to the projection onto the dimensional span of the of . Hence , is upper bounded by the entropy of a uniformly distributed vector over the same support , i . e ., by , where is the dimensional volume of this support . In turn , is upper bounded by the sum of the volume of all dimensional in the box in which un is confined , which we already by , and which is upper bounded where a because is an orthogonal matrix . correspond to the jointly sequence with the same second order as , and that the distribution differential entropy for a given covariance , we obtain the upper bound where a since the are independent , and from the fact that and from the Courant theorem . Since is bounded for all , we obtain by substituting into that . The combination of this with , the proof . We re state here for completeness and convenience the unnumbered lemma in the proof of , Theorem as : Lemma . Let the function be as defined in but for a transfer function with no and only a finite number of , of which lie outside the unit circle . Then , where the in the sequence an , are positive and increase or decrease at most Lemma . Let be rational transfer function of relative degree , with initial a rational transfer function of initial state s . Let where the initial state of is x and the initial state of can be taken to be x s . Combining the above , it is found related to the input u by the following recursion :