. Notice that . For any two we write the standard squared norm and inner product as and , 
 respectively , where complex conjugation . For one sided random and , the term the 
 mutual information rate between and , provided the limit . Similarly , for a stationary random process 
 A source pair a source into binary , from which a reconstruction is . The end to end effect 
 of any pair can be by a series of reproduction , such that , for every 
 where we write as a short notation for . Following , we say that an pair is causal if and only if it the following definition . 
 Definition Causal Source Coder : An pair is said to be causal if and only if its reproduction are such that 
 It also from Definition that an pair is causal if and only if the following chain for every possible random input process : 
 It is worth that if the are random , then this equivalent causality constraint must require that is satisfied for each realization of the 
 be the total number of that the received when it the output subsequence . Define as the random binary sequence that the that the received when is . Notice that is , in general , a function of all source , since the binary may be , i . e ., may be only after the received enough to reproduce 
 , with . We highlight the fact that even though may contain which depend on with , the random may still satisfy , i . e ., the pair can still be causal . Notice also a random variable , which on , the , and on the manner in which the source is into the binary sequence sent to the . 
 For further analysis , we define the average operational rate of an pair as 
 In the sequel , we focus only on the as the distortion measure . Accordingly , we define the average distortion associated with an pair as 
 The allow us to define the operational causal as : 
 We note that the operational causal defined previously to the of all causal . 
 where the last inequality turns into equality for a causal pair , since in that case . Thus , combining , , and 
 This lower bound the study of an information theoretic causal , as defined in the following . 
 Definition : The information theoretic causal for a source , with respect to the average distortion measure , is defined as 
 The definition is a special case of the nonanticipative epsilon entropy by and , which was shown to converge to , for stationary and in the limit as the rate goes to infinity , 
 In the case , it is known that for any source and for any single letter distortion measure , the the information theoretic . Unfortunately , such a strong equivalence between the and the information theoretic 
 . One exception is if one is to jointly and causally encode an asymptotically large number of parallel , of causal . Nevertheless , as outlined in Section I , it is possible to obtain lower and upper to the of causal from . Indeed , and to begin with , since , it directly from and that 
 The previous inequality in is strict , in general , and becomes equality when the source is white or when the rate to infinity . Also , as it will be shown in Section , for , does not exceed by more than 
 For completeness , and for future reference , we recall that for any distortion , the for a stationary source with is equal to the associated , given by the reverse water filling a 
 Although , in general , it is not known by how much , for stationary one can readily 
 More generally , it is known that , for any source , the mutual information across an channel which noise with variance , 
 Until now it been an open question whether a bound than can be for with memory and at general rate . In , we show that for , this is indeed the case . But before on upper for , its operational importance will be established by showing in the following section that , for , the does not exceed by more than approximately . sample . 
 and , an upper bound to can be readily from by approximately . per sample to . This result is first formally stated and proved for finite of any source . Then , it is extended to stationary . We start with two . 
 Definition : The causal information theoretic for a zero mean random vector of length is defined as 
 where the is taken over all output satisfying the causality constraint 
 Definition : The operational causal for a zero mean random vector of length is defined as 
 Lemma : Let be two random with zero mean and the same covariance matrix , i . e . and the same cross covariance matrix with respect to , that is If 
 If furthermore , then equality is in if and only if with and being jointly . 
 Notice that if one Lemma to a reconstruction error with which the output sequence the causality constraint , then the version of the same reconstruction error will also produce an output causally related with the input . To see this , let 
 be the factorization of the covariance matrix of the random vector where is a lower triangular matrix . This one to write as 
 where satisfy . Then , there a set of reproduction satisfying the of Definition which generate each partial vector . Specifically , for any given , there a function , such that . From and given that 
 and thus , for some function . From this and the fact that is independent of , we have that 
 where probabilistic independence . On the other hand , for each let be the minimum mean square error linear estimator of 
 given . Then , the notation for the left corner of a matrix , we have that 
 where from and all the subsequent stem from and from the fact that is lower triangular . Therefore , since the residual is uncorrelated to , it that 
 The result stated in Lemma for random vector is extended to stationary in the following theorem the second main result of this section . 
 The fact that for one to find upper to the of causal by explicitly finding or upper bounding . This is accomplished in the following . 
 In this section , we will find when the source is a Gauss process . More precisely , we will show that the information theoretic causal , which is associated with an average distortion constraint , with the expression for the on the of in for a per sample distortion constraint . To do so , and to provide also a constructive method of realizing the as well as , we will start by an alternative derivation of the for scalar source of length . In this case , from its definition in . . ,. , the the following form : 
 where the is over all conditional of given satisfying the causality constraint and the distortion schedule 
 Before proceeding , it will be convenient to introduce some . , to denote the random column vector and adopt the shorter notation . For any two random , 
 It was already stated in Lemma that the reconstruction vector which mutual information between a source vector and for any given distortion constraint , must be jointly with the source . This , in particular , for a realization of the with distortion schedule . In the next theorem , we will obtain an explicit expression for this and prove that in its realization , the sample 
 Fig . . Recursive Procedure at its th iteration . Starting from known covariance matrices , their next partial and are found . The indicate the step in the algorithm which the corresponding part of the matrix . 
 step is responsible of revealing the partial and by number in the figure . 
 The are formally stated in the following theorem , which also an exact expression for the of first order Gauss . 
 the . Under the latter interpretation , nothing one from choosing an arbitrarily large value for , say , yielding an arbitrarily large value for the second term in the summation on the of , which is , of course , inadequate . 
 We are now in a position to find the expression for for first order Gauss . This is done in the following theorem , whose proof is in Section . 
 The technique applied to prove and does not seem to be extensible to Gauss of order greater than . In the sequel , we will find upper to for arbitrary any order stationary . 
 In order to upper bound the difference between and for arbitrary stationary , we will start this section by an upper bounding function for , by . then derive three closed form upper bounding to the rate loss , applicable to any stationary process . Two of these are 
 Definition Causal Stationary : For a stationary , the information theoretic causal 
 Next , we derive three closed form upper bounding to that are applicable to arbitrary zero mean stationary with finite differential entropy rate . This result is stated in the following theorem , proved in Section : 
 Theorem : Let be a zero mean stationary source with with bounded differential entropy rate and variance . Let denote for given by , and let denote the quadratic for source uncorrelated for the source defined in . Let denote the information theoretic causal see Definition . Then , for all 
 Notice that is independent of , being therefore numerically simpler to evaluate than the other bounding in Theorem . However , as is away from and , becomes very loose . In fact , it can be seen from a that for , the gap between and is actually upper bounded by , which is of course than , but one to evaluate . 
 It is easy to see that time between two causal an output process which causality with a rate distortion pair corresponding to the linear combination of , 
 . Thus , in some , one could get a bound than by considering the boundary of the convex hull of the region above and then . However , such bound would be much more involved to compute , since it to evaluate not only , but also the already convex hull . 
 It is also worth that the first term within the on the of becomes smaller when is reduced . This difference , which from inequality is 
 Fig . . channel within a perfect reconstruction system by the causal filter . 
 always , could be taken as a measure of the of the of especially when . Indeed , as a white process , to zero . 
 It can be seen from that the upper bound for the information theoretic among all so far . Although it does not seem to be feasible to obtain a closed form expression for , we show in the next section how to get arbitrarily close to it . 
 In this section , we present an iterative procedure that one to calculate with arbitrary accuracy , for any . 
 In addition , we will see that this procedure a characterization of the in a feedback that which sample . 
 To derive the previously , we will work on a scheme of an channel and a set of causal , as in Fig . . In this scheme , the source is and stationary , with , and is assumed to 
 is a zero mean process with i . i . independent of . Thus , between and the channel . The filter is stable and strictly causal , i . e ., it at least a one sample delay . The are causal and stable . The idea , to be in the remainder of this section , is to first show that with the that minimize the variance of the reconstruction error for a fixed ratio , the system of Fig . a mutual information rate between source and reconstruction equal to , with a reconstruction equal to . We will then show that finding such is a convex optimization problem , which naturally an iterative procedure to solve it . 
 In order to analyze the system in Fig . , and for notational convenience , we define 
 The perfect reconstruction condition a division , convenient of the optimization problem associated with it . On the one hand , because of , the net effect of the channel and the and is to introduce 
 colored stationary additive noise , namely , independent of the source . The of this noise is given by 
 The diagram in Fig . how the signal transfer function and the noise transfer function act upon and to yield the output process . 
 On the other hand , by looking at Fig . , one can see that also the role of a filter , which can be to reduce additive noise at the expense of linear upon the stationary corrupted by additive stationary noise 
 On the of , the first term is the variance of the additive , source independent , noise . The second term to the error due to linear distortion , that is , from the deviation of from a unit gain . 
 Since we will be interested in , for any given and , the and in Fig . are chosen so as to minimize in , while still satisfying . From the viewpoint of the subsystem comprised of the and and the channel , as an error frequency weighting filter , see . Thus , for any and , the that minimize are those in , Prop . , by setting in b equal to . With the minimizer in , the variance of the source independent error term is given by 
 On the other hand , the filter needs to be strictly causal and stable . As a consequence , it that 
 which from formula see also the Bode Integral Theorem in . 
 Thus , from and , if one to minimize the reconstruction by choosing appropriate causal in the 
 where the space of all frequency that can be with causal . 
 From the lemma , whose proof can be found in Section , one can find either by the minimization in Definition or by Optimization Problem . In the following , we will pursue the latter approach . As we shall see , our formulation of Optimization Problem a convenient of its decision . In fact , it defined in a with respect to the set of all causal frequency involved . That result can be directly from the following key lemma , proved in Section : 
 Proof : With the change of in , we obtain , see . With this , Optimization Problem to finding the and that 
 Clearly , the space of frequency associated with causal transfer is a convex set . This that is a convex set . In addition , is also a convex set , and from Lemma , is a convex functional . Therefore , the optimization problem stated in , and thus 
 Lemma and the in Optimization Problem allow one to define an iterative algorithm that , as will be shown later , the information theoretic causal . Such algorithm is in iterative Procedure . 
 Notice that after Step in the first iteration of Procedure , the is comprised of only additive noise independent of the source . Step then the by source independent noise at the expense of linear distortion . Each step the until a local or global minimum of the is . Based upon the Problem , the following theorem , which is the main technical result in this section , convergence to the global minimum of the , say , for a given end to end mutual information . Since all the in Optimization Problem are causal , the mutual information at this global minimum is equal to . 
 Theorem Convergence of Iterative Procedure : Iterative Procedure monotonically to the unique and that realize . More precisely , denote the 
 Indeed , after Step for the first time , the resulting rate is the quadratic for source uncorrelated in see also . 
 Fig . . Uniform scalar and dither forming an , the channel of the system from Fig . . 
 Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . 
 after the th iteration of Iterative Procedure at a target rate , we have that 
 Proof : The result directly from the fact that Optimization Problem is strictly convex in and , which was shown in Lemma , and from Lemma . 
 The theorem that the stationary information theoretic causal can be by Iterative Procedure . In practice , this that an approximation arbitrarily for a given can be if sufficient of the procedure are carried out . 
 The feasibility of running Iterative Procedure on being able to solve each of the minimization sub involved in and . We next show how these can be . 
 If is given , the minimization problem in Step of Iterative Procedure is equivalent to a feedback design problem with the constraint and with error weighting filter . Therefore , the solution to Step is given in closed form by , , and b , where in b is by . The latter 
 Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . 
 in characterize the frequency response of the optimal , and given . The existence of rational transfer and arbitrarily close in an sense to such frequency response is also shown in . 
 Finding the causal frequency response that for a given is equivalent to 
 for a given , where is as defined in . Since are convex , is a convex optimization problem . As such , its global solution can always be found iteratively . In particular , if is constrained to be an th order finite impulse response FIR filter with impulse response , such that the discrete time 
 is a convex functional . The latter directly from the convexity of and the linearity of . As a consequence , one can solve the minimization problem in Step , to any degree of accuracy , by over the of the impulse response of , standard convex optimization see , e .., . This approach also the benefit of being amenable to numerical computation . 
 It is interesting to note that if the order of the filter were not a restricted , then , after Iterative Procedure 
 Wiener filter i . e ., the causal estimator for the noisy signal that comes out of the perfect reconstruction system that . Notice also that one can get the system in Fig . to yield a realization of Iterative Procedure by simply to be . This would yield a system equivalent to the one that was analytically in . An important observation is that one could not obtain a realization such a system in one step by simply a Wiener filter by the causal estimator that is , a causal Wiener filter . To see , and would no longer be optimal for . One would then have to change , and then again , and so on , thus to carry out infinitely many recursive optimization . However , a causally truncated version of the non causal Wiener filter that could be used as an alternative starting guess in Step of the iterative procedure . 
 If the channel in the system of Fig . is by an , as shown in Fig . , then instead of the noise 
 , we will have an i . i .. process independent of , whose are uniformly distributed over the quantization interval . The dither signal , by , is an i . i .. sequence of uniformly distributed random , independent of the source . Let be the output of the . Denote the resulting input and the output to the , before and after the dither , respectively , as and , and let be the quantization noise by the . Notice that the of are independent , both mutually and from the source . However , unlike and , the and are not , since they contain of the uniformly distributed process . We then have the following . 
 Theorem : If the scheme shown in Fig . the by Iterative Procedure , and if long of the output of this system are entropy conditioned to the dither in a memoryless fashion , then an operational rate satisfying 
 , and linear time invariant a reconstruction error jointly stationary with the source , it that the operational rate distortion performance of the feedback thus is within 
 Remark : When the rate goes to infinity , so does . In that limiting case , the transfer function to unity , and it from that the optimal asymptotically satisfy 
 Moreover , when , the system of Fig . which , in this asymptotic regime , with 
 If the requirement of zero delay , which is than that of causality , was to be satisfied , then it would not be possible to apply entropy to long of . This would entail an excess bit rate not greater than bit per sample , see , e .., , Sec . . . Consequently , we have the following result . 
 Theorem : The of zero delay , say , can be upper bounded by the operational rate of the scheme of Fig . when each output value is entropy independently , conditioned to the current dither value . Thus 
 The . sample in , commonly to as the space filling loss of scalar quantization , can be reduced by vector quantization , . Vector quantization could be applied while causality and without delay if the of the source were dimensional . This would also allow for the use of entropy over 
 dimensional of , which the extra bit sample at the end of to sample see , Th . . . . 
 It is worth that Lemma and the an interesting fact : the rate loss due to causality for with memory , i . e ., the difference between the of causal and , is upper bounded by the sum of two . The first term is . sample , and from the space filling loss associated with scalar quantization , as was also pointed out in for the high resolution situation . This term is associated only with the . For a scalar stationary source , such excess rate can only be by jointly of consecutive source vector quantization , i . e ., by for or by several parallel . The second term can be to the reduced of causal , to those of or smoothing . The contribution of the causal filtering aspect to the total rate loss is indeed . This latter gap can also be associated with the performance loss of causal . 
 As a final remark , we note that the architecture of Fig . , which us to pose the search of as a convex optimization problem , is by no the only scheme capable of the upper and . For instance , it can be shown that the same performance can be removing either or in the system of Fig . , provided an entropy coder with infinite memory is used . Indeed , the theoretical among causal of the differential pulse code , with estimation at the end , been shown in a different setting . 
 To illustrate the upper in the previous , we here evaluate and , and calculate an approximation of via Iterative Procedure , for two zero mean AR and AR . These were by the recursion 
 where the of the process are i . i .. zero mean unit variance random . 
 Iterative Procedure was carried out by to be an eight tap FIR filter . For each of the target considered , the procedure was stopped after four complete . 
 The first order source Source was chosen by setting the of the in to be . This to zero mean , unit variance white noise through the coloring transfer function . The second order source Source of zero mean , unit variance white noise through the coloring transfer function . The resulting upper for Source and Source are shown in . and , respectively . As by and , all the upper for derived in to 
 in the limit of both large and small i . e ., when and , respectively . 
 For both , the gap between and is significantly smaller than . sample , for all at which was . Indeed , this gap is smaller than . 
 For the first order source , the magnitude of the of the FIR filter rapidly with coefficient index . For example , when running five of Iterative Procedure , a tenth order FIR filter for , for Source at sample , the was 
 Such fast decay of the impulse response of that , at least for AR , there is little to be by be an FIR filter of order . It is worth that , in the iterative procedure , the initial guess for is a unit scalar gain . The frequency response magnitude of is plotted in Fig . , together with and the resulting frequency after for a target rate of . 
 Notice that for Source , after four of Iterative Procedure , the for are almost identical to 
 , according to . This that Iterative Procedure fast convergence . For example , when four of Iterative Procedure to Source with a target rate of . sample , the after each iteration were . , . , . , and . , respectively . For the same source with a target rate of . sample , the distortion took the . , . , . , and . as the . A similar behavior is for other target , and for other of in as well . Thus , at least for AR , one close to the global optimum after just three . 
 In this paper , we have and upper to the causal and zero delay rate distortion function for stationary and as the distortion measure . We first that for with bounded differential entropy rate , the causal does not exceed the information theoretic by more than approximately . sample . After that , we derived an explicit expression for the information theoretic under per sample distortion a constructive method . This result was then for a closed form formula for the causal information theoretic of first order Gauss under an average distortion constraint . 
 We then derived three closed form upper bounding to the difference between and . Two of these bounding are than the previously best known bound of . sample , at all . We also provided a fourth upper bound to that is constructive . More precisely , we provide a practical scheme that this bound , based on a noise shaped predictive coder of an channel surrounded by , post , and feedback . For a given source spectral density and desired distortion , the design of the is convex in their frequency . We an iterative algorithm , which is to converge to the optimal set of unique . Moreover , the mutual information across the channel , monotonically to . Thus , one to solve the more complicated minimization of the mutual information over all possible conditional satisfying the distortion constraint . To achieve the upper on the operational , one may simply replace the channel by a scalar and memoryless entropy conditioned to the dither . 
 We will first show that can be by a vector channel between two square matrices . It was already established in Lemma that an output to a realization of only if it is jointly with the source 
 where the inverse of from the fact that bounded differential entropy . It is clear from and the joint between and that the causality condition is satisfied if and only if the matrix 
 We will now describe a simple scheme which is capable of the joint statistics between and any given jointly with satisfying . 
 Suppose is first by a matrix yielding the random vector . Then a vector with i . i .. with unit variance , independent from , say , is added to , to yield the random vector . Finally , this result is by a matrix to yield the output 
 On the other hand , the joint second order statistics between and are fully by the matrices 
 It can be seen from these that all that is for the system previously to reproduce any given pair of covariance matrices is that the matrices and 
 Thus , can be chosen , for example , as the lower triangular matrix in a factorization of . With this , a tentative solution for could be as , which would satisfy if and only if . The latter if and only if recall that is nonsingular since bounded differential entropy . We will now show that this condition actually by a contradiction argument . Suppose 
 . Since , the former supposition is equivalent to . If this were the case , then there would exist such that and . The latter , combined with , would imply 
 . One could then construct the scalar random variable , which would have nonzero variance . The of from is given by 
 From this , and in view of the fact that is with nonzero variance , we conclude that would be unbounded . However , by construction , the chain , and therefore by the Data Inequality we would have that 
 , that is unbounded too . This the assumption that is a realization of , leading to the conclusion that . There 
 is to satisfy , and thus for every , there exist matrices and which yield an output vector satisfying . 
 The first equality from the data inequality and the fact that is from . To prove that the second equality in , we will prove that 
 . We first have , from , that , which combined with the identity immediately that 
 where the orthogonal projection operator onto a given subspace . Since and is orthogonal to the other two on the of , we have that 
 where the last equality from the fact that is i . i .., which that is independent of the other two in the expression . On the other hand , from 
 Thus , we have , shown at the bottom of the page , where comes from the data inequality , and 
 from the fact that and from . To complete the proof of the second equality in , we note that the data inequality also . 
 Finally , and and replace the noise by the vector of noise with unit variance by independently operating , with their being jointly conditioned to the dither , then the operational data rate would be upper bounded by 
 where is the output of the channel . Since the distortion by the is the same as that with the original channel , we conclude that 
 First , following exactly the same proof as in Lemma in the Appendix , it is straightforward to show that 
 Now , consider the following family of . For some positive integer , the entire source sequence is in of contiguous . and of each block is independent of the and of any other block . As in the scheme in the second part of the proof of Lemma , each source block is by the optimal matrix , the resulting block being and parallel and independent , with their jointly entropy conditioned to the dither . When , the result is then by the optimal post matrix in the proof of Lemma . 
 For such an pair , and from , the operational rate after have been reconstructed is 
 where rounding to the nearest integer since the th sample is reconstructed only after of length are . On the other hand , since the variance of each reconstruction error sample cannot be than the variance source , we with the first is upper bounded as 
 where rounding to the nearest smaller integer . Therefore , for any finite , the average distortion of this scheme when i . e ., when we consider the entire source process . Also , from and we conclude that 
 for every finite . Our aim is to use this result to show that . Since is valid only for 
 finite of , we must resort to the convergence of as . First of all , since is bounded , 
 From Lemma , for any given reconstruction error covariance matrix , the mutual information is if and only if the output is jointly with the source . In addition , for any information between and a jointly 
 , the variance of every reconstruction error sample is if and only if is the estimation error resulting from from , that is , if and only if 
 Thus , hereafter we restrict the analysis to output jointly with and causally related also satisfy . For any such output process , say the following : 
 and thus equality in if and only if the following chain is satisfied : 
 is lower bounded by the of , which in turn only on the error associated with . We shall now see that this lower bound is by a unique set of error , and then show that the resulting bound is achievable while these error . , we have that 
 With this , and since the of when any error variance , the minimum value of the of subject to the 
 see . Therefore , for all causally related to and jointly with satisfying the distortion con 
 Now , we will show that for any distortion schedule , the output by the recursive algorithm of Procedure is such that the lower bound , thus being a realization of . 
 and the , and Source Past Independence which are necessary and sufficient to attain equality in . 
 pose causality . Then , since , it from that the top left square is lower triangular , being 
 This that the top in the th column of depend only on the of above its th row . that , we conclude that is also lower triangular , and thus also causality . Notice that for any given causality up to sample , the by Step is the only vector consistent with satisfying causality up to the th sample . 
 : for . , and mean all . Therefore , the reconstruction vector by the above algorithm for all . 
 Source Past Independence : Since all are jointly , condition is equivalent to 
 Since the above algorithm an output which , , and , for all , this output equality in , thus being a realization of . Notice that once the are given , each step in the recursive algorithm the only and that satisfy , and . Therefore , for any given distortion schedule , the latter algorithm the unique output that . This the proof . 
 Consider the first of input and output . The average distortion constraint here the form 
 Then , we have , shown at the bottom of the page , where the last inequality from inequality and the fact that is a convex function of . Equality is if and 
 . Given that the of is when constraint is active i . e ., by making , we can attain equality in and minimize its by 
 The first inequality in directly from and . For a plain channel with noise variance , the mutual information between source and reconstruction is 
 In both the end to end distortion can be reduced by a scalar gain after the test channel . The optimal minimum gain is . The mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it . However , now the resulting end to end distortion is . Therefore , for a given end to end distortion , the distortion between the source and the signal before the optimal scalar gain is 
 where from , , and and by that , from , and from inequality . Notice that the of the first term on the of . 
 The middle term on the of directly from . Finally , for close to , a bound than can be from a as : 
 which is precisely the third term on the of . In the above , a trivially since , and 
 b from inequality . Therefore , equality in b if and only if is white . The validity of the chain of in directly from and . This the proof . 
 Immediately afterward we prove that , despite the distortion and causality , the scheme in Fig . enough of freedom to turn all the above into . That that if we are able to globally over the of the system while satisfying the distortion and causality , then that , say , must satisfy 
 We now proceed to demonstrate the validity of and to state the under which are . The first equality in from the fact that is a i . i .. process . Inequality from the following : 
 where is the signal at the output of , see Fig . . In the above , from the fact that and are independent and from the fact that is strictly causal . As a consequence , is independent of , for all . Inequality from the property , with equality if and only if and are independent , i . e ., if and only if is white . Similarly , since the of are independent . By that is a linear combination of and , it immediately that is independent from upon knowledge of , which to . On the other hand , from the fact that 
 . Equality in from the fact that , if is known , then can be from 
 , and vice , see Fig . . Equality from the fact that there no feedback from to , and thus the chain . On the other hand with equality if and only if is invertible for all for which . Finally , directly from the 
 Since is by definition an , it that , for every , there an output process jointly with , satisfying the causality and distortion and such that . 
 Such output can be by its noise , say , and its signal transfer function , say , by the model in Fig . . 
 chosen in a for simplicity and because , as we shall see next , we have enough of freedom to do so without compromising rate distortion performance . the system of formed by a , c , and b , we obtain 
 that their squared equal their in . To do so , we will make use of the Wiener theorem Theorem in the Appendix . 
 Substitution of the of the second equation of c into the above , together with the Wiener theorem , that there a causal , stable and minimum phase transfer 
 Since can be chosen to be arbitrarily small , it can always be chosen so that , which . Therefore 
 from Theorem that if we construct an output process by the recursive algorithm of that theorem , with the choice , for all , then this output process is such that . 
 Proposition Column Correspondence : Let be a random vector source with covariance matrix . A reconstruction random vector 
 , with , be a random source vector with covariance matrix . A reconstruction random vector 
 Proof : Let us first introduce the notation , the top left of any given square matrix 
 Lemma that , if the reconstruction is the output of a causal Wiener filter applied to the noisy source for some noise vector a condition equivalent to , then and have identical on and above their main . 
 Theorem see ,. : Let be a function defined on . There a unique stable , causal and minimum phase transfer function such that if and only if 
 In ,. , it is stated that is a sufficient condition for such a to exist . However , from , Note ,. and the discrete continuous equivalence in ,. , it that is also necessary . 
 We study the increase in per sample differential entropy rate of random and after being through a non minimum phase discrete time , linear time invariant filter . For discrete time and random , it long been established that this entropy gain ,, the integral of . It is also known that , if the first sample of the impulse response magnitude , then the latter integral the sum of the logarithm of the of the non minimum phase of i . e ., its outside the unit circle , say . These have been derived in the frequency domain as well as in the time domain . In this note , we begin by showing that time domain , which consider finite length and then to infinity , have significant mathematical and , therefore , are inaccurate . We discuss some of the of this oversight when considering random . We then present a rigorous time domain analysis of the entropy gain of for random . In particular , we show that the entropy gain between equal length input and output is upper bounded by and if and only if there an output additive disturbance with finite differential entropy no matter how small or a random initial state . Unlike what with linear , the entropy gain in this case on the distribution of all the involved . Instead , when the input differential entropy to that of the entire longer output of , the entropy gain irrespective of the and without the need for additional exogenous random . We illustrate some of the of these by their in three different . Specifically : a simple derivation of the rate distortion function for non stationary , for 
 In his seminal paper , gave a formula for the increase in differential entropy per degree of freedom that a continuous time , band limited random process u after passing through a linear time invariant continuous time filter . In this formula , if the input process is to a frequency range ,, differential entropy rate per degree of freedom u , and the filter frequency response , then the resulting differential entropy rate of the output process is given by , Theorem 
 The last term on the right hand side of can be understood as the entropy gain entropy amplification or entropy boost by the filter . proved this result by that an filter can be seen as a linear operator that selectively scales its input signal along infinitely many , each of them an orthogonal component of the source . The result is then by writing down the determinant of the of this operator as the product of the frequency response of the filter overfrequency , logarithm and then taking the limit as the number of frequency to infinity . 
 An analogous result can be for discrete time input u and output , and an discrete time filter by them to their continuous time , which 
 is the differential entropy rate of the process u . Of course the same formula can also be by the frequency domain proof technique that in his derivation of . 
 The rightmost term in , which to the entropy gain of , can be related to the structure of this filter . It is well known that causal with a rational transfer function such that i . e ., such that the first sample of its impulse response unit magnitude , then 
 where i are the of and , : is the open unit disk on the complex 
 plane . This a straightforward way to evaluate the entropy gain of a given filter with rational transfer function . In addition , that , if , then such gain is greater than one if and only if outside . A filter with the latter property is said to be non minimum phase ; conversely , a filter with all its said to be minimum phase . 
 appear naturally in various . For instance , any unstable system via linear feedback control will yield transfer which are , . Additionally , also appear when a discrete time with zero order hold equivalent system is from a plant whose number of its number of by at least , as the sampling rate , Lemma . . On the other hand , all linear phase , which are specially for audio and , are , . The same is true for any all pass filter , which is an important building block in signal , . 
 An alternative approach for the entropy gain of is to work in the time do 
 where yn , y y yn and the random vector un is defined likewise . From this , it is clear that 
 regardless of whether i . e ., the polynomial g with magnitude greater than one , which clearly and . Perhaps surprisingly , the above contradiction not only been in previous works such as , , but the time domain formulation in the form of been as a to prove or disprove see , for example , the reasoning in ,. . 
 A reason for why the contradiction between , and can be from the analysis in for an a noisy feedback loop , as the one in Fig . . In 
 Figure . Left : a noisy feedback loop . Right : equivalent system when the feedback channel is noiseless and unit gain . 
 this scheme , a causal feedback channel which the output an exogenous noise random process c to generate its output . The process c is assumed independent of the initial state of , by the random vector x , which finite differential entropy . For this system , it is shown in , Theorem . that 
 with equality a deterministic function of . Furthermore , it is shown in , Lemma . that if 
 x and the steady state variance of asymptotically bounded as , then 
 where pi are the of . Thus , for the case in which , the output y is the result of filtering u by a filter as shown in Fig . right , and the resulting entropy rate of will exceed that of u only if there is a random initial state with bounded differential entropy see a . Moreover , under the latter , , Lemma . that if is stable and x , then this entropy gain will be lower bounded by the right hand side of , which is greater than zero if and only . However , the result in b does not provide under which the equality in the latter equation . 
 Additional and intuition related to this problem can be from in . There it is shown that if is a two sided stationary random process by a state space recursion of the form 
 for some A , , , with unit variance i . i .. u , then its entropy rate will be exactly i . e ., the differential entropy rate of u plus the of with i now being the of A outside the unit circle . However , as noted in , if the same system with zero or deterministic initial state is excited by a one sided infinite 
 i . i .. process u with unit sample variance , then the asymptotic entropy rate of the output process y is just i . e ., there is no entropy gain . Moreover , it is also shown that if is a random sequence with positive definite covariance matrix and , then the entropy rate that of u by the of . This that for an system which a representation of the form , the entropy gain for a single sided i . i .. input is zero , and that the entropy gain from the input to the output plus disturbance is , for any disturbance of positive definite covariance matrix no matter how small this covariance matrix may 
 The previous analysis that it is the absence of a random initial state or a random additive output disturbance that the time domain formulation yield a zero entropy gain . But , how would the addition of such finite energy exogenous random to actually produce an increase in the differential entropy rate which asymptotically the of In a sense , it is not clear from the above what the necessary and sufficient are under which an entropy gain equal to the of the analysis in only a set of sufficient and on second order statistics and to derive the previously . Another important observation to be made is the following : it is well known that the entropy gain by a linear is independent of the input statistics . However , there is no reason to assume such independence when this entropy gain as the result of a random signal to the input of the , i . e ., when the by itself does not produce the entropy gain . Hence , it remains to characterize the set of input statistics which yield an entropy gain , and the magnitude of this gain . 
 The first part of this paper to these . In particular , in Section explain how and when the entropy gain in the above , starting with input and output of finite length , in a time domain analysis similar to , and then taking the limit as the length to infinity . In Section it is shown that , in the output plus disturbance scenario , the entropy gain is at most the of . We show that , for a broad class of input not necessarily or stationary , this maximum entropy gain is only when the disturbance bounded differential entropy and its length is at least equal to the number of non minimum phase of the filter . We provide upper and lower on the entropy gain if the latter condition is not met . A similar result is shown to hold when there is a random initial state in the system with finite differential entropy . In addition , in Section we study the entropy gain between the entire output sequence that a filter as response to a shorter input sequence in Section . In this case , however , it is necessary to consider a new definition for differential entropy , effective differential entropy . Here we show that an effective entropy gain equal to the of is provided the input finite differential entropy rate , even when there is no random initial state or output disturbance . 
 In the second part of this paper we apply the in the first part to three , namely , control , the rate distortion function for non stationary , and the channel capacity with feedback . In particular , we show that equality in b for the feedback system in Fig . left under very general even when the noisy . For the problem of finding the quadratic rate distortion function for non stationary auto regressive , previously in , we provide a simpler proof based upon the we derive in the first part . This proof the result stated in , to a class of non stationary . For the feedback capacity problem , we show that capacity based on a short random sequence as channel input and on a feedback filter which the entropy rate of the end to end channel noise such as the one in , crucially depend upon the complete absence of any additional disturbance anywhere in the system . Specifically , we show that the information rate of such capacity to zero in the presence of any such additional disturbance . As a consequence , the relevance of the robust i . e ., in the presence of feedback capacity of , which to be a fairly unexplored problem , becomes evident . 
 For any system , the transfer function to the transform of the impulse response g , g ,..., i . e . For a transfer function , we denote by n 
 the lower triangular matrix g as its first column . We write as a shorthand for the sequence x ,..., and , when convenient , we write in vector form as , x x , where transposition . Random are non , such as non and , such as . For matrices we use upper case , such as A . We write i A to the note the i th magnitude eigenvalue of A . If An , then 
 Figure . Linear , causal , stable and time invariant input and output , initial state and output disturbance . 
 Ai , the entry in the intersection between the i th row and the th column . We write , with i i , to refer to the matrix formed by the i to i of A . The expression m A m to the square sub matrix along the main diagonal of A , with its top left and bottom right on Am , m and Am , m , respectively . A diagonal matrix whose are the as 
 Consider the discrete time system in Fig . . In this setup , the input u is a random process and the a causal , linear and time invariant system with random initial state vector x and random output disturbance z . In vector notation , 
 where n is the natural response the initial state x . We make the following further the around it : 
 Assumption . is a causal , stable and rational transfer function of finite order , whose impulse 
 It is worth that there is no loss of generality in considering g , since otherwise one can write as g g , and thus the entropy gain by would be plus the entropy gain due to g , which an impulse response where the first sample . 
 Assumption . The disturbance z is independent of u and to a dimensional linear subspace , for some finite . This subspace is by the of a matrix 
 where for the countably infinite size of , such that z . Equivalently , z 
 , where the random vector s , z finite differential entropy and is independent of u . 
 As in the Introduction , we are interested in the entropy the presence or absence of the random u , by 
 In the next section we provide geometrical insight into the behaviour of for the situation where there is a random output disturbance and no random initial state . A formal and precise treatment of this scenario is then in Section . The other are considered in the subsequent 
 In this section we provide an intuitive geometric interpretation of how and when the entropy gain defined in . This understanding will justify the introduction of the notion of an random process in Definition below , which will be shown to play a key role in this and in related . 
 Suppose for the moment Fig . is an FIR filter with impulse response g , g , , i . Notice that this choice , and thus one non minimum phase zero , at . The associated matrix for is 
 whose determinant is clearly one indeed , all its are . Hence , as in the introduction , u , and thus G and in general does not introduce an entropy gain by itself . However , an interesting phenomenon becomes evident by looking at the singular value decomposition 
 of G , given by , where Q and R are unitary matrices and D , d , d , d . In this case , D . , . , . , and thus one of the singular of G is much smaller than the although the product of all singular , as . As will be shown in Section , for a stable such uneven distribution of singular only when non minimum phase . The effect of this can be by looking at the image of the cube 
 , through G shown in Fig . . If the input u were uniformly distributed over this cube of unit 
 Figure . Image of the cube , through the square matrix with , and . 
 volume , then would distribute uniformly over the unit volume parallelepiped in Fig . , and hence u . 
 Now , if we add to a disturbance z , with distributed over . , . independent of u , and with R , the effect would be to thicken the support over which the resulting random vector y z is distributed , along the direction pointed by . with the direction along which the support of is given by q , , the first row of Q , then the resulting support would have its volume significantly , which can be associated with a large increase in the differential entropy of y with respect to u . Indeed , a relatively small variance an still produce a significant entropy gain . 
 The above example that the entropy gain from un to yn as a combination of two . The first of these is the uneven way in which the random vector is distributed over . The second factor is the alignment of the disturbance vector with respect to the span of the subset , i i of of , associated with singular of , indexed by the in the set . As we shall discuss in the next section , non minimum phase , then , as , there will of going to zero exponentially . Since the product of the singular of for all , it that , i must grow exponentially with , where , i is the i th diagonal entry of . This that the span of , i i , compensating its shrinkage along the span of , i i , thus keeping un for all . Thus , as , any small disturbance distributed over the span of , i i , added to , will keep the support of the resulting distribution from shrinking along this subspace . Consequently , the expansion of the span of , i i is no longer , yielding an entropy increase proportional to log , i . 
 The above analysis one to anticipate a situation in which no entropy gain would take place even when some singular of tend to zero as . Since the increase in entropy is made possible by the fact that , as , the support of the distribution of along the span of , i i , no such entropy gain should arise if the support of the distribution of the input un accordingly along the pointed by the , i i of . 
 An example of such situation can be easily as : Let in Fig . have phase and suppose that u is as , where u is an i . i .. random process with bounded entropy rate . Since the determinant of n for all , we have that un u n , for all . On the other hand , yn nu n u n . Since for some finite 
 The preceding discussion that the entropy gain produced the situation shown in Fig . on the distribution of the input and on the support and distribution of the disturbance . This in stark contrast with the well known fact that the increase in differential entropy produced by an invertible linear operator only on its , and not on the statistics of the input . We have also seen that the distribution of a random process along the different within the space which it a key role as well . This the need to specify a class of random which distribute more or less evenly over all . The following section a rigorous definition of this class and a large family of belonging to it . 
 We begin by formally the notion of an entropy balanced process u , being one in which , for every finite , the differential entropy rate of the orthogonal projection of un into any subspace of dimension the entropy rate of u . This idea is precisely in the following 
 Definition . A random process is said to be entropy balanced if , for every , 
 for every sequence of matrices , with . Equivalently , a random process is entropy balanced if every unitary transformation on 
 sequence yn that one cannot predict its last with arbitrary accuracy by its previous , even to infinity . 
 We now characterize a large family of entropy balanced random and establish some of their . Although intuition may suggest that most random such as i . i .. or stationary should be entropy balanced , that statement rather difficult to prove . In the following , we show that the entropy balanced condition is met by i . i .. with per sample probability density function being uniform , piece wise constant or . It is also shown that to an entropy balanced process an independent random independent of the former another entropy balanced process , and that filtering an entropy balanced process by a stable and minimum phase filter an entropy balanced process as well . 
 Proposition . Let u be a i . i .. random process with positive and bounded per sample 
 Lemma . Let u be an i . i .. process with finite differential entropy rate , in which each is distributed according to a piece wise constant in which each interval where this is constant measure greater than o , for some bounded away from zero constant o . Then u is entropy balanced . N 
 Lemma . Let u and v be mutually independent random . If u is entropy balanced , then 
 The working behind this lemma can be intuitively by that to a random process another independent random process can only increase the spread of the distribution of the former , which to balance the entropy of the resulting process along all in space . In addition , it from Lemma that all i . i .. a per sample which can be by uniform , piece wise constant or as many times as are entropy balanced . It also that one can have non stationary which are entropy balanced , since Lemma no for the process v . 
 Our last lemma related to the of entropy balanced that filtering by a stable and minimum phase filter the entropy balanced condition of its input . 
 Lemma . Let u be an entropy balanced process stable and minimum phase filter . Then the also an entropy balanced process . N 
 This result that any stable moving average auto regressive process from is also entropy balanced , provided the of the and regression correspond to a stable filter . 
 We finish this section by pointing out two of which are non entropy balanced , namely , the output of a filter to an entropy balanced input and the output of an unstable filter to an entropy balanced input . The first of these a central role in the next section . 
 In this section we formalize the which were qualitatively outlined in the previous section . Specifically , for the system shown in Fig . we will characterize the entropy gain defined in for the case in which the initial state x is zero or deterministic and there an output random disturbance of possibly infinite length z which Assumption . The following will be instrumental for that purpose . 
 Lemma . Let A be a causal , finite order , stable and minimum phase rational transfer function with 
 Lemma . Consider the system in Fig . , and suppose z Assumption , and that the input process u is entropy balanced . Let the of , where , ,... are the singular of , with , , ,, such that . the number of these singular which tend to zero exponentially as . Then 
 The previous lemma precisely the geometric idea outlined in Section . To see this , notice that no entropy gain is if the output disturbance vector is orthogonal to the space by the . If this were the case , then the disturbance would not be able fill the subspace along which is shrinking exponentially . Indeed , if for all , then 
 , and the latter sum out the one on the of , while limn n nun since u is entropy balanced . On the contrary and loosely speaking , if the projection of the support of onto the subspace by the is of dimension , then remains bounded for all 
 n , and the entropy limit of the sum on the of the possible entropy gain . Notice that because , and thus 
 this entropy gain from the uncompensated expansion of along the space by the 
 Lemma also the following corollary , which that only a filter with outside the unit circle i . e ., an transfer function can introduce entropy gain . 
 Corollary Minimum Phase do not Introduce Entropy Gain . Consider the system shown in Fig . and let u be an entropy balanced random process with bounded entropy rate . Besides Assumption , suppose that is minimum phase . Then 
 Proof : Since is minimum phase and stable , it from Lemma that the number of singular of which go to zero exponentially , as , is zero . Indeed , all the singular vary with . Thus and Lemma directly that the entropy gain is zero since the of is zero . 
 In this section we show that random satisfying Assumption , when added to the input u i . e ., before , do not introduce entropy gain . This result can be from Lemma , as stated in the following theorem : 
 Theorem Input do not Introduce Entropy Gain . Assumption . Suppose that u is entropy balanced and consider the output 
 where b a , with a being a random vector satisfying a , and where . Then , 
 Proof : In this case , the effect of the input disturbance in the output is the forced response it . This response can be as an output disturbance . Thus , the argument of the differential entropy on the of is 
 The proof is by substituting this result into the of and that 
 Remark . An alternative proof for this result can be given based upon the of an sequence , as . Since , , we have that un un . 
 Let and be a matrices with which satisfy and such that is a unitary matrix . Then 
 which is upper bounded for an and , the latter due to u being entropy balanced . On the other hand , since is independent of un , it that un un , 
 We show here that the entropy gain of a transfer function with outside the unit circle is at most the sum of the logarithm of the magnitude of these . To be more precise , the following assumption is . 
 Assumption . The and its transfer , of which are . the number of distinct , given by , i . e ., such that , with li being the multiplicity of the i th distinct zero . We denote by i , where : ,.. ,...,, the distinct zero of associated with the i th non distinct zero of , i . e ., 
 As can be from the previous in this section , we will need to characterize the asymptotic behaviour of the singular of . This is accomplished in the following lemma , which these singular to the of . This result is a generalization of the unnumbered lemma in the proof of , Theorem in the appendix as Lemma , which for FIR 
 transfer , to the case of infinite impulse response transfer i . e ., transfer . 
 where the in the sequence an , are positive and increase or decrease at most 
 Theorem . In the system of Fig . , suppose that u is entropy balanced and that and z satisfy and , respectively . Then 
 where , min , and is as defined in Assumption . Both are tight . The upper bound is if limn n n , where the unitary matrices the 
 Theorem . In the system of Fig . , suppose that u is entropy balanced and that 
 Assumption . Let z be a random output disturbance , such that i , i , and that . Then 
 Here we analyze the case in which there a random initial state x independent of the input u , and zero or deterministic output disturbance . 
 The effect of a random initial state in the output as the natural response it , namely the sequence n . Thus , yn can be written in vector form as 
 This that the effect of a random initial state can be as a random output disturbance , which us to apply the from the previous . 
 Recall from Assumption that is a stable and rational transfer function with . As such , it can be as 
 where is a filter only all the of , and is a FIR filter , all the of . 
 We have already established recall Theorem that the entropy gain by the minimum phase system is zero . It then that the entropy gain can be only by the of and an appropriate output disturbance . Notice that , in this case , the input process w to i . e ., the output sequence to a random input u is independent of since we have the natural response after , hose initial state is now zero . This condition us to directly use Lemma in order to analyze the entropy gain that u after being 
 Theorem . Consider a stable th order filter , and with a random initial state x , such that x . Then , the entropy gain due to the existence of a random initial 
 Proof : Being a and stable rational transfer function , can be as 
 where is a stable transfer function only all the of and with all its at the origin , while is stable and FIR filter , all the of . Let and be the natural of their common random initial state x , respectively , where , . Then we can write 
 Since is stable and , it from Corollary that un for all , and therefore 
 Therefore , we only need to consider the entropy gain by the possibly non minimum to a random output disturbance n , which is independent of the input . Thus , the of Lemma are met considering , where 
 the for , and , , ,. Consequently , it to consider the differential entropy on the of , whose argument is 
 where , un bounded entropy rate and is entropy balanced since is the natural response of a stable system and because of Lemma . We remark that , in , is not independent of x , which one from the proof of Theorem directly . 
 On the other hand , since is FIR of order at most , we have that , where 
 is a non singular upper triangular matrix independent of . Hence , can be written as 
 , where and , . According to , the entropy gain in as long as is lower bounded by a finite constant or if it sub linearly as . Then , we need m n to be a full row ranked matrix in the limit as . However , 
 where m the the . We will now show that these 
 do not go to zero as . Define the matrix such that m . Then , it that , 
 Hence , the minimum singular value of m is lower bounded by the singular value of , for all . But it was shown in the proof of Theorem see page that limn min . this result in and taking the limit , we arrive to 
 is upper and lower bounded by a constant independent v is entropy balanced , m , and , which that the entropy rate in the of to zero . The proof is finished by Lemma . 
 Theorem us to formalize the effect that the presence or absence of a random initial state on the entropy gain similar to those in Section . Indeed , if the random initial state x finite differential entropy , then the entropy gain , since the alignment between x and the is . This us to characterize the behavior of the entropy gain due only to a random initial state , when the initial state x can be written as pst , with , which that x an undefined or differential entropy . 
 Corollary . Consider an FIR , order filter , such that its random initial state can be written as x , where and . Then , 
 where , min ,. The upper bound in is when is a non singular matrix , with defined by n as in Theorem . 
 Proof : The effect of the random initial state to the output sequence y can be written as yn , 
 remains bounded , for , if and only if limn . 
 Define the rank of as ,... If m , then the lower bound is by in . Otherwise , enough such that , 
 We then proceed as the proof of Theorem , by considering a unitary matrix , and a matrix An such that 
 that the lower limit in the latter sum when is a full row rank matrix . the latter into the proof . 
 Remark . If the random initial state x is with , then the entropy gain by an FIR minimum phase at least log . Otherwise , the entropy gain could be identically zero , as long as the of fill only the orthogonal space to the span of the row in m , where En , and m are defined as in the proof of Theorem . 
 Both , Theorem and Corollary , reveal that the entropy gain as long as the effect of the random initial state with the first of , just as in the of the previous section . 
 If there are no and the initial state is zero , then the to an input un is given by . Therefore , the entropy gain in this case , as defined in , is zero , regardless of whether 
 Despite the above , there is an interesting question which , to the best of the knowledge , not been before : Since in any filter the entire output is longer than the input , what would happen if one the differential of the complete output sequence to that of the shorter input sequence As we show next , a proper definition of this question recasting the problem in of a new definition of differential entropy . After providing a geometrical interpretation of this problem , we prove that the new entropy gain in this case is exactly . 
 Suppose u is uniformly distributed over , , . the conventional definition of differential entropy of a random sequence , we would have that 
 In other , the problem in that although the output is a three dimensional vector , it only two of freedom , i . e ., it is restricted to a dimensional subspace of R . This is in Fig . , where the set , , is shown with the u plane , together with its image through as defined in . 
 As can be seen in this figure , the image of the square , through is a dimensional rhombus over which y , y , y uniformly . Since the intuitive notion of differential entropy of an ensemble of random such as how difficult it is to compress it in a fashion to the size of the region by the associated random vector , one could argue that the differential entropy of y , y , y , far from being , should be somewhat than that of u , u since the rhombus , a area than , . So , what does it mean that and why should y , y , y Simply put , the differential entropy to the volume by the support of the probability density function . our example , the latter three dimensional volume is clearly zero . 
 Figure . Support of u laying in the u plane to that of u the rhombus in R . 
 From the above discussion , the comparison between the differential of R and u R of our previous example should take into account in a two dimensional subspace of R . Indeed , since the multiplication by a unitary matrix does not alter differential , we could consider the differential entropy of 
 the matrix with in the singular value decomposition of 
 and is a unit norm vector orthogonal to the of and thus orthogonal well . We are now able to compute the differential entropy in R for , corresponding to the rotated version that its support is now with R . 
 The preceding discussion the use of a version of the notion of differential entropy for a random vector which the number of actually 
 Definition The Effective Differential Entropy . Let be a random vector . be written as a linear transformation , for some u , , then the effective differential entropy defined as 
 It is worth that differential entropy of a vector , whose support is greater than zero , from considering it as the difference between its absolute entropy and that of a random variable uniformly distributed over an dimensional , unit volume region of . More precisely , if in this case the probability density function of y y is integrable , then . . , 
 where is the discrete valued random vector resulting an dimensional uniform with cubic quantization with volume . However , if we consider a support to an dimensional subspace of , i . e ., AT , as in Definition , then the entropy of its version in , say , is distinct from Ay , the entropy of Ay in . Moreover , it turns out that , in general , 
 despite the fact that A . Thus , the definition given by does not yield consistent for the case wherein a random vector a support dimension i . e ., its number of of freedom smaller that its length If this were not the case , then we could redefine by , in a spirit similar to the one behind dimensional entropy . To see this , consider the case in which u uniformly over , and . Clearly , uniformly over the unit length segment the origin with the point . Then 
 The latter example further why the notion of effective entropy is appropriate in the setup considered in this section , where the effective dimension of the random does not coincide with their length it is easy to verify that the effective entropy not change if . Indeed , we will need to consider only which can be by multiplying some random vector u , with bounded differential entropy , by a tall matrix , with as in , which are precisely the by Definition . 
 Theorem . Let the entropy balanced random sequence u be the input of an filter , and let y be its output . Assume that is the transform of the length sequence . Then 
 Theorem that , when considering the full length output of a filter , the effective entropy gain is by the filter itself , without the presence of external random or initial . This may seem a surprising result , in view of the made in the previous , where the entropy gain only when such random exogenous were present . In other , when observing the full length output and the input , the maximum entropy gain of a filter can be in of the volume expansion by the filter as a linear operator , provided we measure effective differential instead of differential entropy . 
 Proof of Theorem : The total length of the output , will grow with the the input , FIR , and will be infinite , . Thus , we define the output length function 
 It is also convenient to define the sequence of matrices , with 
 G ni , i , ni , i . This one to write the entire output of a causal 
 Let the , where , is 
 where the first equality from the fact that un can be written as , which that un un . But 
 G . 
 The product , is a symmetric matrix , with its first column , h h , given by 
 Thus , the sequence to the to of those resulting from the complete convolution , even when the , where the time reversed perhaps infinitely large response . Consequently , the and  theorem , it that 
 In order to finish the proof , we divide by , take the limit as , and replace in the 
 In this section we obtain a simpler proof of a result by Gray , and , which the rate distortion function of a non stationary auto regressive process x of 
 Figure . Block diagram representation of how the non stationary source x is built and then reconstructed as u . 
 a certain class to that of a corresponding stationary version , under distortion . Our proof is based upon the in the previous , and the class of non stationary for which the in are valid . 
 To be more precise , let and be the impulse of two linear time invariant A and A with rational transfer 
 where pi , i ,...,. From these it is clear that A is unstable , A is stable , and 
 A A , ,. Notice also that lim A and lim A pi , and thus 
 Consider the non stationary random source x and the asymptotically stationary source by passing a stationary process w through A and A , respectively , which can be written as 
 A block diagram associated with the construction in Fig . . Define the rate distortion for these two as 
 where , for each , the are taken over all the conditional probability density 
 The above rate distortion have been in for the case in which w is an i . i .. process . In particular , it is explicitly stated in , that , for that case , 
 We will next provide an alternative and simpler proof of this result , and extend its validity for general not necessarily stationary w , the entropy gain of non minimum phase established in Section . Indeed , the approach in is based upon asymptotically equivalent matrices in of the covariance matrices . This w to be and i . i .. and A to be an all pole unstable transfer function , and then , the only non stationary is that from unstable . For instance , a innovation by an unstable filter A would yield a source which cannot be Gray and approach . By contrast , the reasoning behind our proof w be any process , and then let the source be Aw , with A unstable and possibly and stable as well . 
 Theorem . Let w be any stationary process with bounded differential entropy rate , and let 
 Thanks to the in the previous , it is possible to give an intuitive outline of the proof of this theorem given in the appendix , page by a sequence of block . More precisely , consider the shown in Fig . . In the top diagram in this figure , suppose the for the non stationary source . The sequence u is independent of , and the linear filter is such that the error a necessary condition for minimum . The filter is the product of A see in the appendix a stable , filter with unit frequency response magnitude such that . 
 If one now the filter towards the source , then the middle diagram in Fig . is . By doing this , the stationary source with an additive error signal u that the same asymptotic variance as u , reconstructed as u . From the invertibility of , it also that the mutual information rate between and that . Thus , the channel u the same rate and distortion as the . 
 However , if one now a short the error signal u as in the bottom diagram 
 Figure . Block diagram representation of the of in the proof of Theorem . 
 of Fig . , then the resulting additive error term u u will be independent of and will have the same asymptotic variance as u . However , the differential entropy rate of u will exceed that of u by the of . This will make the mutual information rate between and to be less than that between and by the same amount . Hence , be at most . A similar reasoning can 
 Here we revisit the setup shown in Fig . and in Section I . Recall from b that , for this general class of control , it was shown in , Lemma . that 
 By the in show next that equality in b provided the feedback channel the following assumption : 
 Figure . Top : The class of feedback by Assumption . Bottom : an equivalent form . 
 A stable rational transfer such that is , the same unstable as , and the feedback the plant . 
 F is any possibly non linear operator such that , , for all , and 
 An illustration of the class of feedback satisfying this assumption is on top of Fig . . Trivial of satisfying Assumption are a additive channel and by linear . Indeed , an system with a strictly causal transfer function , the feedback channel that Assumption is widely known as a noise shaper with input and post filter , used in , e .. . 
 Theorem . In the control system of Fig . , suppose that the feedback channel Assumption and that the input u is entropy balanced . If the random initial state of the plant , with , x , then 
 Proof : Let and , A . Then , from Lemma in the appendix , the output yn can be written as 
 where the initial state s to yn , the initial state x to the output of , and the initial state x of to yn . Since u is entropy balanced and c finite entropy rate , it from Lemma that u is entropy balanced as well . Thus , we can proceed as in the proof of Theorem to conclude that 
 The feedback information capacity of this channel is by a input , and is given by 
 Figure . Block diagram representation a non white the scheme considered in . 
 where is the covariance matrix of and , for every , the input is to depend upon the channel since there a causal , noise less feedback channel with one step delay . 
 In , it was shown that an auto regressive moving average process of th order , then can be by the scheme shown in Fig . . In this system , is a strictly causal and stable finite order filter and v is with for all such that is with a positive definite covariance matrix . 
 Here we use the in Section to show that the information rate by the capacity scheme in to zero if there any additive disturbance of length at finite differential entropy affecting the output , no matter how small . To see this , notice that , in this case , and for all n , 
 since In . From Theorem , this gap between differential is precisely the entropy gain by In to an input when the output is affected by the disturbance . Thus , from 
 Theorem , the capacity of this scheme will correspond to , where are the of , which is precisely the result stated in , Theorem . . 
 However , if the output is now affected by an additive disturbance d not passing through such that , , with , then we will have 
 But limn n In In , which directly from Theorem to each of the differential . Notice that this result irrespective of how small the power of the disturbance may be . 
 Thus , the capacity scheme in and further studied in , although of theoretical importance , would yield zero rate in any practical situation , since every real signal is unavoidably affected by some amount of noise . 
 This paper provided a geometrical insight and rigorous for the increase in differential entropy rate to as entropy gain by passing an input random sequence through a discrete time linear time invariant filter such that the first sample of its impulse response unit magnitude . Our time domain analysis us to explain and establish under what the entropy gain with what was by , who a approach to a related problem in his seminal paper . In particular , we that the entropy gain only if outside the unit circle i . e ., it is non minimum phase , . 
 This is not sufficient , nonetheless , since the input and output be u and , the difference 
 is zero for all , yielding no entropy gain . However , if the distribution of the input process 
 u a certain regularity condition defined as being entropy balanced and the output the form , an output disturbance with bounded differential entropy , we have shown that the entropy gain can range from zero to the sum of the logarithm of the of the of , depending on distributed . A similar result is if , instead of an output disturbance , we let have a random initial state . We also considered the difference between the differential entropy rate of the entire and longer output of and that of its input , i . e .,, where is the length of the impulse response of . For this purpose , we the notion of effective differential entropy , which can be applied to a random sequence whose support dimensionality smaller than its dimension . Interestingly , the effective differential entropy gain in this case , which is intrinsic to , is also the sum of the logarithm of the of the of , without the need to add or a random initial state . We have some of the of these in three . Specifically , we used the fundamental here to provide a simpler and more general proof to characterize the rate distortion function for non stationary and distortion . Then , we applied our to provide sufficient for equality in an information inequality of significant importance in control . Finally , we that the information rate of the capacity scheme in for the channel with feedback to zero in the presence of any additive disturbance in the channel input or output of sufficient finite length , no matter how small it may be . 
 yn , . Then , where In is the identity matrix . 
 Proof of Lemma : Let be the the sample is constant . Let be the of these . Define the discrete random process c , where i if and 
 where the inequality is due to the fact that un and yn are deterministic of un , and hence un yn . un from we obtain 
 where the last equality from Lemma see Appendix whose are met because , given , the sequence un independent each of them distributed uniformly over a possibly different interval with bounded and positive measure . The opposite inequality is by following the same as in the proof of Lemma , from onwards , which the proof . 
 Proof of Lemma : Let yn , , where is a unitary matrix and where and have . Then 
 Substituting this result into , dividing taking the limit as , and that , since 
 where n is a jointly sequence with the same second order moment as . Therefore , 
 that a bounded second moment at each entry i , and the latter inequality in , 
 Proof of Lemma : Let yn , where is a unitary matrix and where and have . Since , we have that 
 Let be the of , where An is an orthogonal matrix , and is a diagonal matrix with the singular of . Hence 
 It is straightforward to show that the diagonal in are lower and upper bounded by the and singular of , say and , respectively , which 
 where the last equality is due to the fact that u is entropy balanced . This the proof . 
 Proof of Lemma : The fact that limn is upper bounded directly from 
 the fact that A is a stable transfer function . On the other hand , An is positive definite with all its equal to , and so is positive definite as well , with limn . Suppose that limn . If this were true , then it would hold that limn A nA . But A n is the lower triangular matrix associated with A , which is stable since A is minimum phase , that limn A nA , thus leading to a contradiction . This the proof . 
 Notice that . Thus , it only remains to determine the limit of as . We will do this by a lower and an upper bound for this differential entropy and show that these converge to the same expression as . 
 where a from as well to the set , while and stem from the independence between u and z . Inequality is a consequence of , and e from to the set in the second term , and that is not reduced upon the knowledge of . 
 then , by and in , dividing by , and taking the limit , we obtain 
 n un Xi , i m un 
 where the last equality is a consequence of the fact that u is entropy balanced . 
 Notice that by Assumption and thus is restricted to the span of of dimension , for all . Then , for , one can construct a unitary matrix , such that the of An span the space by the of and such that B 
 . Therefore , from , z 
 where and KAn z are the covariance matrices of A and A , 
 respectively , and where the last inequality from . The fact that and are bounded and remain bounded away from zero for all , and the fact that min either sub exponentially since singular decay exponentially to zero , with , imply in that 
 Proof of Lemma : The transfer function can be as , where is stable and minimum phase and is stable with all the non minimum phase of , both being rational . From Lemma , in the limit as , the of are lower and upper bounded by min and T , respectively , where min 
 . Let and the , respectively , with , , , and , , , being the diagonal of the diagonal matrices respectively . Then 
 the i th row of by , i be , we have that , from the Courant theorem that 
 Notice that the of the matrix m n span a space of dimension , ,.., which that one can have m n if . In this case i . e ., if limn m n then the lower bound is by the latter expression into and Lemma . 
 We now consider the case in which limn m n . This condition that there large such that for all . Then , for exist unitary matrices 
 The first differential entropy on the of the latter expression is uniformly upper bounded because u is entropy balanced , m , and . For the last differential entropy , 
 notice that m . Consider the An m , being unitary , being diagonal , . We can then conclude that 
 that A and that is unitary , it is easy to show by the 
 with equality if and only if A . Substituting this into and then the latter into we arrive to 
 Substituting this into , the fact that u is entropy balanced and Lemma the upper bound in . Clearly , this upper bound is if , for example , n n is non singular for large , since , in that case , and we can choose An I 
 Proof of Theorem : As in , the transfer function can be as , where is stable and minimum phase and is a stable FIR transfer function with all the phase of in total . u n , nun , we have that yn nu n , u n un , and that is entropy balanced from Lemma . Thus , 
 This that the entropy gain of due to the output disturbance z to the entropy gain due to the same output disturbance . One can then evaluate the entropy gain of by Theorem to the filter instead of , which we do next . 
 Since only the z are non zero , it that in this case see Assumption . Therefore , m n m n and the sufficient condition given in Theorem will be satisfied for if limn , where now is the left unitary matrix in the . We will prove that this is the case by a 
 Then , there a sequence of unit norm , with for all , such that 
 such that an and . Then , from this definition and from , we have that 
 where the last equality from the fact that , by construction , is in the span of the , together with the fact that is unitary which that . Since the top m 
 where we have applied and the fact that m is bounded and does not depend on . Now , notice that is a matrix with the convolution the impulse response its time reversed version , respectively on its first row and column . It then from , Lemma . that 
 the inequality is strict because all the of are strictly outside the unit disk . Substituting this into we conclude that 
 which . Therefore , to a contradiction , the proof . 
 where b is the first sample in the impulse response of . Notice that that limn n limn n kunk for every sequence of random u with uniformly bounded variance . Since only stable and its coincide exactly with the of A , it that A is a stable transfer function . Thus , the asymptotically stationary process defined in can be as 
 where is a lower triangular matrix with its main diagonal equal to b . 
 The fact that is with b as in that for any un with finite differential entropy 
 For any given , suppose that is chosen and and un are distributed so as to minimize I ; un subject to the constraint E E I E kunk i . e ., , un is a realization of ,, yielding the reconstruction 
 Since we are considering mean squared error distortion , it that , for rate distortion , un must be jointly with . From these , define 
 y n , x n u n , n , n n u n . 
 where is a zero mean vector independent of u n , n with finite differential entropy such 
 that , k . Then , we have that , I ; yn a I ; I n ; n 
 where a from being invertible , is due to the fact that n n u n , because un . The equality from u n un see . Equality in e because n u n , and in because of . The last inequality because n n and n . But from Theorem , limn n u n un , and thus , limn n n ; n . 
 At the same time , the distortion for the source n when reconstructed as n is 
 where a because is bounded , and is due to the fact that , in the limit , is a unitary operator . the of and , we conclude that limn n n ; n ,, and therefore 
 purpose , consider now the asymptotically stationary source n , and suppose that n un ,. Again , n and un will be jointly , satisfying un the latter condition is for minimum . From this , one can propose an alternative realization in which the error The change of and the in this chain of is by the block shown in Fig . . sequence is u , , yielding an output n n u n with n u n . Then 
 where a by that n un and because un , from , is a consequence of n u n , from the fact that n n u n . Finally , e because is invertible for all . Since , asymptotically as , the distortion by yn for the non stationary source is the same which is when n is reconstructed as recall , we conclude that 
 Lemma . Let u be a random process with independent , and where each element is uniformly distributed over possible different , such that ai amin , i , 
 Proof : Without loss of generality , we can assume that ai , for all i otherwise , we could scale the input by amin , which would scale the output by the same proportion , increasing the input entropy by amin and the output entropy by log amin , without the result . The input 
 vector un is confined to an box Un the support of un of volume and entropy 
 these is determined by un to ai , and freely over . Thus , the volume of each box is the product of sizes ai of the associated selected free sweeping . But that ai for all i , the volume of each box can be upper bounded by . With this , the added volume of all the in the original box can be upper bounded as 
 have . From this definition , will distribute over a finite region 
 , corresponding to the projection onto the dimensional span of the of . Hence , is upper bounded by the entropy of a uniformly distributed vector over the same support , i . e ., by , where is the dimensional volume of this support . In turn , 
 is upper bounded by the sum of the volume of all dimensional in 
 the box in which un is confined , which we already by , and which is upper bounded 
 where a because is an orthogonal matrix . correspond to the jointly sequence with the same second order as , and that the distribution differential entropy for a given covariance , we obtain the upper bound 
 where a since the are independent , and from the fact that and from the Courant theorem . Since is bounded for all , 
 we obtain by substituting into that . The combination of this with , the proof . 
 We re state here for completeness and convenience the unnumbered lemma in the proof of , Theorem as : 
 Lemma . Let the function be as defined in but for a transfer function with no and only a finite number of , of which lie outside the unit circle . Then , 
 where the in the sequence an , are positive and increase or decrease at most 
 Lemma . Let be rational transfer function of relative degree , with initial a rational transfer function of initial state s . Let 
 where the initial state of is x and the initial state of can be taken to be x s . 
 Combining the above , it is found related to the input u by the following recursion : 
