This paper deals with control system design subject to average data-rate constraints. By focusing on SISO LTI plants, and a class of source coding schemes, we establish lower and upper bounds on the minimal average data-rate needed to achieve a prescribed performance level. We also provide a specific source coding scheme, within the proposed class, that is guaranteed to achieve the desired performance level at average data-rates below our upper bound. Our results are based upon a recently proposed framework to address control problems subject to average data-rate constraints. 
 The study of control systems subject to communication constraints has recently received much attention in the control community (see, e.g., the papers in the special issue [1]). Within this framework, a key question relates to the trade-offs between control objectives and communication constraints. This paper focuses on the interplay between average data-rate constraints (in bits per sample) and stationary performance in a class of networked control systems (NCSs). 
 When stability is the sole control objective, the results of [2] guarantee that, for a noisy LTI plant model and subject to mild conditions on the noise sources statistics, it is possible to find causal coders, decoders and controllers such that the resulting closed loop system is mean square stable, if and only if the average data-rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. This result establishes a fundamental separation line between what is achievable in NCSs over digital channels and what is not, when the problem of interest is mean square stability (see also the thorough discussion in the survey paper [3]). 
 When performance bounds subject to average data-rate constraints are sought, there are relatively fewer results available. There exist lower bounds on the mean square norm of the plant state that make explicit the fact that, as the average data-rate approaches the absolute minimum for stability, the performance becomes arbitrarily poor when disturbances are present [2,3]. This holds irrespective of how the coder, decoder and controller are chosen. Unfortunately, it is unclear whether or not these bounds are tight in general [3]. 
 A more general performance-oriented approach has been pursued in [3,4]. In those works, conditions for separation and certainty equivalence have been investigated in the context of quadratic stochastic problems for fully observed plants with data-rate constraints in the feedback path. If the encoder has a specific recursive structure, then certainty equivalence and a quasi-separation principle hold [3]. This result is interesting, but [3] does not give a computable characterization of the optimal encoding policies. A similar drawback is shared by the results reported in [4]. In that work, performance related results are expressed in terms of the so-called sequential rate-distortion function, which is very difficult to compute in general. For fully observed Gaussian first order autoregressive systems, [4] provides an expression for the sequential rate-distortion function. However, it is not clear from the results in [4] whether or not the sequential rate-distortion function is operationally tight (see Section IV-C in [4]). 
 Other works related to the performance of control systems subject to data-rate constraints are reported in [5] and [6]. The first work focuses on noiseless state estimation subject to data-rate constraints under three different criteria. The case most relevant to this work uses an asymptotic (in time) quadratic criterion to measure the state reconstruction error. For such a measure, it is shown in [5] that the bound established in [2] is sufficient to achieve any prescribed asymptotic distortion level. This is achieved, however, at the expense of arbitrarily large estimation errors for any given finite time. This feature of the solution makes the conclusions in [5] too optimistic. On the other hand, [6] considers non-linear stochastic control problems over noisy channels, and a functional (i.e., not explicit) characterization of the optimal control policies is presented. 
 In this paper, we focus on SISO LTI plants. Our main contribution is a characterization of upper and lower bounds on the minimal average data-rate that allows one to attain a given performance level (as measured by the stationary variance of the plant output). To that end, we focus on a specific class of source coding schemes that contains, as special cases, the coding schemes studied in [7,8]. We also provide a specific source coding scheme, within the proposed class, that is guaranteed to achieve the desired performance level at average data-rates below our upper bound. Instrumental to our results is the characterization of the minimal signal-to-noise ratio (SNR) that guarantees a prescribed closed loop performance level in a related class of NCSs (cf. [9,10]). 
 The remainder of this paper is organized as follows: Section 2 presents the problem of interest in this paper, and defines the class of considered source coding schemes. Section 3 establishes, for the class of source coding schemes introduced in Section 2, a relationship between average data-rates and an internal SNR. Section 4 focuses on the interplay between SNR constraints and closed loop performance. These results are then used in Section 5 to present our main contribution. Section 6 draws conclusions. 
 Notation: R denotes the set of real numbers, R+ denotes the set of strictly positive real numbers,   stands for the magnitude (absolute value) of x. Rp is the set of 
 all proper and real rational SISO transfer functions,RH2	RH8 contains all stable transfer functions in, and U8 contains all the biproper andRp, contains all the strictly proper transfer functions in RH8 
 minimum-phase transfer functions in RH8. L1 and L2 are defined as usual, and the associated norms are denoted by ||·||1 and ||·||2, respectively [11]. 
 This paper focuses on the NCS of Figure 1. In that figure, G is a SISO LTI plant, u is the control input, y is the plant output, and d is an input disturbance. The feedback path in Figure 1 comprises an error-free digital channel, and thus quantization becomes mandatory. This task is carried out by an encoder whose output correspond to the binary words sc. These symbols are then mapped back into real numbers by a decoder. It is clear that the encoder and decoder also embody a controller for the plant. As will become clear as we proceed, a distinction between the controller task and the encoder-decoder task is rather artificial in our setup. 
 Assumption 1 The plant G is SISO, LTI, strictly proper, free of unstable hidden modes, and has no poles or zeros on the unit circle. The initial state of the plant xo, and the disturbance d, are such that 
 (xo,d) is jointly second order Gaussian; d is stationary and has spectral factor ?d ? U8.  In this paper we focus on source coding schemes (i.e., encoder-decoder pairs) of the following type: 
 Definition 1 The source coding scheme of Figure 1 is said to be linear if and only if its input y and output u are related via 
 where q is a second order zero-mean i.i.d. sequence, q is independent of (d,xo), and Tq,Ty ? Rp are the transfer functions of LTI systems with deterministic initial states. 
 The class of linear coding schemes extends the class of so-called independent and i.i.d. coding schemes introduced in [7,8]. We acknowledge that it is a restricted class of coding schemes. However, its simplicity allows one to actually address control system design problems subject to average data-rate constraints. 
 Any linear source coding scheme can be written as shown in Figure 2(a), where E and H¯ are proper transfer functions. The design of E and H¯ is however non-trivial. Indeed, it is easy to see that optimally designing E and H¯ amounts to solving an optimal control problem subject to sparsity constraints [12]. To the best of our knowledge, the only known sufficient conditions (see [12]) that allow one to pose such problems as convex ones are not satisfied in our case. 
 Motivated by the previous discussion, we introduce the alternative rewriting of a linear source coding scheme shown in Figure 2(b), where H,E,C,F ? Rp are the transfer functions of filters with 
 deterministic initial states. In the scheme of Figure 2(b), F is redundant. However, as will become clear below, this over-parametrization of linear source coding schemes allows one to construct a convex optimization problem that is equivalent to that of optimally designing H¯ and E in Figure 2(a). 
 When the linear source coding scheme of Figure 2(b) is used in the NCS of Figure 1, the feedback system of Figure 3 arises. For future reference we define 
 Remark 1 In the remainder of this paper, when we refer to a linear source coding scheme, it must be understood that we refer to the specific architecture of Figure 2(b), where q satisfies the conditions in 
 In this section we make a connection between the average data-rate across a linear source coding scheme, and the stationary second order properties of the auxiliary signals w and v in Figure 2. In 
 order to do so, we first make the relationship between those signals and the channel symbols sc in Figure 1 explicit. 
 Without loss of generality, we assume that the link between v and w in Figure 2 is given by the scheme of Figure 4 [14]. In that figure, E, D, H and H-1 are causal systems such that 
 where Ek, Dk, Hk, Hk-1 are (possibly nonlinear time-varying) deterministic mappings, and SE(k), SD(k), SH(k) correspond to side information that becomes available at time instant k at the encoder side, decoder side, and both at the encoder and decoder sides, respectively. The range of Ek is assumed to be countable, and that of Hk, to be a countable set of prefix-free binary words [15]. The mappings Hk and Hk-1 are chosen so as to satisfy 
 for any , and any k ? N0. Condition (4) makes explicit the fact that the blocks H and H-1 act as a transparent link between the output of E and the input of D in Figure 4. Since we assume an error free digital channel, and (4) holds, it follows that ˆs = s. 
 It is clear that, when one uses the scheme of Figure 4 as the link between v and w in a linear source coding scheme, one needs to focus on mappings Ek,Dk,Hk,Hk-1 such that the process 
 satisfies the conditions in Definition 1. We also note that, since ˆs = s, the feedback link between w and the encoder side in Figure 2 does not require a physical channel. (To make w available at the encoder side, it suffices to replicate D at the encoder side.) 
 We denote the expected length of the symbol sc(i) by R(i), and define the average data-rate across the considered source coding scheme as 
 (b)	The mapping Di is such that, ?i ? N0, there exists another deterministic mapping). gi such that   (i.e., D is invertible upon knowledge of SD 
 Assumption 2(a) is motivated by the sensible requirement that the block D in Figure 4 uses only past and present symbols, and side information not related to the message being sent, to construct its current output. On the other hand, if, for some mappings Ek and Dk, Assumption 2(b) does not hold, then one can define another set of mappings that achieve a coding noise q with the same statistics as in the original situation, but at the expense of a lower average data-rate R.  Accordingly, if one aims at minimizing R, then one can focus, without loss of optimality, on mappings Ek and Dk satisfying Assumption 2(b). 
 The main focus of this paper is on the minimal average data-rate that allows one to attain a given performance level, as measured by the stationary variance of the plant output y. With the definitions introduced above, we are now in a position to formally define the problem of interest as follows: Problem 1 Consider the NCS of Figure 1, suppose that Assumption 1 holds, that the source coding scheme is linear, and that the scheme of Figure 4 is used as the link between v and w. Denote by Dinf the minimum stationary variance of y that is achievable in the NCS of Figure 3 when q = 0. Find, for a given performance level D ? (Dinf,8), 
 where  is the stationary variance of y, and the optimization is carried out with respect to: 
 •	All causal blocksin Definition 1, andE,DD,Hsatisfies Assumption 2(b).,H-1 described by (3)-(4), such that the coding noise q (see (5)) is as 
 Remark 2and the auxiliary noiseThe definition of independent source coding scheme guarantees that, providedq satisfies the conditions in Definition 1, the NCS considered in Problem 1(H,C,F,E) ? 
 In order to solve Problem 1, we will first establish a lower bound on the average data-rate across a linear source coding scheme in terms of stationary second order properties of the auxiliary signals w and v (see Figure 4). The existence of this bound motivates Section 4, where we study the optimal design of linear source coding schemes subject to SNR constraints. These results are then used in Section 5 to give both upper and lower bounds on RD. 
 This section extends the results of Section V-A in [8] to show that, when a linear source coding scheme is used in the NCS of Figure 1, the minimum average data-rate across it (subject to a performance constraint) is bounded from below by a simple function of the minimum ratio between the stationary variances of v and q (subject to the same performance constraint). To do so, we start by noting that the following holds: 
 Theorem 1 Consider the NCS of Figure 1 where the source coding scheme is linear, and the link betweenhold, thenv and w is given by the scheme of Figure 4. If (H,C,F,E) ? S and Assumptions 1 and 2 
 where I8(v ? w) denotes the directed mutual information rate [16,17] betweenw, and sq2 is the variance of q. Moreover, equality holds in the lastv and w, Sw is the stationary power spectral density of inequality if and only if q is Gaussian. 
 Proof: The first inequality follows immediately from Theorem 1 in [8]. The second inequality follows from Part 3 of Lemma 5.2 in [7], and from the proof of Theorem 3 in [8] (see also Theorem 4.6 
 Theorem 1 gives a lower bound on the average data-rate across a linear source coding scheme in terms of a simple function of the spectrum of w and the equivalent noise variance sq2. This key result, upon which the remainder of this paper is based upon, can be further simplified. Note that Jensen’s inequality yields 
 where sv2 is the stationary variance of v, and ? is the SNR of the linear source coding scheme. By finding the minimal SNR ? subject to a performance constraint (e.g., an upper bound on the stationary variance of the plant output y), one is also calculating an upper bound on the minimal value of the right hand side of (8), subject to the same performance constraint. If, in addition, the optimal solution to the former SNR minimization problem is such that the gap between the left and right hand sides of the inequality in (9) is arbitrarily small (or can be made so without compromising optimality), then, by virtue of Theorem 1, one would immediately get a lower bound on RD by solving the much simpler SNR minimization problem. The following result guarantees that this is actually the case: 
 Lemma 1 Consider the NCS of Figure 1, where the source coding scheme is linear and has a fixed noise source q. Suppose that Assumption 1 holds and define  . If the choice (H,C,F,E) = 
 (H0,C0,F0,E0) ? S(H,C,F,Eis such that) = (sy2 H=1s,Cy,2 01,Fand1,Ef1=) ?f0S, then, for any arbitrarily smallsuch that sy2 = sy,2 0 f = f0 and, in addition,? > 0, there exist 
 Proof:	The proof of this result goes along the lines of the proof of Theorem 4 in [8]. 
 Motivated by the discussion preceding Lemma 1, we will now focus on the following problem: 
 Problem 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumption 1 holds. Define, where pi is the ith unstable pole of G. Find, for a given G ? (?inf,8), 
 Remark 3 It follows from Theorem 17 in [10] that ?inf corresponds to the minimal SNR compatible with mean square stability in the NCS of Figure 3. 
 We note that Problem 2 is concerned with the best achievable performance, as measured by sy2, that is achievable when an upper bound G is placed on the SNR ?. As shown in Lemma 4 below, this problem is equivalent to the problem of finding the minimal achievable SNR ? subject to an upper bound on sy2. Our momentary change of focus is only due to technical reasons (see Remark 4 below). The next lemma states necessary conditions for the 4-tuple (H,C,F,E) to belong to S: 
 Lemma 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumption 1 holds. Define 
 and consider a coprime factorization of G over RHY N8, i.e., consider= 1.	If (H,C,F,EX, Y), ?NS, , thenD ? RHF ?8, withRH2, 
 Proof: It is clear from Figure 3 that F is in open loop. Thus F must be stable to ensure the internal stability of the loop. On the other hand, q models a possibly non-linear and discontinuous mapping between v and w and, accordingly, F must be strictly proper for the architecture to be 
 well-posed (the same conclusion applies to(H,C,F,E) ?must contain as non-minimum phase (NMP) zeros all the unstable poles ofS, then K must be an admissible one degree-of-freedom controller forC). Therefore, F must belong to RHK2. HEG. (1H,E,GThus- C)-S1Kand. If 
 (1-CS)-=1 (Ssee Lemma 3.1.3 in [18]). Denote all the unstable poles ofK(1 - C)-1 we have from the Bode Integral Theorem [18] that, for anyH,E and G by ¯piF, i??RH{1,2···,	,np¯}. 
 To complete the proof, we note that HSGE is the closed loop transfer function from a disturbance at the input of H to the output y. By invoking the Youla-Kucera parametrization [19], we thus conclude that HSGE must be as claimed. 
 By virtue of Lemma 2, we are now ready to present a lower bound on the best achievable performance  . To that end, we will make use of the following mild additional assumption: 
 If EH were identically zero at the optimum, then optimal performance would be achieved by leaving G in open loop. The cases where this happens are clearly of no interest in a networked control setting. 
 Theorem 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumptions 1 and 3 hold. Then, 
 , where ci denotes the ith NMP zero of G. Moreover, the optimization problem on the 
 right hand side of (14) is convex (i.e., JG(·,·) is a convex function of its arguments, and M × RH8 is a convex set). 
 Proof: Under our assumptions we have that, for any (H,C,F,E) ? S and sq2 ? R+, sy2 and ? exist and are given by 
 A simple contradiction-based argument shows that, since Assumption 3 holds, the SNR constraint in (10) is active at the optimum. Hence, at the optimum, 
 where the first equality follows upon solving (18) for sq2 with ? = G, and the inequality follows from the Cauchy-Schwartz inequality and the fact that HSGE+1 = (1-C)S. If one defines f  |(1 - F)S| and2. 
 notes that |?| = 1, then the first part of the result follows immediately from (21), (19) and Lemma 
 To complete the proof we need to show that the optimization problem on the right hand side of 
 (14)Lemma 4 in [20] thatis convex. Since WJGQ(·is an afine function of,·) is a convex function of its arguments. The proof is thus completed.Q, and both RH8 and M are convex, it follows from 
 Theorem 2 shows that one can calculate a lower bound on the best achievable performance subject to an SNR constraint by solving a convex optimization problem. Convexity guarantees, among other things, that the problem of finding the bound0 JG,inf, and also Qd ? RH8 and fd ? M such that, for any d > , 
 Remark 4 As an alternative to Problem 2, one can also consider the problem of finding the minimal SNR subject to a performance constraint. By doing so, and by proceeding as in the proof of Theorem 2, one arrives at an auxiliary optimization problem whose convexity we have not been able to prove yet. 
 We will next show that a solution of the optimization problem on the right hand side of (14) actually yields a solution to Problem 2. To that end, we start by noting that the following holds: 
 1.	Equation (23) follows by mimicking the proof of Lemma 1 in [22], and by using the definition of 
 2.	The Youla-Kucera parametrization guarantees that Kd is an admissible one degree-of-freedom controller for G. Thus, (1 - GKd)-1 has as zeros all the unstable poles of both G and Kd. Accordingly, Sd is stable, Sd(8) = 1, and Sd has as NMP zeros the unstable plant poles only. Given the definition of Fˆd,1, we thus conclude that Fd,1 ? RH2 and that (1 - Fd,1)-1 is also stable. Since, by assumption, ?d is stable, we have that num{Kd}?d(zn(1 - Fd,1))-1 ? L2 is analytic outside and on the unit circle and can thus be approximated to any degree of accuracy 
 Theorem 3 Consider the NCS of Figure 1 where the source coding scheme is linear, and recall the notation introduced in Lemma 3. If Assumptions 1 and 3 hold, then? RHsy2G = JG?,infM. Moreover, for any  => 0, there exists1 1,2,d > 0 such that choosingˆd,1 satisfying the conditions in Part 1 of Lemma 3, andQd 8 and fd satisfying 
 Proof:	Our first claim follows immediately from the second, which is proved below. 
 Since H,E ? RH8 and F,C ? RH2 (recall the definition of den{·}), it follows that the proposed choice of parameters defines a well-posed feedback loop. We next show that the proposed choice of parameters achieves internal stability. To do so, we first note that the open loop transfer function from y to u is given by HE(1-C)-1 = KGd. As such, there exist no unstable pole-zero cancellations between. From the proof of Lemma 3, we know that Kd is an admissible one degree-of-freedom controller for 
 Kd and G [18]. Moreover, it is clear by construction (see (28) and (29)) that there are no unstable pole-zero cancellation among any two transfer functions in the set {H,E,(1-C)-1}. The above facts imply that the closed loop is internally stable. 
 Thus, since||HS1 -dGFˆ?d,d1is stable since||22f<d ?GM+ 1, it follows that there exists a su, whereKd is admissible forFˆd,1 satisfies the conditions in Part 1 of Lemma 3. On the other hand,G and Hffi?ciently smalld is stable. As a consequence of the above,1 > 0 such that ||(1 - F)Sd||22s=q2 
 The fact that our choice of parameters guarantees ? = G follows upon using (30) in (18), and noting that S = Sd. 
 To complete the proof, it remains to show that the proposed set of parameters is such that  JG,inf +  for any  > 0. By definition of Ed,2 (see (27)), the choice for E in (28) is such that there exists ?E ? L2 such that 
 and the numerator of the second term on the right hand side of (20) can be written as 
 In (34), the first equality follows upon using (32) and (33), the inequality follows from the CauchySchwartz inequality, and the last equality follows from the fact that our choice of parameters guarantees 
 thatKd), and from our choice forS = Sd and thus num{ 	}	d,1GK. By using the above facts and the identityW	(use the definition of WQd and 
 The result follows immediately from (36). (Note that 2 depends upon 1 and, hence, has to be chosen, once 1 is chosen.) 
 Theorem 2 provides a characterization of the solution of Problem 2. In the next section, we will use this result to characterize bounds on RD. 
 5 Bounds on the Minimal Average Data-rate needed to Achieve a Given Performance Level 
 This section presents a characterization of the solution of Problem 1. In particular, we present both upper and lower bounds on RD, and also a specific implementation of a linear source coding scheme that achieves an average data-rate that is guaranteed to be below our upper bound. 
 Lemma 4 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumptions 1 and 3 hold. Define 
 Proof: By solving Problem 1 one finds a set of parameters such that sy2 = ˆsy2˜G and ? = G (recall that Assumption 3 guarantees that the SNR constraint is active at the optimum). It is therefore immediate to see that, by definition, ?[sy2]G> G is impossible. Assume now that Qo and fo are the parameters that minimize J?[sy2]G (f,Q). Then, since the SNR constraint in Problem 2 is active at the optimum, we have that 
 Lemma 4 shows that Problem 2 is equivalent to the problem of finding ?D in (38). That is, if some choice of the parameters (H,C,F,E) and   solve Problem 2 and thus achieve a performance level and an SNR ? = G, then the same parameters optimize the SNR ? subject to the constraint 
 Theorem 4 Consider the setup and assumptions of Problem 1. If, in addition, Assumption 3 holds and  , then R . Moreover, there exists a linear source coding scheme such that  , while satisfying for any  > 0. 
 We now prove our second claim. Use Theorem 3 to find filters (H,C,F,E) = (Ho,Co,Fo,Eo) and a noise variance sq2 = so2 such that ? = G and sy2 = D* +  for the desired value of  > 0. Consider an entropy coded dithered quantizer (ECDQ) [23] as the link between v and w, i.e., pick blocks E, D, H and H-1 in Figure 4 such that 
 where dh is a dither signal available at both the encoder and decoder sides (accordingly, SE(k) = SD(k) = SH(k) = dh(k)), Q : R ? {i?;i ? Z} corresponds to a uniform quantizer with step size ? ? R+, Hk corresponds to the mapping describing an entropy-coder (also called loss-less encoder [15, Ch.5]) whose output symbol is chosen using the conditional distribution of s(k), given dh(k), and Hk-1 corresponds to the mapping describing the entropy-decoder that is complementary to the entropy-coder at the encoder side. If dh is an i.i.d. sequence, independent of (xo,d), and uniformly distributed on 
 (-?/2,?/2), with ? = 12so2, then our claim follows from Corollary 3 in [8].	 Theorem 4 establishes a lower bound on the minimal average data-rate that is needed to achieve a given performance level, when linear source coding schemes are employed to control SISO LTI plants. This lower bound is not tight, but the worst case gap is given by  + ln2 nats per sample 
 (i.e., ˜ 1.254 bits per sample). The first term of this gap corresponds to the divergence of the ECDQq from Gaussianity, and appears due to the fact that ECDQs generate uniform quantization noise 
 and not Gaussian coding noise. The second term arises because practical entropy-coders (see proof of Theorem 4) are not perfectly efficient [15, Chapter 5]. (A detailed discussion of these facts can be found in [7,8].) Since we constrain ourselves to a simple class of source coding schemes, this gap seems inescapable but, in our view, it is a fair price to be paid given the simplicity of our approach. 
 A key aspect of our results is that they are built upon the solution of an SNR optimization problem. This is a key feature of our work, and allows one to easily provide average data-rate guarantees in feedback loops, by using standard control system design techniques. 
 Remark 5 If in Problem 1 one removes the performance constraint , then the problem reduces to the calculation of the minimal average data-rate that is compatible with mean square stability, say 
 RMSS. By using Theorem 17 in [10], and proceeding as in the proof of Theorem 4, it follows that, for the setup and assumptions of the latter theorem, 
 where pi is ith unstable pole of G. That is, independent source coding schemes can achieve MSS at average data-rates that are at most   nats per sample away from the absolute lower bound established in [2]. 
 This paper has studied control systems closed over noiseless digital channels. By focusing on a class of source coding schemes, we have established lower and upper bounds on the minimal average data-rate needed to achieve a prescribed performance level. Instrumental to our result was the characterization of the minimal SNR that guarantees a given closed loop performance in a related LTI control system. 
 Future work should focus on situations that include causal but otherwise unrestricted source coding schemes. Extensions to the MIMO case are also under study by the authors. 
 ﻿ In this work we obtain capacity for a class of pair bidirectional relay , where one relay can help between the corresponding user . For the , we apply the generalization of successive compute strategy for the linear of the of each user pair at the relay . The channel is considered as a broadcast network . It is shown that for all channel gains , the achievable rate boundary within of N and N sec below the cut set upper bound for restricted and nonrestricted , respectively . These tend to sec per user to infinity . We first derive a comprehensive formulation for the step asymmetric and use it to derive the capacity result for our problem . 
 In , a compute and forward strategy been for relay with equal power and asymmetric channel gains based on lattice . The receiver can recover the associated linear which are closer to the channel fading . This strategy simultaneously protection against noise and the opportunity to exploit interference for gains . In , a scheme been for relay with unequal power and symmetric channel gains based on lattice where it an achievable cast rate within a constant gap below the capacity . In , a successive scheme been for the same network as defined in . In this method after a linear combination of , the receiver can combine it with its channel observation to obtain a new effective channel that is better for the next targeted linear combination . In and an asymmetric been for and relay , respectively , with unequal power and asymmetric channel gains based on lattice . 
 In and , a asymmetric strategy been . The method a set of scaling to decode integer linear of . The use of scaling in fact is equivalent to non integer linear of lattice . It different to have different . Thus , by appropriately those , different on the boundary of the rate region can be . Also , in , the idea of non integer linear of lattice been which can be considered as a special case of the of and . 
 In this paper we generalize the method of to . We also consider scaling as defined in . Then we use these to address Pair Relay , where one relay can help between the user of each of the relay . We present an achievable rate region for the based on the generalization of the strategy . It is shown that this method within log N and log N of the cut set upper bound for the restricted and non restricted , respectively . to infinity , these tend asymptotically to . Some of the previous works about these are as : 
 In , an achievable rate region was for two way relay based on the of lattice and . In that network , a relay node the communication between two of . They assume a complex channel model . It was shown that for all channel gains , to within sec per user of the cut set upper bound are achievable . In , we studied the same network model as in but with the assumption of real channel model , a achievable rate region for that network by lattice and the successive compute and forward approach of . It was shown that this method within and sec per user of the cut set upper bound for restricted and non restricted network , respectively . The paper is organized as : In Section the system model for the general relay network is defined . In Section we present our generalization of the approach . The system model , achievable rate and capacity for the pair two way relay are in Section . Some concluding are provided in Section . 
 In this section we consider a relay network . We use the same model as defined in , but with the assumption of unequal power and asymmetric channel gains . Each relay indexed by , a noisy linear combination of the through the channel , 
 where are the channel gains and is i . i .. noise , , In . Let denote the vector of channel gains from relay . Each channel input ` is a length sequence subject to the average power constraint `, for all `, i . e . The channel gains are assumed to be known and constant . 
 ; denote the set of , and to the nearest integer value . Throughout the paper , are shown with . 
 In this section we propose a straightforward and comprehensive upper bound to the rate of the step asymmetric successive compute and forward strategy . We consider the system model as defined in Section , which unequal power and asymmetric channel gains . We also consider real valued scaling for the non integer linear combination of lattice . 
 In step , at each step , the receiver can combine its estimation from the previous step with its channel observation to obtain a new effective channel that is better for the next targeted linear combination . This for . The general formulation step is expressed in the following theorem . 
 Theorem . Consider the relay network defined in Sec with unequal power P and equation coefficient ami , where ami ami , ami , and i ;. Let real . By step strategy , if the computation rate R is achievable , then 
 Remark . For , and ` , for all `, to the in . 
 Remark . Equal power and asymmetric channel For ,` , and ` , for all `, to the in and . 
 Remark . Unequal power and symmetric channel For , , ` for all `, and a I , to the in . 
 As it is seen from the above , the formulation in is capable of both unequal power and asymmetric channel even if we omit the scaling from the . 
 In this section we present the system model for a class of pair two way relay channel of . We express the cut set upper bound , achievable rate based on the asymmetric strategy and calculate the capacity . We also show that the according to the channel gains the achievable rate region . 
 A shown in Fig . , where i and i denote the pair of which can communicate to each other via node a relay . There is no direct link between . The relay is assumed to be able to listen and transmit at the same time . and related to the pair are as and 
 For each ;, let ; denote the set of all the of , where i , and , such that if , nil then , i . e ., include two of a pair . For example for , , denote 
 the set of i , such that , for ;, number of the of . For , , let , denote the other node in a pair which with , i . e ., if then . Let ; denote the message that pair i to 
 Fig . : The pair two way relay channel . node of pair i . This message is into a an function , for ;, where , the number of channel , is a realization of a real random variable , satisfying the power constraint , and , the previously received at node , of pair i ;. This mode , non restricted , can introduce some dependency between the by different . In contrast , restricted the function . The are assumed to be independent . The rate of transmission is defined as . 
 The received at node , for all i ,, and at the relay node ,, at time instant , are given by 
 respectively , where the signal by the relay at instant . and are independent and of i . i .. real valued noise , . and denote the and channel gains , respectively , for all i ,. Node a function to decode as , where . 
 In this section the cut set upper bound for restricted and non restricted are given . These are of the cut set upper bound in and for two pair two way relay . 
 Theorem . The capacity region of the non restricted is upper bounded by , 
 Proof : Consider the cut ;, the first and second in the right hand side of b denote the upper for the multiple access and the broadcast of the relay network , respectively . 
 Corollary . For the capacity region of the restricted , we have the outer bound a but b is as , 
 where ;. The proof can be considering the fact that the are mutually independent in the restricted network . 
 Lemma . The non restricted cut set upper bound is within bit sec of the restricted cut set upper bound . 
 Proof : The proof can be done by the same way as done for two pair two way relay network in . 
 In this section we present an achievable rate region for the . In our scheme , we use a restricted function , so it will be an achievable rate region for both restricted and non restricted network . The idea is based on the fact that in the communication , can be considered as an network with N and one receiver , and so we can apply the asymmetric successive compute and forward method to decode the linear of the of each pair successively . 
 In the , is considered as a broadcast channel with one transmitter at the is done by successive interference cancellation . Note that we of N , because of the fact that each pair is a two way relay channel and the of each pair is not considered as a broadcast channel since each receiver about its own message and can subtract it from the its signal . 
 at the : N are such that N . For each ` : N , ` second moment ` , for some distinct i : and , , where P P . All are assumed to be simultaneously good . For each message of , the ` ` is , where ` the region of `, and ` : N . The transmitter i 
 where ` ` and ` is a random vector uniformly distributed in `. So , and to an ensemble 
 of lattice with sizes , with independent of ` and uniformly distributed in `. 
 at the Relay : at the Relay is done according to the successive compute and forward method in Theorem . Here we have one relay , so . Without loss of generality , we assume , 
 At the stage of successive compute and forward , the relay the linear of lattice code 
 Theorem . The coefficient ai can be successfully at the stage successive compute and forward , so long as , 
 Proof : The combination of the channel gains and node the following vector of incoming signal at the relay : is defined as 
 More similarity between and in a achievable rate region see Theorem . So we select and . Also in order to attain simpler and also a achievable rate , we consider power allocation of the as , 
 at the relay : The ti to from with sizes for i ;, respectively , where 
 at : at is done by the successive interference cancellation method . In this way , the user with the channel gain first the related to the with worse channel gain . Then , it the message related to itself , i . e ., the message of the other user which to its pair . Thus , at each user with a specific channel gain , only the of other with higher channel gains can cause interference . For each , let ; i denote the such that for each , for some j , . By the strategy , it can be seen that the following message are : 
 For example , let , and assume that , so we have S , S , , S , , S , S , , S , , and at the is done as : 
 at the of the first pair , i . e ., , , with channel gains and can be done with an arbitrary small probability of error if the message associated to 
 User of the first pair the highest channel gain , thus it first the other and then them from its received signal . Then it the message of user of the first pair . For user of the first pair so the 
 part and part of make interference in the of part . 
 By the same reasoning , the following message are for at the other , 
 In this section we state the capacity gap for the and of the restricted in the following Lemma : 
 Lemma . By the strategy , for any rate i ;, , satisfying 
 where ;, there a choice of power such that of at the in the be done with arbitrary small error probability for all channel gain . 
 Proof . We must show that if , and a d are satisfied , all power scaling between zero and are . Next we prove the theorem for a and b , the proof for the can be done similarly . We need the following inequality : if then 
 From the above Lemma , it is seen that the maximum gap between inner and outer is , and from Lemma for the non restricted , this gap is sec per user . 
 For , i . e ., the two way relay channel , the gap for the non restricted channel model is equal to which is compatible with the result of in the literature . For , i . e . the two pair two way relay , the 
 for restricted and non restricted channel , respectively which are the same as . 
 As , the for restricted and non restricted channel , i . e ., and asymptotically goes to . 
 In this work , we a comprehensive asymmetric step successive compute and forward scheme which the previously . Based on the approach , we the capacity for a class of pair two way relay , where one relay can . It was shown that for all channel gain , the compute and forward scheme to within constant log N and log N sec per user of the cut set upper bound for the restricted and the non restricted network , respectively . These tend asymptotically to sec per user to infinity . 
  
 ﻿This paper presents novel results on scalar feedback quantization (SFQ) with uniform quantizers. We focus on general SFQ configurations where reconstruction is via a linear combination of frame vectors. Using a deterministic approach, we derive two necessary and sufficient conditions for SFQ to be optimal, i.e., to produce, for every input, a quantized sequence that is a global minimizer of the 2-norm of the reconstruction error. The first optimality condition is related to the design of the feedback quantizer, and can always be achieved. The second condition depends only on the reconstruction vectors, and is given explicitly in terms of the Gram matrix of the reconstruction frame. As a by-product, we also show that the the first condition alone characterizes scalar feedback quantizers that yield the smallest MSE, when one models quantization noise as uncorrelated, identically distributed random variables.
 IndexTerms— Frames, Quantization, Sigma-Delta Modulation.
 INTRODUCTION
 In many signal processing applications, signals have to be represented by a series of numbers (samples), so that they can be processed, transmitted or stored in digital form. This paradigm requires sampling, quantization and reconstruction.
 The quantization of the samples, namely the sequence {cj}Nj=1, N ? N, yields a sequence  whose elements are constrained to belong to a discrete set of scalars. We focus our attention on uniform quantization, and thus require that
  
 where U is the quantization alphabet.
 The simplest and most common paradigm to recover the signal from the numbers is linear reconstruction. Here, one is able to recover the original signal, say a, via
 	a  ,	(2)
 In (2), {?j}Nj=1 is a set of vectors (a frame) in the reconstruction Hilbert space W (typically a subspace of 2 or of L2). Thus, the samples  are the frame expansion coefficients of a. Examples of linear reconstruction are the Shannon-Whittaker reconstruction formula, the reconstruction stage in filter-banks, and the inverse wavelet-transform.
 Throughout this work, we will be concerned with the squared 2-norm of the reconstruction error, i.e.,
 
 	 ,	(4)
 for every c ? RN. Unfortunately, minimization of (3) subject to (1) is a non-convex optimization problem. Moreover, the complexity of solving this problem grows exponentially with the number of coefficients to be quantized. In addition, unless   forms an orthogonal set, one would need to “preview” the entire input sequence before being able to calculate any optimal quantized value for µj. This is incompatible with delay sensitive applications.
 For the above reasons, in practice quantization is often accomplished via simpler sub-optimal methods that operate sequentially. The simplest of these correspond to scalar feedback (SF) quantizers. At the i-th iteration, these A/D converters obtain the output sample ui by simple scalar quantization of an auxiliary sequence, which is a linear combination of input and output samples, i.e.,
 ui = Q“X	ai,jcj + X	ßi,j(uj - cj)”.	(5a) j:j=i	j:j<i
 In (5), the real scalars ai,j,ßi,j, i,j ? {1,2,...,N} are design parameters, and Q(·) is the nearest neighbour scalar quantization function
 	Q(v)  argminµ?U |v - µ|,	?v ? R.	(5b)
 The above expressions can be used to describe many scalar quantization schemes, including PCM, DPCM and (multi-bit) Sigma-Delta (S?) converters [2]. The latter have been well studied in the context of shift-invariant reconstruction spaces (wherein reconstruction is done by LTI filters), and recently also for frame expansions (see, e.g., [3,4]).
 Not surprisingly, for a given reconstruction frame, and in return for the above mentioned shortcomings, optimal vector quantization generally outperforms SFQ. However, it is not known under what conditions this performance gap exists. In this paper we derive those conditions. More precisely, we state necessary and sufficient conditions for SFQ to be optimal, i.e., to yield, for any input c, the quantized output sequence µ that minimizes D(c,µ) in (3). Our results extend the work documented in [5,6] to more general situations.
 1-4244-1484-9/08/$25.00 ©2008 IEEE	ICASSP 2008
 Notation We use bold lowercase letters, e.g. x, to denote both the sequence  and the column vector [x1 ··· xN]T, where the meaning is clear from the context. We also use bold letters to represent matrices (uppercase) and their corresponding column vectors (lowercase). For example, if G is a matrix, we use gi to refer to the i-th column of G, and gi,j to refer to the j-th element of gi. The null space and the Moore-Penrose pseudo-inverse of a matrix G are denoted respectively via N(G) and G†. The notation Gi refers to the sub-matrix obtained by removing the first i columns and i rows from G. Similarly, xj denotes the vector x without its first j elements. The symbol 0N denotes an N-length column vector of zeros. We use the short-hand notation G for the quadratic form xTGx. We write ”iff” as an abbreviation for ”if and only if”. Z corresponds to the integers, and we use ZN to denote the set of all N-length vectors with integer elements. We say a matrix or vector is integral iff all its elements are integers.
 PRELIMINARIES
 2.1. Brief Overview of Frames
 Here we will first present some facts regarding frames that will be used in our subsequent analysis .
 A finite frame for a Hilbert space W is an ordered set of vectors   such that, for every w ? W,
 	 ,	(6)
 where the scalar constants A,B satisfy 0 < A = B < 8.
 The synthesis operator ? :  of the frame  is
 defined via
 	 ,	(7)
 where 2 denotes the set of square-summable sequences. The Gram matrix G ? RN×N of the frame  is defined element-wise via
 	 .	(8)
 It thus follows that
 	 	(9)
 which implies that G is positive semi-definite, and, in particular, that
 	 .	(10)
 2.2. Feedback Quantization of Frame Expansions
 It is easy to show from (5) that an SF quantizer cannot yield   for all c ? RN unless3 ai,j = di,j, ?i,j, where di,j is the Kronecker delta function. If the latter holds, then (5) can be written as follows:
 	u  Q(v);	v  c - Fn;	n  u - v,	(11)
 where Q(v) = [Q(v1) ··· ,Q(vN)]T, F is the feedback matrix and n is the vector of quantization errors. In order for the above equations to be well defined, F needs to be lower strictly-triangular, i.e., lower triangular with all main diagonal elements equal to zero4. Notice also that F is the only degree of freedom in the design of an SF quantizer.
 In order to determine D(c,u) for SF quantizers, it is convenient to define the noise shaping matrix
 	S  (IN - F),	(12)
 where IN denotes the N × N identity matrix. Clearly, S is constrained to be lower unit-triangular, i.e., lower triangular with all its main diagonal elements equal to 1.
 Substituting (12) into (11) yields u = c + Sn. Using this, and substituting (10) and (7) into (3), the distortion achieved by SFQ can be written as
 	  nTSTGSn.	(13)
 MAIN RESULT
 We can now state the main result of this paper.
 Theorem 1 For any given reconstruction frame with Gram matrix
 G ? RN×N, the distortion D(c,u) of an SF quantizer equals   for all c ? RN iff the following two conditions hold:
 The columns of the associated feedback matrix F satisfy
 fii = mi + ?i,	?i ? {1,...,N},	(14a) where m ,	?i ? {1,...,N},	(14b)
 and where the vectors  satisfy ?i ? N(Gi), but are otherwise arbitrary.
 For every i ? {1,...,N}, ??i ? N(Gi) such that
 mi + ?i ? ZN-i, (15) i.e., such that mi + ?i is an integral vector. 
 Notice that (i) describes a “matching” condition between the feedback matrix and the reconstruction frame. Thus, (i) can always be satisfied by a proper choice of F (which is given explicitly by (14)). On the other hand, condition (ii) depends only on the reconstruction frame, or more precisely, on its Gram matrix.
 The proof of Theorem 1 will be given in Section 6, based on preliminary results given in Sections 4 and 5. The latter provide valuable insight into the SFQ problem, and stem from two alternative approaches: lattice quantization and dynamic programming.
 LATTICE QUANTIZATION FORMULATION
 In this section we use the fact that minimization of (3) subject to (1) is equivalent to a lattice quantization problem. To show this, we first note that any symmetric positive semidefinite matrix G ? RN×N can be decomposed as
 	G = HTH,	(16)
 where H ? RN×N is lower triangular (see., e.g., [8]). It then follows directly from (9) and (16) that 
 2. Thus, one can analyze the relationship between the images in W through ? of any group of sequences by looking at their images in 2 through H. In particular,
  c 
 see (13) and (8).
 Since the quantization alphabet U is uniform (see (1)), the images through H of all the sequences µ ? UN constitute the reconstruction lattice
 	 .	(18)
 Accordingly, we say that H? is the generating matrix for L. Every lattice has a basic Voronoi cell, V0, associated with it, i.e., the region of points closer to the origin than to any other point in the lattice. More precisely,
   Hc :  
 The Voronoi region around a lattice point Hµ ? L is the region
  . Similarly, we define the quantization cell around Hµ ? L of an SFQ converter as
 	C(Hµ)  Hµ + HSY,	(20)
 where the hyper-cube ? : ? [- 2 , 2 ], 1 is the set containing all possible  quantization noise sequences. Thus, C(Hµ) is the set of all target points Hc for which an SF quantizer outputs the sequence µ.
 With the above definitions, we can now prove the necessity of condition (i) of Theorem 1.
 Lemma 1 For a reconstruction frame with gram matrix G, the distortion D(c,u) of an SF quantizer can equal  for all c ? RN only if condition (i) in Theorem 1 holds.	
 Proof 1 In view of (17) and (19), an SF quantizer is optimal (i.e.,
  . A key property of V0
 is that it has the minimum second moment among all the cells whose L-translations form a tessellation , see [1]. Thus, an SF quantizer is a candidate to be optimal only if its matrix F minimizes the second moment ofNC(0N). This second moment can be readily shown to be given by   trace   (I - F)TG(I -
 F). By using (16), the i-th element of the trace can be written as
 T
 f, since H is lower triangular and F is lower strictly-triangular. The fact that each trace term depends only on its corresponding column of F implies that the trace is minimized iff each fii minimizes i i i .
 Clearly, this happens iff
 	fii = Hi†hii + ?i, ?i ? {1,...,N},	(21)
 where ?i is an arbitrary vector in N(Hi) (and, thus, in N(Gi) as well). Substitution of the identity A† = (ATA)†AT into (21) yields (14), thus completing the proof. 
 Remark 1 If one models n as a vector of uncorrelated, uniformly distributed (u.u.d), random variables, one gets MSE =   trace trace{(I - F)TG(I - F)}. On the other hand, an SF quantizer whose feedback matrix satisfies (21) happens to characterize one of the noise shaping quantizers for frame expansions proposed in [3]. More precisely, condition (i) is satisfied by the variant in which the error associated with each quantized coefficient is projected onto all the coefficients ahead of the current iteration coefficient. Thus, Lemma 1 also shows that, using an u.u.d model for quantization errors, the latter scheme achieves the minimum MSE among all SF quantizers . 
 DYNAMIC PROGRAMMING FORMULATION
 Sequential quantization methods, such as SFQ, decide upon the value of each output coefficient sequentially. Insight can be gained by analyzing them from a dynamic programming point of view. The key point is that each of the decisions contributes additively to the cost defined in (3), leaving, after each step, a sub-problem similar in form to the original one. In turn, each of these sub-problems is determined by the decisions already made. The following result allows us to formalize these observations
 Lemma 2 (Cost Decomposition) Let G ? RN×N be a positive
  , and
  ,
 where the scalars Ki are defined as
 	iT	i i
 G gi, ?i ? {1,...,N}, (22) and where the vectors  satisfy (14). 
 Proof 2 The result follows from direct algebraic manipulation, using the identity A†AA† = A† and from the fact that giiT = giiTGi†Gi, ?i ? {1,...,N} (which stems from G being positive semidefinite).	
 Recursive application of Lemma 2 to (17) allows one to split the total cost D(c,µ) as follows:
  t 
 see (17), where the scalars Kj are defined in (22), and where
 	 ;	t 	(24)
 with t0  c.
 The summation on the right hand side of (23) represents the (irreducible) reconstruction error stemming from the first i - 1 decisions. The cost-to-go after decision i - 1 is the last term in (23). It has the same form as the original cost, but it contains the updated target vector tii. The latter can be regarded as a state vector which summarizes the effect of ci, and of previous decisions, on the costto-go.
 PROOF OF THEOREM 1
 Joint Sufficiency of (i) and (ii) It is well known in lattice theory that any two lattices L1 = M1ZN and L2 = M2ZN, with M1 and
 M2 non-singular, are equal iff there exists an integral matrix T with det{T} = ±1 such that M1 = TM2 (see, e.g., [9]). On the other hand, if conditions (i) and (ii) hold, then there exists a lower strictlytriangular matrix ? ? N(H) such that S + ? is integral. Since S is lower unit-triangular, we have that det{S} = det{S+?} = 1, and thus ZN = (S + ?)ZN. It then follows that L = HZN = H(S + ?)ZN = HSZN. On the other hand, if condition (i) holds, then it follows directly from (21) that the product HS yields an orthogonal, lower triangular matrix. This in turn implies that L is a rectangular lattice. Moreover, it is easy to verify that the associated Voronoi cell V0 is given by the hyper-rectangle HSY, which is precisely the quantization cell of the SF quantizer, C(0N) (see (20)). Therefore (i) and (ii) guarantee that the corresponding SF quantizer is optimal.
 Necessity of Conditions (i) and (ii) The necessity of (i) was shown in Lemma 1. Thus, it suffices to prove the necessity of (ii) assuming that (i) holds. If (i) holds, then the target vectors tj given in (24) can be written in terms of the feedback matrix F as follows
 	t .	(25)
 Since F is lower strictly-triangular, we have from (25) that
 	tj,j = cj - [Fj,1 ···Fj,N]?.	(26)
 If µj = Q(tj,j), then tj,j = vj, and ?j = nj (see (11)).
 Now let us consider a vector c such that the target vector, at iteration i - 1, satisfies 
  	(27a) t ,	(27b)
 for some ? ? UN-i+1 and some e ? (0,?). With the above target, an SF quantizer would choose µi-1 = Q(ti-1,i-1) = ?1 - ?, and thus. Then, from (23), the cost-to-go for the SF quantizer after i - 2 iterations can be split as
  ,
 where Lemma 2 and (27) have been used. On the other hand, from
 Lemma 2 and (27), the cost-to-go for the choice µi-1 = ? is
 . Therefore,
 the minimum difference between the cost-to-go achievable by SFQ and that of the choice µi-1 = ? is
  
 If (15) is not satisfied for some i ? {1,2,...,N}, then fii? ?/
 Usuch that (µi + fii?) ? N(Gi). As a consequence, the first term on the right hand side of (28) is strictly positive. It then follows that D(c,u) is strictly larger than , for sufficiently small values of e, completing the proof. 
 ANALYSIS OF THE RESULT
 Lattice Quantization Interpretation It has been shown in the proof of Theorem 1 that (ii) is a sufficient condition for L to be rectangular and have a hyper-rectangular Voronoi cell. It is important to note that this can happen for a non-diagonal reconstruction Gram matrix, i.e., reconstruction vectors that are non-orthogonal, and even linearly dependent, since G is not required to be non-singular. It is also important to note that the converse does not necessarily hold, that is, a rectangular L does not ensure that condition (ii) is satisfied. More precisely, the fact that a lattice L = HZN is rectangular implies the existence of an integral matrix M with det{M} = ±1 such that HM is orthogonal. It doesn’t guarantee M to be also lower unit-triangular, as required by (ii). On the other hand, (i) alone implies HS is orthogonal, and thus C(0N) = HSY is hyperrectangular. For a uniformly distributed c, the MSE gap between such an SF quantizer and a lattice vector-quantizer is given by the difference between the second moments of C(0N) and V0. Although no closed from expressions are known for the second moment of V0 of arbitrary lattices, preliminary results suggest that it is possible to derive lower bounds for this gap from the non-integer part of the vectors mi defined in (14b).
 Reconstruction by a Single LTI Filter By letting N ? 8 (and considering the distortion per sample D(c,µ)/N as the cost function), our results can be applied to cases where reconstruction is achieved using a discrete-time LTI filter, say R(z). Without loss of generality, we assume that limz?8 R(z) = 1. In this case, the reconstruction frame vectors take the form ?k = [0Tk-1 r(0)r(1)···]T, where r(·) is the impulse response of R(z). This setup turns F and H into infinite dimensional Toeplitz matrices, the first column of H being ?1. In turn, f1 can be seen as the impulse response of a filter F(z). It then follows that the orthogonality of the columns of HS stemming from (i) is equivalent to having 1-F(z) = R(z)-1. This corresponds to a whitening noiseshaping quantizer, which yields minimum MSE, in the alternative white quantization noise paradigm [2]. Similarly, an SFQ satisfying (i) also minimizes the MSE, see Remark 1. On the other hand, for this case, all the vectors mi (see (14b)) are equal to the impulse response (first sample removed) of F(z). Thus, (ii) translates into having the impulse response of 1 - R(z)-1 being integer-valued. Hence, the standard L-th order multi-bit S? converter is optimal for R(z) = (1 - z-1)-L. This extends the results reported in [6], obtained for U = {-1,1}.
 CONCLUSIONS
 We derived necessary and sufficient conditions that make scalar feedback quantization deterministically optimal, in the sense of generating, for any input, the quantized sequence that minimizes the 2-norm of the reconstruction error. The first condition, which depends only on the design of the scalar feedback quantizer, happens to characterize the best quantizer of this class, when a stochastic framework is adopted. The second condition depends only on the Gram matrix of the reconstruction frame, and can be satisfied for non-orthogonal, and even linearly dependent, reconstruction vectors.
 
 ﻿ We show that the structural similarity index , which is used in image to assess the similarity between an image representation and an original reference image , can be as a locally quadratic distortion measure . We , furthermore , show that recent of Linder and on the rate distortion function under locally quadratic distortion are applicable to this distortion measure . We finally derive the high resolution and provide a simple method to numerically compute an approximation of the of real . 
 A vast majority of the work on source with a fidelity criterion i . e ., rate distortion theory on the mean squared error fidelity criterion . The fidelity criterion is used mainly due to its mathematical tractability . However , in a human observer it been noted that distortion which include some of human perception generally perform better than the . A great number of perceptual distortion are distortion and , unfortunately , even for simple , their corresponding rate distortion , that is , the minimum to attain a distortion equal to or smaller than some given value , are not known . However , in certain it is possible to derive their . For example , for a process with a weighted squared error criterion , where the are restricted to be linear time invariant , the complete was first found in and later by several , . Other include the special case of locally quadratic distortion for fixed rate vector and under high resolution , which are extended to variable rate vector in , , and applied to perceptual audio in , . 
 In , Wang al . the structural similarity index as a perceptual measure of the similarity between an image representation and an original reference image . The index into account the between the image and its representation as well as the first and second order . It been shown that this index a more accurate estimate of the quality than the . The index was used for image in and was cast in the framework of compression of and image in . The relation between the rate of a fixed rate uniform and the distortion measured by the index was first in . In particular , for several of source and under high resolution , upper and lower on the index were provided as a function of the operational rate of the . 
 In this paper , we present the high resolution for with finite differential entropy and under an index distortion measure . The is particularly important for and within the image area , since it a lower bound on the number of that any coder , for example and so forth , will use when an image into a representation , which an index not smaller than a level . Thus , it one to compare the performance of a architecture to the optimum performance theoretically attainable . The is and does not appear to admit a simple closed form expression . However , when the rate is high , that is , when each of the image is by a high number of , say more than . , then we are able to find a simple expression , which is asymptotically as the bit rate exact . For finite and small bit , our an approximation of the true . 
 In order to find the , we first show that the index can be as a locally quadratic distortion measure . We then show that recent of Linder and on the under locally quadratic distortion are applicable , and finally obtain a closed form expression for the high resolution . We end the paper by showing how to numerically approximate the high resolution of real . 
 In this section , we present an important result on rate distortion theory for locally quadratic distortion and also present the index . We will need these when proving our main , that is , and in Section . 
 . . Rate Distortion Theory for Locally Quadratic Distortion . Let be a realization of a source vector process and let be the corresponding reproduction vector . A distortion measure , is said to be locally quadratic if it a series i . e ., it of all in a neighborhood around the of interest and furthermore , if the second order of its series dominate the distortion asymptotically as corresponding to the high resolution regime . In other , if , is locally quadratic , then it can be written as , O y , where is an input dependent positive definite matrix and where to , the quadratic term i . e ., is . We use upper to the stochastic process generating a use to denote the differential entropy of , provided it . The determinant of a and E the expectation operator . 
 The for locally quadratic distortion and smooth was found by Linder and and is given by the following theorem . 
 Theorem see . Suppose , some mild technical see a in Section . A in 
 where is the of in per block under distortion ,, and the differential entropy of . The distribution of image and image of natural can in general be sufficiently well by smooth , . Thus , the regularity of Theorem are satisfied for many naturally . 
 . . The Structural Similarity Index . Let , where . We define the following empirical : the sample 
 mean ni , the sample , and the 
 sample cross variance . We define and similarly . 
 where , i , . The index between and , where positive close to indicate a small perceptual distortion . We can define a distortion measure as one minus the index , that is , 
 which between and and where a value close to a small distortion . The index is locally applied the image . Then , all block are to yield the index of the entire image . We treat each block as an dimensional vector where N . 
 In this section , we present the main theoretical of this paper . We will first show that , is locally quadratic and then use Theorem to obtain the for the index . 
 Theorem . The high resolution for the the distortion measure ,, defined in and where and EX , is given by 
 Proof . Recall from Theorem that , is locally quadratic . Moreover , the weighting matrix in , which is also known as a sensitivity matrix , is given by A . , see the appendix . In the appendix , it is also shown that is positive definite since a , a , for all , where a and are given by and , respectively . From A . , it that 
 At this point , we note that the main technical for Theorem to be applicable is boundedness in the following sense : , 
 E log , and E trace and furthermore uniformly bounded third order partial of ,. The first two are satisfied by the of the Theorem . The next two follow since all of are bounded for all see the proof of Theorem . Moreover , due to the positive stabilization C and C , trace is clearly bounded . Finally , it was established in the proof of Theorem that the third order of , are uniformly bounded . Thus , the proof now simply by in . 
 . . the Rate Distortion Function . In this section we propose a simple method for the in practice based on real . Conveniently , we do not need to encode the in order to find their corresponding high resolution . Thus , the in this section as well as the in the previous are independent of any specific architecture . 
 In practice , the source statistics are often not available and must therefore be found from the image data . Towards that end , one may assume that the individual i Mi where i the image total number of in the image of the image constitute approximately independent of a vector process . In this case , we can approximate the expectation by the empirical arithmetic mean , that is , 
 where a i and i that the a in and are used on the 
 Table : n E log log for some bit grey and block sizes N , , , and . 
 Table : in dim or equivalently per for different bit grey and block sizes N , , and . 
 x i . Several of n E log log are shown in Table , for various commonly considered in the image literature . 
 In order to obtain the high resolution of the image , according to Theorem , we also need the differential entropy of the image , which is usually not known a in practice . Thus , we need to numerically estimate , for example , by the average empirical differential entropy over all of the image . In order to do this , we apply the two dimensional on each of the of the image in order to reduce the correlation within the since the is an orthogonal transform , this operation will not affect the differential entropy . Then we use a entropy estimation approach to approximate the marginal differential of the within a . Finally , we approximate by the sum of the marginal differential , which the in Table . 
 In this section , we use the on the and measure the corresponding of the reconstructed . In particular , we use the coder implementation available via the function in . Then , we compare these operational to the information theoretic high resolution as in the previous section . We are interested in the high resolution region , which to small , i . e ., close to zero or equivalently large i . e ., close to one . Figure the high resolution for , below . , corresponding to above . . Notice that the rate becomes negative at large i . e ., small , which because the high resolution assumption is clearly not satisfied and the are therefore 
 Figure : High resolution under the similarity measure , , for different and an block size . 
 not accurate . Thus , it does not make sense to evaluate the asymptotic of Theorem at large . 
 The information theoretic high resolution by Theorem a lower bound on the achievable minimum rate for a given distortion value . As in , the high resolution could require the use of optimal compounding , which may not be feasible in some . Thus , the of whether the in Theorem is achievable and how to achieve it , remain open . Nevertheless , we can obtain a loose estimate of how close a practical scheme could get to the high resolution by the operational performance of , for example , the . Figure the operational for the coder used on the image and block sizes of . For comparison , we have also shown the . It may be that the operational curve is up to above the corresponding a similar behavior is for the other four in the test set . 
 The gap between the and the operational based on as can be in Figure can be by the following . First , the coder at a frequency weighted rather than the index . Second , is a practical algorithm with reduced complexity and is therefore not optimal even for the weighted . Third , the differential entropy as well as the expectation of the log of the determinant of the sensitivity matrix are found based on a finite amount of image data . Thus , they are only of the true . Finally , the becomes exact in the asymptotic limit where the rate 
 Figure : Operational the coder on the image under the similarity measure , , for block size . For comparison we have also shown the high resolution thin line . 
 towards infinity i . e ., for small . At finite , it is an approximation . Nevertheless , within these , the numerical evaluation of the here that significant compression gains could be by an optimal image coder , at least at high rate . To obtain further insight into this question , the corresponding under distortion for the image is shown in Figure . We can see that the excess rate of with respect to the at high is not greater than . . This that a like algorithm at distortion could reduce at least a fraction of the bit rate gap seen in Figure . 
 It is interesting to note that , in the case , we have I , which that log . 
 Thus , the difference between the and the , under high resolution , is constant e .., independent of the bit rate . In fact , if the is measured per dimension , then the rate difference is given by the in Table , that is , n E log log . It that the is simply a version of the at high . Moreover , the gap between the the fact that , in general , a representation of an image which is optimal is not necessarily also optimal . 
 We have shown that , under high resolution , the for a range of natural under the commonly used index a simple form . In fact , the only upon the differential entropy of the source image as well as the value of a function of the sensitivity matrix of the image . Thus , it is independent of any specific 
 Figure : Operational the coder on the image under the distortion measure . For comparison we have also shown the high resolution thin line . The horizontal axes on the top and the bottom show the and , respectively . 
 architecture . Moreover , we also provided a simple method to estimate the in practice for a given image . Finally , we the operational performance of the image coder to the and by approximate numerical that potentially significant perceptual rate distortion could be by optimal . 
 We need to show that the second order of the series of , are in the high resolution limit where . In order to do this , we show that the series of the zero and first order vanish whereas the of the second and third order are nonzero . Then , we upper bound the remainder due to , by its second order series . This upper bound is established via the third order partial of ,. We finally show that the second order decay more slowly towards zero than the remainder . 
 Let us define C C and C C and let h . It that , and we note that the second order partial with respect to and for any i ,, are given by 
 Clearly , where that the expression , y yis at the , . Since 
 xi , it is easy to show that , for all i . Thus , the of the first order of the series of , are zero . Moreover , it from A . that h 
 g , for all i ,. With this , and after some algebra , it can be shown that 
 We now let denote the partial derivative respect to note that from 
 generalized product rule f . When at that , this and 
 an dimensional ball of radius centered at , let , and let T be the second order series of , centered at i . e ., at . It that 
 T i , i , i , A . where is given by half the second order partial of ,, that is see A . , 
 Thus , a and i a , i ,..., . Since is symmetric , the quadratic form is lower bounded by 
 where min min i ni min a , a , which that is positive definite . 
 On the other hand , it is known from theorem that for any , the remainder R , where 
 that is , is upper bounded by the over the set of third order of the series of . Since for real , the are finite , and since , i , , it from A . A . that the third order are uniformly bounded therefore finite . Moreover , for all such that e , it A . , A . , and A . that 
 where A . since i ,..., i , and the sum in A . over all possible of third order partial of a vector of length , that is , 
 i ,, n . Furthermore , A . . Finally , by use . from A . and the fact that i 
 the fact bounded by A . . Since the limit of A . and is zero , we deduce that the second order of the series of , are asymptotically as 
 The work of . is by the Research Council for Technology and Production , Grant no . . The work of . is by the Project no . and the Project no . ACT . 
  
 ﻿The performance of a noisy linear time-invariant (LTI) plant, controlled over a noiseless digital channel with transmission delay, is investigated in this paper. The ratelimited channel connects the single measurement output of the plant to its single control input through a causal, but otherwise arbitrary, coder-controller pair. An informationtheoretic approach is utilized to analyze the minimal average data rate required to attain the quadratic performance when the channel imposes a known constant delay on the transmitted data. This infimum average data rate is shown to be lower bounded by minimizing the directed information rate across a set of LTI filters and an additive white Gaussian noise (AWGN) channel. It is demonstrated that the presence of time delay in the channel increases the data rate needed to achieve a certain level of performance. The applicability of the results is verified through a numerical example. 
 Rate-constrained NCSs are generally studied from two points of view; control theory and information theory. In the former case, classical nonlinear control methods are employed. In the latter case, the key idea is extending information-theoretic notions to the case of closed-loop control. In both frameworks, stability analysis of linear timeinvariant (LTI) systems is well-studied (see, e.g., [1], [2] as early results and [3], [4] as recent contributions). 
 However, studies analyzing system performance from an information-theoretic viewpoint are less abundant in the literature. Fundamental results are presented in [5]. In this work, for a discrete-time LTI plant, the well-known Bode’s integral is extended to the case of causal rate-limited arbitrary feedback. Along the lines of [5], research reported in [6], [7] has investigated bounds on the minimum data rate which is needed to attain a quadratic performance level in NCSs with delay-free channels. For the lower bound, [7] shows that the rate-constrained optimization to find desired infimal data rates over causal but otherwise arbitrary coder-controller pairs, is reduced to a convex SNR-constrained optimization problem over an auxiliary LTI feedback loop closed through an AWGN channel. In [6], [7], it is furthermore shown that the directed information from the plant output and to the 
 control input provides a lower bound on the coding rate for any coding policy, and that it suffices to use linear coding policies, when the initial state and external disturbances are jointly Gaussian and the plant is linear. These findings were used by [8], to establish a general SDP framework for the problem of LQG control for fully observable multiple-input multiple-output (MIMO) LTI plants. 
 In this paper, we address output feedback control of an NCS comprised of a noisy LTI plant and a causal encoder-controller-decoder set connected through a noiseless digital channel with a constant transmission delay. More specifically, the problem is obtaining the bounds on minimal average data rate required to guarantee that the steady-state variance of an error signal does not become larger than a certain value. Motivated by its merits such as simplicity and practical appeal, we use the approach pursued in [6], [7] to gain outer bounds and build upon [7]. However, the main departure of this work from [7] is considering a channel which is not delay-free. So, as the first contribution, we rederive fundamental information inequalities of the system under the delay assumption. Secondly, we characterize the trade-off among performance, delay, and minimal desired average data rate. It is shown through a numerical example that greater transmission delay necessitates greater minimal average data rate needed to guarantee achieving the considered quadratic level of performance. Simulation indicates that by employing a simple scalar uniform quantizer in the LTI architecture that gives the lower bound, the quadratic performance is attained by operational average data rates at most 0.3 bits away from the lower bound. 
 The outline of this work is as follows. Section II presents the notation and some preliminaries. Then the problem of interest is formalized in Section III. Section IV is dedicated to the lower bound characterization. An illustrative numerical simulation is provided in section V. Finally, Section VI concludes the paper. 
 The set of real numbers is denoted by R with subset R+ as the set of strictly positive real numbers. N represents the set of natural numbers, based upon which N0 = N ? {0} is defined. Furthermore, k is the time index and for random processes considered in this paper, k ? N0 holds. Magnitude and H2-norm of a signal are symbolized by |.| and k.k2, respectively. The set U8 is defined as the set of all proper and real rational stable transfer functions with inverses that are 
 stable and proper as well. E denotes the expectation operator and log stands for the natural logarithm. The entry of matrix S on the i-th row and j-th column is denoted by [S]i,j. Moreover, ?min(S) and ?max(S) represent eigenvalues of S with the smallest and largest magnitude, respectively. 
 All random variables and processes in this paper are assumed to be vector valued, unless otherwise stated. A random process ? is said to be asymptotically wide-sense stationary (AWSS) if it satisfies   and limk?8 E[(?(k + t) - E[?(k + t)])(?(k) - E[?(k)]) ] = R?(t) hold, where ?? is a constant. C? = R?(0) denotes the corresponding steady-state covariance matrix upon which the steady-state variance of ? is defined as s?2 , trace(C?). The covariance matrix for a scalar random sequence   is defined as  . Considering 
 Pn,Qn ? Rn×n as two square matrices, the sequences   and  are asymptotically equivalent if and only if the following holds for finite %: 
 The structure considered in this work can be found in Fig. 1 where G is an LTI plant with u ? R as control input and y ? R as sensor output. Moreover, there is a disturbance represented by w ? Rnw and the output signal z ? Rnz, with nw,nz ? N, upon which the desired performance is characterized. The plant has the following transfer-function matrix description: 
 (nz × nw, nz × 1, 1 × nw and 1 × 1 for G11, G12, G21 and G22, respectively). The input alphabet of the channel is represented by A and is defined as a countable set of prefixfree binary words. Due to the delay, the output of the channel uq(k) follows uq(k) = yq(k - h) for k = h where yq(k) belongs to A. The average data rate across the channel is specified as follows: 
 where R(i) denotes the expected length of the i-th binary word yq(i). The channel input is provided by the encoder E based on the following dynamics: 
 in which ?e(k) is the side information at time k at the encoder with Ek representing an arbitrary (possibly nonlinear or time-varying) deterministic mapping. It should be noted that ßk is a shorthand for [ß(0),··· ,ß(k)]. On the decoder side, we have 
 Dk is assumed to be an arbitrary deterministic mapping, like Ek, and ?d(k) signifies the side information available at the decoder at time k. It should be emphasized that E and D in Fig. 1 are possibly time-varying or nonlinear causal systems. 
 Assumption 3.1: The plant G is LTI, proper and free of unstable hidden modes. Moreover, the open-loop transfer function from u to y is single-input single-output (SISO) and strictly proper. The disturbance signal, w, is a zero-mean white noise with identity covariance matrix Cw = I and jointly Gaussian with x0 = [x(-h),··· ,x(0)]T, the initial condition, having finite differential entropy. 
 Assumption 3.2: Each of processes ?e and ?d is jointly independent of (x0,w). So regarding the dynamics of the system, I(u(k);yk-h | uk-1) = 0 holds for 0 = k < h. Moreover, upon knowledge of ui and ?di, the decoder is invertible. It means that there exists a deterministic mapping 
 Now, suppose that Assumption 3.1 holds. Let Dinf(h) denote the infimum steady-state variance of the output z over all settings u(k) = Kk(?k) for 0 = k < h and u(k) = Kk(yk-h) for k = h with ?k independent of x0 and w. 
 for any D ? (Dinf(h),8), where the search is to be restricted to encoders with mapping Ek and decoders with mapping Dk which satisfy Assumption 3.2 and make the NCS of Fig. 1 strongly asymptotically wide-sense stationary (SAWSS) (this notion of stability is defined in [7]). Moreover,  denotes the steady-state variance of z. The optimization problem in (5) is feasible if D ? (Dinf(h),8) (see Appendix A for the proof). 
 Theorem 4.1: For the feedback loop depicted in Fig. 1 and satisfying Assumptions 3.1 and 3.2, the following holds: 
 (see [9] for the definition). Moreover, as defined in [10],   denotes the directed information rate across the forward channel from y to u with delay h. The proof can be found in [11]. 
 Now, a lower bound can be derived on the directed information across the coding scheme of Fig. 1. 
 (x(0),w,u,y) form a jointly second-order set of processes and that Assumptions 3.1 and 3.2 hold. Moreover, take yG and uG into account as the Gaussian counterparts of y and u where (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y). 
 where (a) follows from a slight modification in [7, Lemma B.4], (b) follows from [7, Lemma B.1], (c) holds form the Markov chain   based upon (51b) in [7, Theorem B.3], (d) is a consequence of Assumption 3.1 and yGi being a deterministic function of uiG-1,x(0) and wi, and (e) stems from (51a) in [7, Theorem B.3] and Assumption 3.1. This completes the proof. 
 In what follows we will relate the directed information from yG to uG to their corresponding power spectral densities: 
 Lemma 4.2: Consider y and u as jointly Gaussian AWSS processes. Moreover, suppose that u is SAWSS with   where µ > 0. Then the following 
 Proof: Having Gaussianity and joint AWSS-ness of (u,y) in mind and based on [12, Theorem 2.4] with a little modification, we can conclude that ? is Gaussian and AWSS as well. We start by the following equalities: 
 where (f) follows from the definition of mutual information and (g) from the definition of ?, (h) stems from [7, Property 2] and (9), and (i) holds from [7, Property 3]. So the directed information rate can therefore be rewritten as follows: 
 where (j) follows from the chain rule of the differential entropy and ?(k) being independent of ?k-1. Since the process u is SAWSS with   for some µ > 0, [7, Lemma B.5] will approve the validity of the leftmost term in (k). The rightmost term is self-explanatory because ? is Gaussian and AWSS. 
 It follows from Theorem 4.1, Lemma 4.1 and Lemma 4.2 that the rate-performance pair yielded by any encoderdecoder scheme which renders the NCS SAWSS, is attainable with a lower rate by a coding scheme comprised of LTI filters and an AWGN noise source. Such a scheme is depicted in Fig. 2. 
 The NCS of Fig. 2 is defined under the same conditions (Assumption 3.1) as the main system in Fig. 1 except for one thing; the arbitrary mappings are replaced by proper LTI filters B and J. Moreover, the communication channel is a delayed AWGN channel with noiseless one-sample-delayed feedback. The dynamics of this auxiliary coding scheme can be summarized as follows: 
 in which ? is the AWGN with zero mean and variance  . This noise is assumed to be independent of (x0,w). Additionally, we suppose that the initial state of B, J, and the delay are deterministic. 
 Theorem 4.2: For the NCS depicted in Fig. 1 and satisfying Assumptions 3.1 and 3.2, R(D) is lower bounded as follows if D ? (Dinf(h),8): 
 where the feasible set for the optimization problem defining ?0u(D) is all LTI filters B and the noise ? with s?2 ? R+ rendering the feedback loop of Fig. 2 internally stable and well-posed when J = 1. In these expressions,   and Su0 denote the steady-state variance of z0 and the steady-state power spectral density of u0 in Fig. 2, respectively. 
 Proof: Since D > Dinf(h), there is at least one pair, say Eˆ and Dˆ, satisfying Assumption 3.2, rendering the NCS of Fig. 1 SAWSS and producing zˆ, yˆ and uˆ where  holds and 
 can be concluded based on Theorem 4.1, if the conditions in Lemma 4.1 and Lemma 4.2 are met. It should be noted that Su? denotes the steady-state power spectral density of uˆG. A coding scheme comprised of linear filters with a unit-gain noisy channel and delay h can generate yˆG and uˆG which satisfy those conditions and keep  within (Dinf(h),8). Such a scheme is described as follows: 
 where ?ˆG(k) represents a Gaussian noise with zero mean and independent of  . Regarding the causality and linearity of Lk, uˆkG can be written as follows: 
 According to the causality in (16), joint SAWSS-ness of (yˆG,uˆG) and transitivity of asymptotic equivalence for products and sum of the matrices noted in [13], the sequences {Qk} and {Pk} are asymptotically equivalent to sequences of lower triangular Toeplitz matrices. Moreover, Lk renders the NCS internally stable and well-posed. With all of this in mind, by setting J = 1 and B as a concatenation of linear filters with the steady-state behaviour of Lk in (15) and considering a variance for ? equal to s?2ˆG, the system of Fig. 2 will be rendered well-posed and internally stable where Su0 = Su? and  are resulted. Then according to Lemma 4.2, the following can be concluded: 
 Lemma 4.3: Consider the LTI loop of Fig. 2 with fixed s?2 ? R+. Define  as follows: 
 in which Sr represents the steady-state power spectral density of r. Then for any ? > 0, upon the existence of the pair (B,J) = (B1,J1) making the system of Fig. 2 internally stable and well-posed, there exist another pair, comprised of 
 the biproper filter J2 and B2, which renders the feedback loop of Fig. 2 internally stable and well-posed, preserves the the steady-state power spectral density of z0 and satisfies the following: 
 Proof: It is well-known that the system of Fig. 2 is well-posed and internally stable if and only if the transfer function T from [?,w,?1,?2]T to [z0,y0,r,u0]T in Fig. 3 belongs to RH8. By Ti and ri, we refer to the transferfunction matrix T and signal r when B and J are set such that (B,J) = (Bi,Ji),i ? {1,2}. Moreover, Byi and Bri represent elements of B (B = [Br By]) in the situation where B = Bi. Now, consider the following set of filters: 
 in which d1 indicates the relative degree of J1 and V ? U8 is chosen in such a way that V (8) = 1. Consequently, J2 is biproper and T2 can be written as follows: 
 So regarding the definition of d1 and properties of V , T2 ? RH8 if and only if T1 ? RH8. Moreover, based on the same argument, using (B2,J2) would give the same power spectral density for z0 as for the case where (B1,J1) is utilized. Let d1,...,dm represent the zeros of Gr1 lying on the unit circle. Now for ? ? (0,1) we define the following: 
 Hence, V? ? U8 and V?(8) = 1 can be deduced for every ? ? (0,1). By following the same procedure as for the proof of [6, Theorem 5.2], the existence of ? ? (0,1) will be shown in such a way that for any ? > 0, setting V = V? will give a pair (B2,J2) that satisfies (19). 
 Corollary 4.1: If Assumptions 3.1 and 3.2 hold for the NCS of Fig. 1 and D ? (Dinf(h),8), then 
 where the optimization is done over all LTI filter pairs (B,J) and the noise variance  making the system in Fig. 2 internally stable and well-posed. 
 Proof: According to the feasibility of finding ?0u(D) (see [11] for the proof), there exist the triplet  , with B? a proper LTI filter and , that guarantees sz20 = D for the system of Fig. 2. Furthermore, based upon the definition of  and   in (13) and (18), the following can be derived for any ? > 0: 
 So regarding Lemma 4.3, since there exist a biproper filter J˜? and a proper one B˜? making the LTI feedback loop of Fig. 2 internally stable and well-posed and keeping   intact, the following can be concluded: 
 To characterize ?0(D), we will mostly use properties of linear systems and some results on H2 optimization with input-delay. Consider the auxilary structure of Fig. 4, where except for shifting the delay block to the plant model, which leads to 
 the same assumptions as Fig. 2 hold. The NCS of Fig. 4 is internally stable and well-posed if and only if the transfer function Ta from [?,w,?1,?2]T to [za,ya,ra,ua]T in Fig. 5 is a member of RH8. It can be easily shown that Ta = T. So the feedback loops of Fig. 4 and Fig. 2 are equivalent in the sense of internal stability and well-posed-ness. Moreover, the SNR and output variance of the NCS depicted in Fig. 2 can be stated in terms of H2-norms as follows: 
 (1 - Brz-1 - G22Jz-hBy)-1. Likewise, the following holds for the structure of Fig. 4: 
 and . So every triplet   that can infimize the SNR while making the system output satisfy   for the NCS of Fig. 4, can do the same for the the LTI system of interest, in Fig. 2, and vice versa. In other words, the NCSs in Fig. 2 and Fig. 4 are equivalent regarding the SNR-perfomance optimization problem in (23) as well. This problem is studied for such feedback systems as auxiliary system of Fig. 4 in [7]. Consequently, it can be concluded that the problem of finding ?0(D) is equivalent to an SNR-constrained optimal control problem which was proved to be convex. As another result, ?0(D) being a monotonically decreasing fuction of D can be deduced. All in all, the interplay between the desired performance, the average data rate and the time delay is characterized through 
 Consider the the following transfer function representation for the plant G in NCS of Fig. 1: 
 where (x0,w) satisfies Assumption 3.1. Using the results of the previous section, we simulate the lower bound on R(D) obtained in (23) regarding five different values of delay (h = {0,1,2,3,4}) and for each h, over a range of D > Dinf(h). Fig. 6 demonstrates the behaviour of the lower bound with respect to D and h. Additionally, it shows the operational rates when using scalar uniform quantizers for h = 0 and h = 4. First, as expected, ?0(D) in (23) is a monotonically decreasing function of D. Secondly and more importantly, Dinf(h) increases when h grows (see [14]). So greater delay yields worse best performance. The most significant 
 outcome is associated with the behaviour of R(D) in (5) with respect to delay. It can be observed from Fig. 6 that for a fixed D, ?0(D) is increasing in h. Therefore, a delay in the channel forces an increase in the required infimum data rate to achieve a quadratic level of performance. The greater delay, the higher rate to be spent in order to get a certain level of performance. Indeed, this finding extends the delay-free results of [7]. Another observation is the convergence of the obtained infimal data rates to the minimum rate required for stabilizability as D ? 8. As illustrated in Fig. 6, high rates are required to attain the ideal non-networked performance Dinf(h). Along the lines of [6], [7], we now simply replace the AWGN ? in the independent coding scheme depicted in Fig. 2, by a uniform scalar quantizer in order to assess the operational performance caused by a simple coding scheme. It is interesting to note that the obtained operational average data rate in Fig. 6 is at most around 0.3 bits away from the derived lower bound at all performance. 
 In this paper, rate-constrained networked control systems comprising noisy LTI plants, causal but otherwise arbitrary coding-control schemes and digital noiseless communication channels with time delay, have been studied. For such NCSs, a certain level of performance is attainable if and only if the average data rate does not fall below a minimal value. A lower bound on this infimum rate has been obtained. Through a numerical example, it has been illustrated that the channel’s time delay increases the infimum average data rate needed to achieve a prescribed level of performance. Moreover, by using a simple scalar quantizer, operational average data rates fairly close (around 0.3 bits) to the lower bound have been obtained. 
 G, x0 and w satisfy Assumption 3.1 and K follows u(k) = Kk(yk-h). Regarding the Gaussianity of x0 and w and the fact that G is LTI, it can be implied from some results in 
 in which   denotes the variance of output z and ? is the set of all proper LTI filters which render the system of Fig. 7 internally stable and well-posed. The assumptions considered for G guarantee that finding Dinf(h) is feasible. The feasibility of finding ?0u(D) in (13) and  in (23) follows from the feasibility of Dinf(h) [11]. 
 ﻿We obtain the maximum average data rates achievable over block-fading channels when the receiver has perfect channel state information (CSI), and only an entropy-constrained quantized approximation of this CSI is available at the transmitter. We assume channel gains in consecutive blocks are independent and identically distributed and consider a short term power constraint. Our analysis is valid for a wide variety of channel fading statistics, including Rician and Nakagami-m fading. For this situation, the problem translates into designing an optimal entropy-constrained quantizer to convey approximated CSI to the transmitter and to define a rate-adaptation policy for the latter so as to maximize average downlink data rate. A numerical procedure is presented which yields the thresholds and reconstruction points of the optimal quantizer, together with the associated maximum average downlink rates, by finding the roots of a small set of scalar functions of two scalar arguments. Utilizing this procedure, it is found that achieving the maximum downlink average capacity C requires, in some cases, time sharing between two regimes. In addition, it is found that, for an uplink entropy constraint H <¯ log2(L), a quantizer with more than L cells provides only a small capacity increase, especially at high SNRs. 
 Index Terms—Channel state information feedback, Information rates, fading channels, quantization, radio communication. 
 T is well known that the achievable data rates for reliable communication over a fading wireless channel depend on the availability of channel state information (CSI) at the transmitter and receiving end [1], [2]. For single-input singleoutput (SISO) flat fading channels, the CSI consists of channel gain and phase. If perfect CSI is available at the transmitter (perfect CSIT) and at the receiver (perfect CSIR), the channel is slowly fading and the transmission is subject to a long-term average power constraint, then the average capacity is achieved by adapting rate and power to the channel gain in a time waterfilling fashion [3], [4]. By contrast, if an instantaneous (per block) maximum power constraint is imposed, the fades are ergodic and the transmission blocks are long enough so that the fade statistics over each block converge to their ensemble statistics, then the ergodic channel capacity is achievable without CSIT [3], [5]. Else, if the fading is so slow that channel gain can be regarded as constant within each block (which corresponds to a block-fading scenario) then CSIT is beneficial. In this case, with perfect CSIT and per-block power constraint, the capacity is achieved by transmitting at maximum power, with only the data rate being adapted to the channel gain in each transmission block [3]. 
 If perfect CSIR is available and the receiver feeds back this CSI via an uplink with limited information throughput, then only imperfect CSI will be available at the transmitter. In a block-fading situation, the uncertainty at the transmitter about the true channel gain in each block implies a trade-off between throughput and reliability: the larger the data-rate chosen by the transmitter, the higher the probability of exceeding the channel capacity during the transmission block [6]. This poses the problem of encoding the CSI at the receiver and decoding it at the transmitter (i.e., choosing rate and power) in a rate-distortion optimal fashion, where the distortion is some measure of the decrease in downlink throughput, as in [3], [7] or the increase in error probability, as in [8]. 
 The capacity of memory-less block-fading SISO channels with long-term power constrained downlink transmission and fixed-rate constrained CSI feedback was studied in [9]. A similar situation was considered in [10], assuming a multilayer downlink coding scheme in which data-blocks are decoded perfectly or totally lost if transmission data-rate is, respectively, below or above the channel capacity during the block. The idea in [10] was to design a quantizer with a fixed number of quantization cells so as to maximize the expected downlink rate, i.e., the expected number (or long term average) of successfully decoded bits. Also for a constraint in the number of CSI quantization cells, [11] studied the maximization of downlink throughput considering a noisy feedback channel. There exist also numerous results related to downlink throughput maximization problems for multipleinput multiple-output (MIMO) wireless channels (see, e.g. [2], [12]–[14] and the references therein). Although not directly related to the SISO problem (which is the focus of this work), it is worth mentioning that, in all the MIMO results in [2], [12]–[14] and the references therein, the only constraint on the quantizer (where there is a quantizer) is its cardinality. In [15], the maximum SISO downlink average throughput under a long-term power constraint and for a fixed number of quantization cells is analyzed. The performance of zerooutage schemes (referred to as MASA schemes) was compared against that of average reliable throughput schemes (referred to as ART schemes), which allow for outages to occur. It is shown in [15] that, in some regimes, when the additional feedback load of the ART policies (associated with informing the transmitter of a previous outage) is counted in, MASA schemes outperform ART schemes. In that context, the feedback load refers to the entropy of the messages (quantized CSI plus ACKs and NACKs) that are sent to the transmitter. However, in [15] this entropy is evaluated a posteriori, i.e., after the quantizers have been optimized without considering uplink entropy as a constraint. 
 Thus, in all these papers, the design of optimal CSI quantizers has been addressed only considering a constraint on the number of quantization intervals (or cells). However, if one addresses the question “what is the maximum throughput that can be attained if there is a constraint on the amount of information that can be sent to the transmitter for representing the CSI?”, then it is more appropriate to consider an entropy constraint (instead of a cardinality constraint) for the CSI quantizer. On the other hand, the entropy of the quantized output, say H, is a lower bound to the average number of bits required to represent this output. At the same time, by using Huffman coding, it is possible to find prefix-free bitwords for each quantized CSI outcome with an average length not greater than H + 1 bits per CSI realization. Moreover, in a situation in which K i.i.d. CSI realizations are quantized at a time (which would happen, for example, in an OFDM system with K independently fading carriers), joint entropy coding would yield bitwords with an average length of K times H plus, at most, 1 bit. Since, in general, having fewer information bits to feed back for each CSI realization requires less average power, bandwidth or time, the latter benefits can be directly associated with a low entropy. This constitutes a practical motivation for considering entropy, instead of cardinality, as a constraint for the CSI quantizer. However, to the best of the authors’ knowledge, there are no available results on average downlink throughput maximization in which the quantizer utilized to encode CSI for the transmitter is to be designed subject to a constraint on the entropy of its output. 
 With the motivations stated in the previous paragraph, in this paper we study the problem of finding entropy-constrained quantizers with any given number of quantization intervals, for encoding block channel gains for the transmitter, that yield the largest average downlink data rate. In our setup, the downlink channel is assumed to experience i.i.d. block fading, with associated gains and phases perfectly known to the receiver. We consider a wide family of fading statistics, general enough to include Ricean and Nakagami-m fading channels with one or more degrees of freedom. As in [10], the uplink over which quantized CSI is fed back is an errorfree, zero-delay channel. To solve this problem, we propose a numerical method which yields the optimal quantization thresholds and reconstruction points for any given number of quantization intervals and average channel signal-to-noise ratio (SNR). The optimization problem is partly similar to the quantizer design problems addressed in [16]–[19] because of the common entropy constraint. However, as we shall see, since the distortion measure in this case is the decrease in average downlink rate (not mean squared error), the resulting situation is vastly different from the one encountered in standard entropy-constrained quantization. The CSI entropyconstrained coding problem turns out to be non-convex, and our analysis reveals that it has, in general, several local optima. Its Lagrangian formulation, for L = N +1 quantization cells, leads to a system of 3N + 2 non-linear equations in 3N + 2 unknowns, each of which taking values over the non-negative real numbers. Since each of these equations must be solved numerically, and due to the high-dimensionality of the search space, direct solution of this system of equations constitutes a high numerical complexity task. The numerical procedure introduced in this paper greatly reduces this complexity by turning the problem into finding the roots of a small set of scalar functions of two scalar arguments, only one of which having unbounded support. The evaluation of each of these functions involves solving N line-search problems with respect to monotonic functions. By applying this procedure, it is found that, in general, the maximum average downlink capacity C for a given uplink entropy H¯ is a non-concave function. Since in our formulation time sharing between two regimes yields an average capacity and entropy equal to the weighted averages of the capacity/entropy values of each regime, the region of all achievable C, H¯ pairs is given by the convex hull of the C v/s H¯ curve. On the other hand, it is found that if H¯ is log2 of the number of available quantization cells, then arranging the thresholds so as to obtain equiprobable cells is nearly optimal. Our results also allow one to find the gain in downlink average throughput of using an optimal entropy-constrained quantizer instead of a cardinality constrained optimal quantizer. For instance, when the average downlink SNR is 0 dB, then an entropy-coded quantizer with 3 levels and an average rate of 1 bit per CSI realization yields an 8% increase in average downlink throughput over an optimal fixed-rate quantizer with two levels (i.e., requiring the same average rate). The performance of the latter fixed-rate quantizer corresponds with the one found in [10]. It is also found that for any given maximum uplink entropy constraint H <¯ log2(L), the increase in maximum downlink capacity obtained by using a quantizer with more than L cells is relatively small. Moreover, our analysis also suggests that, for any given H¯, the maximum average downlink capacity is achieved using a quantizer with a finite number of cells. This contrasts with what is obtained also for an exponentially distributed source but with MSE as the distortion measure, wherein the optimal quantizer turns out to be uniform with infinitely-many levels [16]. 
 In the following section we present a precise model description, introduce some notation, and formally state the problem of interest. To illustrate some of the properties of this problem and its solutions, we first analyze the case N = 1 (two quantization cells), which can be solved explicitly, in Section III. Then we extend the analysis to the case N > 1 in Section IV, where we introduce the numerical procedure to solve the problem in its generality. Section V shows and analyzes the results obtained with this procedure for the case N = 2 and N = 3 under Rayleigh fading. Finally, Section VI draws conclusions. 
 We consider a block-fading downlink additive white Gaussian noise (AWGN) channel, a transmitter, a receiver and an error-free, zero-delay uplink channel, as depicted in Fig. 1. In the transmitter, the binary message sequence W is mapped into consecutive blocks of K symbols. During each block b ? N, a real-valued sequence   is transmitted over the downlink channel. The random block-channel gain magnitude for the b-th block, , is assumed constant within each block. Channel gains in consecutive blocks are i.i.d. according to a probability density function (PDF) satisfying the following: 
 Assumption 1: The PDF of the the channel gain’s squared magnitude, fg, has the form 
 for appropriate constants K1,K2 > 0, where the differentiable function  is such that the ratio  dßß(u(u)/du) is non increasing with respect to u over [0,8). 
 The structure of the PDF of g in (1) is fairly general. For example, if channel gain magnitude is Ricean distributed, then the PDF of g has the form 
 where I0(·) is the modified Bessel function of the first kind of order zero. From direct comparison with (1), we obtain, for this case,, K2 = 1/(2s2) and ß(u) =  . It can be verified (numerically) that the latter form of ß(·) satisfies the conditions required by Assumption 1. Likewise, if channel gain magnitudes are governed by a Nakagami-m distribution, then the PDF of g takes the form 
 and we have K1 = G(mm)m?m, K2 = m/? and ß(u) = um-1. If m = 1, it is easy to verify that ß(u) also satisfies the 
 Returning to Fig. 1, the real-valued random process nb[k], k ? {1,...,K}, is AWGN with sample variance N0. Thus, if  were the samples, taken at Nyquist frequency, of continuous-time AWGN band-limited to B [Hz], then the 
 1The necessity of the condition upon ß(·) in Assumption 1 will become evident in Lemma 1 (Section III), in which it allows us to prove the convexity of a function playing a key role in the problem under study. 
 two-sided PSD of the latter would be N0. On the other hand, the information-bearing signal xb[k] is subject to a per-block power constraint of the form 
 where M > 0. With this constraint, if the block-length K is large, then the maximum achievable data-rate during any given block b can be well approximated by Shannon’s capacity formula [20] as Cb = ln(1 + ? gb) nats/s/Hz, where 
 is the mean SNR at the receiver for a channel power gain g with unit mean value. 
 At the other end of the downlink channel, the receiver is assumed to acquire a perfect estimation of gb prior to (or at the beginning of) the b-th transmission block. This channel power gain is instantaneously quantized and entropy coded, with the resulting bits being sent over a zero-delay, error-free uplink channel. These assumptions about the feedback channel have been considered before in [7]–[10], [21]. The zero-delay condition can be expected to be a good approximation when the time spent to feed the quantized CSI back to the transmitter is much shorter than the duration of a downlink frame. In turn, it is possible to have an almost error-free feedback channel if the feedback SNR is sufficiently large and/or strong forward error correction is employed for the CSI bits. And naturally, if in a given situation these latter conditions are not present, then our results would provide upper bounds to achievable performance. 
 As foreshadowed in the Introduction, it is possible to translate a small entropy of the quantized CSI into using less average power, bandwidth or time to convey this CSI to the transmitter. At this point, it is perhaps worth noting that if only a single CSI realization is quantized and fed back at the beginning of each downlink block, then attaining these benefits may require one to match the channel coding and modulation scheme in the feedback link to the variable bitword lengths coming out of the entropy coder. For instance, placing an “off-the-shelf” channel coder and modulator in the feedback channel would yield an uplink that conveys only sequences of fixed-length data blocks. Such choice which would entail significant inefficiencies when transmitting variable-length bitwords, in comparison to sending fixedlength bitwords. However, in this scenario wherein a single CSI realization is quantized and fed back at a time, the Vi 
 feedback channel coding and modulation can be chosen so as to handle variable-length bit words (or the associated unequal probability outcomes of the quantizer) as efficiently as it is possible for fixed-rate quantizers. This can be done, e.g., by employing variable-length error-correcting codes [22] or joint source-channel coding (see, e.g., [23]–[25] and the references therein). Although the design of such coders and modulators is beyond the scope of this work, we illustrate this fact with an example (presenting a simple scheme similar in spirit to [26]), which can be found in Section VII-A in the Appendix. 
 Upon receiving the quantized CSI, the transmitter chooses a transmission data-rate rb from a discrete set of data-rates. To define the quantizer and its reconstruction values, let N + 
 1 be the number of quantization intervals (or cells), and let   denote the thresholds set, where 
 Define also the quantization cells Vi  [µi,µi+1), i = 0,...,N.  As in [10], whenever the channel power gain gb falls within cell Vi, the transmitter outputs a codeword   satisfying (4), belonging to the i-th codebook amongst N + 1 codebooks, one for each cell. This codebook is capacity-achieving for some nominal channel power gain ui associated with the cell Vi, i.e., 
 Thus, the power-gain levels  can be seen as the set of reconstruction points (or codepoints) of the quantizer, as represented in Fig. 2. 
 Since in each block the transmitter sends information over the downlink using a capacity-achieving code for a nominal channel gain ui, all the transmitted bits are correctly decoded if gb = ui. Else, if gb < ui, then ri is not supported by the channel, and the receiver declares an outage, discarding all the information received during the b-th block. From this, it is clear that a necessary condition for a set of reconstruction values to be optimal is 
 Let the random variable ub, taking values in   with probabilities Pr{ub = ui} = Pr{gb ? Vi}, denote the output of the quantizer for the b-th block. As already mentioned in Section I, we focus on quantizers that satisfy an entropy constraint, which we now formally state as 
 with H¯ = 0 representing the maximum entropy allowed for the quantizer’s output. 
 We are interested on finding the quantizer (i.e., the thresholds   and reconstruction points  ) satisfying the entropy constraint (9) and maximizing the average datarate in the downlink channel, defined as the average number of correctly decoded bits. The average number of correctly decoded bits (sometimes referred to as average “goodput” or average reliable throughput) is also the the objective function in [7], [10], [12]. It is a reasonable figure of merit if one assumes forward error correction followed by interleaving has been applied to the data being sent over the downlink, so that, with high probability, downlink blocks lost due to outage do not cause irrecoverable errors. Otherwise, and if all data is to be decoded correctly, ACKs and NACKs would have to be sent back over the uplink to request retransmission of lost data. As we shall see from the examples in Section V, at least for Rayleigh fading and for ? equal 0 dB and 30 dB, the optimal entropy-constrained CSI quantizers are such that only the first cell (V0) allows for outage events. The latter suggests that if sending ACKs and NACKs is necessary, then it would mean adding at most Pr{gb ? V0} bits per CSI realization to the uplink datarate. (The extra bit-rate is upper bounded by Pr{gb ? V0} bits/block because only when gb ? V0 it becomes necessary to send an ACK or NACK during block b + 1, which means at most 1-extra bit every time gb ? V0.) 
 Since fades in consecutive blocks are i.i.d. and ergodic, averages over a large number of blocks converge to ensemble averages. Thus, for notation simplicity, in the following we drop the frame and channel-use indexes b and k. With this, the quantizer design problem can be stated as finding the thresholds   and codepoints   satisfying (6) and (8), that maximize the average data rate in the downlink channel without exceeding the entropy constraint in the uplink. More precisely, combining (6), (7), (8) and (9), we state the optimization problem in canonical form as minimize: 
 with µ0 = 0 and µN+1 = 8, and where  is the cumulative distribution function (CDF) of g. 
 This optimization problem is difficult to solve primarily because the entropy constraint (10c) is non-convex. As we shall see, this leads to the existence of several local solutions, which, in principle, requires one to run an optimization program several times with a potentially large number of different starting values. In the following sections we solve this optimization problem, first explicitly for the case N = 1, and then numerically by means of Lagrangian optimization and a novel procedure which greatly reduces the overall complexity of the task. 
 We now address the optimization problem stated in (10) for the case N = 1, corresponding to two quantization cells. In this case, (10) reduces to: minimize: µ1,{ui}i1=0 
 This problem can be solved explicitly without using Lagrange multipliers by noticing that the entropy associated with the two cells depends only on the threshold µ1. Supposing µ1 is given, the optimal value of u0 is found by differentiating the objective function J with respect to it and equating to zero: 
 We see from this equation that for every u0 ? R+ there exists a unique µ1 > u0 for which u0 minimizes J. On the other hand, in order to determine the optimal value of u1 given µ1, we notice that this value has to minimize the term ln(1 + ?u1)(F(u1) - 1) in (11a). Although, in general, such value cannot be found explicitly, the following lemma guarantees that it is unique and that it can easily be found numerically: Lemma 1: Let f be a PDF satisfying Assumption 1. Define u 
 The proof of this lemma, which will play a key role later in Section IV, can be found in the Appendix, at the end of this document. 
 It turns out that the value of u which minimizes ?(u,µ)  ln(1 + ?u)[F(u) - F(µ)] increases monotoni- cally with µ. More precisely, define the function U(µ) 
 for any ? = 0, with the inequality being a consequence of the fact that F(·) is non-decreasing. Recalling from Lemma 1 that ?(u,µ) is convex in u, we conclude that  becomes zero at a single value of u greater than or equal to  . This proves that   holds for all µ ? (0,8). 
 The latter result implies that the optimal value for u0 must belong to the interval (0,?(?)), where . 
 Also, by applying Lemma 1 with µ = 8, it is readily found that the unique value of u1 that minimizes J in (11a) also equals ?(?). The convexity granted by Lemma 1 means that the latter function can be easily obtained numerically by line search. Moreover, Lemma 1 implies that sgn(?J/?u1) = sgn(u1-?(?)), which leads to the conclusion that the optimal value of u1 given µ1 is 
 It then follows that, if u0 is part of an optimal quantizer for some entropy constraint, then the optimal µ1 can be explicitly obtained from u0 by using (12), and then the optimal u1 can be directly derived from µ1 using (15). In this manner, by increasing u0 from 0 to ?(?) and evaluating µ1 and u1 with the latter equations, one generates a family of quantizers containing the optimal quantizer for every value of H¯. 
 Fig. 3 (top) shows the curve of downlink capacity versus uplink entropy obtained by the method described in the pre- 
 vious paragraph, for two quantization cells, under an average SNR ? = 1 (0 dB, left) and ? = 1000 (30 dB, right). More precisely, for every , the corresponding µ1 was calculated using (12). Then, for this value of µ1, the optimal codepoint u1 was determined using (15). In this way, for each value of u0, a different quantizer was obtained. The pair {downlink capacity, output entropy} associated with each quantizer yielded a single point in the top two graphs in Fig. 3. In this example, channel gain magnitudes distribute Rayleigh, so gb has exponential PDF, chosen to yield unitmean, i.e., fg(u) = e-u, ?u ? [0,8). Notice that, for every value of H <¯ 2, there exist two solutions to (12) and (15), corresponding to different local constrained minimizers of J in (11). There is one quantizer associated with each of these two solutions. One of these yielded the upper section of the curve in each of the plots shown in 3-top, and the other one yielded the lower section. Of course, the optimal quantizers are those responsible for the upper part of their respective curves, i.e., those which yield the maximum downlink capacity for a given uplink entropy constraint. As expected, in general, downlink capacity grows when the entropy of the quantized CSI available to the transmitter (uplink entropy) is increased. 
 Fig. 3. Solutions to (12) and (15) (two quantization cells) for Rayleigh fading with unit-mean channel power gain. Top: Downlink capacity v/s uplink entropy under an average SNR ? = 1 (0 dB, left) and ? =1000 (30 dB, right). Bottom: Thresholds and code-points for the optimal solution as a function of uplink entropy under an average SNR ? =1 (0 dB, left) and ? =1000 (30 dB, right). 
 Note that the maximum downlink capacity for ? = 1 (SNR = 0 dB), 0.52347 bits/s/Hz, occurs at an entropy of 0.85644 bits/block, not coinciding with maximum uplink entropy, which, for a two-cell quantizer is 1 bit/block. This is expectable for a quantizer with a fixed number of cells, since there is no reason why all cells being equally likely (the only situation in which the entropy is maximized) should yield the largest downlink capacity. Also, the maximum downlink capacities for a two-cell quantizer at 0 dB and 30 dB SNRs coincide with what was obtained in [10], where average downlink capacity was maximized without an entropy constraint on the quantized output. At an average SNR of 30 dB, Fig. 3 (top right) shows that the maximum downlink capacity occurs closer to maximum uplink entropy, and that equallylikely cells, corresponding to this maximum entropy, are near optimal. Interestingly, the maximum capacity curve at this SNR shows a breakpoint at an entropy of about 0.5 bits/block (where the curve crosses itself in Fig. 3 top right), which makes it non concave. For this case, this implies that better performance for entropies below   bits/block can be achieved by doing time-sharing between two quantizers: one with a single cell and zero entropy, and another with two cells and an entropy of about 0.88 bits/block. By choosing one regime more frequently than the other, it is possible to achieve a downlink capacity and an uplink entropy equal, respectively, to the weighted averages of the capacities and entropies of both regimes. In this manner, all capacity/entropy points within the convex hull of the capacity v/s entropy curves can be achieved. 
 Fig. 3 (bottom) shows the evolution of thresholds and codepoints for the optimal solution, as the entropy of the quantized output varies. For ? = 1 (0 dB), on the left, u1 = µ1 at all entropies. We see also that higher capacities are achieved by bringing thresholds and code-points closer together. For the 30 dB case, shown in Fig. 3 (bottom right), the breakpoint in capacities coincides with a change in the arrangement of thresholds and code-points in the quantizer. Except in a neighborhood to the right of this breakpoint, we find that the codepoint u1 coincides with its left-boundary threshold µ1. 
 The straightforward approach presented in the previous section cannot be extended directly to the case in which there are more than two quantization cells. Lagrangian optimization can be utilized instead, which, as we shall see, leads to a system of non-linear equations that must be solved numerically. The non convexity of the optimization problem (10) and the existence of several local minimizers satisfying the constraints imposes the need to solve this system of non-linear equations possibly many times with different initial values. However, we will introduce an algorithm, in the same spirit as the strategy illustrated in the previous section, that allows one to simplify the Lagrangian optimization problem to a sequence of simple line search problems, each with a single solution. 
 Before stating the Lagrangian associated with (10), we note from (10a) that, at the optimum, the inequality constraints stated in (10e) are not active. Similarly, constraint (10d) for the case i = 0 is not active since increasing u0 above µ0 = 0 would raise the average downlink data rate without increasing the entropy of the quantized output. 
 Taking the above observations into account, the Lagrangian associated with (10) adopts the following form 
 Lagrange multipliers. Differentiating L with respect to u0 and equating to zero gives 
 Notice that (17a) is identical to (12), which implies that u0 is a solution to (17a) only if 
 Differentiating L with respect to the other codepoint values and equating to zero yields 
 On the other hand, differentiating the Lagrangian with respect to the thresholds we obtain 
 Finally, the Karush-Kuhn-Tucker (KKT) conditions [27], [28] provide another set of N + 1 equations, 
 Although it is possible to solve this system of 3N + 2 non-linear equations by standard numerical algorithms, the existence of numerous local minima requires one to apply these algorithms repeatedly, each time with different initial values. This shortcoming is worsened by the fact that the vector of initial values (Lagrange multipliers plus threshold and code-point values) lies in a (3N + 2)-dimensional space, which implies a large number of initial guesses is required to obtain a “reasonably good” coverage of the search space. In the following section we will show how these inconveniences can be avoided by using an approach similar to the one presented in Section III. More precisely, we derive a method which, for this problem, allows one to find all local minima in a systematic, sequential manner, greatly reducing the number of required computations. 
 In this section we exploit the recursive nature of (17) to reduce its induced system of non-linear equations into a sequence of line-search problems over bounded intervals, in a spirit similar to the one behind the approach followed in Section III. 
 To begin with, recall that the KKT conditions imply that, for every constrained (local) minimizer of (10a), the multiplier ?j > 0 only if uj = µj (i.e., only if the associated constraint is active). The next corollary of Lemma 1 provides an easyto-verify condition for uj = µj to hold in such a minimizer: 
 Suppose   are a solution to optimization problem (10). Then uj = µj holds for some j ? {1,...,N} if and only if 
 Proof: The result follows directly from Lemma 1, since it implies the convexity of the function -ln(1 + ?u)(Fg(µj+1) - Fg(u)), and from (10a), upon recalling that the entropy of the quantized output does not depend on the choice of codepoint values. 
 Corollary 1 will allow us to find uj and µj+1 from ?, uj-1, µj and µj-1 in a simple manner. For this purpose, define, for  ,...,N where 
 is the complementary CDF of g evaluated at µj. With these functions, the combined conditions (17c), (17d), (17g) and (17i) can be written in the following equivalent form 
 ?j = ?j(uj,?j+1) = sj(?j+1) = 0, j = 1,...,N (20a) ?j(µj - uj) = 0, j = 1,...,N. (20b) 
 Fig. 4. Plots of ?j(µj,?j+1) and sj(?j+1) (defined in (19)), as functions of ?j+1. Left: a case in which  . Right: a case in which 
 Figure 4 shows a (qualitative) description of ?j(µj,?j+1) and sj(?j+1) as functions of ?j+1. It can be seen that both functions are monotonically increasing, the first one being affine, the second one convex. 
 A look at Fig. 4 immediately suggests that, for any given j, and depending on the values of the parameters ?, ?, uj-1, µj-1 and µj, there will be, in general, more than one pair of values of uj and µj+1 satisfying (20). Let us find out which solutions actually exists by first considering the conditions under which the constraint uj = µj is active or inactive: 
 • Inactive Constraint: In this case, uj > µj, which implies ?j = 0 (see (20b)). In view of (20), this is equivalent to having 
 The first equality of (21) is satisfied by a unique value of the argument of s(·), say  , shown in Fig. 4. 
 From the definition of ?j+1, this quantity must be nonnegative. On the other hand, Corollary 1 implies that uj > µj if and only if 
 situation exemplified and illustrated in Fig. 4-right. In addition, Lemma 1 states that, if this inequality is satisfied, then there exists a unique uj satisfying the second equality of (21). Thus, there exists a solution to (20) for which the constraint uj = µj is inactive if and only if   and (23) holds. In this case, we say solution (0) exists for j. 
 • Active Constraint: In this case, uj = µj, and ?j can be positive. By looking at (20), a solution satisfying this condition exists if and only if there is ?j+1 for which 
 which correspond to intersections of the plots of ?(µj,·) and s(·) occurring on the first quadrant in Fig. 4. Since, with respect to ?j+1, the function ?j(µj,?j+1) is affine and increasing, and the function s(?j+1) is convex and monotonically increasing, it follows that (24) is satisfied for either none, one or two values of ?j+1. Indeed, these properties imply that two solutions to equality (a) in (24) will exist if and only if 
 is the unique value of x at which ??(µj,x)/?x = ?s(x)?x, see Fig. 4. It is also easy to show that if (25) holds, these two solutions, say  and , will lie at opposite sides of . Also, it is straightforward to verify that both solutions are not larger than ?j. Therefore, 
 problem, each solution to equality (a) in (24) will also be a solution to (20) if and only if it is non-negative and it yields a non-negative value for s(·). This allows one to discard solution   or if 
 . On the converse, if these two conditions do not hold, solution   will be non-negative and yield 
 , i.e., it will be a valid solution to (20). In this case, it is easy to verify that 
 , with i = 0,1,2, be the threshold value associated with ?j+1, we can devise the following procedure to find the solutions to (20) for a single, given j: 
 Procedure 1: Suppose ? = 0, and that uj-1, µj-1 and µj are given, with 0 = µj-1 = uj-1 = µj. Then, in order to find the solutions to (20) for j, 
 a) Find  by solving equality (a) in (24) with respect to ?j+1 by line search over  , 
 , find   solving equality (a) in (24) by line search over   (unless , in which case  ). Set uj = µj, and 
 The last step, where solutions yielding ?j+1 < 0 are discarded whenever j < N, responds to the fact that ?j+1 is a complementary CDF value. This requirement is dropped only if one is calculating the last threshold (i.e., when j = N), to allow a higher-level routine to iteratively adjust ? so that ?N+1 = 0. (Such a routine is exemplified by Procedure 3 below.) Of course, unless j = N, solution (0) can be discarded before doing the corresponding line search if  . The same applies for solution (1) if . 
 In each of the line searches mentioned in Procedure 1, there exists a single solution over the corresponding search interval, since, in all cases, the involved function is monotonic within it. This makes each step straightforward to execute. Notice also that for every j = 1, and depending on the values of uj-1, µj-1 and µj, there are between zero and three pairs of values for uj, µj+1, namely solutions (0), (1) and (2), which satisfy (20) for that j. 
 The above procedure can be applied sequentially to find all pairs uj, µj+1, for j = 1,2,...,N, that satisfy (20). More precisely, one can first choose values for u0 ? (0,?(?)] and  , with which one can calculate µ1 explicitly from (17a). Then, setting j = 1, Procedure 1 can be carried out to find at most three pairs of values for u1, µ2 satisfying (20) for j = 1. Each solution can be considered a branch in a tree structure. Increasing j by one and repeating the procedure for each emerging branch, until j = N, yields the complete tree of valid solutions, each of which is associated with a path in the tree. Thus, for each choice of u0 and ?, there can be at most 3N different paths, each associated with a sequence of codepoints and thresholds satisfying (20) for all j = 1,2,...,N. However, we shall see in the following that the number of valid paths obtained in practice is much smaller than 3N. 
 Following our notation for solutions adopted in Procedure 1, we label each solution path in the tree using the numbers of the solution types associated with its segments, thus referring to a sequence . For a given choice 
 It is important to note that there is a one-to-one relationship between every solution   satisfying (20) for a given choice of u0,? and every path in O(u0,?). Indeed, the path associated with any such solution can be easily determined by analyzing each of its code-point/threshold pairs using the the reasoning that led to Procedure 1. 
 Now, suppose one wants to know whether a given path  is associated with a valid solution to (20) and then find this solution. Instead of applying Procedure 1 to find the entire tree of valid paths and then checking if  is one of them, one can apply the following algorithm, derived directly from Procedure 1, for this purpose: 
 Procedure 2: Let u0 ? (0,?(?)], ? = 0 and the corresponding µ1 be given by (17a). Let  be a path. 
 Then the following steps can be taken to determine whether   and find its associated thresholds and code- 
 With the above procedure, the resulting values of all thresholds and codepoints are a function of  . For convenience, we denote this function by , defined 
 where we have chosen † as a special symbol to indicate that the path  does not yield a valid solution. For future use, we also define 
 Although being a solution to (20) is a necessary condition for code-points and thresholds to be a solution to (17), two additional conditions must be satisfied for sufficiency. The first one is the entropy constraint, expressed in the KKT equation (17f). The second is the construction condition µN+1 = 8 or, equivalently, ?j+1 = 0. For a given path , u0 and ?, this condition can be expressed as . 
 is the set of all paths , values for the first codepoint u0 and Lagrange multiplier ? associated with solutions to (17) for some entropy constraint, while S    corresponds to the set of all 
 , respectively, as the entropy and capacity associated with , the original optimization problem (10) can be stated as 
 Thus, we have reduced the problem from solving a set of (3N + 2) non-linear equations, over a (3N + 2)-dimensional space, into a moderate number of searches (one per valid path), over two dimensions, requiring the evaluation of a scalar function that takes N line searches to compute. 
 Define a grid of values of u0 over (0,?(?)), say U, and a grid of values for ? ? [0,?max], say ?, for some ?max. 
 Detect pairs of consecutive values of ? between which a sign change of ?N+1 occurs for some valid path. In each of the intervals formed by such pairs, find the value of ? for which ?N+1 = 0 by line search, running Procedure 2 for the corresponding path. 
 Calculate  and  for each of the   values found in the previous step. Select the combination that yields the largest capacity with an entropy not greater than H¯. 
 In the following section we present an example in which Procedure 3 was utilized to find the solution to (31) for N = 3. 
 In this section we apply Procedure 3 to find the set G (see (30)), for the case N = 3, i.e., for quantizers having four cells, assuming vg  is Rayleigh fading with g being unitmean. The latter set contains the paths , values for u0 and ? that characterize a solution to (17), i.e., quantizer thresholds and code-points that yield a local maximum (or minimum) downlink capacity for some fixed maximum uplink entropy constraint H¯. The obtained results are presented in Fig. 5, for ? = 1 (SNR=0 dB), on the left, and for ? = 1000 (SNR=30 dB), on the right. 
 The two top graphics in this figure correspond to downlink capacity v/s uplink entropy plots for all solutions to (17). It can be seen that, although for each SNR there exist several such solutions, for each entropy constraint the number of these solutions is significantly smaller than 3N = 27. For the 0 dB case, the absolute maximum downlink capacity is 0.6466 bits/s/Hz, attained with an uplink entropy of 1.842 bits/block, by a solution with associated path . The curve that yields this maximum is ended at that point and, to the eye, it seems as if there was a missing segment which would connect it to the curve that reaches the right boundary of the plot, at an entropy of 2 bits/block. The absence of this segment can be attributed to the fact that in the Lagrangian formulation of the problem, the uplink entropy is an inequality (and not an equality) constraint. 
 The solution yielding the maximum downlink capacity for a given uplink entropy is given by the highest curve in each of the capacity v/s entropy plots. Interestingly, for entropies somewhat below log2(3) and log2(2) bits/block, the highestcapacity curve coincides with the optimal solution with 3 and 2 quantization cells, respectively. This can be seen by noticing from the bottom plots in Fig. 5 that a few bits/block below these entropy values, the optimal solution is such that one, two or three thresholds, respectively, tend to infinity, effectively leaving three, two and then one cell, as the entropy is decreased.  Such behaviour suggests that for a finite uplink entropy constraint, the maximum downlink capacity over all quantizers is achieved with a quantizer having a finite number of cells. 
 As already observed for the same SNR and two cells, the region of achievable downlink average capacities is given by the convex hull formed by all the curves in the capacity v/s entropy plots. Unlike what is observed for ? = 1, since the composite maximum capacity curve for ? = 1000 (Fig. 5 top-right) is not concave, achieving the maximum downlink capacity for some entropy constraint values requires timesharing between two regimes. 
 The point of maximum capacity for each number of quantization levels corresponds to the solution obtained when a quantizer with that number of levels is optimized for maximum average downlink throughput without an entropy constraint. Therefore, those peak points are the solutions found in [10] for single-layer coding, where the quantizer was optimized under a cardinality constraint only. From this fact, we can conclude from Fig. 5 (top left) that at SNR = 0 dB, and for the same average uplink rate of an optimal fixed-rate quantizer from [10] with two levels (that is, 1 bit per CSI realization), which achieves 0.52347 bits/s/Hz, an optimal entropy-coded quantizer with 4 quantization cells yields (approximately) 0.56 bits/s/Hz. This represents an increase of roughly 8% in downlink average throughput for the same average uplink rate. The corresponding increase with respect to a fixed-rate quantizer with 3 cells goes from 0.6 bits/s/Hz (at a fixed 
 Fig. 5. Downlink capacity v/s quantized output entropy for the solutions to (17), for Rayleigh fading with unit-mean channel power gain, using up to 4 quantization cells (N = 3). Left: Average SNR ? = 1 (0 dB). Right: Average SNR ? = 1000 (30 dB). 
 0.63 bits/s/Hz (using an optimal entropy-coded quantizer with 4 cells). For an SNR of 30 dB, Fig. 5 (top right) reveals that an optimal entropy-constrained quantizer provides smaller gains over fixed-rate optimal quantizers. Notice also that fixedrate quantizers can only operate at a limited set of uplink rates (given by log2 N bits/CSI realization), Thus, variablerate entropy-coded quantization is the only scheme which allows one to send quantized CSI using other average uplink rates (for example, rates below 1 bit/CSI realization). 
 We have proposed a numerical procedure to find the maximum downlink average capacity over block-fading channels, under a fixed per-block power constraint, when the receiver has perfect CSI and an error-less, delay-free, entropyconstrained uplink channel is available to convey quantized CSI to the transmitter. This procedure, which has a smaller numerical complexity than trying to directly solve the Lagrangian equations associated with the problem, also yields the quantizer thresholds and codepoints that achieve the optimal solution. Our results are valid for a broad class of channel fade distributions, including Nakagami and Ricean fading. We have applied the procedure proposed here to find optimal quantizers having 2 and 4 quantization cells. The obtained results revealed that, for a given number of quantization cells, say L, maximum capacity is achieved at an uplink entropy slightly below log2(L) bits/block. Furthermore, our results show that for any uplink entropy below log2(L) bits/block, there is little to be gained (in average downlink capacity) by using more than L quantization cells or intervals. This suggests that for any finite uplink entropy, the optimal quantizer has a finite number of quantization intervals. Our analysis also revealed that for high average SNRs, achieving the maximum average downlink capacity requires time sharing between two regimes with different uplink entropies and associated capacities. 
 As a final remark, we would like to mention that, after several attempts, the authors have found that the results and strategies developed here for a short-term power constraint do not seem to be applicable for the long-term power constrained version of the problem. Indeed, the latter problem appears to be significantly harder to solve than both the one addressed in this paper and the long-term problem without the uplink entropy constraint. 
 Here we provide an example to illustrate how a discrete random source with small entropy (which can be associated with the variable-length bitwords coming out of an entropy coder) can be efficiently transmitted using a matched channel coder. The latter coder is able to transmit the low-entropy source using less power and with a smaller message error probability than what is obtained when transmitting equiprobable symbols using 4-QAM modulation and maximum likelihood decoding. For this purpose, suppose we have two quantizers, each with four quantization cells. The first quantizer is not entropy coded, and each of its outcomes, represented using two bits, has equal probability. For simplicity, suppose that this quantizer is followed by a rate 1/2 error-correcting channel coder and that 4-QAM is utilized. Assume each symbol in the 4-QAM constellation has unit energy and that there is a memoryless channel in the uplink, with complex circularly symmetric white Gaussian noise with variance sn2. Therefore, each quantized outcome (or message) is mapped into channelsymbol sequences of length two (that is, two consecutive 4QAM symbols are sent for each message), which yields an average energy of 2. At the other end, the decoding is done by by picking the most likely transmitted symbol sequence given the received signal. For this scheme, it can be found (either analytically or via simulations) that at an SNR of 10 dB, the message error probability is approximately 0.9 × 10-3. 
 For the entropy-coded quantizer, suppose that its four possible outcomes, say m1, m2, m3, m4, have probabilities 1/2, 1/4, 1/8 and 1/8, respectively. An entropy coder for this quantizer (for example, a Huffman coder) would output 1, 2, 3 and 3 bits, respectively, for each of its outcomes. In order to send these four outcomes (or messages) over the feedback channel, consider a time-varying digital modulator generating symbols from the constellations shown in Fig 6 (top). Each of these symbols (except symbol o) has unit energy. 
 An outcome from the entropy-coded quantizer is fed back to the transmitter by sending a sequence of three channel symbols, each coming from the first, second and third constellation, as specified in the table of Fig 6. Notice that messages with higher probability are mapped to symbol sequences with smaller total energy, the latter being proportional to the length of the bitwords yielded by a Huffman entropy coder (i.e., proportional to -log2(pmi), where pmi is the probability of the i-th message or quantizer’s outcome). This corresponds to the sequence-length (or energy) distribution that minimizes average energy, the latter energy being in this case equal to 7/4. Therefore, the average energy required by the channel coding scheme for the entropy-coded quantizer is just 7/8 of the mean energy associated with the fixed-rate quantizer. On top of this power reduction gain, if this “variable-rate” scheme matched to the entropy-coded quantizer is combined with a maximum-likelihood sequence decoder at an SNR of 10 dB, then each message is decoded with a message error probability not greater than 0.6 × 10-3. This shows that if the outcomes of the CSI quantizer have uneven probabilities (which amounts to a smaller entropy than with equi-probable outcomes), then it is possible to transmit the quantized CSI using less power and with a smaller probability of error than when transmitting the outcomes of a fixed-rate quantizer. 
 where we use f as a short-hand notation for df/du. For every u such that f(u) = 0, it readily follows from the structure of  ·) (see (1)) that f (u) = 0 and therefore (33) immediately yields d2a(u)/du2 = 0. The same holds for u = 0. Thus, it is only left to consider values of u > 0 and such that f(u) > 0. If da(u)/du = 0, then, from (32), 
 Since we are only considering the cases in which f(u) > 0, (34) implies F(µ) - F(u) > 0. This allows one to 
 Since we are considering values of u such that f(u) > 0, we can substitute (36) into (33), obtaining 
 It then follows that (35) also holds if da(u)/du > 0. We therefore conclude that, irrespective of the sign of da(u)/du, 
 Fig. 6. Top: Three consecutive constellations of a digital modulator adapted for transmitting the outcomes of an entropy-coded quantizer with 4 quantization cells. Symbols a,b,c,d have unit energy. Bottom: A mapping between quantization outcomes (messages m1 to m4) and channel symbol sequences. The 
 Since   is a non-increasing function, it follows that the RHS of (42) is positive. Therefore, we obtain from (39) that a(u) is also convex for all u ? (0,µ] such that f(u) > 0. This completes the proof. 
 ﻿ We analyze the behavior of the mean squared error achievable by , uniform scalar quantization feedback , and post of unrestricted order , when wide sense stationary discrete time random possibly unbounded support . Our are based upon the use of uniform scalar . We consider the number of quantization to be given and fixed , which itself to fixed rate , and focus on the in which is insufficient to avoid overload . In order to guarantee the stability of the closed loop , we consider the use of a clipper before the scalar . Our are valid for zero mean independent whose satisfy some mild , which are met by infinite support such as and . We show that , for fixed , the can be made to decay with the ratio as when to infinity , where . We note that the latter bound is asymptotic in but not in , and that it clipping . 
 T is well know that can reduce the magnitude of the reconstruction error that from the of an source , see ., e .., . This reduction is by to digital such as sigma delta , which have been successfully in audio and image quantization . 
 , as , where is the ratio and the order of the feedback filter assumed fixed for all of . In their analysis , the of an additive noise model , in which quantization are assumed to form a wide sense stationary ... random process , white and uncorrelated with the input . Also the , it was recently shown in that by different of unrestricted order for each value of , the 
 can be made to decay as , where the signal to noise ratio of the scalar . The analysis in and restrict to the where the effect of overload is negligible , which cannot be if when the source unbounded support unless infinitely many quantization are available . Indeed , an important body of literature related to quantization overload either by careful design of the 
 or by simply assuming there exist enough quantization to avoid overload , see e .., , and the therein . 
 of bit two level in which the , , by following a deterministic approach . The in yield a continuous time reconstruction error that can be term proportional to , where 
 is independent of . In turn , the continuous time reconstruction error with the in can be uniformly bounded as when . This immediately to an that as , when . To the best of the author knowledge , the latter is the decay rate of the reconstruction error with available in the literature . 
 However , the in and have not been extended to with more than two quantization , and rely upon the input being uniformly bounded . On the other hand , available on the quantization of unbounded the effects of overload do not consider , see , e .., and the therein . 
 In this letter , we study the behaviour of the with increasing ratio when the source is a possibly unbounded wide sense stationary ... band limited process . Our analysis is based upon the use of a uniform scalar , by a clipper , together with feedback , and post of unrestricted order see Fig . . We focus on the in which the number of quantization is insufficient to avoid overload . We show that , for this architecture , the 
 Assumption : The source process independent , with zero mean and symmetric probability density function . Moreover , there a constant such that the th of each satisfy 
 This letter the work in by taking account of clipping in the analysis . 
 Our are related with the feedback quantization architecture shown in Fig . . In this scheme , the 
 form a zero mean ... process , from sampling a ... band limited signal . For each ratio 
 In and is the square root of the of the input process when . It is assumed that the input process finite power , i . e ., that . For simplicity , we shall further restrict the analysis to the in which . Notice also from that the total power of in of variance per sample , remains constant for all . 
 . interval and reconstruction . is a random process with i . i .. independent of and uniformly distributed over the interval dither to the input of the the range for the input signal over which overload cannot occur . Since the dither is distributed over , this range is . It is well known that such a dither signal a quantization error process with i . i .. which are also independent of the source , , provided 
 overload . Quantization error appear in the output as the stationary process . In order to keep as shown in Fig . . The clipper the value of the input signal so that , if , or 
 thus stability , see . The key point here is that , unlike overload , clipping , given by , are not into the feedback loop . Instead , clipping appear in the output after being by , to yield the process . Unless the source 
 is a stationary process , one cannot guarantee that the of the clipping error will form a stationary , or even a ..., random process . In order to quantify the contribution of clipping to the for not necessarily stationary , we define the average power of clipping in the output as 
 Two important the under which the combination of clipper and operate are the signal to noise ratio 
 It from and that , if and are kept fixed , then can only be at the expense of reducing the at 
 For the scheme of Fig . , it was shown in that the reconstruction due to granular quantization only , which here to , can be upper bounded as 
 i . e ., on , for every , where . In and , can be any bounded , nonzero gain . With the optimal in , to and the variance via 
 Notice the upper bound on the due to granular quantization in exponentially with . However , the behaviour of the average power of clipping with increasing is unknown . Therefore , in view of , the exponential decay of given by does not necessarily hold for , for sufficiently large . In the next section we find an upper bound to the total average power of the reconstruction error , clipping . 
 Proof : From one of , given in , Sec . . , we have that 
 every , the bound for the first inequality in is with . Substituting this into 
 . The latter , together with the fact that directly to , the proof . 
 The following theorem an upper bound for applicable but not restricted to in which the source unbounded support . 
 Theorem : Suppose there a scalar such that see . Suppose that Assumption 
 : A BOUND ON THE OF QUANTIZATION WITH FEEDBACK 
 Thus , we have an upper bound on the due to clipping that linearly with and exponentially with provided the product does not tend to zero as , see . 
 The above upper bound for does not tend to zero with increasing unless one the loading factor grow with fast enough . Substituting and into 
 . From the latter , we have that , where . Thus , the term due to clipping in 
 can be reduced only at the expense of operate at a lower . This , in turn , the term due to granular decay more slowly with increasing . 
 The optimal decay rate when is by choosing and so as to make granular and clipping error decay at the same asymptotic rate . This is if and only if and are chosen so that 
 . Before the above limit , note that from we obtain since , being a random variable uniformly distributed over , standard deviation and with . rule to twice and substituting by , we obtain that 
 We have studied the asymptotic behaviour of the reconstruction of fixed rate quantization with feedback as the ratio to infinity , for ... possibly unbounded support . It was shown that with the proper choice of and loading factor for each , the can decrease with at least as fast as , where does not depend on . 
  
 ﻿In this work, we deal with zero-delay source coding of an unstable vector Gaussian Auto-Regressive (AR) source under average mean square error fidelity criterion. To begin with, we consider zero-delay coding of a vector Gaussian AR(1) source modeled in state space form. It turns out to be convenient to use an asymptotically stationary time-domain scheme with feedback, which was recently proposed in [1] in the context of filtering theory applications via the Information Nonanticipative Rate Distortion Function (INRDF). In this scheme, the feedback path comprises a Kalman filter, which produces an estimate of the source. Our idea is then to encode the vector innovations due to Kalman filtering via lattice quantization with subtractive dither and memoryless entropy coding. We show that the resulting system is stable and furthermore provide new bounds to the zerodelay RDF. We then generalize our results to vector Gaussian AR sources of any order. With this, we are able to establish a timedomain approach with feedback, which by use of a single filter gets very close to the zero-delay RDF for vector Gaussian AR sources of any order. Interestingly, for infinite dimensional vector sources, the INRDF coincides with the Optimal Performance Theoretically Attainable (OPTA) by zero-delay codes.
 INTRODUCTION
 Zero-delay processing of information in source coding is the reproduction of each source sample at the same time instant that the source sample is encoded. Zero-delay source coding is desirable in various applications, like for instance, signal processing [2] and network control systems [3], [4]. The class of zero-delay codes is a subclass of the so-called causal source codes [5] where the reproduction of the current source sample depends only on the present and past source samples but not on the future source samples. A preference for zero-delay source coding over causal source coding stems from the fact that the latter does not exclude the possibility of long blocks of quantized samples, which allows for arbitrary end-to-end delays. Of course, every zero-delay code needs to be causal, but the opposite is not true.
 It is well known that zero-delay codes (and causal codes) in constrast with non-causal codes cannot achieve the classical rate distortion function (RDF) [6], i.e., the optimal performance theoretically attainable (OPTA) by the class of noncausal codes. Indeed, an open problem in information theory is quantifying the gap between the OPTA by non-causal, denoted hereinafter by R(D), and the OPTA by causal and zerodelay codes, hereinafter denoted by rcop(D) and RZDop (D), respectively. Notable exceptions where this gap is explicitly found are memoryless sources [5], stationary sources in high rates [7], and zero mean stationary scalar Gaussian sources with average mean square error (MSE) distortion [8].
 Throughout the years, the interest in studying information theoretic rate distortion functions that performs as tight as possible to the OPTA by causal or zero-delay codes has been very high. For example, the authors in [9] introduced the so-called nonanticipatory -entropy, hereinafter denoted by  , to demonstrate its utility in real-time (zero-delay) applications. Perhaps the most striking result inspired by  , is the work of [10] where the author introduced the so-called sequential RDF, hereinafter denoted by RSRD(D), to investigate control related applications. A decade later, the authors in [1], introduced the so-called information nonanticipative RDF (INRDF), denoted by Rna(D), in the context of filtering theory applications . The work of [1] complemented with the work of [12] implies that under squared error distortion constraints, for any AR(1) source Xt, the optimal reproduction distribution of the output Yt, conditioned upon Xt , {X0,...,Xt} and Y t-1 , {Y0,...,Yt-1}, simplifies to P(Yt|Y t-1,Xt), i.e., P(Yt|Y t-1,Xt) = P(Yt|Y t-1,Xt) . Using the latter structural result, the authors in [12] used an unstable Gaussian AR(1) source and showed a lower bound to RZDop (D). This bound can be achieved by a feedback realization that was first proposed in [1], where a Kalman filter provides a prediction Xˆt|t-1 of Xt to the encoder, which then “encodes” via scalings that act as amplifiers the prediction error (innovations) Xt - Xˆt|t-1. Interestingly, it turns out that the Kalman filter in [12] is obtained from the noisy observations and thereby parallel a result of Zamir et al. [13], on achieving the noncausal RDF of a stable, stationary, and scalar Gaussian process by noisy prediction. Note that for stable stationary Gaussian sources, the use of Kalman filter is not mandatory. In this paper, we show the following:
 For the case of zero mean unstable  vector fully observed Gaussian AR(1) sources with average MSE fidelity, the feedback realization scheme of [1] complemented by the work in [12], implies that an upper bound to the RZDop (D) can be obtained by quantizing the innovations due to using a Kalman filter in a feedback loop, which predicts the source Xt based on its noisy past Y t-1. We provide a simple quantization strategy and assess the rate loss as compared to the achievable lower bound (see Theorem 1). In addition, we show how to generalize our scheme to vector Gaussian AR sources of any order (see Theorem 2).
 When the vector source is unstable, it is necessary to guarantee that the feedback realization is stable under quantization. We provide an upper bound to the RZDop (D) due to finite-dimensional quantization of the innovations. We show that the rate loss is finite and directly linked to the space-filling loss of the quantizer as well as the loss of the entropy coder due to zero-delay constraints. If the vector dimension of the Markov source tends to infinity, it is possible to completely eliminate the rate loss and thereby show that in this limit, the OPTA coincides with the INRDF (see Section IV-B).
 This paper is structured as follows. In Section II we cast our problem. In Sections III and IV we derive upper and lower bounds to the OPTA by zero-delay codes and we discuss possible extensions and related results. In Section V we draw conclusions.
 Notation: We let R = (-8,8), N0 = {0,1,...}. For t ? N0, we denote random variables and p-dimensional vectors with boldface letters, i.e., xt ? R and xt ? Rp, respectively.
 T
 The transpose of a matrix or vector Z is denoted by Z . We denote by {·}G any term that is Gaussian. For a square matrix Z ? Rn×n with entries ?ij on the ith row and jth column, we denote by diag{Z} the matrix having ?ii, i = 1,...,n, on its diagonal and zero elsewhere. We denote the time index with “t” and the dimension index with “i”.
 PROBLEM STATEMENT
 In this paper we consider the zero-delay source coding setting illustrated in Fig. 1. In this setting, the p-dimensional (vector) Gaussian source is governed by the following discretetime linear time-invariant state-space model
 	xt+1 = Axt + Bwt, t ? N0,	(1)
 where A ? Rp×p, and B ? Rp×q are known, x0 ? Rp ~
 N(0;Sx0) is the initial state, and the noise process wt ? Rq is an i.i.d. Gaussian N(0;Iq×q) sequence, independent of x0.
 The system operates as follows. At every time step t = 1,2,..., the encoder observes the source xt and produces a single binary codeword zt from a predefined alphabet set Zt of at most a countable number of codewords. Since the source is random, zt and its length lt are random variables. Upon receiving zt, the decoder produces an estimate yt of the source sample. We assume that both the encoder and decoder process information without delay and they are allowed to have infinite memory of the past.
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable-length binary codes zt. The countable set of all codewords (codebook) Zt is time-varying
 
 Fig. 1: A zero-delay source coding scenario using variablelength binary codewords.
 to allow the binary representation zt to be an arbitrarily long sequence. There is no loss by restricting to uniquely decodable codes [3]. The encoding and decoding policies are described by sequences of probability density functions as {P(zt|zt-1,xt) : t = 0,1,...} and {P(yt|yt-1,zt) : t =
 0,1,...}, respectively.
 The design in Fig. 1 is required to yield an asymptotic average distortion  
 D, where D > 0 is the pre-specified distortion level, d(xn,yn) , . The objective is to minimize the expected average codeword length denoted by
  , over all zero-delay encoding-
 decoding policies. These design requirements are formally cast by the following optimization problem:
 		(2)
 	s. t.	
 i.e., the OPTA by zero-delay codes. Note that in (1) we assumev
  
 that (A, BBT) is stabilizable (see, e.g., [14]) to ensure that the limit exists in (2) (i.e., RZDop (D) < 8).
 LOWER BOUNDS ON THE OPTA BY ZERO-DELAY CODES
 In this section, we present a lower bound on the OPTA by zero-delay codes using the INRDF, Rna(D). In general, the expression of the OPTA by zero-delay codes given by (2) is very hard to find and often bounds are obtained (see, for example, [7], [8]).
 First, recall that since the OPTA by zero-delay codes is a subset to the OPTA by causal codes (see Section I) then
 	rcop(D) = RZDop (D).	(3)
 Moreover, it is also known that for general sources the following bounds hold (see, e.g., [8, equation (11)]).
 	(a)	(b)
 	R(D) = Rna(D) = rcop(D).	(4)
 Note that, inequality (a) is strict, in general, and becomes equality when the source is i.i.d. or when the rate tends to infinity. In contrary, inequality (b) is strict at high rates (high resolution) due to space-filling loss and becomes equality at zero rate. Also, for unstable sources like the source model of (1), Rna(D) has to be greater or equal to the sum of the unstable eigenvalues of matrix A, i.e., whose magnitudes are greater than one (see e.g., [15]).
 By combining (3) and (4) we obtain
 	R(D) = Rna(D) = RZDop (D).	(5)
 Since, the INRDF is a tighter lower bound on the OPTA by zero-delay codes compared to the classical RDF, we can use this information measure to obtain tighter bounds on (2).
 In nonanticipative rate distortion theory (see e.g., [1], [8], [9]) the mutual information [16] can be written as
 
 where E{·} is the expectation with respect to the joint probability distribution
 P(xn,yn) , P(xn) ? P(yn||xn),
 with P(xn) , ?P(xt|xt-1), P(yn||xn) , ?P(yt|yt-1,xt), while P(yt|yt-1) is the marginal distributions induced by the joint probability distribution P(xn,yn). We now state the definition of INRDF as given in [11,
 Definition 6.1].
 Definition 1. (INRDF)
 The finite time horizon INRDF is defined by
 		(6)
 s.t. 
 where the infimum in (6) is taken with respect to the sequence of conditional probability density functions {P(yt|yt-1,xt) : t = 0,1,...,n}.
 The INRDF is defined by
 		(7)
 s.t. 
 If one replaces liminf by inf lim in (7), then an upper bound to Rna(D) is obtained, defined as follows.
 		(8)
 s.t. 
 It is shown in [11] that provided the limit in (8) exists, and the source is stationary then Rna(D) = R¯na(D). The optimization problem of (7), in contrast to the one given in (2) is convex (see [17]). In addition, Rna(D) < 8 due to the assumption of stabilizability on the pair (A,B) of (1).
 A. Feedback Realization Scheme via Kalman Filtering
 The authors in [1] the causal and rate-distortion optimal
 “test channel” {P*(yt|yt-1,xt) : t = 0,1,...} characterizing
 AR(1) sources such as the source model of (1) with an
 {encoder, channel, decoder} via an asymptotically stationary feedback realization scheme illustrated in Fig. 2 to find the optimal causal and zero-delay information based filter. A detailed analysis on this scheme is provided in [1, Section VI]. Next, we briefly explain the methodology presented in Fig. 2.
 Preprocessing at Encoder: Introduce the estimation error
 {kt ? Rp : t ? N0} of {xt : t ? N0} based on the previous estimates {y0,...,yt-1} defined by
 		(9)
 and its (error) covariance ?8, defined by
 	.	(10)
 The covariance matrix ?t is diagonalized by introducing a unitary transformation E (invertible matrix) such that
 T
 	E?8E = diag{?1,...?p}.	(11)
 To facilitate the computation, we introduce the scaling process
 , where ?t , Ekt, t ? N0, has
 independent Gaussian components.
 Preprocessing at Decoder: Analogously, we introduce the estimated error process {k˜t : t ? N0} defined by k˜t , yt - xˆt|t-1 and the scaling process {?˜t : t ? N0} defined by ˜?t,Tßt, with ßt , (FEkt + vt), vt ~ N(0;V ), and
 F, T} being diagonal scaling matrices.
 The fidelity criterion  at each t is not affected by the above processing of {(xt,yt) : t ? N0}, in the sense that the preprocessing at both the encoder and decoder do not affect the form of the distortion function, that is,
 	.	(12)
 Using basic properties of conditional entropy (see, e.g., [1, Eq. (IV.35)]), it can be shown that
 		(13)
 s.t. 
 and
 		(14)
 	s.t.	
 are equivalent expressions.
 UPPER BOUNDS ON THE OPTA BY ZERO-DELAY CODES USING ENTROPY CODING
 
 Fig. 2: Realization of the optimal minimizing distribution P*(yt|yt-1,xt) (see Section III-A).
 In this section, we derive upper bounds to the OPTA by zero-delay codes using a universal quantization scheme based on a subtractive dither with uniform scalar quantization (SDUSQ) [18] on the feedback realization scheme illustrated in Fig. 2. Toward this end, we consider unstable vector Gaussian AR(1) source modeled as in (1), and we quantize each time step t over p independently operating SDUSQ, with their outputs being jointly entropy coded conditioned to the dither. Our approach is far from new in the literature. In fact, it is adopted in a variety of papers, e.g., [3], [4], [8]. Compared to these works, our scheme deals with a purely information theoretic setup while at the same time it gives more general and tighter bounds.
 Before we proceed, we state the definition of a scalar uniform quantizer with subtractive dither [18]. A scalar quantizer function is defined as
 Q?(x) = i? for 
 where ? > 0 is the quantization step which is freely designed by the designer. A scalar universal uniform quantizer with subtractive dither is defined as
 	r	(15)
 where r is the realization of a uniformly distributed random variable R over the interval .
 The execution of  requires a common randomness both at the encoder’s and the decoder’s ends. In practice, the dither r acts as a synchronized pseudo-random noise generator that can be used at both encoder and decoder’s end.
 A. Componentwise Scalar Uniform quantization
 Next, we use use the asymptotically stationary feedback realization scheme illustrated in Fig. 2 to design an efficient
 {encoder/quantizer,decoder} pair. This procedure is described next.
 We select the quantizer step size ? so that the covariance of the resulting quantization error meets V . The encoder does not quantize the observed state xt directly. Instead, it quantizes the deviation of xt from the linear estimate xˆt|t-1 of xt. This method is known in least squares estimation theory as innovations approach [19]. As a result, we name the encoder innovations encoder.
 We consider the zero-delay source coding setup illustrated in Fig. 2 with the additional change of the p-parallel AGWN channel with p independently operating SDUSQ. This change is illustrated in Fig. 3. Note that, all matrices and scalings adopted in Fig. 2 still hold when the aforementioned change is applied.
 For each time step t, the input to the quantizer, is a scaled estimation error defined as follows
 at = Akt, A = FE, kt , (x - xˆt|t-1).
 
 p-parallel AWGN channels.
 
 Realization over p independently operating SDUSQ.
 Fig. 3: Perfoming componentwise scalar uniform quantization by replacing a p-dimensional AWGN channel with p independently operating SDUSQ.
 Moreover, at is an Rp-valued random process. The parallel p-dimensional AWGN channel is replaced by p independently operating SDUSQ, hence we can design the covariance matrix V of the AWGN corresponding to the p-parallel AWGN channels in such a way, that for each t, each diagonal entry of Vi,i = 1,...,p, i.e., V , diag{V1,...,Vp} to correspond to a quantization step size ?i,i = 1,...,p, such that
 		(17)
 This results into creating a multi-input multi-output (MIMO) transmission of parallel and independent SDUSQ. We apply SDUSQ to each component of at, i.e.,
 		(18)
 and we let rt be the Rp-valued random process of dither signals whose individual components {rt,1,...,rt,p} are mutually independent and uniformly distributed random variables independent of the corresponding
 source input components at,i, ?t,i. The output of the quantizer is given by ß˜t,i = Q?i(at,i + rt,i), i = 1,...,p. (19)
 Note that ß˜t = {ß˜t,1,...,ß˜t,p} can take a countable number of possible values. In addition, by construction (see Fig. 2), the sequences {at : t = 0,1,...} and {ß˜t : t = 0,1,...} are not Gaussian any more since by applying the change illustrated in Fig. 3, {at : t = 0,1,...} and {ß˜t : t = 0,1,...} contain samples of the uniformly distributed process {rt : t = 0,1,...}. As a result, the Kalman filter in Fig. 2 is no longer the least mean square estimator since the obtained quantized signals may no longer be Gaussian.
 For completeness, we illustrate in Fig. 4 the relation between a SDUSQ and a scalar uniform additive noise channel, a result that was first pointed out in [18].
 Entropy coding: In what follows, we apply joint entropy coding across the vector dimension p and memoryless coding across the time, that is, at each time step t the output of the quantizer ß˜t is conditioned to the dither to generate a codeword zt. The decoder reproduces ßt by subtracting the dithered signal rt from ß˜t.
 Specifically, at every time step t, we require that a message ß˜t is mapped into a codeword zt ? {0,1}lt designed using Shannon codes [16, Chapter 5.4]. For a random variable x, the codes constructed based on Shannon coding scheme give an instantaneous (prefix-free) code with expected code length that satisfies the following bound
 	H(x) = E(l) = H(x) + 1.	(20)
 If x is a p-dimensional random vector then the normalized version of (20) gives
 	.	(21)
 
 Fig. 4: An equivalent model to Fig. 3b based on scalar uniform additive noise channel.
 In view of the assumption that the scalar uniform quantizer with subtractive dither operates using memoryless entropy coding over time, the following theorem holds.
 Theorem 1. (Upper bound)
 Consider the realization of the zero-delay source-coding scheme illustrated in Fig. 2 with the change of AWGN channel with p-parallel independently operating SDUSQ illustrated in Fig. 3. If the vector process {ß˜t : t = 0,1,...} of the quantized output is jointly entropy coded conditioned to the dither signal values in a memoryless fashion for each t, then the operational zero-delay rate, RZDop (D), satisfies
 		(22)
 where p is the dimension of the state-space representation given in (1), while the average MSE distortion achieves the end-to-end average distortion D of the system.
 	Proof: See Appendix A.	 
 Evidently, by combining the lower and upper bounds to the OPTA by zero-delay codes, we obtain
 
 In the next theorem we explain how the bounds derived in
 (23) generalize to Gauss AR sources of any order.
 Theorem 2. (Generalization)
 The bounds derived in (23) based on the scheme of Fig. 2 hold for vector Gaussian sources of any order.
 	Proof: See Appendix B.	 
 In the next remark, we comment on (23) and draw connections to existing results in the literature.
 Remark 1. (1) For scalar Gaussian AR sources, i.e., p = 1, then (22) degenerates to
 	 .	(24)
 This result was first obtained in [8, Theorem 7] but only for stable scalar Gaussian AR sources.
 (2) Note that the zero-delay rate distortion performance per dimension is given by the following expression
 
 B. Vector Quantization
 It is interesting to observe that if instead of scalar uniform quantization we quantize over a vector lattice quantizer followed by memoryless entropy coded conditioned to the dither, then the upper bound in (22) becomes
 		(26)
 where Gp is the normalized second moment of the lattice [20]. If we take the average rate per dimension then (26) becomes
 	.	(27)
 Additionally, by assuming an infinite dimensional vector Gaussian source, then by [20, Lemma 1], , and we obtain
 	.	(28)
 As expected, using vector quantization for infinite dimensional vector Gaussian source, the term due to space-filling loss and the loss due to entropy coding asymptotically goes to zero.
 Hence, utilizing this result, and the lower bound of (5) we obtain
 1
 p?8 p
 i.e., Rna(D) is the OPTA by zero-delay codes.
 CONCLUSIONS
 In this work, we considered zero-delay source coding of an unstable vector Gaussian AR source under average MSE fidelity criterion. Based on a simple feedback realization scheme that quantizes the innovations of a Kalman filter with a SDUSQ, we derived new bounds to the OPTA by zero-delay codes. We discussed the performance of this scheme when using lattice quantization. For infinite dimensions we observed that the INRDF is in fact the OPTA by zero-delay codes.
 APPENDIX A PROOF OF THEOREM 1
 In the realization scheme proposed in Fig. 2, with the change of AWGN channel with p-parallel independently operating SDUSQ, the operational rate for each t is equal to the conditional entropy H(ß˜t|rt) where ß˜t = {ß˜t,1,...,ß˜t,p}, ß˜t,i = Q?i(at,i + rt,i), i = 1,...,p, i.e., the entropy of the quantized output ß˜t conditioned on the t-value of the dither signal rt. This leads to the following analysis.
 
 where (a) follows from [18, Theorem 1]; (b) follows from the fact that the quantization noise is ?t = ßt - at (see Fig. 4); (c) follows from the fact that the relative entropy D(x||x0) = h(x0)-h(x), see, e.g., [16, Theorem 8.6.5]; (d) follows from the fact that D(at+?t||aGt +vt) = 0, with equality if and only if {?t : t = 0,1,...} becomes a Gaussian distribution; (e) from the fact that the differential entropy h(vt) of a Gaussian random vector with covariance V , diag{V1,...,Vp} is
 
 and the entropy h(?t) of the uniformly distributed random vector  is
 .
 Since we have that , the result follows.
 Next, note that for n = 0,1,..., the following inequality holds in Fig. 2.
 n
 I(xn;yn) (=a) XI(xt;yt|yt-1)
 t=0
 n (=b) XI(k ;k˜ |k˜t-1) t	t
 t=0 n
 (c) X
 	=	I(at;ßt)	(31)
 t=0
 where (a) follows from the structural properties of specific extremum problem resulting in the realization of Fig. 2 (see, e.g., the analysis in Section II and [1, Remark IV.5]); (b) follows from the analysis in [1, Equation (35)]; (c) follows from the fact that E,F,T are invertible matrices and as a result the information from kt to k˜t given k˜t-1 is the same as from at to ßt (information lossless operation).
 Since we are assuming joint memoryless entropy coding of p independently operating scalar uniform quantizers with subtractive dither, then by (20), for t = 0,1,...,n, we obtain
 
 where (a) follows by (30) and (b) follows from (31).
 Then, by first taking the per unit time limiting expression in (32) and then the infimum, we obtain
  (33)
 where (a) follows by (2) and (8) respectively, and R¯na(D) is the upper bound expression of Rna(D) for the unstable vector Gaussian AR(1) source model given by (1).
 Finally, by assumption, (i.e., stabilizability) the Gaussian source model given by (1) is asymptotically stationary, hence as n -? 8 the source becomes stationary. Utilizing this, we have R¯na(D) = Rna(D) and the result follows. This completes the proof.	 
 APPENDIX B PROOF OF THEOREM 2
 This can shown by augmenting the state of the Gaussian source model given in (1) as follows. Let x˜t ? Rs denote a new state vector of the additional dynamics. Then the model of (1) is modified as follows.
 xt+1 = Axt + A1x˜t + Bwt	(34)
 x˜t+1 = A2x˜t	(35)
 where the matrices (A˜1,A˜2) are the coefficient matrices of augmented dynamics. At this point, (34), (35) can be written as
 	.	(36)
 But (36) can be written as an augmented state space form as follows.
 	Xt+1 = A˜Xt + B˜Wt	(37)
 where
 X ,
 The state space representation of (37) with the same assumptions on the augmented coefficient matrices (A,˜ B˜) can be used precisely as the feedback realization scheme of Fig. 2 via a Kalman filter where the structure of the filter is the one derived in [12, Section IV]. This completes the proof.  
 ﻿ The main purpose of this note is to show that in a realization of the causal information 
 rate distortion function for a th order source , under a single letter sum distortion 
 constraint , the . This result is 
 derived under the assumption that the have a joint probability density function . 
 INTRODUCTION 
 Consider the causal information rate distortion function for a random source , 
 defined as 
 , 
 where the minimization is over all conditional satisfying the distortion constraint 
  
 and the causality 
 y 
 If the is by some conditional distribution , the associated pair a realization of . Here we assume that such distribution and that the corresponding realization a joint . This assumption is satisfied if , for example , is and , . 
 The first purpose of this note is to show that in a realization of the causal for a th order source , under the average distortion constraint , and supposing that in such realization the have a joint , it that 
 ne 
 a 
 where is the of and 
 b 
 The given in are a special case of the given by , , , for abstract , where their derivation is not included . The value of our first result in that 
 We provide a proof for the validity of absent in . 
 In this proof , we pose the causal optimization problem with as the decision variable instead of the collection as would be the case in for probability an associated . Accordingly , we impose an explicit causality constraint on , instead of causality structurally by to be the product of , as done 
 in , . 
 The second and main goal of this document is to note that from a it is clear that 
 y 
 , and that 
 y 
 does not hold , except for . Crucially , does not become true by supposing that the joint stationary , thus , Remark . and what is stated in the discussion paragraph at the end of , Section . 
 . PROOF 
 The causal under the above is by the solution to the following optimization problem : 
 minimize : a subject to : b 
 c 
 d 
 where the minimization is over the conditional . Notice that d is an explicit causality constraint equivalent to . 
 Let be any conditional , and define 
 , 
  
 , 
  
 where o , . 
 Before writing the and taking its differential , let us obtain the differential 
 of in the direction , given by 
  
  
 where 
 R , 
  
  
  
 On the other hand , for each i ,...,, the causality constraint d in the as 
  
 It will be convenient to manipulate this expression so as to give it a structure similar to the other in the . For this purpose , notice that 
  
  
  
  
  
  
 where 
 Substituting this into we obtain 
  
  
 We can now write the associated with optimization problem as 
  
  
  
 From the theory of optimization on vector is a solution to Optimization Problem only if 
  
  
 for every function gyn as defined in , i . e ., for every conditional . This if and only 
 if for every 
  
  
 The multiplier function must enforce the constraint b . Hence , 
 e 
 , 
 where 
  
 over we obtain 
 e 
  
 rule we can write 
  
  
 R e , x , y 
 where 
 , e 
 These can be written as 
 yn yn a 
 , e , , e , , b 
 In order attain causality in , the must depend only on and . Since for each , the function does not depend on with i , the causality constraint 
 is met if and only if we choose in b such that , for each ,..., n 
  
 for some function . 
 For , the causality constraint is satisfied automatically since 
 see a . Suppose now that i . e ., causality is satisfied for , for . In such case , 
 one can replace in and , 
 , 
 write as 
  
 Multiplying both sides by and over we obtain 
  
  
 This that the recursion the form 
  
  
 If th order , then , for all ,...,, in 
 which case becomes b . Substituting the latter into and then in a . Finally , from a , it that in a realization of the causal it must hold that 
 y 
 and that 
 y 
 does not hold , except for . This the proof . 
  
  
 ﻿ The present an achievable rate region for two pair bidirectional relay based on successive compute and forward method . In these , one relay in the communication between two of . In their scheme , the use lattice for the at the , and random for the at the relay . They use the successive compute and forward strategy to decode two integer linear of the lattice in the , and successive interference cancellation for the in the . The channel can be considered as a broadcast channel with two receiver , but within each group , a pair of is considered as an additive white noise channel instead of a broadcast channel because each node its own message . It is shown that for all channel gains of and all channel gains of symmetric channel , the strategy to within constant of and bit per user of the cut set upper bound for restricted and non restricted , respectively . These are than those previously for this network , which have not the successive compute and forward method . 
  
  
 Introduction 
 Nowadays , the capacity of wireless is one of the main research in network information theory . As an interesting and important application , two way via have been , where the two way relay channel is used as a building block . Several were for the in , by extending and combining the basic for the relay channel , such as decode and forward , compress and forward and amplify and forward . When or , the effect of the noise remains on the signal by the relay . In contrast , by at the relay , the noise is completely removed from the signal . However , from loss . 
 Compute and forward is another strategy for the which lattice . In this method , the receiver need not decode directly ; instead , it a linear equation of the based on the fact that integer of also belong to the . In , an achievable scheme was for the based on lattice , which was shown to achieve within bit from the cut set bound , for all channel . This strategy noise suppression without loss . In , the capacity of the with relay private been within constant of and bit per user from the single sided genie upper bound for restricted and non restricted channel , respectively . 
 The extension of the to been done in , where more than two communicate via a relay in a directional manner . The way relay channel was studied in , where upper and lower for the capacity were given . In this setting , are divided into several , where each user in a cluster a single message intended to other in the same cluster . A similar symmetric setup with one cluster and equal channel gains , noise and power was considered in , where the sum capacity was . In , a type of three user was considered , where each user two independent , each of them intended to one of the other . 
 In , a compute and forward approach was to exploit interference between the in an additive white noise network . In their approach , are free to select integer that match the channel as closely as possible , thus reducing the effective noise and increasing the achievable . In , the successive compute and forward approach was , where after a linear combination , the relay can combine it with its channel observation to obtain a new effective channel that can be better for the next targeted linear combination . 
 In , the capacity of two pair two way relay for both linear shift deterministic and complex channel were . In these , a relay node the communication between two of . It was shown that the cut set upper bound is tight for the linear shift deterministic channel model . The strategy for the deterministic channel was to a specific superposition of lattice and random at the source and successive interference cancellation at the for the network . It was shown that for all channel gains , that strategy the cut set upper bound to within bit per user . In that approach , the lattice of were , so it was necessary for them to assume equal power for lattice of one pair , and to use for the excess part of the signal with extra power . However , the did not show that successive interference cancellation was actually feasible for lattice and together . 
 In this paper , we study two pair two way bidirectional relay , the same model considered in , where one relay can help the communication between two of see Fig . . For brevity , here we name it . Here , we present a achievable rate region for a class of full duplex by successive compute and forward method , which the ability to efficiently decode multiple linear of the . In contrast to the method in 
  
 Fig . Asymmetric 
 , which is based on the superposition of random and lattice at the , and successive interference cancellation at the , in our method , it is enough to consider only lattice for at the , and successive compute and forward for at the relay in the . In the , random are used for at the relay , and successive interference are applied for at the . Moreover , in contrast to , we do not use and assume that all can transmit simultaneously . It is shown that for all channel gains and symmetric channel , the method within and bit per user of cut set upper bound for restricted and non restricted , respectively , which are than the and bit per user in . 
 The remainder of this paper is as . In Section , the system model , the cut set upper bound and some about compute and forward are . The achievable rate region for is in Section . Capacity gap and some are in and , respectively . Some concluding are provided in Section . 
 System model and 
 In this section , we review the system model for the asymmetric and its associated cut set upper bound . We also review some about lattice , and outline the successive strategy . The material of this section , for completeness , is a brief summary of the and from , in which the interested reader can find a detailed exposition . Consider an asymmetric as shown in Fig . , where and a pair of can communicate with each other via node a relay . There is no direct link between . The same situation for and . The relay is assumed to be able to listen and transmit at the same time . We use a real valued channel model for all in this network . Let log log , , log , U , , , , , , , , and for each i U , let denote the node on the other side of the relay which with i . The forward and backward of each pair are assumed to be different . Channel gains are written on each link in Fig . as , where i , and i . 
 Node i U , .., to be to node , where . Each user is assumed to choose its transmit message independently of the other . The user at node i U its message into a xi , xi .., an function , , for , …,, the length in channel , and the of the relay into a , .., an function , for , …,, is a of a real random variable , satisfying the power constraint i , and , .. the previously received at transmit at the same time , . that 
 transmission of each simultaneously . In this setup , non restricted , some dependency between the by different can be . In contrast , in restricted , apply the fi , thus yielding independent for all i . 
 The rate of an , code is defined in bit as 
  
 n 
 The rate is said to be achievable if , for any and for large , there an , code with such that the probability of error is less than . 
 The received at node i U and at the relay given by the following , respectively 
 , , .., 
  
 , , .., 
 i 
 where is a of independent and identically distributed real valued noise , i . e . with zero mean and unit variance . For each i U , and denote the and channel gains , respectively . Each node i U , a function to decode as where , .., yin . 
 . Cut set upper bound 
 The cut set upper bound for the was in . We rewrite a reduced and equivalent version of it for both restricted and non restricted in the following theorem and corollary . For the proof and further , the reader can refer to . 
 Theorem : The capacity region of the non restricted is upper bounded by 
 P , P , i U 
 min P , 
 C , i , U , i 
 Corollary : The capacity region of the restricted is upper bounded by 
 P , P , i U 
 min , , , i , U , i 
 Remark : In , lemma , it is proved that and are within bit per user of each other . The latter was shown by the 
 following 
 C P C 
 C 
  
 C C , 
 C , 
 When the channel is complex valued , the capacity to C so the previous yield a gap of bit for two or , equivalently , bit per user . However , in our problem , we have assumed real valued , for which the capacity is simply . Thus , the gap between upper for restricted and non restricted would be bit for two or , equivalently , bit per user . 
 . Compute and forward approach 
 Next , we review some on lattice and the compute and forward approach . For more see and the therein . 
 A lattice code is defined in of two dimensional and , which form a lattice partition , that is , . and are usually fine and coarse , respectively . The lattice code is a lattice code which as and the region of as a shaping region which is by . For , the set of coset is defined as . is chosen so that its average power per dimension is , that is , the second moment per dimension of a random variable uniformly distributed over is . The coarse and fine should be good for and channel , respectively for definition of lattice goodness see . The rate of the lattice code is defined as 
  
 R log n 
 Let , dither , be a random variable uniformly distributed 
 . Given the message , the 
 X 
 In , Theorem , an achievable rate was for real valued based on the compute and forward strategy to decode one linear combination of . In , the successive compute and forward method was , where more than one linear combination of is desirable . We rephrase these in the following theorem for the case one receiver . 
 Theorem : For real valued with , one receiver and channel coefficient h , h .., and equation coefficient vector a a , a .., aL , the following computation rate is achievable 
 log a , al 
 P 
 To successfully decode two linear , for example , a , , the following computation rate is achievable see 
 Finally , the rate of as 
 al , a , 
 In other , the relay can first aim to obtain a linear combination that is easy to decode and then use it to create a better effective channel for the second linear combination . In Theorem , a selected according to the design where different linear of are desirable . In the next section , we present an application for that approach . 
 Achievable rate for the 
 In this section , we present an achievable rate region for the . In our scheme , we consider restricted . The achievable rate region is also an achievable region for the non restricted . 
 In the , based on the fact that the of two , we use the idea of successive interference cancellation for lattice in which was in the previous section . By considering appropriate for a and , the linear function of the of each pair is , respectively , at the relay . 
 We would like to mention that in the superposition of and lattice from four is received at the relay in the . The relay then the successive interference cancellation method originally thought for for all , regardless of the fact that the are of different , that is , they have used the equation of for the lattice of each pair but they consider the other as in the rate equation . There is no proof for this performance and it does not seem to be a correct approach . 
 In contrast , we use the interference cancellation method that been for lattice in . Moreover , in this method , lattice and have been used instead of lattice alone . The motivation for this is that the former method been shown in to yield the rate log for equal power which is higher than the one in which is log . 
 In the , we extend the method for the in to the two of present in the . In this method , each pair of can be as an channel instead of a broadcast channel . This is due to the fact that each user of each pair its own signal , thus it can subtract this part from the received signal and can extract the message of the other user of that pair . This method higher than the method used in which as a broadcast channel with four . The full strategy is next . 
 . at the 
 According to the strategy , the transmit signal at user i U is given by , which a lattice code ensemble of lattice with size , where , , , , . should be good for and its second moment per dimension is . i should be good for channel . We consider the following channel vector between and the relay 
 h 
 . : at the relay 
 The relay two linear of lattice code and , with the following 
 a a a , b b , a , a b , b 
  
  
 b ah a ba 
 T 
 The coefficient a by successive compute and forward individually with transmitter as shown by the following 
  
 ah a 
  
 P 
  
 b 
  
 These can be via the following 
 T T 
 a a b 
 a a a 
 h 
 By substituting the above in and , and are , respectively . The are shown below 
 t P a h 
  
 log a a P 
  
  
 log a a r P 
 log r P 
  
 T 
 b ah a 
  
 log b a P Pah P 
  
  
 log b b a a aa r 
 a a P 
 log a Pa a a h r ra h P 
 a a a a 
  
 Remark : As stated in , the closer a a a a a and are , the higher the rate by . However , in our problem , the two last of a are fixed to be zero , that is , a , a , , so as shown in the simulation section , not necessarily those of a , a which are closer to , result in higher . 
 Remark : Consider the case where each pair of in the symmetric channel gains , that is , and . For this case , a and higher as shown by the following 
 a a r P 
  
  
  
 b 
  
 More generally , let us consider the channel gain vector as , where air for all i . It is possible to find , , and such that we have symmetric for each pair of , that is , 
  
 ah , we can 
 . For this case , by assuming and , and are simplified as 
 log P 
 P 
 . at the relay 
 The relay two , from of sizes and , respectively . The signal by the relay is given by 
 art , art 
 . : at the 
 The two following of all different of channel gains are sufficient for the analysis to be carried out . The other can be and will lead to similar by the indices . 
 . . Case : It is shown that , for any choice of channel gains , this can be done successfully as long as the following are satisfied . 
 at user 
 P 
 R ar P 
 R , R Car P t 
 Proof : User first by treating as noise . This can be done with arbitrarily small probability of error as long as is satisfied . Then , the signal is from the received signal and is . This can be done with arbitrarily small probability of error as long as is satisfied , because by the same conclusion in for , in the , each pair of in the can be as an channel instead of a broadcast channel . As before , this is due to the fact that each user of each pair its own signal , thus it can subtract this part from the received signal and can extract the message of the other user of that pair . Hence , the can be as a broadcast channel with two receiver , and , instead of a broadcast channel with four , as was considered in . 
 The proof of the other following can be done similarly . 
 at user 
 P 
 R , R Car P 
  
 at user R Cart P 
 P 
 R Car P 
  
 at user 
 P 
 R Car P 
  
 Clearly , and are redundant in view of and , respectively , because is an increasing function of . 
 . . Case : It is shown that for any choice of channel gains , this can be done successfully as long as the following are satisfied . 
 at user 
 P 
 R , R Car P 
  
 at user R Cart P 
 P 
 R Car P 
  
 at user 
 art P 
 R Car P 
  
 at user 
 P 
 R Car P . 
  
 Clearly , is redundant in view of and . 
 Capacity gap 
 In this section , we compute the capacity between the achievable and the cut set upper bound by the strategy . For the , we obtain the capacity by considering symmetric channel gains for each pair of . For the , we consider the general case . 
 Lemma : : For the with symmetric channel gains for each pair of , for any rate , satisfying 
 P 
 P 
  
 there a choice of power such that of 
 with error probability for all channel gains . i U , can be done with arbitrary small 
 Proof : From we have 
 P 
 P 
 From and we have 
 P 
 P 
 P 
 Table for a , a , b , b , for each channel coefficient vector h 
 No . a , a b , b 
 R , R 
 R , R 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 . , , . , , , . . 
 , , , , , . . 
 , , , , , . . 
 , , , , , . . 
 . , . , . , . , , . . 
 Lemma : : For any rate i U satisfying 
 P , i U 
 , , 
  
 i , U , i 
 there a choice of power such that of 
 with error probability for all channel gains . i U , can be done with arbitrary small 
 Proof : Consider and for : 
 From , we have 
 art 
 r 
 From , we have 
 art 
 r 
 Substituting in , we have 
 P 
 P 
 P r r P 
  
 P 
  
  
  
 Similarly , substituting into in 
 P 
 P 
 substituting into in 
 P 
 P 
  
  
 Fig . Non restricted and restricted cut set upper and achievable for , , , and different for a and b 
 Remark : Note that if I , I : unit vector , and , then the rate I the of and . Thus , the capacity of the restricted with symmetric channel can be within bit per user of the cut set upper bound . By against , we see that the gap is bit with respect to the cut set upper bound for non restricted . 
 Simulation 
 In this section , we illustrate the performance of the method for the via and discuss the . All the rate shown in the and the table are in bit . 
 . 
 Fig . Cut set upper and achievable for . , Assuming , we consider several shown in . , . , . and a , a , and b , b , Table , and for each case , we compute the of the a , a and b , b which yield the highest and . As it is from the , not necessarily 
 and substituting into , in those of a , a and b , b which are closer to , and , result in higher . The first row of the table 
 r the of which are , and , , respectively . a , a and b , b the the for and . As it is seen , not necessarily with a , a , and b , b , Other can be proved similarly . in higher . of the table the for 
  
 Fig . Non restricted and restricted cut set upper and achievable for , , , by our method and the method of for the communication 
  
 . It can be seen that , for most , a , a , in higher . of the table show the for and , in this case , with a , a , and b , b , in higher . 
 Fig . the cut set upper bound and the achievable for an channel gain vector equal to , , , and different for a and . It is seen that the choice a , a , and b , b , in higher . This result is also shown at row of the table . 
 Fig . the cut set upper bound and the achievable for a symmetric channel gain vector equal to . , . , . , . , and for a , a , and b , b , , which yield the rate region . As it is seen from the figure , the distance between the achievable rate , . , . the last row of Table and the cut set bound is bit , which is than the found in Lemma . 
 . 
 Assume the channel gain vector to be , , , , and . Fig . the cut set bound , the achievable by the method and the of for the communication . As it is seen from the figure , for most , the method that are within those by Lemma . 
 Conclusion 
 In this paper , we studied the capacity region of an asymmetric full duplex with no direct link between . We established an achievable rate for this network by lattice for at the and random for at the relay . For , we successive compute and forward and successive interference cancellation at the relay and at the , respectively . It was shown that , for the with symmetric channel , the achievable rate is within constant of and bit per user of the cut set upper bound for restricted and non restricted , respectively . In this paper , we have used real valued . For complex valued , we have C instead of in the capacity expression , so the would be twice the for real valued , that is , and . bit per user of the cut set upper bound for restricted and non restricted , respectively . These are one half of those in , that is , and bit per user for restricted and non restricted , respectively . 
  
 ﻿ In , Theorem . it is that , for a one sided random source , the 
 search for the non anticipative i . e ., causal rate distortion function can be restricted to y which are jointly stationary with x . In this technical report we show that the proof of , Theorem . is invalid because it on , Theorem . , the proof of which , as we also show , 
 is flawed . 
 INTRODUCTION 
 The manuscript , Theorem to prove the claim that , for one sided x , the non anticipative i . e ., causal rate distortion function can be by a reconstruction process y which is jointly stationary with x . To do so , it on , Theorem . . 
 In this note we argue that the proof of , Theorem . , and hence that of , Theorem . , are flawed . For that purpose , we will first recall the and in . After that , we will present the in and show , under the stated there , the by , Theorem the basis of , Theorem . of are not met . 
 A BRIEF REVIEW OF 
 Throughout , the search in the associated with various of i . e ., causal rate distortion is stated over of joint probability between source and reconstruction as opposed to the usual , in which the search is over conditional , see , Chapter , . Since the distribution of the source is given , it is that for every k k , all the joint to be considered yield the 
 same given distribution of the source for the corresponding block , say . This requirement can be as that , for a set of admissible joint , k 
 defined as 
 , 
 where and are , respectively , the to which and belong , and is a 
 s algebra over . In , this admissibility requirement is in the definition of the of which meet the distortion constraint , next . 
 The fidelity criterion for every pair of k k is expressed in as to belong to a non empty set of hereafter to as distortion feasible set , k , a condition written as . In this definition , the number an 
 admissible distortion level . Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation . 
 As above , the admissibility requirement is expressed in the in . . The latter equation can be written as 
 , k , k . 
 In . and . , the distortion feasible are assumed to satisfy the concatenation condition 
 . 
 With this , . defined the epsilon entropy of the set of 
 , k as 
 , 
 where the is taken over all of random such that the 
 causality 
 x 
 are satisfied . Then . the message generation rate as 
  
 The analysis in considered both and continuous time , but here we only refer to the discrete time 
 scenario . 
  
 The actual term employed in is epsilon entropy of the message where the term message 
 to the random in . 
 when the limit . 
 An alternative message generation rate is also considered in by the set of distortion admissible process as : 
 Definition . The set of all two sided random process for which there exist k k such that and 
 . 
 N 
 With this , . 
  
 when the limit , where the is taken over all of satisfying the causality 
 x . 
 THE WITH 
 The proof of , Theorem . on the claim stated in , Theorem . , namely , that an equality similar to 
  
  
 . 
 We demonstrate that the proof of , Theorem . is not valid and hence that of , Theorem . 
 is flawed . We do this by showing next that , Theorem . two , namely : a one of 
  
 the causal considered in it does not coincide with , and the proof of , Theorem . 
 is invalid . 
 A . The First Problem 
 The already first problem of , Theorem . as a basis for , Theorem . 
 na as from the fact that its alternative causal function R 
  
  
 where as defined in the text just below equation . in Q , is the set of conditional of y given x such that the causality 
 x 
 and the asymptotic distortion constraint 
  
 Next , in its equation . that defined 
 Re 
 e although this equality is not explicitly Thanks to , it readily that R 
 stated in . 
  
 Since the only causal defined in as an lim is , one must conclude that Re as equivalent to D . However , in view of Definition and , such equivalence is not valid 
 since the distortion feasible of Definition are not compatible with the distortion constraint . 
 na and hence , 
 Therefore , when in , Theorem . it is stated that R 
  
 it does not mean that . As a consequence , one of the necessary for , Theorem is not shown to hold . 
 B . The Second Problem 
 The second issue with , Theorem . is the validity of its proof . To begin with , the only 
 argument used in it is that the source is stationary and , Theorem . However , the latter theorem 
  
 only that , and thus the proof of , Theorem . there is flawed . 
 Although not to in that proof , the reverse inequality in , Lemma . would be all that is to show that . However , the proof of , Lemma . , 
 below , is clearly invalid . It by that , by definition , 
 . 
 Then it proceeds by saying that taking the limit on both sides we obtain 
  
  
 and then that the claim by taking the over Q , . The problem with this reasoning is that does not follow from . A rigorous reasoning that when taking the limit as 
 n , to 
 n such that y 
  
 Thus , one cannot choose to the of this inequality over Q , and expect the in 
 equality to hold , since one can easily find a pair of whose conditional distribution 
 to Q , and yet because the on 
 the of are to reach the above . 
 In order to arrive to , one should first show that 
 . 
  
 Unfortunately , the latter is not true since , as already , Q , of random 
 such that E , for all thus reaching the limit distortion from above , and thus such that , for all . Therefore , does not hold . 
 Indeed , the latter reasoning that 
 , 
 leading to an inequality in the same direction as the one provided by , Theorem , i . e ., that 
 na Re . 
 R 
  
 simbolos_extraer=['?','|','¯',' >','<','≤','˜','	','∞','+','Δ','∈','∀','⇐','⇒' ,'≥' 
 "⊥", "∥","‰","ø", "≪","≫","~", "⊢", "⊨", "ℂ", "ℕ","ℙ","ℚ","ℝ","ℤ","Γ","Δ","Λ", "Ξ", "Π", 
 "⊕","∧","∀","∃","−","±","·","×","÷","²","√","³","∛","≠","≈","≡","≝","≤","≥","°","∠", 
 "©","[]",', ,','. .',', .','. ,','@','-','¢','(',')','{','}', '()','{}','"',"''",'   ', 
 if lista[i]!='Tratamiento de textos para BERT.ipynb' and lista[i]!='.ipynb_checkpoints'and lista[i]!='tratamiento.py': 
 ﻿We consider a networked LTI system subject to an average data-rate constraint in the feedback path. We provide upper bounds to the minimal source coding rate required to achieve mean square stability and a desired level of performance. In the quadratic Gaussian case, an almost complete rate-distortion characterization is presented.
 I. INTRODUCTION
 This paper focuses on the interplay between average datarate constraints (in bits per sample) and stationary performance for a networked control system comprising a noisy LTI plant and an average data-rate constraint in the feedback path. In such a setup, the results of [8] guarantee that it is possible to find causal encoders and decoders such that the resulting closed loop system is mean square stable, if and only if the average data-rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. This result has been extended in several directions (see, e.g., [7], [9]). However, when performance bounds subject to average data-rate constraints are sought, there are relatively fewer results available. Indeed, to our knowledge, there are no computable characterizations of the optimal encoding policies in networked control scenarios [1], [3], [5], [9], [13].
 In this note, we present upper and lower bounds on the minimal average data-rate that allows one to attain a given performance level (as measured by the stationary variance of the plant output). From a source-coding perspective, we are aiming at characterizing the rate-distortion function in closed-loop systems. This extends beyond causal rate-distortion theory [2] due to being subject to a stability constraint. Our results exploit a framework for networked control system design subject to average data-rates developed in [10], [11].
 II. PROBLEM SETUP
 Consider the NCS of Figure 1, where P is an LTI plant with state x ? Rnx and initial state xo, u ? R is the control input, y ? R is a sensor output, e ? Rne is a signal related to closed loop performance, and d ? Rnd is a disturbance. We assume that (xo,d) are jointly second-order and Gaussian (with finite entropies). The feedback path in Figure 1 comprises a delayfree noiseless digital channel, a causal encoder whose output yc is a sequence of binary words, and a causal decoder.
 
 Fig. 1. Networked control system. average data-rate across the channel is defined as
 	R ,	(1)
 where R(i) referes to the expected length (in nats) of yc(i).
 We do not restrict the complexity of the encoder or the decoder a priori, and only assume them to be causal, and to have access to independent side information SE and SD. Our aim is characterizing
 	R ,	(2)
 where se2  trace{Pe}, Pe is the stationary variance matrix of e, D > 0 is a desired level of performance, and the optimization is carried out with respect to all causal encoders E and decoders D that render the resulting NCS (asymptotically) mean square stable (MSS), i.e., that render (x,u,d) jointly second-order and asymptotically wide-sense stationary processes.
 III. AN INFORMATION-THEORETIC LOWER BOUND ON AVERAGE DATA-RATES
 Theorem 3.1: Consider the NCS of Figure 1. Under suitable assumptions,
 R = I8(y ? u) = I8(yG ? uG), (3) where I8(a ? ß) denotes the mutual information rate [6] between a and ß, and (yG,uG) are jointly Gaussian processes with the same second order statistics as (y,u).  
 Thus, in order to bound R(D) from below, it suffices to minimize the directed mutual information rate that would appear across the source coding scheme, when all signals in the loop are jointly Gaussian.
 Lemma 3.1: Suppose that (yk,uk) in Fig. 1 are second order and jointly Gaussian random sequences. Then uk can be constructed from yk as u(i) = Li(yi,ui-1) + s(i), i = 1,...,k (4) where, for each i = 1,...,k, s(i) is a zero-mean Gaussian random variable such that s(i) ?? (ui-1,yi-1,si-1), and
  
 Fig. 2. NCS that arises when, in Figure 1, the encoder E and decoder D form a linear source coding scheme.
 where Li : Ri×(i-1) ? R is a linear operator such that Li(yi,ui-1) is the minimum mean-square error estimator of u(i) given (yi,ui-1).  
 We conclude from the above that, for a given performance level D, the minimum of I8(yG ? uG) over all causal encoders and decoders is achievable by an encoder/decoder pair which behaves as a linear system plus additive white Gaussian noise sk such that s(i) ?? (yi,ui-1), ?i.
 IV. LOWER AND UPPER BOUNDS ON RD
 We next define the class of linear source coding schemes, which are capable of yielding a relationship between y and u of the form given by (4).
 Definition 4.1: A source coding scheme is said to be linear if and only if, when used around a noiseless digital channel, is such that its input y and output u are related via
  
 where v and w are auxiliary signals, q is a second-order zeromean i.i.d. sequence, both F and K are proper LTI systems, and q is independent of (xo,d).  
 When a linear source coding scheme is used in the NCS of Figure 1, the LTI feedback system of Figure 2 arises.
 Lemma 4.1: Consider the NCS of Figure 1 and assume that the encoder E and the decoder D form a linear source coding scheme. Under suitable assumptions, I8(y ? u) = I8(v ? w) and
 	 ,	(6)
 where Sw is the stationary power spectral density of w and sq2 is the variance of the auxiliary noise q.  
 Linear source coding schemes have sufficient degrees of freedom to allow one to whiten w without compromising optimality. Thus, our results lead to:
 Theorem 4.1: Consider the NCS of Figure 1 under suitable assumptions. Define, with reference to the feedback scheme of Figure 2, the infimal signal-to-noise ratio function
 	 ,	(7)
 where sa2, a ? {v,q,e}, is the stationary variance of a in Figure 2, and the optimization is carried out with respect to all sq2 ? R+ and all proper LTI filters F and K which render
 2
 the feedback system of Figure 2 internally stable and wellposed. Then:
 	 .	(8)
 Moreover, there exists a linear source coding scheme such that
 R 
  
 Theorem 4.1 characterizes the minimal average data-rate that guarantees a given stationary performance level, in terms of ?(D), i.e., in terms of the minimal SNR that guarantees the desired performance level in a related LTI architecture. Interestingly, the upper bound in (9) is valid even if one removes the assumption of (xo,d) being Gaussian
 To find ?(D), one can resort to the results in [4]. A case where an explicit solution is available is when D ? 8, i.e., when only stabilization is sought. In that case, it follows from Theorem 4.1 and [12] that
 	 ,	(10)
 where p1,...,pnp are the unstable poles of P. If one uses (10) in (8) and (9), then one recovers, within a modest gap, the absolute minimal average data-rate compatible with stability derived in [8].
 ﻿ This paper with the design of feedback to encode plant output in control with data rate constrained . Starting form a nominal design made under the assumption of transparent communication links , we show how to design a feedback so as to systematically reduce the impact of quantization on closed loop performance . To obtain our , we model quantization as additive white noise with a signal to noise ratio constraint . As a , we obtain a simple characterization of the minimal noise ratio that one to design a feedback that stability . This bound only upon the plant and controller unstable . If the plant is strongly , then the bound is consistent with the absolute minimal data rate for stabilization in previous work . 
 Standard control theory with where the communication links between plant and controller can be as transparent see , e .., , . There exist , however , where the links in a control system are far from being transparent and may become in the achievable performance . Control where this are collectively to as Control see , e .., , , , and the many therein . Clearly , unless the channel are explicitly taken into account at the design stage , the performance of an may be far from optimal and sometimes completely unsatisfactory . The main that need to be considered when dealing with include data rate i . e ., quantization , data loss and random . A framework for the treatment of the general design problem does not exist . Nevertheless , there been significant progress in the study of specific that focus on . For example , data rate have been studied in , , , , and design to deal with quantization have been in , e .., , . The issue of data loss been studied in , , , among many , and have been considered in , e .., , , , . 
 In this paper we focus on linear time invariant plant , and concentrate on the effects of quantization on closed loop performance . Within this framework , a key result in the minimal data rate which is necessary and sufficient to achieve stabilization of an unstable plant , to its in very simple way . This bound been linked to information theoretic where it been given an interpretation akin to that of entropy see , e .., , , . Furthermore , , established that the minimal data rate for stabilization is sometimes consistent with minimal noise ratio in standard one degree of freedom control that employ . More precisely , , that , if the plant is defined in discrete time , relative degree one and is minimum phase , then a memoryless channel a signal to noise ratio equal to the lower bound derived in , would exhibit a channel capacity equal to the data rate bound in . 
 The above give absolute lower on the admissible channel data rate which cannot be by by any control law . It , however , quite difficult to obtain practical design from such as those in , , . This some to move towards a simplified treatment of quantization . For example , quantization as a sector bound uncertainty and standard robust control . On the other hand , a simple white noise model for quantization . The latter model for quantization close to the signal literature , where it been successfully used to design high performance quantization see , e .., , , , , . 
 In the present work we assume that a controller already been designed under the assumption of transparent communication links . However , we subsequently extend the set up by assuming that the control loop to be a bit rate limited channel in the plant to controller communication link . Thus , the plant output have to be prior to transmission . To that end , we borrow from the signal literature and employ a feedback to encode the plant output see , e .., , . a fixed signal to noise ratio additive noise model for quantization , we show how to design the feedback so as to systematically reduce the impact of quantization on closed loop performance , as measured by the error variance . We show via that our approach very good even for bit as low as one bit per sample . We also study stability for this linear model . As a , we obtain a simple characterization of the minimal signal to noise ratio that one to design a feedback system that stability . This result is expressed in of the plant and controller unstable only . For stable , and regardless of the plant or relative degree , our suggest a minimal data rate for stabilization that is consistent with the bound in . 
 The idea of designing to embellish given controller is not new . For example , our previous work in a scheme that turns out to be a special case of the one considered here . On the other hand , the same architecture as the one studied in this paper , but the design procedure in that quantization effects are relatively small . The methodology used in the current paper does not require this assumption . Also , the stability analysis included in the current paper goes beyond the of , . Another related line of work been in , , . The latter work a precise deterministic stability analysis when the system is constrained to be a modulator or thereof ; see , e .., , but does not address performance . Another recent publication closely related to the current paper is . In that work , the propose a architecture similar to the one in this paper , but restrict the to have infinitely many and a quantization step . The latter are not here . Interestingly , the optimal coder in which on a time domain functional turns out to have a structure that is a special case of the architecture considered here . 
 The remainder of this paper is organized as : Section the notation employed in the paper . Section the architecture of interest and a linear model that is suitable for analysis and synthesis linear system theoretical . Section stability of the linear model , while Section the design procedure . Section a simulation study . Concluding are included in Section . 
 We use standard vector space notation for , i . e ., N . We also both the argument of the transform and as the forward shift operator , where the meaning is clear from the context . Given any matrix , and denote conjugate transposition and transposition , respectively . Given any complex scalar , and magnitude and complex conjugation , respectively . 
 The set of all discrete time real rational transfer is by . We define six : all proper transfer , all strictly proper transfer , all stable and proper transfer , U all matrices in that have in , all stable and strictly proper transfer , all transfer that have only outside the unit circle and are either proper or improper . For any A we define A , A . We say that A is unitary if and only if A A I . We also define A , A A . 
 Every A with no on the unit circle to L in which case we define the norm of A via see , e .., 
 For each such A , we can always find and A such that A A 
 Any transfer matrix A an inner outer factorization of the form 
 where Ai is unitary i . e ., Ai is inner and is a scalar transfer function , that no in i . e ., is scalar and outer . Moreover , if A no on the unit circle , then U see , e .., . 
 Given any wide sense stationary process , we denote its power spectral density by , its variance by and its standard deviation by . We note that if , in addition , an always positive rational spectrum , then we can always find a spectral factor U such that , ,. We also recall the well known fact that see , e .., . 
 In this paper , we will consider the architecture in Figure . In that figure , is the plant model , is a controller , is the plant output , is the reference signal , do 
 output and to measurement noise . Unlike standard non see , e ., , , the feedback path in Figure a communication channel and a source system and . The main focus of the current paper in designing this system , performance in mind . To that end , we utilize as the performance assessment quantity the stationary variance of the error e , defined via 
 Since our aim is to design , we will assume that the controller in Figure been already designed assuming transparent communication links . The control loop formed by and when transparent communication links are in place i . e ., when ym in Figure will be to as the nominal loop or nominal design . 
 The transfer function is the nominal loop sensitivity function and is the nominal loop complementary sensitivity function see . 
 Assumption Plant and nominal design The plant model to , whilst the controller ,, to , is non zero and is such that the nominal loop is stable and well in the standard sense ; see , e .., , . 
 The assumption that the nominal loop is stable and well defined is , of course , sensible in our context where the system is designed a . We assume that is strictly proper for simplicity . In principle , this can be removed at the expense of additional technical care . On the other hand , the assumption of being non zero non interesting , where the nominal loop is such that is left in open loop i . e ., uncontrolled . 
 Assumption The , do and are mutually independent scalar zero mean , each a rational power spectral density that , if not identically zero , a spectral factor in U . 
 In this paper , we will focus on error free bit rate limited . As a consequence , the input to the channel , i . e ., see Figure , must be prior to transmission . To that end , we will consider a standard feedback as in Figure also known as a noise shaping ; see , e .., , . In that figure , A , and are in that need to be designed anda uniform see , e .., , , i . e ., 
 the dynamic range , , V the number quantization . 
 We recall that a is said to be if and only if the absolute value of its input is greater than its dynamic range , i . e ., for some N . If the does not overload , then the quantization noise , defined via 
 As already in Section . , we are interested in designing for nominal . In this setting , it is natural to employ that , in the absence of channel , have unit transfer function . That is , we will utilize that achieve perfect reconstruction . In our case , the channel is assumed to be error free and hence . As a consequence , it is straightforward to see from Figure that 
 It from that perfect reconstruction is tantamount to A for every . On the other hand , in order to have a properly defined feedback loop around the it is necessary to have a strictly proper see , e .., Chapter in . We summarize the previous discussion as : 
 Constraint Structural on the feedback The feedback are such that A and . 
 Quantization is a deterministic non linear operation and hence , the exact analysis of is difficult see , e .., , , , . It thus become standard , particularly in the signal literature see , e .., , , , , , , to approximate quantization noise by an additive white noise source uncorrelated with the input of the . Here , we adopt this paradigm and assume the following : 
 Assumption Quantization noise model The quantization noise signal defined in is a sequence of i . i .. random uniformly distributed in , and uncorrelated with . 
 Note that we do not assume that the quantization noise is uncorrelated with , which is certainly not the case since the quantization noise is fed back to the input of the and , moreover , the system is inside the main feedback control loop . Instead , we adopt a milder assumption that only with the exogenous in . We stress that the previous model is valid only if is small enough , the does not overload anda smooth probability density see , e .., . These usually do not hold in the case of that are in feedback see , e .., the discussion regarding stand alone feedback in . Nevertheless , one can make use of see , e .., , to render the model in Assumption exact provided no overload . Despite the above , we will see in the simulation study included in Section , that , even if one a non uniform with as few as , the made the simple model in Assumption are surprisingly accurate see also simulation in , , . 
 In order to guarantee that the does not overload , in principle one needs to consider infinite quantization or assume that the input is bounded , which is seldom the case in a stochastic framework . In practice , it is standard to choose a dynamic range such that the probability of overload is negligible see , e ., . Indeed , and is any positive real , then one can always find a finite a such that choosing that the probability of overload is less than ; a is the loading factor . With such a choice for the factor , it is immediate to see that 
 where we have used the fact that , according to Assumption This the following additional assumption : 
 Assumption Fixed signal to noise ratio For a fixed number of quantization , the variance of the quantization noise is proportional to the variance of the signal being , i . e ., the a fixed signal to noise ratio defined via 
 Assumption is a key constraint . As before , it one to guarantee that the dynamic range is always properly scaled . In addition , it a effect on the optimization based design of the system . Indeed , if this constraint were not in place i . e ., assumed to have some statistics , then it would be optimal to choose and A with . This is , of course , not a sensible choice since A and grow unbounded when . 
 Remark We would like to stress that , in some , overload may become the dominant quantization effect in feedback . Indeed , overload may trigger limit cycle that are , of course , not by the linear model for quantization above see , e .., , , . As by and , we assume in this paper that overload is infrequent enough and , accordingly , that it no significative effect on overall closed loop performance . Careful design of the loading factor may act as a safeguard against overload . 
 Considering the model for quantization above , together with the nominal loop description in Section . , it is easy to derive the linear model shown in Figure for the considered . Note that we have made the perfect reconstruction constraint explicit . In Figure , and , and , do , satisfy Assumption . We will refer to this model as the linear model . It will be the basis of the remainder of this paper . 
 In this section we study stability of the linear model for the considered derived in Section . In particular , we characterize all and A that lead to stable linear in an appropriate sense for a given signal to noise ratio . As a , we characterize the minimal signal to noise ratio that one to find and A such that the resulting linear model is stable . 
 We begin by that , a dimensional vector that the of ,, A , A and see Figure , then the evolution be by a linear state space model : 
 where A and are matrices of appropriate that depend on the particular of ,, A , A and . Next , since we are considering a stochastic system , we need an appropriate notion of stability : 
 Definition Mean Square Stability , , The linear system in is Mean Square Stable if and only if there exist a finite and a finite , , both not dependent on the initial state , such that 
 Theorem for Mean Square Stability If hold , and is an independent random variable with finite mean and finite variance matrix , then the linear model in Figure is if and only if A U , and 
 Proof : Define as the variance matrix of . Since the spectral factor of , , to , we lose no generality if we restrict attention to the case where , for every i . e ., if we assume white noise . The general case the same , but an augmented description of the system that additional stable . 
 Consider the state space description of the system under study given by . Standard allow one to conclude see , e .., Chapter in that under our working 
 where and , are defined as and ,, but of . 
 • If the is , then both and are finite and unique . Therefore , that A must be stable . On the other hand , we also see from that ,, i . e ., the stationary variance of , say , must be positive semi definite , finite and unique . 
 Since the nominal loop is stable , a simple calculation that A being stable that both A and A must be stable , and moreover , that is stable . Of course , both A and A must be proper and , on the other hand , is constrained to be strictly proper recall Constraint . Therefore , it that A U and . 
 If A is stable , then it is easy to see that the stationary variance of see also Section 
 where is defined in , and where we have used the fact uncorrelated is such that . the definition of in 
 Therefore , we conclude that , provided A is stable , being positive semi definite , finite and unique is equivalent to . 
 • Since the nominal loop is stable , and A U , we have that A is stable . Therefore , it from that is finite , unique and well defined . 
 If , then the when proving the sufficiency part of this theorem imply that is positive semi definite , unique and finite . the definition of , it then that is positive semi definite , finite and unique . Therefore , is positive semi definite , unique and finite and , since A is to be stable see above , then we have that the equation that the limiting value of , in a finite , unique and positive semi definite solution see , e .., Section . in . Therefore , we have proven that is as . This the proof . 
 The condition for given in Theorem is deceivingly simple . This is due to the fact that the nominal loop is assumed stable and we are on that achieve perfect reconstruction recall Section . . It is relevant to note that does not depend on A . Therefore , one can easily characterize the lower bound on that one to guarantee : 
 Theorem Minimal signal to noise ratio for If hold , and is an independent random variable with finite mean and finite variance matrix , then there exist A and that allow one to guarantee if and only if 
 Proof : It to compute see Theorem . To that end , we employ the in detail in , e .., , , . We first note that , . Define 
 where pi i ,, the set of non minimum phase of that lie strictly outside the unit circle i . e ., the unstable of outside the unit circle . It is clear that , is unitary , and is such that the transfer function to , is and as non minimum phase the on the unit circle of i . e ., the on the unit circle of 
  
 where we have used orthogonal in L , the fact that both unitary , the fact that Assumption , and basic of the norm . By construction , and is invertible in except for on the unit circle . Elementary see , e .., Chapter in allow one to conclude from that 
 Use of the Residue Theorem and some simple algebra the desired result . ¤¤¤ 
 Theorem a precise condition that the signal to noise ratio to satisfy in order to be able to find a system that , when inserted in the feedback path of a stable nominal loop , the of the resulting linear model . The bound on only on the unstable of , i . e ., on the unstable of the plant and controller . If the plant model is strongly i . e ., can be a stable controller ; see , e .., , then a stable controller in the nominal loop one to find a feedback coder capable of the resulting linear model if and only if 
 where i ,, the set of unstable of . We note that the same conclusion if the controller is stable except for on the unit circle e .., with integral action . 
 If we fix , then must satisfy , which is a fixed constraint in our framework . If it were possible to redesign the controller under the constraint , then one can use the in to establish that the admissible signal to noise ratio must satisfy 
 where is non negative and on the non minimum phase and on the relative degree of the plant model if and only if is minimum phase and relative degree equal to one . We thus conclude that the inclusion of the system one to reduce the on the signal to noise ratio , at least for strong regardless of the plant or relative degree . This reduction may be very significative if , e .., the plant high relative degree . This is an important indication of the that to control . A question that remains open , however , is whether or not there exist different that allow one to recover for any plant . 
 Remark Relationship to prior work In it is proved that is the minimal signal to noise ratio that one to find one degree of freedom that stabilize a given plant model over an additive noise channel with a power constraint . In a second step , the show that a memoryless channel , with a signal to noise ratio that , would have a capacity Cap see , e .., that 
 where is the minimal data rate which is necessary and sufficient to stabilize an system over an error free bit rate limited channel . Equality in is if and only if the plant is minimum phase and a relative degree equal to one . These suggest that signal to noise ratio in one degree of freedom control are , for a restricted class of , consistent with the minimal data rate of . If the plant non minimum phase , or a relative degree than one , then see . It thus that , in these , data rate by signal to noise ratio may be more demanding than those in . 
 Our can be applied to the channel model in as well , provided error free feedback with a unit delay is available from the channel output to the channel input . When doing so , it turns out that is consistent in the sense above with the minimal data rate derived in . Our even if the plant model arbitrary relative degree and arbitrary , as long as is strongly . We thus conclude that , within the framework , the use of feedback is key to achieve and , accordingly , key to make signal to noise ratio consistent with the in . We stress that the issue of existence of feedback from the channel output to the channel input is inconsequential to the set up used in because the channel is error free , as in our case . Note that the assumption of channel feedback been explicitly made for with stochastic see , e .., , , again the in . If channel feedback is removed from the analysis of , then the minimal data for stabilization do not necessarily coincide with those in see Section in . 
 In this section we go beyond stability and focus on how to actually design a feedback system that the impact that the communication channel on closed loop performance , as measured by the steady state variance of the error . 
 The purpose of this section is to define the performance of interest in a precise way . To that end , we consider the linear model in Figure . Straightforward analysis that the error 
 Therefore , if the linear model is and and hold , then the stationary variance of e and is given by 
 where is a spectral factor of the power spectral density of . Since Assumption , is not a given constant ; indeed , it on the variance of . Proceeding as above and the same , it from Figure that 
 We note that , since is assumed to be given , the choice of the i . e ., A and only the second term in , which we denote as 
 Problem Main problem Given a fixed , , a controller and a plant that satisfy Assumption , and exogenous satisfying Assumption , find defined via 
 and A and that achieve or approximate arbitrarily well . 
 We note that all in the formulation of Problem stem from , as in and . We will use the term admissible A resp . admissible to refer to a filter A resp . that the in Problem . 
 Problem is non trivial . Indeed , the much simpler problem of designing A and so as to minimize the steady state variance of ym , when and been only recently exactly see . This is quite surprising given the fact that feedback have been studied extensively see , e .., , , . Unfortunately , the technique employed in does not seem to yield an explicit characterization of the solution in the present situation . Instead of that line of reasoning here , we will derive an iterative approach that is to yield performance that is arbitrarily close to optimum . 
 Before the design procedure , we note that the following : 
 Fact Asymptotic behavior of Assume that the of Problem hold . Then : 
 . If , then unless all exogenous have zero spectral density , in which case A for every admissible A and . 
 . By definition of , we have that . Thus unless either . Since A U and 
 . Fix A U and . In these , and hence , . 
 As a consequence of Fact , we will omit from our subsequent presentation an explicit analysis of the or . The reader can easily verify that the below are consistent with Fact by or . 
 We begin by showing how to choose A , when an admissible is given . To that end we define , for any given admissible , 
 Theorem Optimal A for a given Assume that the of Problem hold and consider a fixed admissible . If is not identically zero , then : 
 Proof : The definition of the norm one to conclude that , for , the following hold : 
 Both and follow and the inequality in note that is always well defined if Assumption , and is not identically zero . To complete the proof we note that is a condition on the magnitude of the filter A . Thus , can always be , to any desired degree of accuracy , by a rational filter in U as . ¤¤¤ 
 Remark Of course , assuming that is not identically zero does not hinder the generality of Theorem see Part in Fact . 
 The characterization of given by Theorem , although explicit , is usually not satisfied by any transfer function in U . This is due to the fact that , except in very special , the th root of the right hand side in is irrational . Nevertheless , as in the proof of Theorem , it is always possible to find a filter in U that a performance that is as close as desired to 
 . In practice , it is usually enough to consider reasonably low order to approximate see also . 
 In this section we address the problem of choosing when an admissible A is given . Consistent with the notation before , 
 We begin by that A can be written in a simpler form as : 
 Fact Equivalent formulation for A Assume that the of Problem hold and consider a fixed A U . Then , 
 Proof : the definition the fact that A is fixed , it is immediate to see that 
 Define a new real variable ,, constrained to belong to ,. With this definition , elementary optimization see , e .., Section . . in allow one to write as 
 where we have used the fact that , by definition of , J for any . The result is now immediate . ¤¤¤ 
 Fact is key to derive the main result in this section . Namely , a one parameter characterization for A and the corresponding optimal . Towards that goal , we begin by considering an auxiliary problem . Define the functional 
 Lemma Solution to auxiliary problem in and suppose that Assumption . 
 the relative degree of , i ,, resp . pi i ,, is the set of non minimum phase resp . unstable of that lie strictly outside the unit circle . 
 . If resp . , then the in is achievable in , if and only if no resp . on the unit circle . 
 . We will proceed as in the proof of Theorem see also . As before , we define via , Q . It is easy to see from that 
 Moreover , the same procedure as in the proof , it is also clear that 
 A A z Q A A Q 
 ¡ A A A Q , where we have used the fact that the relative degree and non minimum phase of are the relative degree and non minimum phase of , that unitary , and that , since A U , is such that A to , is and as non minimum phase the on the unit circle of i . e ., the on the unit circle of . From and it that 
 where , i is an inner factor of and , o is the corresponding outer factor . We note that , since Assumption , no on the unit circle for , . Thus , for those of ,, o U . 
 Therefore , orthogonal as those employed before allow one to write 
 . The result upon that , by definition of and , resp . A to if and only if no on the unit circle resp . on the unit circle . 
 The characterization of given in Lemma an essential role in our subsequent discussion . It is worth that the only critical step when calculating is the inner outer factorization of . Since no at infinity i . e ., is , this factorization can be made with the aid of standard see , e .., , . 
 The next theorem a characterization of the optimal in of . 
 Theorem Optimal for a fixed A Assume that the of Problem hold and consider a fixed A U . Then , 
 In , is defined as : If there does not exist , such that J , then . Otherwise , , where is the unique real in , such that J . 
 . We first show how to solve an auxiliary problem related to the inner optimization problem in 
 The well known for this problem see , e .., , allow one to conclude that the optimal , say , if it is a critical point of J J , where , , and , moreover , J . It is immediate to see that this is equivalent to saying that is a critical point of see , with , and J . 
 L is a strictly convex functional and so are J and J . Hence , it an unique critical point given by see . Moreover , the set of in the J versus J plane defined by , when from zero to one , is the set of optimal of the objective problem of simultaneously J and J see , e .., , . This set is a strictly convex and decreasing function when J is seen as function of J . Clearly , the optimal point corresponding to resp . is such that J is minimum resp . J is minimum . Thus , by definition of optimal point , the minimum of J , when J is when J , provided J F . If J F , then the minimum J is when J J F . As a consequence , the optimal solution of the auxiliary problem is given by , where , provided J F , to , and is such that J note that convexity that , in this case , is unique . On the other hand , if J F , then . 
 . We next show how to exploit the above reasoning to prove the result . We first note that , since F J , it is of no use to consider that J F note also that is optimal for the first optimization problem in and , accordingly , constraining J F to be greater than or equal not impede the minimization of . Thus , 
 to exist in , and to be unique . A key feature of this problem is that the optimal of the auxiliary problem considered in Part do not depend on . Thus , in , min , J F is equivalent to , min , if defined in the body of this Theorem does not exist , then pick . It should be clear that the structure of the problem is such that , min , J F . Thus , it to consider , min , . As a consequence , the result . 
 Theorem a one parameter characterization of the optimal and the corresponding minimal cost A , for any admissible A . The scalar parameter can be found any standard line search procedure and , as such , its calculation no additional . This is by the fact that the search for is made over , and that actually in , which is precisely the range of of for which is always defined in see Lemma . 
 In this section we show how to use the in . and . to design a feedback system in an iterative fashion . Of course , one can always choose to fix one of the coder trivial are A or and then use Theorem or to design the free parameter . Obviously , this choice will limit the achievable performance . To exploit the full potential of a feedback system we suggest that one an iterative algorithm such as the following : 
 Algorithm Iterative design procedure For a given plant and controller satisfying Assumption , and a given , proceed as : 
 Pick a tolerance , a transfer function A U and a transfer function F that is admissible . 
 Set A A , F . Set A ,, fix A or , alternatively , fix and set . 
 If at the th iteration A was fixed , then use Theorem to obtain . Set and A ,. Fix . 
 If at the th iteration was fixed , then use Theorem to obtain . Set and A ,. Fix A . 
 It should be clear that it is not certain that Algorithm will converge to the global minimum of . Nevertheless , it is easy to see that , by definition of and , the algorithm the value each iteration . Therefore , Algorithm , necessarily , to a local minimum . Thus , we suggest to use multiple starting so as to find the global minimum . A procedure for getting a good starting point is below . 
 In general , and . Thus , fixing A or and choosing the other filter , will obviously provide a system that closed loop performance when with a non situation . It is also clear that the use of Algorithm one to design that will always outperform that have been designed the in our work in . This is a consequence of the fact that to be identically zero . 
 A more interesting discussion if one the in this paper with the in . 
 In the latter work , it is assumed that is sufficiently high so as to be able to by 
 a second stage , to choose as the minimizer of . A problem with the above approach is that , a , which are high enough to be impossible . In particular , since the procedure in does not take the constraint explicitly into account , the choice for may be not admissible or may be such that A , . Needless to say , this drawback is explicitly in the current paper . It is also clear that choosing A as in and , then , Theorem to choose will always lead to a feedback coder that a error variance that is lower than the one by the in . Of course , if the in are feasible , then they may provide a good starting point for Algorithm . 
 This section a design study that the in this paper . We consider a very simple case that , nevertheless , will allow us to present the main of our proposal . 
 The measurement noise and output disturbance are assumed zero , whilst the reference is considered to have a power spectral density with spectral factor 
 The loading factor is fixed at in all , and the number of quantization ,, b , between and . Since we do not consider the use of channel e .., entropy ; see , , to the rate at which data is sent through the channel in 
 Figure the steady state error variance se see as a function of the number of in Algorithm for two representative of the signal to noise ratio : . and . , which correspond to and , respectively . and refer to that start with A and F . In Case we initially fixed A , whereas in Case we start fixing . Case to that start with the in . We note that to be greater than . in order for the proposal in to be admissible . Accordingly , we 
 Figure : error as function of the number of in Algorithm see text for . 
 Case in Figure when . . It can be seen that rapid convergence of Algorithm and , more interestingly , that the limiting performance does not depend on the order in which the are calculated or on the initial condition . Thus , local minima related do not seem to play a role in this example . 
 In Figure we have three . The first of these point to the performance without and A . The second point to the performance when the optimal system in . The third point to the performance the approximately optimal in . 
 The show that is , indeed , necessary to achieve the best possible loop performance . Compare point with , e .., the value of se for . It is also possible to see that use of Algorithm that perform better than our previous in , , which is consistent with the discussion at the end of Section . . Compare and with the limiting value for se . It is also interesting to mention that , for , the performance provided by the in is substantially closer to the limiting value of se than the case shown in Figure . This suggest , as before , that the in , when feasible , provide good starting for the iterative procedure here . 
 We end this section by the behavior of the error variance as a function of the channel bit rate . The are in Figure , where Nominal performance to the performance by the nominal loop without quantization , No empirical to when no is employed i . e ., when A and , Opt . empirical to with the by Algorithm after , and Opt . analytical to the corresponding made the simplified noise model for quantization . One can see that , as , the effects of quantization vanish as . Interestingly , the made our model turn out to be very accurate for every bit rate : indeed , for the relative are of less than and , for , , the relative are around . We note that turns out to be non admissible for . Accordingly , 
 This paper a methodology to design feedback that encode plant output in a control situation data rate limited . a fixed signal to noise ratio additive noise model for quantization , we have shown how to iteratively design the of a feedback system so as to minimize the impact of quantization on the closed loop error . Our show that feedback quantization are beneficial when to simpler in the literature . An interesting by product of our in the characterization of the signal to noise ratio compatible with stabilization . We have shown that , for a given signal to noise ratio , the class of that are when feedback is employed is significatively than the class of that are when no is used . This result the door to investigating other control and feedback and the associated signal to noise ratio . 
 A very interesting extension of the present work in multiple input multiple output . In that case , it is worth exploring how may help the well known performance that arise when constraining the structure of the controller see , e .., , . A second immediate extension in the problem of joint controller and coder design . The study of how to apply similar to the case of prone to data loss is also interesting see preliminary work in . 
 Consider a a on and extensible to . If X and X is achievable in , i . e ., if such that X , then , X . On the contrary , if such that X , then defined as above should be understood as limn , where is a sequence in whose limit to such that limn X . Therefore , if we write and , it is implicit that one can find a sequence as above . In these , it is clear that one can always pick that is as close to X as desired . 
  
 import nltk
 from nltk.corpus import words
 nltk.download('words')
 import os
 lista=os.listdir()
 simbolos_extraer=['?','|','¯',' >','<','≤','˜','	','∞','+','Δ','∈','∀','⇐','⇒' ,'≥'
                  ,'Φ','μ','ν','→','Ω','θ', 'ω', 'ε', 'ρ'
                   ,'τ', 'ψ', 'υ', 'ι', 'ο', 'π', 'α', 'σ', 'δ', 'φ', 'γ', 'η',
                   'ς', 'κ', 'λ', 'ζ', 'χ', 'ξ', 'ω', 'β', 'ν', 'μ','⊂' 
                  '⟨', '⟩', '⟦','⟧', "⌊", "⌈", "⌋","⌉","↑","⇐","←","↦",
                   "ℰ","ℓ","ℒ","ℳ","’","“","”","–","—","½","¼","∕","∤" 
                   "⊥", "∥","‰","ø", "≪","≫","~", "⊢", "⊨", "ℂ", "ℕ","ℙ","ℚ","ℝ","ℤ","Γ","Δ","Λ", "Ξ", "Π", 
                   "Σ", "Φ", "Ψ", "Ω", "ℏ", "∞","∘","∂","∫","∮","∯","∇","′","″","‴","⇒","→","⇔",
                   "↔","∈","∉","⊂","⊆","⊄","⊈","⊃","∪","∩","∖","∅","∏","∑"," ̂"," ⃗","¬","∨",
                   "⊕","∧","∀","∃","−","±","·","×","÷","²","√","³","∛","≠","≈","≡","≝","≤","≥","°","∠",
                   "©","[]",', ,','. .',', .','. ,','@','-','¢','(',')','{','}', '()','{}','"',"''",'   ',
                   '    ','     ','      ','=','%','!','#','$','&','/','\\','_','^','[ ]','*']
 espacios=['  ']
 words=set(nltk.corpus.words.words())
 for i in range(len(lista)):
     if lista[i]!='Tratamiento de textos para BERT.ipynb' and lista[i]!='.ipynb_checkpoints'and lista[i]!='tratamiento.py':
         archivo=lista[i]
         print(lista[i])
         texto=open(archivo,"r")
         stringa=texto.read()
         stringa=" ".join(w for w in nltk.wordpunct_tokenize(stringa)\
                 if w.lower()in words or not w.isalpha())
         for i in simbolos_extraer:
            stringa=stringa.replace(i,"")
         for i in espacios:
            stringa=stringa.replace(i," ")
         
         stringa=" ".join(w for w in nltk.wordpunct_tokenize(stringa)\
                 if w.lower()in words or not w.isalpha())
         
         texto1=open("tratado_"+archivo,"w")
         texto1.write(stringa)
         texto1.close()
 
 ﻿ We present an based model for the gain of an indoor antenna array . This to the of the model , as applied to outdoor indoor links . Our model is applicable to a few adjacent in an system such as . We find that as much as out of of antenna gain are attainable even in rich indoor scattering . At the same time , no fade margin is when the array . the antenna . To provide a reference for the we consider a simple propagation model , which is theoretically and via simulation . This model is found to match our empirical very well . 
 Index Channel , directive , gain reduction factor , outdoor indoor wireless links . 
 have been used to improve received signal power since the beginning of . However the specification of an antenna effectiveness in such as improvement of signal strength and reduction of interference is typically for propagation devoid of or reflecting , i . e ., under free space . The more recent appearance of cellular wireless service , in propagation where the presence of is the rule rather than the exception . 
 Relatively little been on the interaction between propagation and the antenna gain . The decrement in achievable antenna gain due to local scattering was originally defined in as the Gain Reduction Factor . This factor was as a location dependent random variable and its statistical were for a specific environment , namely suburban outdoor to outdoor links . The very fast growth in wireless traffic in urban forced base coverage to those of , , for which the may no longer hold . 
 While short range outdoor to indoor links for static or nomadic have been , the effect of directional on those links received very little attention . In fact , most have been based on the use of . Moreover as stated in , path loss prediction are much greater when directional instead of . 
 Recent in antenna design the use of have made it practical to consider the application of in relatively small , such as those used in wireless local area . Such will allow taking advantage of propagation where the dominant lie in an angular range that is much narrower than the array . To the best of our knowledge , there is no general model available to describe angular spread when indoor wireless service is provided from outdoor bases . The actual gains achievable with directional in this setting therefore an empirical study similar to what was in for suburban links . This is the main focus of the work we describe here . We provide the first statistical description of the for this type of environment . 
 The Mean Effective Gain , was in as a useful single parameter to describe the impact of the antenna on the link budget for , primarily for mobile links . This is defined as the ratio between the spatial average power received by the antenna and the sum of the average that would have been received in the same environment by two isotropic , vertically and horizontally . A theoretical method is in to analyze the of a mobile antenna . A general expression based on a statistical model of incident is and empirical data was at to validate the model . A statistical model for incident is also , where model derived and measured of are for street links at . . An analysis of some fundamental of the and the corresponding physical is in for theoretical Rice channel . In contrast to the , as a random variable , the is an average value . 
 . Personal use is permitted , but republication redistribution permission . See : index . for more information . 
 An approach based on modeling from the gain pattern of an antenna as a result of propagation is in . This is defined as the Effective Directivity Antenna Model , which a bin fitted stochastic model to correct the nominal antenna pattern . The based model in the above was for specific directive in outdoor outdoor and indoor indoor links , . and . are carried out to show that the model more accurate path loss prediction than the when directive are used . The however do not focus on the reduction of available gain for beam indoor when are from outdoor bases , as we discuss here . 
 The originally defined in , was subsequently in but the corresponding statistical model is based on empirical data collected only in suburban links , . The approach is based on the arrival distribution of power density which from our methodology , as will be in the next section . 
 Short range outdoor indoor wireless are links for which there is still a lack of empirical , . Several such as and the therein have the subject of path loss characterization for that in some are similar to ours . However , links with directional are only considered in and in . A comparison of maximum received power directive and dipole is in , but this work does not include a statistical characterization of the gain difference between both antenna . In , and of outdoor indoor links that include directive are , but this work does not include a comparison of received power for different antenna . 
 The main objective of this paper is to evaluate the effectiveness of steerable directional at the indoor user terminal in typical short range , fixed wireless , outdoor indoor service . We propose a statistical model for the of links , applicable for instance to a group of within the coherence in transmission . These are important since antenna gain may compensate for the propagation loss disadvantage of indoor outdoor as to indoor indoor links , opening up a much application space for cellular service . The were carried out at . a transmitter and a carefully power measuring receiver . The frequency used is the same as in the fixed wireless service . A total of different links were tested in the urban in Section . The links included in the relative position of outdoor and indoor and variation in path , ranging from to . 
 The remainder of this paper is organized as : Section the theoretical background . Section the measurement hardware and methodology . Section statistical based on our empirical data . Finally , . 
 For convenience we assume the outdoor base to be the transmitter and the indoor user the receiver . The receiver is connected to a directional antenna , capable of searching in azimuth for maximum power . The received power at the antenna in a wireless link with properly can be expressed as 
 where is the transmit power , and are the transmit and receive antenna gains in free space , for feeder cable between the , the transmitter and the receiver and is the path loss associated with the propagation environment . Expression if the angular spread of the is much smaller than the antenna , as may be the case for a base antenna above clutter . For an indoor antenna surrounded by clutter this directivity gain will in general no longer hold . While the antenna can be in the direction of the arrival , its radiation pattern will attenuate from other , which may be by a beam antenna , in particular an element . Further , as in , we define the as the decrease in gain advantage of a directive over an antenna , due to the presence of clutter . Let G and G be the nominal free space antenna gains of a directive and an antenna respectively . We denote as the actual in clutter gain advantage of the directive antenna when for maximum received power . Then , 
 To estimate for a given link , we measure the received power an in azimuth antenna . Let this power be . For exactly the same antenna and transmit power , we repeat the measurement with the directive antenna , rotating it in search of the maximum power . The difference in received power is then the in clutter gain advantage of the directive antenna , , at that spatial position . 
 Since and are the position dependent random received by the , the is by the difference between the spatial of a directive and an antenna , both at the same position , by the difference in free space gains . 
 To establish a basis that can serve as a reference for comparison with our empirical , we use a very simple model for our propagation environment , which nevertheless proved to be quite accurate , as we will see in Section . This model , in that at each indoor measurement location a very large number of equal power at the receive antenna . These are distributed uniformly over the complete azimuth angular range with phases distributed uniformly from to p . In addition to these scattered , a single dominant from one direction , as in Fig . . 
 The average power ratio of the dominant wave over the total of the scattered is the factor . We now consider a directional antenna rotated in azimuth . For simplicity we assume that the directive and the have the same vertical gain and that the horizontal gain pattern for the directive antenna is constant within the and zero elsewhere . If this antenna is pointed in a direction that does not include the dominant path , then the received voltage will be distributed , provided that one can still assume that a large number of fall within the . On the other hand , when the directive antenna the dominant , then the received voltage will be Rice distributed . However the factor for this 
 distribution will be times that the voltage received by the antenna . This is because the dominant wave is subject to the horizontal antenna gain of the directive antenna , while the scattered power is on average unchanged , as the gain increase is exactly offset by the reduction in number of the received diffuse . 
 With these we can statistically describe the ratio of the power received both of . For the antenna at random in the room the output voltage envelope is a Rice distributed random variable by the local factor . We now consider that at the same position the directive antenna is rotated in search of the maximum power , where antenna , with the antenna expressed in . Then the directive antenna output voltage can be written as , where the collection of random i of random and one Rice distributed random variable with a factor equal to . It is reasonable to assume that these random are independent , since they arrive from non angular . From this we can obtain the statistical characterization of the ratio of between the directive and the . 
 As an example we will consider the limiting situation where . We note that in this case the directive antenna will only be able to choose among distributed random and will thus act as a branch selection combiner . This us to use well known . We characterize the statistics of the ratio , as it from that this will determine the statistics of the . the derived for an branch selection combiner , we see that the Probability Density Function of the output power , 
 where we have assumed that the average power for each direction is unity . This can be done without loss of generality since we will only consider power . For the antenna the of output power will then have an exponential distribution with unit average . We will also assume that this random variable is independent of the preceding . 
 Given the statistics of and we now obtain the ratio distribution for . the version , we find that the of for the case can be calculated as 
 The above integral can in this case be in closed form . again we the u and from it the corresponding Cumulative Distribution Function 
 In the above example the directive antenna can only offer a diversity gain . The case nonzero can be the same procedure , instead of in and . However the resulting integral corresponding to does not yield a closed form expression and is thus best numerically . We note that when the these must not be , since the model must consider that the average received are not equal as the vary with angular position . We will in general use when to the random power ratio , given that the factor is equal to . As the is 
 equal to by the ratio of nominal antenna gains , the statistics of the are readily from those of 
 We plot in Fig . the of the for the , , , under the assumption . We have here again assumed that both type of have the same gain in the 
 vertical plane so that . The were calculated for the case , and numerical integration nonzero . In addition we the previously model generating the corresponding random . The were identical . 
 We see that for small of , such as in our , the model that the will be negative with non zero probability i . e ., the array gain advantage can appear to be higher than in free space . This to the condition where the antenna is subject to a large fade while the directive array at the same position can combat such a fade angular selection diversity . Under the we , even for as low as , which a significant fade probability for 
 Fig . . of the for the , , , under the assumption . 
 reception , the directive antenna would almost in every case select the dominant direction and as a result only exhibit very shallow . Thus , we have a combination of classic deterministic gain as well as statistical reduction of fade margin , which can result in an overall gain higher than in free space . 
 Throughout this work we consider that the beam scanning of the directive antenna is only in azimuth and that the elevation of the is than the corresponding angular spread and hence the vertical gain is not reduced . 
 Our approach to calculating the is different from the one originally in see in , which is based on a transmission much than the typical coherence of the links considered . The use of the real valued angle of arrival distribution of power and the antenna power gains used in is under such due to the effect over frequency . Instead , for short range links the coherence is much , in the range of . We thus here consider the case of a transmission , for example a range of within the channel coherence . Although the definition of the is not dependent on the relation between the transmission and channel coherence , in the case the approach used in would not be practical . In fact , calculating the received power from the angle of arrival distribution of the electric field would require phase information , and the complex antenna voltage gain . Instead we found it much simpler to directly measure received power with various an dipole used as reference and to apply . 
 The measurement system of a . continuous wave transmitter and a purpose built receiver coupled to a power meter . The outdoor base station into the antenna terminal of a 
 . gain dipole . The receiver is . This is wide enough to capture any frequency dispersion that the transmission , induced for example by vegetation movement . 
 The measurement procedure is essentially the same already in , a computer to control antenna position and the synchronized acquisition of power . For each antenna position consecutive were . These were then to remove residual temporal , which were typically less than . . Before each measurement we calibration to assure that the transmitter and the receiver were operating with their nominal and gains . In all field the received power was at least above the noise floor . 
 were carried out at urban type in and , Chile . We selected where the base terminal could be positioned at a range of to from the indoor antenna . This space was in some by with and no higher than . Three were chosen in , one of them on the campus of and two in private . The fourth location was the campus of Maria in , which a wide range of indoor that and relatively large . A schematic of one of them is shown in Fig . where we also specify the outdoor base and indoor subscriber unit . Fig . the setup of the indoor measurement system for the analysis of both small scale and statistics . 
 Fig . . Measurement configuration for the case showing the on axis rotating system and the . rotating arm . 
 At these we measured a total of links that to two of , line of sight through a window or non line of sight when the direct path was blocked . This blockage included foliage of , small or a wall . The thickness of the foliated obstruction was typically in the range of . to . The construction was of the brick and mortar type , with non . Window sizes varied in width in the range of . to . and in height from . to . . The minimum window area was m . All were made by the outdoor antenna that the wireless at . height . 
 We firstly at fully our test . This involved the receiver to the dipole antenna on a . length rotating arm stepwise in azimuth , which a displacement of approximately between successive antenna . The power in a rotation at each placement were used to calculate the spatially path loss at that range and to generate the statistics of with respect to that average . The size of the region for the spatial average is well within the shadow fade correlation distance in previous work , , . In fact the in the literature , corresponding to indoor , are in the range of to m . This was repeated over a variety of in our test . The our statistical data with that of typical path loss . We note that these were only at the propagation environment and that spatial was not used in the , as will be below . 
 To directly measure the of the as in the previous section , we used a platform capable of rotating the about their axes in with no displacement other than rotation . Power were acquired as before at each angular position . Four of were used indoor , an . gain dipole used as reference and three different directional . The first was a . gain patch antenna with elevation and azimuth half power of and respectively . The second was a . gain horizontal linear array of four vertically polar 
 with elevation and azimuth half power of and respectively . The third was a . gain array of four vertically with elevation and azimuth half power of . The corresponding horizontal gain for the directive , measured in an chamber , are in Fig . . At each measurement location , the nominally antenna was rotated exactly as the directive to average out minor gain less than . Care was taken to place the of in exactly the same position at the beginning of each rotation and this was repeated for a wide range of as will be later . We power for the full rotation , which in the case of the directional us to obtain the maximum signal strength as well as the direction of arrival of the diverse resolvable . The computer measurement procedure is accurate to of one degree and us to verify that were repeatable , i . e ., that successive at any given location would yield the same result . 
 This section the of the and the statistical data derived from them . 
 We start by our measurement environment from a propagation point of view . To this effect we the data collected with the rotating arm in our to model average path loss and small scale fade statistics . The extensively used log normal model that the spatially path loss in may be as : 
 between the and , d is the free space path loss at a distance d in , is the 
 SUMMARY OF PATH LOSS STANDARD THE AND 
 path loss exponent and is a zero mean distributed random variable in with standard deviation . 
 We applied this model to our data after out over one rotation of the arm that the antenna . As conventionally done , we chose d . of the model in , multiple or choosing the intercept point d different from the free space path loss did not provide a measurably better fit . In addition , we the of the small scale with respect to the local . As , we found that the fit to was very good . We show in Table I the model . The were below . in of the for both and links to a very rich environment . It that our test environment is not by large shadow fade related power , particularly for the case , but that it is rich in propagation , as for an indoor setting . 
 Consistent with the above , we found the indoor propagation environment to be rich in over a broad angular range . The dominant energy in general from the direction of the nearest window . Moreover , when pointing the antenna in the general direction of the dominant path , angular in the range of a typically in a received power pattern that closely that of the specific antenna used . 
 This may be seen in Fig . , where we plotted received power . angle for the case of the patch array in . To avoid a very figure we only show chosen at random among the total of . These were to their maximum power and the angle at which this power was was set to zero . We also plot the antenna gain pattern and the average per angle from all . It can be seen that the dominant path is not resolvable into separate by the we used and that the angular spread of these dominant is narrower than the of the directive . Other were typically to arrive as wall from than the used , as seen in the above figure . This can be with of the propagation model in Section , assuming the rotation of a directional antenna with the same gain pattern as that used in our . Since in this case our are exclusively based on simulation , we chose to also include the randomness of the factor , which for our links was approximately uniformly distributed in the range to . . Thus in Fig . we show from . to . and the average of . As seen , the model match the behavior quite well , although in contrast with the , the average power for the 
 is not constant at angular outside the dominant direction . It may be that in our , the angular distribution of the the direction of the dominant path . 
 To further describe the angular of the , we the angle of arrival of the signal , with the angle of the straight line with . The of this difference is shown in Fig . for both the and . As seen , for links the dominant on a close to direct path from the base . We found that for of the angular deviation was no than . , i . e ., about half a . Instead , for links the angular range for of was . . The latter value is by the angular difference of between the direct path and that of the nearest window in our , which for of was less than . Alternatively , we found that when considering of , aiming along a direct path in a power reduction reaching up to of . for and for links . 
 Fig . . of the difference between the angle of arrival of the signal and the angle of the straight path between and . 
 Our show that , as previously in suburban , the may be quite accurately as a random variable . Fig . the cumulative distribution for the case of the array in and . In the same figure we also show the when the model in Section , considering of . and . for and respectively . 
 Here , the model based need to take into account that the vertical gains of the and the directive are not equal . As a result the increase in the factor that the directive antenna voltage only to the horizontal gain advantage over the antenna or equivalently its horizontal reduction , a factor of for this case . On the other hand the total gains of each antenna were considered when calculating the they receive . As seen , despite the simplicity of the model , it measurement accurately , the error being negligible at the 
 and probability , while at probability it is . for and to . for . We have the corresponding for the , as they are quite similar . In contrast with , the exhibit a considerable likelihood of negative i . e ., the array gain advantage would appear to be higher than in free space . As already , this at where the is subject to a large fade , which the angular selection diversity gain of the directive antenna can significantly reduce . In negative are not possible since the small scale have been out in frequency . 
 The of the best fit random the for all is by their mean and standard deviation in Table . 
 Although our propagation are different , we fitted the model in to these average . The expression used is 
 where is the half power azimuth antenna in . The best fit for A in Table . As in Fig . , the fit is quite good , although the actual average in our case are considerably lower than those measured in suburban . 
 Furthermore , we measured an increasing average with , but this is much less than for the outdoor case . This may be due to the scalar integration of for the case . vector addition of in our case . Therefore in the case the monotonically the total received power , whereas in our case excluding secondary reflected can in some be of benefit . Another manifestation of this phenomenon is the fact that we measured a significant fraction of negative , which contribute to lowering the average . 
 We also found a relatively large standard deviation of the with no statistically significant variation with as to . This is consistent with our angular spread , which is typically dominated by a strong component that is narrow with the directive antenna . Therefore when aiming these towards 
 the signal , there will be little difference among them resulting from the effect of secondary . Equivalently , all directive see the same strongly dominant , from which they extract power in accordance with their respective effective . The variation of the , which is similar for all directive , may instead reflect the fact that in our measurement the antenna used as the common reference is subject to considerable Rice spatial fading . 
 We also study the possible correlation between the and the shadow plus small scale fade at the location , as measured by the dipole . It cannot be assumed a that the is a random variable independent of these . We calculated the total of the dipole with respect to the average path loss regression as well as the for the same measurement point . The correlation coefficient between these random is then easily . We found that for all used and both and , the ranged between . and . . The negative in all the fact that deeply faded for the dipole antenna will be those where the directive can offer the benefit , due to angular diversity gains . The negative correlation between the spatial and the , that a directive array is capable of partially that would affect a dipole at the same position . We discuss this further in what . 
 To illustrate the actual gains that were with directive we plot in . and the received power for our with the array and the dipole , considering the and separately . We recall that for the dipole these do not include spatial , while for the directive they represent the peak received power over a rotation at the same measurement position of the dipole . This is consistent with a scenario where the user the antenna at a random position , without searching to optimize 
 Fig . . Received power power of . Straight and dashed represent the best fit linear regression for a dipole and for a array respectively . 
 Fig . . Received power power of . Straight and dashed represent the best fit linear regression for a dipole and for a array respectively . 
 location and hence , small scale Rice fade must be included in the link budget . 
 In these we also include the linear for both of , considering an intercept at based on free space propagation with the respective . The least fit are in Table . As seen , the are consistent with those for when out small scale . In the case , which is dominated by small scale , the negative slope is marginally , as the lack of spatial deep when measuring power in . The of the difference of received power , with respect to the regression , were in both very close to . The difference between the two regression in the distance range where our were concentrated is as almost identical to the nominal gain difference of the after the average . 
 SUMMARY OF RECEIVED POWER STANDARD THE AND 
 We observe that the standard deviation of received power is about the same for the directive and the dipole . While the antenna array may benefit from fade reduction due to diversity effects , this is more or less by the variability of the . As a result , similar link fade can be used for both antenna . 
 In summary , we found that indoor able to search for the signal with a directive antenna instead of an one , can achieve very significant . For example a nominally . gain antenna will in a indoor setting be able to provide an average . power gain . At the same time , the choice of a directive over an antenna over does not increase the . As already noted , part of the gain is attributable to classic and part to the angular diversity advantage of the directive antenna . As a reference we note that for a purely fading environment , the average diversity gain of the antenna , would have been only . , corresponding to a branch selection combiner . 
 The of an extensive measurement campaign , covering a variety of short range outdoor indoor links , show that significant power gains are available to indoor a steerable directive antenna . For the case that we considered , the antenna gains that should be used for link budget are within . of the nominal , free space gains for example . instead of . for the array . Moreover , while the directive exhibit considerable randomness in the , they also benefit from angular diversity and the combination of both effects in fade statistics that are very similar to those of the antenna . Thus , the use of a directive antenna indoors an increase in average received power , without the need for increasing the fade margin . 
 The wish to acknowledge the help provided by . Hector in the design and calibration of the used in this work . 
  
 ﻿ We present empirical results on the achievable gains stemming from the use of wireless remote radio heads (RRH) in a typical urban environment. Our work is based on simultaneous path-loss measurements of the base station and RRH links to outdoor street level users. We statistically characterize the increase in received power, when a RRH is added to improve the coverage achieved by a base station. We consider diverse expected coverage areas for the mobile terminal, evaluating the effect of RRH position with respect to the intended users. We also compare the power gains that would be obtained in practice from combining the signals from the base with those of the RRH, using schemes such as selection combining and maximum ratio combining. We conclude that under practical conditions, the benets of using RRHs will depend very strongly on the existence of line-of-sight links between the RRH and the intended users. For RRHs placed at low heights, below the clutter, only users in a street-canyon position with respect to the RRH will obtain a signicant benet. Our data also shows that the gains in signalto-noise ratio achieved when using maximum ratio combining are only marginally better than those of the much simpler selection combining. 
 One of the great challenges for today’s wireless communication networks is to provide adequate spatial coverage in a cost-effective way, while achieving bandwidth efciency and interference levels adequate for high frequency re-use [1]. To meet this challenge, during the last few years there has been a growing interest in the study of femtocells, relay stations (RSs) and low-complexity repeaters [2], [3]. These may constitute relatively simple, low cost and easy-to-install alternatives when compared to the deployment of an additional base station to serve mobile stations (MSs) within the cell. 
 While femtocells are conceived around a small-coverage base station (BS) with low transmit power connected to a wired network, a RS is connected to a BS through a wireless link, being able to repeat or re-code the data by using amplify-andforward or decode-and-forward algorithms, among others [4]. A repeater may be thought of as an amplify-and-forward relay with no decoding or scheduling ability [3]. In this context, there are two basic types of repeaters: wireless-repeaters (WRs), and remote radio heads (RRHs, also known as ber repeaters, connected to a BS by an optical ber) [3]. WRs 
 will suffer large and small scale fading at both BS-RS/WR and RS/WR-MS links. In contrast, for RRH assisted wireless networks, achievable gains will only depend on the quality of the RRH-MS connection, which makes it an attractive solution to provide connectivity to wireless users in dense urban areas, where the links to the BS may experience signicant shadowing and ber connectivity for the RRH may be readily available. 
 Proper placement of a RRH is obviously a fundamental factor in dening the compromise between the desired gains and the cost associated with its installation. In this regard, in an urban environment it is reasonable to expect that while coverage will improve with antenna height, this will at the same time have a negative impact on deployment costs and on co-channel interference, thus affecting frequency reuse in a large system [5], [6]. If the RRH is to cover an area of a size comparable to a small cell, then it is reasonable to assume that this is best achieved by placing its antenna in positions normally used by base stations (i.e above the clutter height). In such cases well-established propagation models such as those of Hata [7] and Erceg et al. [8] will be adequate to predict the coverage achieved by the RRH. Alternatively, they may be positioned at lower heights with the aim of achieving improvements in more limited regions, i.e. generating microcells within a large cell. Coverage in microcells has generated growing interest as it can provide local increases in signal to noise ratio (SNR) and thus higher data rates [2], [9]. This may be achieved without generating excessive interference in neighboring cells and at lower implementation costs when compared to a base station [2], [10], [11], particularly when placed below the surrounding clutter height. Numerous empirical models have been proposed for the statistical characterization of path-loss in relatively lowheight (3-20 m) radio links [12]–[17]. Analytical models based on optical geometry have also been proposed for microcells in urban environments [18], [19]. Goldsmith et al. [20] proposed, based on a collection of measurements, a mathematical description of the radius of coverage of a microcell. Such models may be used to characterize the RRH-MS link portion of a repeater. Haneda et al. [21] analyzed relays based on outage capacity for a very short-range outdoor-indoor environment. In [3], the authors simulated different uses of repeaters and RRHs in cellular networks, concluding that careful placement can improve capacity substantially by transferring trafc from heavily loaded to lightly loaded sectors. 
 The IEEE 802.16 Relay Task Group [22] and the WINNER Consortium [23] have suggested the use of both empirical and theoretical models to predict path-losses for wireless links with transmission units below the clutter of scatterers, also applicable to RRH-MS links. While the accuracy of pathloss models for diverse environments has been the subject of extensive studies, their use in RRH-assisted wireless networks requires more than treating each link individually, since the joint statistics of BS-MS and RRH-MS links may not correspond to those of independent random variables. Statistics for power gains stemming from the use of relays or RRHs based on simultaneous measurements in urban environments, as presented here, have to the best of our knowledge not been published. 
 In this work, we report on path-loss measurements of the BS-MS and RRH-MS links in a typical outdoor urban environment. We evaluate diverse aspects related to the performance improvement that can be expected when adding a RRH in an area served by a base station. We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user. Our results will be useful to calculate the achievable benets in coverage and in transmission rates experienced by mobile terminals. Measurements were carried out in an area within a range of 200 m of the RRH, which is placed at lamp-post height (3 m), considering that this type of setting will be typical for a practical deployment of low complexity repeaters. To avoid biasing our statistics we randomly chose LOS and NLOS placements for the MS at various distances from the RRH. The maximum range was dened by the requirement that at all placements, particularly those lacking LOS, the received power would be within the limits imposed by our channel sounding system. We note that our system was capable of measuring path-losses that would considerably exceed those compatible with a typical link budget in a wireless network. We also considered two types of RRH placements. One was chosen so as to maximize the likelihood of users having LOS to the RRH and the other chosen in close proximity to the rst, but obstructed from direct street view by construction. In this way we were able to consider the effect of elements blocking a RRH, as may occur when a surrounding construction is modied after the placement of the ber repeater. Thus our statistics allow us to quantify the benets of using a RRH for randomly placed mobile users within a given range considering improvements in coverage and in transmission rates. From the joint data of path-loss for the BS-MS and RRH-MS links we were able to compare the effectiveness of Selection Combining (SC) and Maximal Ratio Combining (MRC) [24] at the mobile terminal. To this effect we considered typical transmit powers and antenna gains at the BS and RRH and calculated received signal power at the MS under the condition of equal noise power for both schemes. It was found that the statistical gain of MRC over SC is less than 1 dB under all conditions tested, due to the random power imbalance between the links. 
 Our measurements also allowed us to calculate correlations between fades for the BS-MS and RRH-MS links. The low values observed validate an approach based on considering them as independent random variables. 
 The measurement campaign extended over a period of 4 months during summertime. The urban area used as test-bed in Vin˜a del Mar, Chile, contains a mix of high-rise buildings and two-story houses with altitudes ranging between 8 and 60 m, built on a plane region at sea level. Measurements of received power at MS positions were carried out at street-level. The area is overlooked by nearby hills that allowed positioning a transmitter at a location typical for a base station covering a relatively large urban area. The BS transmitter height was 180 m above the measurement region at a distance of 2 km to the RRH position. The RRH was mounted in two positions on the exterior walls of a 62 m high building. The streets in the measurement area are lined with trees with heights ranging between 6 and 8 m. All BS-MS links were non-line-of-sight (NLOS), with blockages due to the surrounding constructions. All users were located within a radius of 200 m centered at the low-height RRH. Regarding the distance of these locations to the RRH, 8 were within 60 m, 8 between 60 and 100 m, and 14 between 100 and 200 m, which allowed us to also obtain range-dependent statistics. A schematic description of the terrain prole and of our measurement scenario are presented in Figs. 1 and 2. 
 At the BS position, a continuous wave (CW) transmitter based on a synthesized oscillator transmits at 3.5 GHz with 19 dBm output power through a sector-type antenna. The antenna used had 60? azimuth and 8? elevation beamwidths at -3 dB points. All MS locations were within this beamwidth. At the RRH, the antenna used was a vertically polarized dipole with 2 dBi gain, transmitting a CW signal at 3.57 GHz with 19 dBm output power. Power measurements at the MS positions were carried out using an Agilent model N1996A-506 spectrum analyzer that simultaneously tracked the carriers at 3.5 and 3.57 GHz. A noise oor of -125 dBm in a 10 kHz bandwidth was achieved using low-noise preampliers. The antenna used was a 2 dBi gain vertically polarized dipole placed at a 1.8 m height. The MS measurement system included a laptop computer that acquired the spectrum analyzer scans at a rate of 2 per second. Within the selected area all power measurements exceeded the noise oor by at least 10 dB. 
 The RRH was placed at an altitude of 3 m at two different positions: one at a street intersection so as to simultaneously illuminate two streets and the other one within 5 m of the rst placement but with no direct path to the adjacent streets. In the rst case there are LOS and NLOS user locations, while in the second all MS positions are NLOS. The measurement campaign for both RRH placements involved moving the MS along a straight path of about 4.5 m at each of the 30 locations that were selected at random within 200 m of the RRH, as illustrated in Fig. 2. This was done using a very slow-moving vehicle in such a way that approximately 300 equally-spaced power samples were collected. At the specied frequency, this implies more than 100 received power samples at positions spaced at least half a wavelength apart. From these measurements we obtained the path-loss at each receiver position, which allowed the calculation of average path-loss and small-scale fade statistics at each of the 30 locations. 
 To evaluate achievable power gains under realistic conditions we calculated the received power at the mobile station assuming typical values for transmit powers and antenna gains. Using the path-losses for all points measured we then generated the cumulative distribution function (CDF) of the received power at the MS, considering the BS alone, the RRH alone, the BS with the RRH under a selection combining scheme (SC) and the BS with the RRH considering a maximum-ratio combiner (MRC). We assumed the existence of a BS with 45 dBm of transmission power. The BS and MS antennas are assumed to be the same as the ones used in our measurements (with gains of 17dBi and 2dBi, respectively). For the RRH we assumed that in most practical cases the RRH would be attached to a wall and consequently only illuminate a sector of 180 in azimuth. All our MS locations are in fact within a sector of such angle. We thus considered that a typical 7 dBi, 180? sector antenna would be used, consistent with calculations reported in [25]. For the transmit powers at the RRH we considered two values that cover what could be considered the high and low extremes of a practical deployment: 20 and 40 dBm. 
 To quantify the gains we dene as ?(y) the dB increase in received power that can be guaranteed to users at a given availability level y. Let P[dBm] be the received power at the MS. We denote the CDFs of power received P[dBm] from the BS alone as FBS(P) and the CDF of the power received under SC as FSC(P). The gain in received power when using SC is then: 
 The same type of denition of course applies to the gain obtained through MRC. We will use (1) to calculate, at various availability levels, the power gain resulting from the use of a RRH. 
 In this section discuss the results that can be expected when using a low altitude RRH (3 m height). We consider a random selection of MS placements within a 200 m radius of the RRH. In order to verify that our selected urban scenarios do not signicantly deviate from those characterized by wellestablished models for wireless channels, we compared our path loss results with those predicted by such models. For the NLOS links between RRH and MS, we found a good match to the COST-Hata model [24] and to the alternative WINNER path-loss model for the F-NLOS case suggested in [22] using an operating frequency of 3.57 GHz. With respect to these models, our measured losses were slightly higher, 2 and 4 dB on average, with r.m.s. errors of 6 and 10 dB respectively. The measured losses for the street canyon links were far lower than those for NLOS links at the same distance, from 4 to 34 dB, with an average of 18 dB. They were however higher by about 8 dB on average than those predicted by the advanced LOS model for Type-F LOS scenarios (with both RS and MS below rooftops) [22]. For the collection of mobile locations that we considered, we also modeled empirical pathlosses for both LOS and NLOS links according to a singleslope log-distance model, with free-space losses at 1 m. The resulting slopes were 26 and 38 dB/log(m) respectively. We note that most of our LOS links were partially obstructed by relatively large trees. To evaluate the effect of choosing a RRH position, we considered two placements. One had an uncluttered path to two intersecting streets, while the other was in close proximity (5 m) but obstructed by construction. The rst placement is denoted here as Non-Obstructed RRH (N-RRH) and the second as Obstructed RRH (O-RRH). The N-RRH position provides LOS links to some of the chosen MS locations. For a setting such as this, where both antennas are below clutter, LOS links are described as being in a streetcanyon [26]. Correspondingly, we use this notation and refer to the NLOS links as non-street canyon. In the 200 m radius, half (15) of the N-RRH links are of the street canyon type while all O-RRH links are non-street canyon. When considering a 100 m radius all links to the N-RRH become of streetcanyon type. This classication (according to the RRH-MS link characteristics) is aimed at determining which users can be expected in practice to benet most from a given RRH placement. 
 As an example of the achievable gains we present in Fig. 3 the CDF of the received power at the MS within the 200 m radius, for the case where the RRH transmits at 20 dBm for both RRH placements. All other parameters are as described in section II. As seen, the CDF for MRC and SC are virtually indistinguishable, the shift being less that 1dB. We observe that the benets resulting from the use of a RRH very much depend on its placement. For example, at 50% coverage, the BS will guarantee a power of at least -63dBm. Using an ORRH will only provide a slight improvement, guaranteeing the same power to 56% of users, while an N-RRH will increase this coverage to 76%. Repeating this calculation under the assumption of a RRH transmit power of 40 dBm results in coverage increases to 77% and 84% respectively. In what follows we summarize the power gains for various conditions illustrating the importance of proper placement of the RRH with respect to the intended users. 
 Figure 3.	CDF of received power at MS for 200 m coverage radius and 20 dBm RRH power. RRH height was 3 m. 
 To evaluate the effect of coverage range we divided the locations where the power measurements were carried out into three groups with ranges of up to 200 m, 100 m and 60 m, referred to the RRH. Table I presents the power gain ? (as dened in (1)) obtained with a SC at three availability levels for two RRH transmit powers and the two placements described. We recall that for radii of 100 m or less the NRRH placement provides street-canyon-type links to all users, while only 50% of the users are in that condition for the 200 m radius. The consequence, clearly observed in the table, is that the gains available to 90% of the users increases sharply when the coverage radius is reduced to 100 m, i.e. when all of them are in a street canyon with respect to the RRH. Both Fig. 
 Figure 4.	CDF of received power at MS street canyon positions for 200 m coverage radius and 20 dBm RRH power. 
 3 and Table I illustrate the importance of proper positioning of the RRH. We further explore this below. 
 Based on our above observations we divided the results for all links with up to 200 m length into street canyon and nonstreet canyon types, rather than by distance as before. Table II presents the results for the power gain ? under this classication, using again the parameters previously detailed in section II. The N-RRH columns illustrate that very considerable power gains are available for street canyon links, even at low RRH powers. In contrast, the row corresponding to the non-street canyon links shows very modest gains regardless of the RRH position. The latter results were obtained considering only MS positions that are of non-street canyon type under both RRH placements, so that comparisons are based on a common set of data. 
 To illustrate the gains achievable under the most favorable RRH and MS placements, we present in Fig. 4 the CDFs for received power considering all street canyon locations within the range of 200 m, at a RRH power of 20 dBm. When comparing this gure with Fig. 3, it becomes clear that the street canyon locations are those that contribute most signicantly to the observed increases in received power. For example the BS will provide at least -65 dBm to 50% of users. This same minimum power will be available to 96% of users if a RRH is included. Repeating this for an assumed 40 dBm transmit power increases coverage to virtually 100%. 
 Finally we calculated correlation of fades for the BS-MS and RRH-MS links, for O-RRH and N-RRH as 
 where PL(BS) and PL(RRH) represent respectively the path-losses, including the small-scale fades, from the BS and 
 the RRH to a specic MS position. µBS = E[PL(BS)] and µRRH = E[PL(RRH)] are the statistical averages of these path-losses at the distance under consideration, obtained from a linear regression of path-loss vs. distance. sBS and sRRH are the standard deviations of the fades with respect to these averages. 
 In both cases (O-RRH and N-RRH) these correlations were found to be below 0.01. When small-scale fades are averaged out, this increases to 0.27. 
 Our empirical results suggest that a RRH allows important gains in received power even when placed as low as 3 m and operating at the relatively low transmit power of 20 dBm. However, these gains are basically associated with the existence of LOS (or, equivalently, street-canyon) links to users, and thus heavily depend on site-specic RRH and user placements. In all scenarios, the gains achievable using MRC are only marginally larger than those obtained by a SC, attesting to the fact that, in practice, the likelihood of the RRH and the base providing comparable powers is very small. 
 Virtually no correlation (< 0.01) was observed between the fades of the RRH-user link and the base user link. This allows treating them as independent random variables when modeling a wireless system assisted by RRH units, using the appropriate statistical description of the individual links. 
 ﻿ The characterization of ` compressible random is and extended to the case of stationary and ergodic . The main result of this work a check necessary and sufficient condition for a stationary and ergodic sequence to be ` compressible in the sense by Amini , and . Furthermore , for non ` compressible random , we provide a closed form expression for the best term relative approximation error given a rate of as the block length to infinity . 
 I . INTRODUCTION 
 of compressibility for a stochastic process , meaning that every realization can be well in some sense by its best term sparse version , been a topic of recent interest , , . compressibility for random can play an important role in regression , reconstruction for instance in the classical compressed analysis and reconstruction setting , Th . , inference and decision making . One important case is such a compressibility notion for i . i .. where the probability is with a density function . In this context , every realization of the process is clearly non sparse almost surely , and conventional ways of compressibility for finite dimensional , based on the power law decay of the best term approximation error or that belong to the weak ` ball , are not applicable either , as shown in , . 
 by this , Amini al . and al . have recently new for compressible random . These are not based on the typical absolute approximation error decay pattern of the , but on a relative ` best term approximation error behavior . In particular , Amini al . formally define the concept of ` compressible process in Section below . This new definition a meaningful way of i . i .. random or in of the probability that almost all the ` relative energy of the process is concentrated in an arbitrarily small sub dimension of the innovation domain , as the block length to infinity . Under this umbrella , they provide two important the theory of order statistics . On the one hand , , Theorem that a concrete family of i . i .. is ` compressible the generalized , ‘ t and log logistic , while on the other side , 
 , Theorem that with exponentially such as Generalized are not ` compressible . Therefore , it is interesting to ask about the compressibility of i . i . not considered in this analysis . In this direction , we highlight the work of al . , which under an alternative notion of relative ` compressibility almost sure convergence instead of convergence in measure criterion adopted in and a different analysis setting fixed rate instead of the variable rate used in , an exact dichotomy between compressible and non compressible i . i . Therefore , it is an interesting direction to connect Amini al . ` with the more refined almost sure convergence analysis of the ` best term relative approximation error in , Prop . , with the idea of the analysis of and . 
 To address this question , as well as to extend the analysis to more general random , this work precisely all ` compressible random , in the sense of Amini al . , within the family of stationary and ergodic . Our main result Theorem a necessary and sufficient condition for an ergodic process to be ` compressible , for any arbitrary . Furthermore , for the case of non ` compressible ergodic , we provide a closed form expression for an achievable rate ` approximation error function . The key element in the proof is the application of the ergodic theorem and the derivation of intermediate almost sure convergence Lemma and in Section that match the approximation result by al . , Prop . for the i . i .. case . A corollary of Theorem a necessary and sufficient condition to categorize i . i .. random in of ` , which the analysis in and . 
 . 
 For a finite dimensional vector x ,.., in , let , ,.. denote the ordered vector such that 
 , , ... For some and 
 ,..,, let 
 , , ... , p 
 denote the ` norm of the best term approximation of , where by definition ` In addition , 
  
 the best term ` approximation error of , in the sense that if is the collection of sparse , then , is the solution of minx `. For the analysis of infinite , Amini al . and al . the following relative best term ` distortion indicator : 
  
 From this , Amini al . a notion of critical dimension for a finite length signal and a notion of ` for infinite . 
 Definition : For and , , let us define , min ,.., :, . 
 Then , a sequence is ` compressible if , , 
 , 
 where x ,.., is the truncated finite block vector of . 
 A . Approximation Error for Random 
 Let X ,.... be a random sequence with in , and by its consistent family of finite dimensional : , where X ,.., and the space of probability for the measurable space , . As a short hand , we denote by 
 : the process distribution of . Let us define the measurable set An , 
 :, : , , 
  
 where the last equality is by . Then in analogy with Definition , Amini al . the following : 
 Definition : and Let us consider a process 
 with and , . Then 
  
 is the critical number of with which the set typical with respect to . With this , the process and , respectively is said to be ` compressible , if , , , 
 . 
 Alternatively , we can consider the following fixed rate : 
 Definition : Let be a process by 
 P , and let us consider and , . 
 We say that the rate distortion pair , is ` achievable for with probability , if there a sequence of positive such that and 
  
 Definition : The rate distortion approximation function of , with probability is given by : 
  
 , is ` achievable for with probability . , 
  
 In the next section , we will study the class of stationary and ergodic , where the best term approximation measured in of the function in will be in closed form . Furthermore , it will be shown for this class of random that 
  
 for all and , . 
 . ANALYSIS OF ERGODIC 
 Let be a stationary and ergodic sequence process with distribution : , where we denote by 
 P its shift invariant distribution . For simplicity , we assume that where de measure 
 . Then is with a probability density function and . 
 For a ,, we say that a measurable function : , , is integrable with respect 
 Z 
  
 R 
 and L the collection of integrable . 
 We are in the position to state the main result : 
 THEOREM : Let be a stationary and ergodic process with marginal shift invariant distribution such that , and let us consider an arbitrary . Then we have the following dichotomy : 
 i If L : then and , , 
 . 
 i . e ., is ` compressible . 
 If L : then is not ` compressible . Furthermore , if we introduce the induced probability measure in , by : 
 , 
 then , and 
 . 
 where , , and is solution of : 
 . 
 The proof is in Section . 
 A . Discussion and Interpretation of Theorem 
 Theorem a necessary and sufficient condition for a stationary and ergodic process to be ` compressible in the sense in Definition . 
 In the case of non ` compressible , i . e ., when L , Theorem , what we call , the achievable rate distortion region for the process , given by the set of critical rate distortion : 
  
 This region solely on the shift invariant measure and its induced measure in . 
 In both i and , the critical rate for a stationary and ergodic is independent of . The justification is that asymptotically to infinity , the characterization of to compute on that belong to the tail field of the process , which is known to be trivial i . e ., their have zero or one probability for the case of ergodic . Therefore , we obtain almost sure convergence that make irrelevant the role of in the characterization of see Section 
 for . 
 A natural order among stationary and ergodic process can established from Theorem . 
 Proposition : If is ` compressible for some , then is ` compressible for all . Proof : If L , then L for all . 
 Proposition : If is not ` compressible for then is not ` compressible for all . Proof : If L , then L for all . 
 For the emblematic i . i .. scenario , we want to highlight in more detail the by Amini al . related to ` compressibility in the sense of . , Theorem that if is such that for some , EX e then the i . i .. process is not ` compressible . On the other hand , , Theorem that if to the domain of attraction of a stable distribution , Chap . . with a , , then the process is ` compressible . First for , Theorem a refined result , revealing a indeed , the complete family of that are not ` compressible . In fact , in addition to that go to zero exponentially and consequently f L Gamma , heavy tail whose density are lower dominated by a power law decay of the form with for , student ‘ t freedom , are not ` compressible either . On the other hand , concerning , Th . , it is simple to verify that any that is in the domain of attraction of an a stable law with a that EX and consequently , part i of Theorem this family of ` compressible i . i . 
 More generally , from Theorem we can state the following : 
 Corollary : Let be a stationary and ergodic process . If where EX e , then is not ` compressible for any . 
 Corollary : Let be an ergodic process with invariant distribution and density , . If as for some , then . is compressible , if and only if ,. 
 For the proof of Theorem , we derive almost sure convergence see Lemma and in Section . In the case when L : if is such that 
 for some , then limn , 
 , a ... Furthermore , for 
 for some , it that , a ... In the case when , then limn , , a .. These are consistent and extend the result by al . , Prop . , which for the i . i .. case the same convergence limit for the object Their proof was based on the ‘ lemma of order statistics see in , Th . , while ours is based on the application of the ergodic theorem . 
 Theorem another interesting dichotomy 
 Corollary : If for some and for some , it that 
 , 
 then the latter also for all and for all , . Likewise , if for some , for some , and for all , the pair , is ` achievable for with probability , then all , with , and , are ` achievable for probability . 
 . PROOF OF THEOREM 
 Let us first state the following : 
 Proposition : For any and , : 
 . 
 Proof : Considering as a short hand , by definition in , where from it that . 
 is lower dominated by , if there x and C such that for that then C . 
 We say that as if there and 
 and 
 , 
 ; and otherwise , 
 Let us first consider the case when L . For the rest , it is important to note that given that , then for all , there such that , and for all 
 d , there such that . For X ,.., , we can define : 
 , 
 where from the ergodic theorem , Th . . , , 
  
 and 
 The second almost sure convergence is from the assumption that L . Then , we can state the following : 
 Lemma : For any , and sequence such that , we have that 
 q 
 lim , , a .. 
 n 
 where is solution of . In addition , , , 
  
  
 where is solution of . The proof is given in Section I A 
 In order to prove , let us fix , . Then there , such that and from Lemma if is such that , then limn , , a ... 
 Let us consider an arbitrary such that , then again from Lemma , if a sequence is such that , then limn , 
  
 , a ... Consequently ,, almost surely to a distortion strictly less than , and then for all : 
  
 Hence from the definition of in , we have that eventually in , which that 
 . 
 This upper bound is valid for any such that , then 
  
 > 
 . 
 In general , the condition on for the rate i . e ., , such that and the distortion , 
 are not unique . 
 The first inequality from Proposition and the last equality from the fact that the function is continuous as . 
 To derive a lower bound , let us consider an arbitrary , . We know that there such that . Again from Lemma , for all such that 
 , then limn , d , a .. Therefore , 
 . 
 This result that eventually in , and 
 consequently , 
 . 
 On the other hand , from , we have that it is necessary that . This last inequality and are valid for any , , then and 
 n 
 , which from . 
 Moving to the case where L , we have that 
 EX , then from the ergodic theorem , 
  
  
 and 
 In other , that limn , a .. Furthermore , we have the following : 
 Lemma : Let us consider an arbitrary , and 
 such that , then 
 lim , , a .. 
 n 
 The proof is in Section I 
 Let fix an arbitrary and , then from Lemma we have that : 
 . 
 Then for all and , , and therefore 
 eventually in . From this , for all , which the result from Proposition . 
 V . 
 The work of .. Silva is by of Grant and the Basal Grant . The work of .. is by Grant , and internal research grant 
 . . . 
 APPENDIX I COMPLEMENTARY 
 A . Proof of Lemma 
 Proof : To begin let us prove the fixed rate result in . 
 It will be first important to concentrate in the case where 
 and show in this context that 
 lim , . a .. 
 n 
 For any , let us define the 
  
  
 where from and , At because 
  
 lim , a .. 
 n 
 Let us fix an arbitrary and let and 
 such that for some . Let us take an arbitrary At . From the zero rate assumption on and the definition of At , such that , which that Therefore considering that , 
 lim , . 
 Doing the same process for the collection , we have that , 
 . 
 Then from the sigma additivity , which the result in . 
 with this result , for an arbitrary , let us consider such that limn . We know that there to such that , and for this to we consider and as defined in . Then for any it that 
 . 
 Furthermore , from definition of the ordered sequence , it is simple to verify that see . : 
 , n 
 This is where the zero rate result in is used . In particular , 
 if we consider , from , and the fact that , is o in 
 , we have that 
 lim , lim 
  
 the last equality in from definition of in . Finally from and , 
 which . 
 Concerning the fixed distortion result in , for , 
  
 let be such that . Let us consider an where and consequently and are mutually absolutely continuous . Again for this , we use the in , where for all , it that such that . 
 Considering , it by definition in 
 that : 
  
 then , . We can do the same for the numerable collection : 
 such that 
 where for any , 
 sup lim , . 
 The first equality from the continuity of the function with respect to . Then from the fact that , we have that , , , a .. Finally , proving that , a .. an equivalent symmetric argument and we omit it . 
 B . Proof of Lemma 
 Proof : For let us consider , and , such that and such that . Considering the and 
 , we have 
 that At from and . Let us fix an arbitrary At . Considering that , then eventually in , and therefore eventually , which 
 from definition of that limn , . The fact the event surely the result . 
  
 ﻿The problem of characterizing lower bounds on data-rates needed for closed loop stability has been solved in a variety of settings. However, the available results lead to coding schemes which are very complex and, thus, of limited practical interest. In this paper, we show how simple coding systems comprising only LTI filters and memoryless entropy coded dithered scalar quantizers can be used to stabilize strongly stabilizable SISO LTI plant models over error-free bit-rate limited feedback channels. Despite the simplicity of the building blocks employed, we prove that the data-rates incurred do not exceed absolute lower bounds by more than 1.25 bits per sample.
 I.	INTRODUCTION
 The study of networked control systems (NCSs), i.e., control systems with communication constraints, has emerged as an active research field during the past years (see, e.g., the special issue [1]). Key questions within this framework are related to the way in which network artifacts affect the stability and performance of control loops that employ non-transparent communication channels. Typical channel artifacts include data-rate limits, random delays and data dropouts. A unifying framework for the treatment of the general NCS analysis or design problem does not exist. Nevertheless, there has been significant progress in the study of subproblems. For example, data-rate constraints have been studied in, e.g., [2]–[5]. The issue of data dropouts has been studied in, e.g., [6] and random time delays have been considered in, e.g., [7].
 In this paper we focus on feedback loops closed over delay- and error-free bit-rate limited channels. Within this context, a key existing result establishes necessary and sufficient conditions on the channel data-rate that allows one to achieve closed loop stability (in an appropriate sense; see, e.g., [2], [3], [5] and the many references therein). This result is given in terms of a lower bound on the channel datarate (which depends on the unstable plant poles only) over which coding schemes can be constructed so as to achieve stability. These coding schemes are quite complex and are not attractive from a practical point of view. On the other hand, showing that the rate at which a stable control system is transmitting data is always greater than the aforementioned bound (i.e., necessity) is fairly simple (see also [8], [9]), whereas constructing an actual coding scheme that achieves stability at any rate above the absolute limit (i.e., the proof of sufficiency) is much more involved (see [2], [5]). Clearly, the need arises to develop the study of simple stabilizing coding schemes that achieve rates close to the bounds in [2], [3], [5], and of approaches that allow one to exploit the insights obtained when deriving the necessity of the bounds in the construction of coding schemes.
 Motivated by the discussion above, this paper shows how simple coding systems comprising only LTI filters and entropy coded dithered quantizers (see, e.g., [10]) can be used to stabilize strongly stabilizable SISO LTI plant models incurring in data rates which exceed the minimal ones established in [2], [3], [5] by no more than 1.25 bits per sample. The excess rate of our simple coding scheme is given by the sum of two terms: a first term due to the divergence of the quantization noise distribution from Gaussianity, and a second term that originates in the inefficiency of the loss-less coder that generates the binary words.
 In our results, the coder and decoder architecture plays an essential role, which, given the results in, e.g., [5], is by no means surprising. Our work also sheds light into the reasons why the results in [11] are not always consistent with the lower bound on data-rates for stability studied in [2].
 The remainder of this paper is organized as follows: Section II introduces the notation used in the paper. Sections III and IV present the considered setup and coding scheme, respectively. Section V focuses on a simplified setting where Gaussianity assumptions are imposed. Section VI uses the results in Section V to derive guaranteed upper bounds on the data-rates that allow one to achieve MSS with the proposed coding scheme. Section VII draws conclusions. Due to space constraints, all proofs have been omitted and can be found in [12] or obtained via e-mail from the authors.
 II.	NOTATION
 We define N0 , {0,1,···} and  x < 8}. We use z as both the argument of the z-transform and as the forward shift operator, where the meaning is clear from the context. Given any scalar x, |x| denotes magnitude.
 The set of all discrete-time strictly proper real rational transfer functions is denoted by Rsp.
 Unless otherwise stated, random processes are always scalar and defined for k ? N0, we abbreviate {x(k)}k?N0, x(k) ? R, by x, and define xk , {x(0),··· ,x(k)}.
 E{·} denotes the expectation operator. The variance, at time instant k, of a process x is denoted via sx2(k); similarly, if x is a random variable, then sx2 denotes its variance. We define sx2 , limk?8 sx2(k), provided the limit exists. If x is
  
 channel
 	decoder	encoder
 Fig. 1. Considered networked control system (generic coding scheme).
 a wide sense stationary (wss) (asymptotically wss) process, then Sx(ej?) denotes its (stationary) power spectral density (PSD) and ?x(z) denotes any spectral factor of Sx(ej?), a second order one if and only if it has finite mean and finite i.e., . We say that a random variable is
 and non-zero variance.
 If x,y,z are continuous (resp. discrete) random variables, then h(x) (resp. H(x)) denotes the differential (resp. discrete) entropy of x; h(x|y) (resp. H(x|y)) denotes the conditional differential (discrete) entropy of x, given y; I(x;y) denotes the mutual information between x and y; I(x;y|z) denotes the conditional mutual information between x and y, given z. We recall that if x is Gaussian, then h(x) =
  . In this paper we use logarithms in base e. Thus, information is measured in nats (1 nat = (ln2)-  bits). For details and basic properties of the quantities mentioned above we refer the reader to [13].
 III.	PROBLEM SETUP
 In this paper we are concerned about the stability of a closed loop system built around a SISO discrete time LTI plant model, which employs a delay- and error-free data-rate limited channel in the feedback path, as shown in Fig. 1. In that figure, G(z) is the plant model, C(z) is an LTI controller, y is the plant output, u is the plant input, r is a reference signal, and d models disturbance signals. In order to make use of the bit-rate limited channel, the feedback path comprises an encoder E which generates binary words based on information regarding the plant output. These words, which we will denote by s, are then sent delay- and errorfree through the channel. At the receiving end, a decoder D uses the received symbols to generate the signal f that is fed back into the controller.
 We will make the following assumptions:
 Assumption 1 (Plant model): G(z) ? Rsp is unstable,1 has no poles or zeros on the unit circle, is strongly stabilizable,2 and its initial state is an independent second order random variable.	N
 Assumption 2 (Reference and disturbance): The signals r and d are second order zero-mean wss processes that admit rational PSDs, and that are mutually uncorrelated. Moreover,
 Sd(ej?) + Sr(ej?) 6= 0.	N
 Assuming that the plant is strictly proper guarantees that the feedback loop in Fig. 1 is well-posed for all causal controllers, decoders and encoders. The assumption regarding
  
 Fig. 2. Proposed coding scheme.
 the plant initial state holds for most cases of interest. The remaining assumptions on G(z) are not essential, but we have made them to maintain a straightforward presentation (see [15] for the general case). Assumption 2 is standard except for the fact that we require r or d to be non-zero. We avoid the case r = d = 0 for brevity.
 For future reference, we define {p1,··· ,pnp} as the set of unstable plant poles, and
 	 .	(1)
 Throughout this paper we adopt the following notion of stability (see also, e.g., [16]–[18]):
 Definition 1 (Mean square stability): Consider a system described by x(k + 1) = f(x(k),w(k),k), where k ? N0, f : Rn × Rm × R ? Rn, x(k) ? Rn is the system state at time instant k, x(0) = xo, where xo is a second order random variable, and the input w is a second order wss process independent of xo.  We say that the system is mean square stable (MSS) if and only if there exist finite µ ? Rn and finite M ? Rn×n, M = 0, such that
  
 regardless of the initial state xo.	N
 Significant work has been devoted to the study of the MSS of the networked control system in Fig. 1, when arbitrarily complex coding schemes are employed (see, e.g., [2], [5]). However, the study of simple and efficient coding schemes has received much less attention. This is indeed one of the main motivations for the present work.
 IV.	PROPOSED CODING SCHEME
 We seek simple, though effective, coding and decoding schemes. We propose to use the coding scheme shown in Fig. 2. In that figure, F1(z) is an LTI filter, and Eˆ, Dˆ are (for the moment) abstract systems that are allowed to exploit the history of their inputs to generate their corresponding outputs. More precisely, for every k ? N0,
 	s(k) = Eˆk(vk,sk-1),	w(k) = Dˆk(sk),
 where Eˆk and Dˆk are (possibly stochastic) mappings that may depend explicitly upon time, and where the range of Eˆk is a collection of prefix-free binary words (see, e.g., [13]). Note that the resulting control loop is well-posed if F1(z) ? Rsp.
 The coding scheme defined above is a restricted instance of much more general schemes. Indeed, it is a special case of a coding scheme that exploits the history of the plant output and of the received symbols in an arbitrary causal fashion (see, e.g., [2], [5]).
 We will assume that the following holds:
 Assumption 3 (Coding scheme): The coding scheme in Fig. 2 is such that, ?k ? N0,
 (a)	knowing sk and vk does not provide more knowledge about w(k) than just knowing sk,
 (b)	the sequence of mappings {Dˆi}i?{0,···,k} is known at the encoder and is such that sk can be recovered exactly from wk (and vice-versa). Also,
 (c)	F1(z) ? Rsp and the corresponding initial state is an independent second order random variable. N At each time instant k the encoder transmits the symbol s(k) using a word of length R(k) (measured in nats). We will be interested in the average coding rate and, accordingly, we define the average data-rate as
 	 ,	(2)
 provided the limit exists. R¯ is the average rate at which the symbols are sent trough the channel and, as such, is a measure of the “information flow” at the physical level.
 A key question that arises when considering any coding scheme is how to characterize lower bounds on the achievable average data-rates. To that end we define the following: Definition 2: Consider two random processes v and w. We define (if the defining limit exists) the average directed mutual information between v and w as4 (see also [19])
 	 .	(3)
 N
 We can now characterize lower bounds on the average data-rate for the proposed coding scheme:
 Theorem 1 (Lower bounds on data-rates): Consider the networked control architecture in Fig. 1 and the specific coding scheme in Fig. 2, where Eˆ and Dˆ are as described above. If Assumptions 3(a-b) hold, then R¯ = I8(v ? w).
 	Proof: See [12].	
 Theorem 1 relates a physical quantity, namely average data-rate, to an information theoretic quantity, namely average directed mutual information. It is important to note that a different bound on the average data-rate would have arisen if we had considered Dˆ and Eˆ with different information available.
 Theorem 1 is a key one. However, it is not straightforward to characterize I8(v ? w) unless one makes suitable assumptions on Eˆ and Dˆ or, equivalently, on the signal
 	q , w - v.	(4)
 In the next section we will make some Gaussianity assumptions. Later, in Section VI, we will consider a simple instance of Eˆ and Dˆ and we will exploit the results for the 4I8(v ? w) is sometimes called directed mutual information rate.
  
 Fig. 3. Model of the networked control system under study.
 Gaussian case to provide guaranteed upper bounds on the rates associated to that specific coding scheme, without using the Gaussianity assumptions.
 V.	THE GAUSSIAN CASE
 As foreshadowed at the end of Section IV, we will assume in this section that the following holds:
 Assumption 4 (Gaussianity): The noise q (see (4)) is an independent second order zero-mean i.i.d. Gaussian sequence. The initial states of all LTI filters in Fig. 3 (including plant and controller) are independent second order Gaussian random variables, and r and d are jointly Gaussian processes.
 N
 Consistent with the fact that the noise q depends on the way in which Eˆ and Dˆ are chosen, we will consider the variance of q, say sq2, as design parameter. Assumption 4, when combined with the networked control system in Fig. 1 and the coding scheme in Fig. 2, yields the linear model in Fig. 3.
 We will associate to every pair {F1(z),C(z)} a vector K(z) defined via
 	  .	(5)
 As long as Assumptions 2 and 4 hold and q has finite variance, the networked control system in Fig. 3 is MSS if and only if K(z) is such that the feedback loop in Fig. 3 is well-posed internally stable in the standard deterministic sense (see, e.g., [20], [21]). Thus, if we define
 S , {K(z) ? Rsp × Rp : the loop in Fig. 3 is
 internally stable and well-posed},
 then MSS is equivalent to  (provided Assumptions 2 and 4 hold; see [20]).  It is easy to see that K(z) ? S if and only if F1(z) is stable, and C(z) is an admissible controller  for G(z) (see, e.g., [21]).
 Using Theorem 1 we can now characterize average directed mutual information in the situation under study:
 Corollary 1 (I8(v ? w) under Gaussianity assumptions):
 Consider the feedback loop in Fig. 3 and assume that Assumptions 1, 2 and 4 hold. If K(z) ? S and  ,
 then
 1	p	Sw(ej?) I8(v ? w) =  4p Z-p ln	 sq2	d?
 	(a) 1	sv2
 	=  2 ln(1 + ?),	? , sq2 ,	(6)
 where Sw(ej?) is the stationary PSD of w, sv2 is the stationary variance of v, and ? is the stationary coding scheme signal-to-noise ratio. Equality in (a) holds if and only if Sw(ej?)sq-2 is constant for every ?.
 	Proof: See [12].	
 Corollary 1 provides an explicit expression for the average directed mutual information across the coding scheme in the considered situation. Also, it establishes a relationship between the average directed mutual information across the coding scheme and the corresponding signal-to-noise ratio. In particular, it follows from (6) that (provided Assumptions 1, 2 and 4 hold) minimizing the coding scheme signal-tonoise ratio amounts to minimizing an upper bound on the average directed mutual information across it. Moreover, if Sw(ej?)sq-2 is a constant, then we have equality in (6). Hence, under the constraint of having Sw(ej?)sq-2 constant, minimizing the coding scheme signal-to-noise ratio also minimizes the average directed mutual information across it. We will show below that, in our setting, one can restrict Sw(ej?)sq-2 to be constant without loss of generality. Towards that goal, we start with the following theorem:
 Theorem 2 (Directed mutual information for MSS): Suppose that Assumptions 1, 2 and 4 holds. Then:
 1)	There exists K(z) ? Rsp × Rp such that the feedback loop in Fig. 3 is MSS if and only if
 np
 I8(v ? w) > I8inf(v ? w) ,Xln|pi|.
 i=1
 2)	Any I8(v ? w) > I8inf(v ? w) can be achieved in a MSS loop if one chooses a stable and strictly proper F1(z) such that 1 + F1(z) is minimum phase (MP), C(z) is any stable admissible controller for G(z), and sq2 is made sufficiently large (but finite).
 	Proof: See [12].	
 Theorem 2 establishes a bound on the directed mutual information across the considered coding schemes whose satisfaction is necessary and sufficient to achieve MSS. It also provides a characterization of the controller C(z), the LTI filter F1(z), and the noise variance sq2, that allow one to achieve any average directed mutual information arbitrarily close to the bound established.
 We can use Theorem 1 to immediately infer that, provided the assumptions in Theorem 2 hold,
 np
 	R¯ > Xln|pi|	(7)
 i=1
 is a necessary condition that average data-rates need to satisfy in order to guarantee MSS when employing the proposed coding scheme. We note that we have recovered, for the class of considered plant models, the bound on average data-rates for MSS derived in [2]. However, we have not provided a proof of the achievability of this bound. This issue will be explored in Section VI.
 We next explore the minimal stationary coding scheme signal-to-noise ratio compatible with MSS. This will enable us to state the main result of this section.
 Theorem 3 (Signal-to-noise ratio for MSS): Suppose that Assumptions 1, 2 and 4 hold.
 1)	There exists K(z) ? Rsp × Rp such that the feedback loop in Fig. 3 is MSS if and only if the stationary coding scheme signal-to-noise ratio ? satisfies
  .
 2)	Any ? > ?inf can be achieved in a MSS loop if one chooses C(z) as any stable admissible controller for G(z), F1(z) is chosen as
 F1(z) = ?p(8)(?p(z)S(z))-1 - 1, (8) where S(z) , (1 + C(z)G(z))-1, and sq2 is made sufficiently large (but finite).
 	Proof: See [12].	
 From Theorems 2 and 3 we have the following corollary:
 Corollary 2: Consider the feedback loop in Fig. 3 and suppose that Assumptions 1, 2 and 4 hold. A solution to the problem of finding the minimal stationary coding scheme signal-to-noise ratio that guarantees MSS provides a solution to the problem of finding the minimal average directed mutual information across the coding scheme that guarantees MSS.
 	Proof: See [12].	
 Corollary 2 is one of the main results in this paper. Indeed, by virtue of Theorem 1, Corollary 2 allows one to conclude that determining the minimal signal-to-noise ratio for MSS provides an immediate lower bound on the average data-rates that allow one to achieve MSS. The key to establishing these facts lies in that we consider a coding architecture with sufficient degrees of freedom. This allows one to make Sw(ej?)sq-2 constant in the limit as sq2 ? 8, without constraining the minimum achievable directed mutual information across the coding scheme, or the corresponding minimal achievable signal-to-noise ratio (a simple calculation based on the results of Theorem 3 reveals this). We feel that the insights provided by these results are of fundamental importance and, to the best of our knowledge, new.
 Remark 1: The work [11] also studies minimal signal-tonoise ratio considerations for stability. However, the authors of [11] constrain themselves to the particular case F1(z) = 0, which does not, in general, allow one to achieve the absolute minimal coding scheme signal-to-noise ratio identified in Theorem 3. Hence, it is not surprising that the results in [11] do not always suggest bounds on average data-rates that are consistent with the results in [2], where coding schemes that exploit the history of y and s are used. N
  
 Fig. 4. Entropy coded dithered quantizer.
 VI.	UPPER BOUNDS ON ACHIEVABLE DATA-RATES
 Section V has focused on establishing a necessary lower bound on the average data-rate that allows one to guarantee MSS. We have, however, provided no proof of the achievability of this bound. It is well known (see [2], [5]) that there exist coding schemes that allow one to achieve MSS with average data-rates arbitrarily close to the bound identified in Section V. However, since we have chosen a specific coding scheme that is much simpler than those proposed in [2], [5], there exists no guarantee that we will be able to actually achieve the bound. This motivates the study of guaranteed upper bounds on the average data-rate for MSS applicable to the proposed coding scheme. In particular, we will use the results in Section V to study achievable rates, when a memoryless entropy coded dithered quantizer (ECDQ) is used to generate the binary words that are sent over the channel.
 Fig. 4 shows the architecture of an ECDQ and its relationship to Dˆ and Eˆ in Fig. 2 (see, e.g., [10]). The ECDQ is such that s+(k) = Q(v(k) + dh(k)),
 where dh(k) is the dither signal which is known at both ends of the channel, and Q corresponds to a uniform quantizer defined via Q(x) , i? for  , i ? Z, where ? is the quantization step. At each time instant, s+(k) is memory- and loss-lessly coded by the entropy coder EC using explicit information about dh(k) and the corresponding binary word is sent over the channel. Upon reception, the decoder ED, which knows dh(k), decodes the received symbol recovering s+(k). Accordingly, the output w(k) becomes equal to s+(k) - dh(k).
 Key properties of ECDQs are the following:
 Lemma 1 (ECDQ (see [10])): Consider an ECDQ as described above. If dh is an independent sequence of i.i.d. random variables, uniformly distributed on  , then the noise q defined in (4) is distributed according to the distribution of -dh, and the scalar mutual information between v(k) and w(k) satisfies I(v(k);w(k)) = H(s+(k)|dh(k)).
 
 Remark 2: The proof in [10] assumes no feedback around the ECDQ. If there is strictly causal feedback around it (as in our case), then the same result applies (see detailed proof in [15]).	N
 The results in Lemma 1 state that, if one employs an appropriately dithered ECDQ, then the difference between the input and output of the ECDQ, namely q, becomes just an i.i.d. source, uniformly distributed on each quantization interval (as in the classical additive white noise model for quantization; see, e.g., [22], [23]). This means that the use of an ECDQ allows one to achieve a noise q that resembles the noise source considered in Section V (except for having a different distribution). Therefore, it is sensible to expect that use of an appropriately designed ECDQ yields average data-rates “close” to the bounds identified in Section V. We will show below that this is indeed true. We note that, given the fact that the dither signal is known at both ends of the channel, choosing dh as in Lemma 1 implies that the ECDQ satisfies Assumptions 3(a-b) (i.e., an ECDQ is a valid instance of Dˆ and Eˆ in Fig. 2).
 We are now ready to prove the main result of this paper:
 Theorem 4 (Achievable rates): Consider the networked control system in Fig. 1 with the coding scheme in Fig. 2, where Dˆ and Eˆ form an ECDQ as described above. Suppose that the memoryless entropy coder-decoder pair ECED inside the ECDQ uses Huffman coding (see [13]) that Assumptions 1, 2 and 3(c) hold, that the controller initial state is an independent second order random variable, and that the dither signal dh is an independent sequence of i.i.d. random variables, uniformly distributed on  . Then, if one chooses K(z) as in Theorem 3 (Part 2; see (5)) and
  , with , then the resulting networked control system will be MSS and the corresponding average data-rate will be upper bounded as
  ,
 where  is a continuous and decreasing function of ? such that lim??8 F(?) = 0, lim??0 F(?) = 8.
 	Proof: See [12].	
 Remark 3 (Bounds are conservative): The upper bound for the average data-rate in Theorem 4 is conservative. Indeed, the rates achieved by Huffman coding are usually closer to the entropy of the source than to the entropy of the source plus ln2 (see, e.g., [24].). As a consequence, the actual data-rates will be usually close to the expression in
 Theorem 4 with the term ln2 omitted.	N
 Theorem 4 is the main result in this paper. It establishes guaranteed upper bounds on the average data-rates that guarantee MSS and that are achievable with the coding scheme in Fig. 2, when Dˆ and Eˆ form a memoryless ECDQ and when the plant model is strongly stabilizable. If one chooses K(z) as suggested in Part 2 of Theorem 3, and ? is chosen sufficiently large (but finite), then one will be able to achieve a rate that is no more than   nats per sample (i.e., 1.25 bits per sample) away from the absolute bound in [2], [5]. This additional rate is composed by two terms: the first one is due to the divergence of the distribution of quantization noise from Gaussianity, and the second one originates in the inefficiency of the loss-less coding scheme employed to generate the binary words. We feel that this extra rate is a fair price to be paid if one constrain oneself to the conceptually simple coding schemes considered in this paper.
 Remark 4: Note that the results in Theorem 4 rely on the insights developed in Section V. In other words, the insights developed when establishing lower bounds on the average data-rates needed for stabilization were key when constructing an actual coding scheme that achieved a datarate close to that bound. It is also worth noting that the quantization scheme considered in this paper constitutes a well studied building block in the Information Theory literature (see, e.g., [10], [23], [25]). All the above stands in stark contrast to the approach in, e.g., [2]. N
 VII.	CONCLUSIONS
 In this paper, we have studied the mean square stabilization of strongly stabilizable SISO LTI plant models controlled over bit-rate limited channels. We have proposed a simple coding scheme which uses only LTI filters and an entropy coded dithered quantizer. Within this setup, we have shown that the excess data-rates, which derive from using this reduced-complexity scheme instead of an arbitrarily complex one, are no larger than 1.25 bits per sample. Extensions to more general cases and to performance related questions can be found in [15].
 
 ﻿ This work a formal connection between density estimation with a data rate constraint and the joint objective of fixed rate universal source and model identification by in TIT , , , . an equivalent learning formulation , 
 we derive a necessary and sufficient condition over the class of for the of the joint objective . The learning framework used here is the skeleton estimator , a rate constrained learning scheme that achievable for the joint and modeling problem by its learning to the specific of the problem . The with the skeleton estimator significantly extend the context where universal source and model identification can be , for that move from the known case of parametric collection of with some smoothness and to the rich family of non parametric L totally bounded . In addition , in the parametric case we are able to remove one of the that constrain the applicability of the original result similar in of the distortion redundancy and per letter rate overhead . 
 : fixed rate source ; joint and modeling ; universal source ; learning with rate ; the skeleton estimator ; L totally bounded classes 
  
 Introduction 
 Universal source a long history in information theory and statistics . seminal work the variable length lossless problem and important information for performance analysis , . In this lossless setting , it is well understood that the entropy the minimum achievable rate in per sample to code a stationary and memoryless source when the probability model of the source is available . When the probability of the source is not known but to a family of the so universal source problem , the focus of the problem is to characterize the penalty or redundancy in per sample that an and pair will experience due to the lack of knowledge about the probability . In the lossless case , a seminal result that the least worst case redundancy over or the solution of the problem for is determined by the information radius of . 
 Building on this connection between least worse case redundancy and information radius of , there are numerous important for lossless , . In particular , it is known that the information radius sub linearly with the block length for the family of finite alphabet stationary and memoryless , which the existence of a universal source code that entropy as the block length goes to a large value for every distribution in . 
 Entropy , , ; : . e journal entropy 
 However universality is not possible for the family of alphabet stationary and memoryless because the information radius of this family is unbounded , , . More recent on lossless over countable infinite have at the analysis to specific of with some tail bounded to achieve universality and also at weak of the lossless source setting . 
 In the fixed rate source problem , assuming first that the probability of a memoryless source is known , the performance limit of the problem is given by the distortion rate function , . Consequently , the universal source problem to compare the distortion of a scheme satisfying a fixed rate constraint with the distortion rate function assuming that the designer only that . The literature on this problem is rich , , with a first result dating back to who the existence of weakly fixed rate universal source code for the class of stationary under certain about the source , the alphabet , and the distortion measure . More refined were in , one of which established necessary and sufficient to achieve weakly universality for the class of stationary and ergodic . To provide a more specific analysis of universal source , Linder al . a scheme with a distortion redundancy that goes to zero as O for the case of independent and identically distributed i . i .. bound . Later Linder al . previous showing a fixed rate 
 construction with a distortion redundancy that as O log and O log finite alphabet i . i .. and bounded infinite alphabet i . i . respectively . Similar convergence were a nearest neighbor vector quantization approach in . 
 It is also understood that universal variable length lossless source is connected with the problem of distribution estimation , , as there is a one to one correspondence between prefix free and finite entropy discrete in the finite and countable alphabet case , , . Building on this one to one correspondence in the lossless case , al . , Theorem that the redundancy in per sample of a given code upper the divergence between the true distribution of the source and the distribution derived from the code . Therefore , the existence of a universal lossless source code existence of a universal distribution free in estimator of the distribution in direct information divergence . This that lossless not only a lossless representation of the data , but it a consistent error free estimator of the distribution at the receiver . 
 The connection between and distribution estimation that is evident in the lossless case is not , however , present in the fixed rate source problem . As in , a fixed rate source code does not offer a direct map with a probability distribution model for the source . In light of this gap between and and by some in adaptive control , where it is relevant to both compress data in a way and identify the distribution of the source at the receiver , , the joint objective of fixed rate universal source and model i . e ., distribution identification in . 
 Inspired by construction in , , a new setting for the problem of fixed rate universal compression of continuous memoryless based on the idea of a two stage joint and model or distribution identification framework . In this context , he a two stage scheme to consider two : fixed rate universal source and source distribution model identification . The first objective of the scheme is to transmit the data in the classical distortion rate sense , while the second objective is to learn and transmit a description version of the source distribution model , . Taking from statistical learning , splitting the data into training and testing . The training data is used in the first stage of the process to construct a estimation of the source distribution and encode it the first stage . Then in a second stage of the process , the first stage are used to pick a with the distribution fixed rate source code to encode the test data the second stage . In this joint and modeling setting , the existence of a zero rate consistent estimator of the density in total variation is sufficient to show the existence of a weakly universal fixed rate source scheme Theorem . , the distortion rate function , , , , for any given rate . This result is for a wide class of single letter bounded distortion and for a family of source : indexed over a bounded finite dimensional space , i . e ., a parametric collection with some smoothness and Theorem . . 
 It is important to highlight that the joint and modeling in did not degrade the performance of the source objective . In fact by the analysis to the source objective alone , the joint and modeling framework in the same state of the art performance as conventional two stage universal source or universal vector , , in of distortion redundancy and per letter rate 
  
 overhead O and O log , respectively as the block a large number . Importantly , the first stage of this joint and modeling scheme are used to achieve model identification at the receiver with arbitrary precision in total variation with a rate of convergence 
  
 of O to infinity , with no extra cost in per letter with conventional fixed rate source . 
 of This Work 
 This work formally the interplay between density estimation under a data rate constraint and the joint fixed rate universal source and modeling problem with training data or memory in . The first main result Theorem a connection between zero rate density estimation and a universal joint and modeling scheme that optimal source in a distortion rate sense and lossless model identification . This result is for the general family of bounded single letter . Remarkably , this connection that the construction of a joint and modeling scheme to the construction of a zero rate density estimator . From this result , the second main result Theorem a necessary and sufficient condition for the existence of a weakly universal joint and modeling scheme . For the part of this result , we used the skeleton estimator as our learning framework . this learning framework we extend the parametric context in to the rich non parametric scenario of L totally bounded . 
 Furthermore , the parametric case studied in , by the skeleton estimator we are able to remove some of the that limit the applicability of the original result . We show that the skeleton estimator the best performance in in of the distortion redundancy and per letter rate overhead , in particular of convergence to 
  
 zero of O and O log , respectively , as the block length to infinity . To obtain this , our result the finite and dimension assumption considered in . On the other hand , when the finite dimension assumption is added in the analysis , 
  
 the skeleton learning scheme a convergence rate of O for the distortion redundancy as the sample length goes to infinity . Finally , the skeleton framework is in the parametric case as its minimum distance decision is carried out on a finite number of and the oracle e skeleton or the e covering in total variation of Chapter can be by a practical uniform covering of the compact index set Theorem . Finally , it is worth that a preliminary version of this work in the context of density estimation under a data rate constraint was in . 
 The rest of the paper is organized as : Section the setting of the joint and modeling with training data . Section the with zero rate density estimation . Section the main joint and modeling result Theorem and the skeleton estimator . Finally , Section a special case where the are indexed by finite dimensional bounded space the parametric context . A summary of the is in and . Finally , the are in Section . 
  
 The fixed rate and modeling problem in is in this section . This joint and modeling problem will be the main focus of this work . In addition , and used in the rest of the paper will be . 
 . . Basic 
 Let be a separable and complete subset of where is the sigma field . Let be the collection of probability on ,, with the sigma field restricted to , and let denote the set of probability absolutely continuous with respect to the measure . For any its probability density function . The total variational distance in is given by to avoid any confusion , a set then its . 
 V A . 
 For to , if we define the set for the pair , by 
 ,, 
 then , , , , . 
 . . Fixed Rate Universal Source with Memory or Training Data 
 Let : be an i . i .. stochastic process or stationary and memoryless source , where Xi in and a distribution in : . is in general an index set for . The problem of source of a finite block of the process X .., to find a or code from to , where is a finite set . Given a constraint on , the design is to make as close as possible to in average for that a distortion function . The standard problem the knowledge of for finding the optimal code for any finite block , , , as well as for the fundamental performance of this task to infinity , , , . 
 A more realistic scenario is the universal source problem , where the source distribution is unknown and a scheme needs to be designed for the family . Here we focus on a specific learning variation of this task by in , where in addition to the data that needs to be compressed and with respect to a fidelity criterion , 
 we have a finite number of i . i . following the same distribution and that can be used to estimate in the process more of this approach in Section . . This additional data can be as memory , training data , or side information about available at the because it is data that is not to be compressed and . The existence of this memory from the standard zero memory setting considered in universal source . However , this information can be seen as a realistic assumption in the context of a sequential block by block of an infinite sequence , where the data is partitioned into of the same finite length and compressed sequentially block by block . Then in a given stage of this sequential process , the data from previous are available at the lossless for the process the current block . 
 More specifically following the fixed rate block and modeling setting by in , we consider an block scheme with finite memory , where there is a distinction between the data Z .., that is available as side information to estimate the source distribution training data and the data that needs to be and source or test data , under the important assumption that both data are i . i .. of the same unknown probability . A systematic exposition of this setting and its connection with the classical setting of zero memory block is in Section . Formally , let us define an , block code by the pair 
  
 Then given a set of training and a finite block of the source is the composition of : a function , that to an element in a finite set conditioned on the training data or memory by , and a function that a 
 ,. In this context , into the reproduction the reproduction space . As a short hand , we denote , : that we the 
 , , the reconstruction of by , and its memory for simplicity , the dependency of or , on the memory will be implicit in the rest of the exposition .. 
 The rate of , in per letter is given by . In general , it is not possible to recover from given the constraint on , and thus a single letter distortion measure : is used to quantify the block discrepancy by 
 n 
 , xi 
 i 
 Finally considering and , the average distortion per letter of , given is 
 , , 
 which is a function of and hence the average distortion per letter of , is 
 , . 
 In universal source the performance of a code , is over a collection of and is point wise with the best code that can be assuming that is known . For this analysis , we need the following : 
 Definition . For a finite block distribution , the order operational distortion rate function of at 
 ,. 
 m with R 
 In this context , the operational distortion rate function , is given by 
 lim . 
 The celebrated source theorem a single letter theoretical characterization for in also known as the . A nice exposition of this celebrated result can be found in , , . 
 It is worth that the operational distortion rate function in is equivalent to the classical order operational distortion rate function given by : such that R 
 Lemma . . Then , a nonzero memory side information at the does not help in the minimization of the distortion when is known . 
 For the rest of the exposition , we will concentrate on the simple case studied in where i . e ., the block length is equal to the memory of the code . To be precise about the meaning of universality in this context , we resort to some standard : 
 Definition . A scheme ,: is weakly universal for the rate , 
 if F 
 lim , 
 n 
 and . Alternatively , the scheme is said to be strongly 
 universal for the 
  
 n F 
 and lim , . 
 the distortion redundancy in two , 
 , 
 the first term is the order distortion redundancy , which is the discrepancy that can be exclusively to the goodness of the scheme . The second term in , i . e ., 
 , to do with how fast to the as the block length 
 to infinity see further in Section and therein . From this observation , 
 we introduce the following definition : 
 Definition . A scheme ,: is strongly finite block universal for the 
 lim sup , i 
 n F 
 and lim , . 
 Note that if ,: is strongly universal then it is strongly finite block universal , but the converse result is not true in general . The missing condition to make these two criteria equivalent is the uniform convergence of to in the class . More discussion about this point in Section . 
 . . Two Stage Joint Universal and Modeling 
 by the work of , a two stage block code with finite memory training data , with the objective of doing both fixed rate source , and identification of the source distribution at the receiver . More precisely , given and the training and the source data , respectively , an , joint and modeling rule is given by 
 , : , :, 
 , 
 where finite set of , in two . In the first stage , the pair , in to do density estimation and finite rate quantization by , and an density in . At the end , the first stage 
 a estimation of given by 
 . 
 the index , the second stage of ,, by , and the source data by 
 , , , . 
 In summary , the outcome of the whole process is the concatenation of the that represent first stage , and the that represent , second stage . The process , on the other hand , the first stage to recover and then the second stage to recover . see Figure in which this process is . The rate in per letter of , is 
 R . 
  
 source reconstruction 
 Figure . Illustration of two stage joint source and modeling scheme . Top figure the process and the bottom figure the respective process . 
 Based on this two stage scheme , we could simultaneously achieve source and density estimation modeling at the . This new joint and modeling objective the introduction of the following definition : 
 Definition . A joint and modeling scheme ,: in is strongly universal for a class of distribution : at the rate , if 
 , 
 •• , . , , and 
 Consequently , if ,: is strongly universal for , it that infinity , density estimation is at the in total and , from the source perspective , ,: is strongly finite block universal the sense of Definition . For the rest of the paper , the strongly universality of Definition will be the main and modeling objective . 
 with Zero Rate Density Estimation 
 This section a connection between the objective of joint and modeling declared in Definition and a problem of zero rate density estimation . 
 . . Density Estimation with a Rate Constraint Let us first introduce the problem of rate constrained density estimation . Let : be an indexed collection of as in Section . . 
 Definition . An , learning rule of a pair of ,, with : 
 and :, a finite set and 
  
 . 
 n 
 The composition of these two : the rate constrained learning rule in the : , where log its description complexity in per training sample . 
 Definition . The rate is achievable for , if a learning scheme , : such that 
 and lim sup , 
  
 where Z , Z . in the left hand side of to i . i .. driven by . In this case , we say that is an rate uniformly consistent scheme or estimator for the class . 
 . . Main 
 Proposition . If for a given , ,: is strongly universal for the the rate Definition , then its induced finite description learning scheme from the first stage in , i . e ., , : , is a zero rate uniformly consistent estimator for Definition . 
 The proof is in Section . . 
 Interestingly , the existence of a zero rate uniformly consistent scheme also sufficient to achieve the joint and modeling Definition if some mild are adopted from the work in . This is stated in the following result : 
 Theorem . Let us assume that 
 : can be expressed by , , where , is a bounded metric in with and 
 for all , for all , and for all , there a , block code , say , that the order operational in . 
 Then the existence of a learning scheme , : that is zero rate uniformly consistent there a joint and modeling scheme ,: that is strongly universal rate Definition . 
 The proof is in Section . . 
 Remark . The construction for ,: at any rate in Section . the zero rate density estimation scheme : that : 
 , and 
 R , , 
 , where is a constant . It is worth that these two summarize the result in Theorem and , importantly , these two are independent of . 
 Remark . An important consequence of the in and is the fact that a learning scheme : with specific of convergence for E , and to infinity a joint and modeling scheme that a uniform rate of convergence to zero over of the overhead in distortion by and a uniform rate of convergence to zero of the overhead in rate by . This observation will be used in all the achievable in and , where , consequently , the problem to determine and for E , and . 
 Joint Source and Modeling 
 From the connection with zero rate density estimation in Section , here we present a set of new for the joint and modeling problem of Section . . In these , the general i and stated in Theorem are assumed . 
 . . Main Result : The Skeleton Density Estimator 
 Let us first introduce some from approximation theory . 
 Definition . Let be a class of . We say L totally bounded if for every e , there is a finite set of : i .., that , 
 N 
 F , 
 i 
 where : , e . 
 Definition . totally bounded , let Ne denote the positive integer that the condition in . Ne is the e covering number e log Ne is the e entropy of . 
 Definition . An e covering Ge that Ge Ne is an e skeleton of . 
 Theorem . There is a strongly universal joint and modeling scheme any rate if , and only if , is L totally bounded . 
 The proof is in Section . . 
 The part of the proof of Theorem on the adoption of the skeleton estimator with its minimum distance learning principle in , which is a zero rate uniformly consistent density estimator for Definition . Furthermore , Theorem can be saying that the construction ,: derived from the skeleton estimator that is a short hand for the process distribution of by under the i . i .. assumption . 
 lim , , almost surely , 
 n 
 lim , , almost surely , 
 n 
 . The argument is in Appendix A . 
 . . of L Totally Bounded 
 Knowing specific for e log Ne , the skeleton estimator can be its design parameter appropriately . In particular , the sequence en see in Section . is selected as the solution of the optimal balance between estimation and approximation see 
 in Section . , which is given by e Chapter . . The of this analysis are in Section . and Chapter . By doing so , an zero rate skeleton scheme , with concrete rate of convergence for , and 
 R pen , can be . From and , these imply specific performance for the induced joint and modeling scheme . To illustrate , we present three interesting below . 
 . . . Finite Mixture Classes 
 Let : with be the class of which are a convex combination of .., , i . e ., A . is 
 L totally bounded with e being O log e Chapter . . From the optimal sequence 
 is O , which the following finite rate performance bound Chapter . : 
 r 
 sup , o , 
 T n 
 universal non negative constant . The rate in per O log . 
 . . . Monotone in , d 
 the collection of with support on , , monotonically decreasing per and bounded by a constant . This class is known to be L totally bounded , and furthermore e Lemma . , with the only on . From being O n 
 is optimal please see in , with the following performance bound , 
  
 . 
 In this case , the rate in per O n . 
 . . . Moment Smooth Class in , 
 the class of defined on the bounded support , , continuous integer greater than zero and satisfying that : for a constant . This class is L totally bounded with e being O er Chapter . . From , the optimal sequence , where , o is O n and the rate in per O n . 
 Notably , the last two are fully non parametric , where e is a polynomial function of 
 e . non parametric of L totally bounded of , where e is even exponentially in e , are in . and . and its . 
 . . Classes with Finite Dimension 
 Looking at the distortion redundancy bound in , totally bounded the rate 
  
 convergence that could be with the skeleton estimator in Theorem is O see Section . and the estimation error bound in . In this section , more specific density 
  
 are studied to achieve this best rate O v for density estimation and distortion redundancy from . We follow the path by in , who of with a finite and dimension the so classes , . Let us first introduce some : 
 Definition . Let : be an indexed collection of . The class for such a collection is given by 
 , 
 where A is the set of with respect to , as defined in . 
 Theorem . Let us assume that 
 F is L totally bounded , 
 the class AT a finite dimension A in , and 
  
 the entropy of associated with the sequence en strictly sub linearly , i . e ., log N is o , then there is a zero rate density estimator scheme , : that 
  
 , o is O , 
 F 
  
 where is the skeleton estimator in with en . Furthermore , is also a zero rate strongly consistent density estimator where F 
  
 V , is O , almost surely . 
 The proof is in Section . . 
 From Definition , log Ne is inversely proportional to e . In fact , depending of , log Ne can go from being O log e , passing from being polynomial in e , to being O e see a number of in Chapter and its . Then the role of in the statement of Theorem is to bound how fast Ne should tend to infinity as e goes to zero , to guarantee a zero rate in the skeleton learning scheme . It is simple to show that Ne being O e e with , is sufficient to achieve that log N is o . This is a condition satisfied by a rich collection of L totally bounded classes in . Concrete are in Chapter . 
 The Parametric Scenario 
 The so far are of theoretical interest because they rely on the skeleton estimator that is from the skeleton covering of see Definition , which is unknown in practice . Moving towards making the zero rate skeleton learning scheme of practical interest , we revisit the important parametric scenario in which , the index set of , is a compact set in a finite dimensional space . Interestingly , in this context we can consider a practical covering by the uniform partition of the parameter space , as used in . Unlike , 
 where a minimum distance estimate is first found and then , here we first quantize the then find the minimum distance estimate among a finite collection of i . e ., over a number of in . Some will be . 
 Definition . Let : with . Let IF : be the index function to . IF is said to be locally uniformly , if there and , such , 
 , 
 V , , 
 where the ball of radius with respect to the norm in centered at . 
 The following lemma L totally bounded under some parametric . 
 Lemma . Let : with . bounded such that 
 bounded . Furthermore and the is O IF : for this family . is locally uniformly Definition , L totally 
 e 
 The proof is in Section . . 
 It is important to note that the e covering in the proof of Lemma to derive an upper bound for Ne is practical see Appendix . This the possibility of a practical skeleton estimator , which is the focus of the following result . 
 The Practical Skeleton Estimator 
 Under the of Lemma , let , e ,, e denote the learning rule of with the minimum distance principle in with parameter e see in Section . , where instead of the e skeleton Ge of in Definition , the see Appendix e covering the proof of Lemma is used . This practical e covering is by e by definition , Ne e O , this last part from Lemma .. With this , let , en , denote our practical learning scheme indexed by the precision en . We are in a position to integrate Theorem and Lemma to state the following : 
  
 Theorem . Under the of Lemma , the practical skeleton estimator en with en that 
 , en , o is O , and , 
 F 
 where , e , e , e . 
 In addition , if the collection a finite dimension equal to 
 J , then 
 , en , o is O , and . 
 F 
 The proof is in Section . . 
 When , that the finite dimension assumption of Theorem is satisfied by the class of mixture in Section . . and a rich collection of exponential of the form : with , where is a reference density , hi : i .., is a set of arbitrary real valued , is a normalization constant see in Section , a compact subset of see in Section . 
 Summary of the 
 We summarize the of the zero rate density estimation approach adopted for the problem of joint fixed rate source and modeling of continuous memoryless . 
 Proposition and Theorem formalize the interplay between the two stage joint fixed rate and modeling objective and the problem of zero rate uniformly consistent in total variation density estimation . 
 Theorem a necessary and sufficient condition on a family of for the existence of a strongly joint and modeling scheme both source and model identification Definition . The result is for the rich non parametric collection of L totally bounded . 
 For the modeling stage , we propose the skeleton estimator , which first the data and then the minimum distance decision on this finite set of density . This is a practical solution in the sense that the inference minimization is carried out over a finite set . 
 By combinatorial regularity on the family of : , 
  
 the skeleton scheme O rate of convergence in the order distortion redundancy , and the same rate in the total variational distance for the modeling part Theorem . • Finally , for a relevant parametric setting , a practical skeleton based joint is that a rate of for the order distortion redundancy Theorem . This rate is slightly better than the O in under the same rate overhead of O log . Furthermore , Theorem the finite dimension assumption over the class AT considered in Theorem . , while the same performance 
  
 in of order distortion redundancy O , uniform risk to learn the 
  
 density O , and rate overhead O log . 
 Concerning the last parametric result , we note that the result in can be by the adoption of Dudley entropy bound , which would yield the same asymptotic rate in this work for the order distortion redundancy . 
 A final remark is that under the bounded distortion metric assumption of Theorem condition i , Linder al . Theorem that , and for every such that , there is a constant such that 
 r 
 log , n 
 where is a sequence that to zero o uniformly in . This result a rate of convergence of the order operational distortion rate function to the as the block length to infinity . In view of , we can adopt this result in and , to say that the average distortion of the respective joint and modeling at rate , i . e ., ,, to the as O point wise . Therefore in the process of 
 , with the function , we lose the O rate of convergence . 
  
 This work the problem of fixed rate universal source and model identification with training data in from a learning perspective . Remarkably , we found that the problem is equivalent to the problem of density estimation of the source distribution with some concrete but non conventional operational data rate in per sample . This learning problem can be seen as the task of and the distribution of with a zero rate in per sample , while a consistent estimation in total of the distribution after the process . From our perspective , the rate constraint density estimation problem is interesting in itself and can have relevant in other such as distributed learning and sensor network . 
 Importantly for the joint and modeling problem , the connection with density estimation a context for the use of the skeleton estimator by in . We highlight two important from its use . First , we extend about universality from the parametric context in to the rich non parametric family of L totally bounded , . This result significantly the where the joint model and objective can be . We this with some in Section . and many more can be found in the literature of density estimation , . 
 Second , in the parametric case studied in , we were able to remove some of the and obtain not only the same performance result in of rate of convergence of the order distortion redundancy but also slightly better convergence . Therefore , the Skeleton estimator , though essentially a non parametric learning scheme , is shown to be instrumental in enriching the applicability of the joint and modeling framework . 
 of 
 . . Proposition 
 Proof . The fact that is uniformly consistent directly from Definition . On the other hand , the rate of is . From the definition of , it is simple to show from the strict of that in order for limn , i , it is that e for any e . Then , from , and since log , lim , that limn . 
 . . Theorem 
 Proof . The proof upon the in Theorem . ,. . Let us consider an arbitrary and let , : be the zero rate learning scheme of the assumption let us construct the joint and modeling rule of : 
 , : , :, 
 . 
 Concerning the first stage of ,: , it is induced directly from the of . 
 For the second stage , , the pair , is picked such that ,, which 
 , 
 is the optimal block code that from the hypothesis in , with , 
 , 
 short hand for the reproduction induced from the first stage pair and satisfying the rate constraint , i . e ., . From construction and the fact that zero rate , 
 R , 
 n 
 then ,: the rate condition . On the other hand , based on the assumption that is zero rate uniformly consistent , it that 
 , n 
 where . Then ,: the modeling objective . Concerning the objective , we use the following key result : 
 Lemma Lemma . . two probability in ,. Let , be a zero memory block coder with the nearest neighbor property i . e ., is nearest neighbor if ,, with the reproduction of .. If we denote the performance 
 of with respect toby 
 , , 
 where the product measure with , , and the condition i of Theorem and is bounded by , then 
 ,. 
 Furthermore , the inequality can be extended for the order operational in , i . e ., 
 ,, 
 . 
 Let us work with the following distortion redundancy , 
  
  
 , 
  
 , 
  
 n 
 For the first equality we use . The inequality in is from the definition in and , and the equality in is from the construction of which is operational optimal for the distribution at rate . Finally , is from . 
 Concluding , , is random a measurable function of and dominated by 
 V . Hence taking the value with respect to on both sides of this inequality 
 see , we have the uniform convergence in that 
 lim sup , i , 
 n F 
 and then the objective is . 
 . . Theorem 
 Proof . Let us first assume L totally bounded and prove the direct part of the statement . 
 We adopt the skeleton estimate by and extended by al . , 
 a complete presentation can be found in Chapter . For any arbitrary e , let us consider the 
 o e e skeleton Ge .., Ne of . We use ie i as short hand for the i th in 
 , and we define 
 Ge 
 .., Ne T 
 to represent the index set of Ge . Let us consider the class of Ge given by 
 n o 
 A 
 e , i : i Ne , 
 where , is the set of ie with respect to in , . 
 Hence , given i . i .. X .., with Xi , let us propose the pair , e e associated with Ae by , 
 , e min , 
 , 
 where is the standard empirical distribution . In this context , 
 , e , e min , 
 ie Tee 
 is the well known skeleton estimate . is the minimum distance approximation of with of Ge , , the measure in the right hand side of that is reminiscent of the total variational distance in . In order to choose a sequence en , we consider the following performance bound . 
 Lemma Theorem . . For any , 
 V . 
 e 
 Equation is valid for any e and , consequently , it a trade off between an approximation error term and an estimation error term . The approximation error is Ge 
 which is bounded by the definition of Ge . For the estimation error , on the other hand , the use of inequality to obtain that Theorem . , 
 . 
 in , it that This last expression is distribution free and it is valid if the approximation fidelity e is a chosen function of . Consequently , for any sequence en , 
 s 
 n o log 
 E , en , 
 for all . Hence , we consider en in 
 Chapter . , which is well defined and to zero infinity . Consequently from , limn . Then the learning scheme 
 the learning requirement in Definition , where in 
 O by construction . To conclude the argument of this part i . e ., the construction of the second stage of a joint modeling scheme , we adopt the result and the construction in the proof of Theorem see Remark for . This result that there is a strongly universal joint and modeling scheme rate . 
 For the other implication the converse part of the statement , let us fix and assume that we have a joint modeling scheme that is strongly universal Definition rate . Then from Proposition , we have a learning scheme , : such that limn and 
 n E . 
  
 For the learning rule of length , we have its reproduction that we denote by . Let us define the minimum distance oracle solution in by 
  
 From , we have that limn , . In other , e , there 
 N e , such that for all e , , e uniformly for every . This that e there e , such that for any arbitrary e , Be , where by 
 . totally bounded , which the proof . construction T 
 . . Theorem 
 Proof . From Lemma , for any arbitrary sequence en 
 V , en sup . 
 B 
 with the class of the skeleton Gen . It is clear that e , Ae AT . Then by 
 , for all e and for any distribution 
 . Here is where we use the assumption that AT finite dimension , which from Theorem . that 
 r 
 E n 
 for some constant . Substituting this result in , the argument by en , a solution which the intended rate of convergence for , o . Finally , the rate of the learning rule is e , which to 
  
 zero by the last hypothesis . 
 For the almost sure convergence part if e , it is sufficient to show that the second term in 
  
 the right hand side of is O almost surely . From the fact that AT finite dimension Definition A , and from the classical inequality Corollary . and Theorem . and Chapter . , it that 
 e , 
  
 and e . Then considering an and M , 
  
 K 
 P sup an n 
 B 
 for some , hence . Then from the 
 Lemma , e almost surely , which 
 n 
 the proof . As an is o , this result the almost sure to zero of , as 
 to infinity . 
 Finally , similar , it is possible to show that , is o almost 
 n 
 surely for any , . 
 . . Lemma 
 Proof . First note in a compact set , consequently , the finite covering property of a compact set , i . e ., e , there a finite covering such that , e 
 . i 
 On the other hand , from the locally uniformly assumption on IF :, there and such that , , , . Then , by considering , it by construction of that 
 K 
 , 
 i 
 i i 
 where :, is the ball centered at , induced from the total variational distance , and the last inequality from the condition . Hence , from , e 
 e o there e min e ,,..., e , such that i , which the result . 
 For the final part , let , be the uniform that characterize the condition of 
 from IF Definition Ne is upper bounded by . Without loss of generality , let us assume the critical regime e , which is the covering number of me hence , 
 R 
 we will work with a uniform partition of to find a bound for e . Let e me , then a product type partition , where in each we have uniform length , we have the e covering . The number of is O , which is O as a function of e e e . 
 To clarify the constructive nature of the e covering used to prove this result , an algorithm with the basic of the construction of this practical covering is in Appendix . 
 . . Theorem 
 Proof . Let e be the e covering induced from the uniform partition Lemma . 
 From this we can construct the minimum distance estimate in the class of e with index set e , i . e ., A e , which , from , 
 e min . 
 ie e A e 
  
 Considering en , from it that 
 n o t 
 . 
 n n 
 The latter upper bound is asymptotically dominated by from the fact that is O log Lemma , which the made in . 
 Concerning part , the in the proof of Theorem , we can obtain that e , 
 . 
  
 From this point , the proof from the of Theorem and the fact that is O log . 
 Author : Conceptualization ,.. Silva and .. ; Methodology ,.. Silva and .. ; 
 Formal Analysis ,.. Silva and .. ; Investigation and ,.. Silva and .. ; 
 Writing Original Draft Preparation ,.. Silva and .. ; Writing .. Silva ,.. ; Project Administration ,.. Silva ; Acquisition ,.. Silva . 
 : The work is by from and , Chile and the Advanced Center for Electrical and Electronic Engineering ACE , Basal Project . In addition ,.. Silva support from Project , Chile . 
 : We want to thank the anonymous for their constructive that were instrumental to improve the technical content and organization of this work . We thank for and proofreading all this material and for Figure . 
 of Interest : The declare no conflict of interest . 
 Appendix A . Proof of and 
 First , we show that the zero rate skeleton estimate en , en en : in and is also strongly consistent . 
 Proposition A . is strongly consistent , i . e ., for any , 
 , , almost surely . 
 Proof . Let us consider the skeleton estimate , where the sequence was chosen by the rule 
 n 
 e . all . From Lemma , 
 V . As by construction , we just need to concentrate on the estimation error term . inequality , 
 e e log e , A 
 where from the lemma , , the estimation error to zero almost surely . 
 Finally considering the inequality in , we have that , which the argument . 
 Appendix . Basic of and Theory 
 Let be a collection of measurable , and x .., be a sequence . Then we define by , the number of different in 
 x , x .., n : , 
 and the shatter coefficient , 
 sup A 
  
 The shatter coefficient is an indicator of the richness dichotomize a finite sequence of in the space , where by definition n . 
 Definition A . The first time in the index where is strictly less than n is the and 
 dimension of . finite dimension then it is a class ; otherwise if n , then the class is said to have an infinite dimension . 
 Appendix . Pseudo Algorithm to Implement the Practical e Covering in Lemma 
 Under the parametric of Lemma , we recognize four structural that characterize : the dimension of the space that , associated with the assumption that ,, and , the associated with the locally assumption of IF . Given these four ,,, and e , there is a constructive e covering in the proof of Lemma that can be in the following : 
 In each of the k , the interval , is partitioned uniformly with sub e 
 of length e m . This a scalar quantization of , with e per . 
 A product partition of , is made with the scalar of the previous step . 
 From the proof of Lemma , this is a e covering . Let us denote this set by i , i .., . 
 From the proof of Lemma , the covering the previous step an e covering the indexing function IF , i . e ., by IF i : i .., . 
  
 ﻿This paper derives novel results on the characterization of the the causal information ratedistortion function (IRDF) Rcit(D) for arbitrarily-distributed one-sided stationary ?-th order Markov source x . It is first shown that Gorbunov & Pinsker’s results on the stationarity of the realizations to the causal IRDF (stated for two-sided stationary sources) do not apply to the commonly used family of asymptotic average single-letter (AASL) distortion criteria. Moreover, we show that, in general, a reconstruction sequence cannot be both jointly stationary with a one-sided stationary source sequence and causally related to it. This implies that, in general, the causal IRDF for one-sided stationary sources cannot be realized by a stationary distribution. However, we prove that for an arbitrarily distributed one-sided stationary source and a large class of distortion criteria (including AASL), the search for   can be restricted to distributions which yield the output sequence y81 jointly stationary with the source after ? samples. Finally, we improve the definition of the stationary causal IRDF   previously introduced by Derpich and Østergaard for two-sided Markovian stationary sources and show that  for a two-sided source ...,x(-1),x(0),x(1),... equals Rcit(D) for the associated one-sided source x(1),x(2),.... This implies that, for the Gaussian quadratic case, the practical zero-delay encoder-decoder pairs proposed by Derpich & Østergaard for approaching  achieve an operational data rate which exceeds Rcit(D) by less than 1 + 0.5log2(2p e /12) ? 1.254 bits per sample. 
 The information RDF (IRDF) Rit(D) for a given one-sided random source process x can be defined as the infimum of the mutual information rate [1, Section 8.2] 
 between source and reconstruction y81 such that a given fidelity criterion does not exceed a distortion value D [1]–[3]. If one adds to this definition the restriction that the decoder output can only depend causally upon the source, one obtains what is known as the causal [4], [5], non-anticipative [6]–[8] or sequential IRDF [9]–[11]. All these are equivalent and will be denoted as Rcit(D), defined in terms 
 where the infimum is taken over all joint distributions of y81 given x81 such that the causality Markov chains (which will be referred to as the short causality constraint) 
 hold and which yield distortion not greater than D, for some fidelity criterion. Notice that, if one is given a two-sided random source process x8-8 = {...,x(-1),x(0),x(1),...} instead, and one is interested only in encoding and reconstructing the samples x81 , then the causality constraints may be stated as 
 as done in [5]–[7]. This notion of causality will be referred to as the long causality constraint. 
 The motivation for considering in this work one-sided instead of two-sided sequences (and thus (3) instead of (4)) arises from the aim of building encoder-decoder systems which operate with zero delay (the same motivation behind the causality constraint). To see this, notice that the causality constraint (4) for two-sided sources corresponds to the situation in which source samples in the infinite past exist and are available to the encoder. This may require an infinite delay before actually beginning to encode and decode. By contrast, the causality constraint (3) describes the case when the source is a one-sided process and yk1 depends only upon x 
 Remark 1. It is important to highlight at this point that even though the causality condition (3) can also be applied to a two-sided source process x , it would not ensure causality in that case. To see why, consider the situation in which x  is a binary i.i.d. source where each xk takes the values 1 or 0 with equal probability. Suppose y81 is built as y(k) = x-k ?x2k, where ? denotes the exclusive “OR” operator. It is easy to see that   satisfies (3), even though y81 depends non-causally on x81 . 
 The above observation reveals that if the source is two sided but only the samples x81 are encoded and the decoded process is one-sided (y81 ), then one needs to impose instead the (more general) 
 which implies (3). Besides causality, these Markov chains guarantee that even if the source is a two-sided process, its encoding and reconstruction proceeds as if it were a one-sided process. 
 Notice that (5) implies (3) and (4). For this reason, (5) will be referred to as the strong causality 
 As we shall see in sections III and IV, this situation, where at time k the encoder can take only xk1 as input, entails significant challenges due to the unavoidable need to deal with transient phenomena. 
 The operational significance of Rcit(D) stems from its relation to the causal operational RDF (ORDF), denoted as Rcop(D). The latter is defined as the infimum of the average data-rates which are achievable by a sequence of causal encoder-decoder functions [4], [5] yielding a distortion not greater than D. Characterizing Rcop(D) is important because every zero-delay source code (suitable for applications such as low-delay streaming [13] or networked control [14], [15]) must be causal. 
 An IRDF is said to be achievable if it equals the ORDF under the same constraints [2], [3]. As far as the authors are aware, the achievability of Rcit(D) has not been demonstrated yet, for any source and distortion measure, and thus the gap between Rcop(D) and Rcit(D) is unknown in general. However, it is known that [5, Section II] 
 and for Gaussian sources it is possible to construct causal codes with an operational data rate exceeding   by less than (approximately) 0.254 bits/sample (1.254 bits/sample for zero-delay codes), once the statistics which realize the latter are known [5]. This underlines the importance of studying the 
 To the best of the authors’ knowledge, no closed-form expressions are known for  , except when considering mean-squared-error (MSE) distortion and for Gaussian i.i.d. or Gaussian autoregressive (AR)-1 sources, either scalar [5, Section IV] or vector valued [16] . However, there exist various structural properties of the causal IRDF that have been found in literature when  admits (or is assumed to admit) a stationary realization. 
 Indeed, the stationarity of the realizations of the causal IRDF has played a crucial role in simplifying the computation of   for Gaussian 1-st order Markovian sources and MSE distortion in [17]. It has also been a key implicit assumption in [10], and an explicit assumption in works such as [8] and [5]. In particular, for a stationary two-sided random source x , Definition 6] introduced the stationary causal IRDF 
 where the infimum is taken over all distributions of y81 given x81 which yield a one-sided reconstruction processes y81 jointly stationary with x81 , satisfying (4) and an asymptotic average MSE distortion constraint on  . For the case of a Gaussian source, it was shown in [5] that an operational data-rate exceeding   by less than 1 + 0.5log2(2p e) ? 1.254 bits/sample was 
 achievable using a entropy-coded subtractively dithered uniform quantizer (ECSDUQ) surrounded by linear time-invariant (LTI) filters operating in steady state. These examples illustrate the relevance of determining whether (or in which cases) the causal IRDF admits a stationary realization. 
 To the best of our knowledge, the only work which has given an answer to this question in a general framework is [6]. Under a set of assumptions (discussed in Section II below), it is shown in [6, Theorem 4] that the search for the causal IRDF for a large class of two-sided sources and distortion criteria can be restricted to reconstructions which are jointly stationary with the source. Unfortunately, as we show in Section II-B, the assumptions on the fidelity criteria utilized in [6] leave out some common distortions (such as the family of asymptotic average single-letter fidelity criteria), and the statement of [6, Theorem 4] contains an assumption whose validity has to be proved. More importantly, the entire analysis of [6] is built for two-sided processes (using the causality constraint (4)), which opens the question of whether its results could apply to one-sided processes as well, with the causality constraint (3). 
 In this paper we give an answer to these questions and use the results to prove some novel properties of the causal IRDF associated with the stationarity of its realizations. Specifically, our main contributions are the following: 
 We show in Theorem 2 that if a pair of one-sided random processes x is jointly stationary, with the latter depending causally on the former according to (5) (but otherwise arbitrarily distributed), then it must also satisfy the Markov chains 
 which is a fairly restrictive condition. In particular, as we show in Theorem 3, if x  are jointly Gaussian and y81 depends causally upon x81 , then joint stationarity implies x81 is an i.i.d. or 1st-order Markovian process. This stands in stark contrast with what was shown in [6] for two-sided stationary processes and constitutes a counterexample of what is stated in [18, Theorem III.6]. 
 Despite the above, we show in Theorem 4 that for any ?-th order Markovian one-sided stationary source x81 and a large class of distortion constraints, the search for the causal IRDF (as defined in (2)) can be restricted to output sequences causally related to the source and jointly stationary with it after ? samples, and such that  . We refer 
 to such pairs of processes as being ?-quasi-jointly stationary (?-QJS) (this notion is formally introduced in Definition 2 below). A consequence of this result is that for any ?-th order twosided Markovian stationary source x  equals  for the corresponding one-sided 
 stationary source x81 . The relevance of this finding is that for Gaussian stationary sources and asymptotic MSE distortion, an operational data rate exceeding  (and thus ) by less than approximately 0.254 bits/sample, when operating causally, and 1.254 bit/sample, in zero-delay operation, is achievable by using a scalar ECSDUQ as in [5]. 
 The remainder of this paper begins with Section II, in which the assumptions leading to [6, Theorem 4] are revisited and the limitations of that theorem are discussed. In Section III we prove that, in general, it is not possible to have two one-sided processes which are jointly stationary and, at the same time, satisfy the causality constraint (3). Section IV presents our main theorem (Theorem 4), which shows that the search for the causal IRDF for one-sided ?-th order Markovian stationary sources can be restricted to ?-QJS processes. Finally, Section V draws the main conclusions of this work. All proofs are presented in section VI (the Appendix), which also contains some technical lemmas required by these proofs. 
 Notation: R denotes the real numbers, Z denotes the integers, N = Z+ is the set of natural numbers (positive integers),  and N0 , {0,1,...}. For every x ? R, the ceiling operator ?x? yields the smallest integer not less than x. We use non-italic letters for scalar random 
 For a random (one-sided) process x81 we will sometimes use the short-hand notation x wherever this meaning is clear from the context. When convenient, we write a random sequence  , as the column vector ykj , [y(j) y(j +1) ··· y(k)]T (the indices j and k are swapped so that the smallest index goes above the largest one, thus mimicking the usual index order in a column vector). The entry on the f-th row and k-th column of a matrix M is denoted as [M]f,k, with [M]jk being the sub-matrix of M containing its rows j to k, j = k. 
 For a random element x in a given alphabet (set) X, we write B(X) to denote a sigma-algebra associated with X and Px : B(X) ? [0,1] to denote its probability distribution (or probability measure). We write x ~ y to describe the fact that y has the same probability distribution as x, and x ?? y to state that x and y are independent. We write the condition in which two random elements a,b are independent given a third random element c using the Markov chain notation a ?? c ?? b. If W is a set of probability distributions, then (W) denotes the set of all random elements whose probability distribution belongs to W. The expectation operator is denoted as E[·]. We write Xk as a shorthand for  . The mutual information between two random elements x ? X y ? Y is defined as [1, Lemma 7.14] 
 where the supremum is over all quantizers q and r of X and Y, and Pq(x),r(y), Pq(x) and Pr(y), are the joint and marginal distributions of q(x) and r(y), respectively. If x,y have joint and marginal probability density functions (PDFs) fx,y, fx and fy, respectively, then [3] 
 The conditional mutual information I(a;b|c) is defined via the chain-rule (cr) of mutual information I(a;b|c) , I(a;b,c) - I(a;c). The mutual information rate   between two processes 
 x81	and y81	is defined as in (1). The variance of a real-valued random variable x is denoted as 
 . The auto-correlation function of a random process x81 is denoted ?x(t,k) , E[x(k)x(k + t)], k = 1, t > -k. 
 The following properties of the mutual information involving any random elements a,b,c will be utilized and referred to throughout this work: 
 Fact 1. Let a,b,c be three random elements with an arbitrary joint distribution. Then, there exists a random element ¯a (equivalently, a joint distribution P¯a,b,c) such that 
 In order to assess whether (or to what extent) [6, Theorem 4] could provide support to the stationarity assumptions made in, e.g. [8], [10], [18], [19], it is necessary to take a closer look at the assumptions made in [6] and the statement of its Theorem 4. For that purpose, the first part of this section is an exposition of the definitions and assumptions leading to [6, Theorem 4].2 The second part is an analysis which reveals the limitations of [6, Theorem 4] and its inapplicability to the case in which the source and reconstruction are one-sided processes. At the same time, this section also introduces definitions and part of the notation to be utilized in the remainder of this paper (for convenience, a summary of these is presented in Table I below). 
 Throughout [6], the search in the infimizations associated with various types of “nonanticipatory” (i.e., causal) rate-distortion functions is stated over sets of joint probability distributions between source and reconstruction (as opposed to the usual definitions, in which the search is over conditional distributions, see (2) and [3, Chapter 10], [2]). Since the distribution of the source is given, it is required that for every k2 > k1 ? Z, all the joint distributions Pxkk21,ykk21 to be considered yield x  having the same (given) distribution of the source for the corresponding block, say P°xkk21. This requirement can be formalized as requiring that  , for a set of admissible joint 
 where  and  are, respectively, the alphabets to which xkk21 and ykk21 belong. In [6], this admissibility requirement is embedded in the definition of the sets of distributions which meet the distortion constraint, described next. 
 The fidelity criterion for every pair of integers3 k1 = k2 is expressed in [6] as requiring  to belong to a non-empty set of distributions (hereafter referred to as distortion-feasible set)  , a condition written as  . In this definition, the number D = 0 represents an 
 admissible distortion level. Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation. 
 As mentioned above, the admissibility requirement   is expressed in the distortionfeasible sets in [6, eqn. (2.1)]. The latter equation can be written as 
 We believe this re-exposition of [6] to be valuable in itself since on the one hand, it selects the minimal set of notions required to formulate and understand its Theorem 4, and on the other hand, it provides an arguably clearer presentation than the one found in [6] (an English translation from Russian), which is not easy to read due to its notation, some mathematical typos and the low resolution of its available digitized form. 3 
 The analysis in [6] considered both discrete- and continuous-time processes, but here we only refer to the discrete-time scenario. 
 In [6, eqs. (2.4) and (2.5)], the distortion-feasible sets are assumed to satisfy the “concatenation” condition 
 With this, [6, eqn. (2.9)] defined the “nonanticipatory epsilon entropy” of the set of distributions4   as 
 where the infimum is taken over all pairs of random sequences   such that the causality Markov chains 
 are satisfied. Then [6, eq. (2.13)] defines the “nonanticipatory message generation rate” as 
 (when the limit exists). An alternative “nonanticipatory message generation rate” is also considered in [6] by defining the set of distortion-admissible process distributions WD as follows: 
 Definition 1. The set (WD) consists of all two-sided random process pairs (x8-8,y8-8) ? (WD) for which there exist integers ··· < k-1 < k0 < k1 < ··· such that limi?±8 ki = ±8 and 
 (when the limit exists), where the infimum is taken over all pairs of processes  satisfying the causality Markov chains 
 Notice that these Markov chains imply (4) and differ from the latter in that here the reconstruction y8-8 is a two-sided random process. 
 Now assume that Xi = X and Yi = Y, for all i ? Z, for some alphabets X and Y. Define, for any given non-negative sequence  such that  , the distribution 
 Px¯kk21,y¯kk12(E) = X a(s)Pxkk21++ss,ykk21++ss(E),	k1 = k2,E ? B(X(k2-k1+1) × Y(k2-k1+1)).	(21) 
 The actual term employed in [6] is “nonanticipatory epsilon entropy of the message” where the term “message” refers to the random ensembles in  . 
 Then, the analysis of the lower bound in (19) can be confined to jointly stationary pairs of random 
 For convenience, Table I presents a summary of the definitions and notation described so far, together with some which will be defined in the following sections. 
 We now discuss three limitations of Theorem 1 which are relevant when trying to establish whether the causal IRDF of a one-sided stationary source admits a stationary realization. 
 Limitation 1: The first obvious limitation is that even if source and reconstruction are twosided processes, every distortion criterion which considers only their “positive-time” part cannot be expressed by a distortion-feasible set WD given by Definition 1 if the sets {WDl,j}l=j?Z satisfy condition ii) in Theorem 1. To see this, notice that if l = j < 1, then such distortion criterion (which neglects non-positive times) would require WDl,j to admit all joint probability distributions satisfying (13). Combining this with condition ii) in Theorem 1 yields that every set WDn,m = WDl,j with m - n = j - l, which amounts to imposing no restriction on the distortion at all. 
 It is natural to think that such elemental shortcoming could be avoided by simply replacing condition ii) in Theorem 1 by a one-sided version of the form: 
 For every t1 ? Z, s,t2 ? N such that t1 = t2: WDt1+s,t2+s and WDt1,t2 are identical sets.	(23) 
 Leaving aside the fact that this alternative condition is not sufficient for Theorem 1 to hold, it is worth pointing out that using (23), the commonly utilized family of asymptotic single-letter fidelity 
 Distortion-feasible set. The set of all joint distributions   which satisfy a given constraint given by D ?R (see comments before ( 
 PD8	Generic distortion-feasible set of probability distributions for pairs of one-sided processes 
 Q?, ? = 1,2,...	The set of all joint distributions   of pairs of one-sided random processes 
 (Cn), n = 1,2,... and	The sets of causally related one-sided pairs of n-sequences (see Definition 3). 
 (C8)	The set of one-sided pairs of processes causally related according constraint (3) (see Definition 3).	to the short causality 
 The set of causal distributions for processes of the form  the long causality constraint (4) (see Definition 4).	. Such processes satisfy 
 criteria [2] can not be expressed by a distortion-feasible set WD given by Definition 1, as the following lemma shows (its proof can be found in Appendix VI-A). 
 Lemma 1. Let ? be any given distortion functional which takes as argument a joint distribution Px,y and yields a non-negative real value. Let (AD) be the set of all pairs of processes  where x  is stationary, with pair-wise distributions {Px(k),y(k)}k?Z which satisfy the asymptotic 
 (24) Then, there doesn’t exist an infinite collection of distortion-feasible sets {WDk1,k2}k1=k2?Z satisfy- 
 ing (23) such that the associated WD given by Definition 1 satisfies (WD) = (AD).	N 
 Limitation 2: The second limitation associated with Theorem 1 is that its application requires 
 one to prove its condition iv), i.e., the unproven supposition that HD0 = HD0 , holds. The only work we are aware of which builds upon Theorem 1 is [18], and, accordingly, [18] provides [18, Theorem III.5], which states that a similar equality holds. Unfortunately, as shown in [20], the proof of [18, Theorem III.5] is flawed. 
 We note that Lemma 3 in Section IV-A below provides two alternative sufficient conditions for an equality similar to   (but for one-sided processes) to hold. 
 Limitation 3: The third limitation of Theorem 1 for its applicability to one-sided sources is the fact that the entire framework built in [6] is stated for two-sided processes (and, crucially, for the corresponding causality restriction given by Markov chain (20)). This difference cannot be simply neglected while expecting Theorem 1 to remain valid. Indeed, as we show in the next section (Theorem 2), a pair of random processes   can be jointly stationary and at the same time satisfy the causality Markov chain (3) only if y(k) is independent of x  when x(k) is given. Moreover, we prove that joint stationarity and causality are incompatible when the source is a ?-th order Markovian Gaussian one-sided process with ? > 1. 
 In this section we address the question of whether there exists a one-sided reconstruction process y81 jointly stationary with a source x81 and which also satisfies the causality constraint (3). 
 Each source random sample x(i) belongs to some given set (source alphabet) X and is allowed to have an arbitrary distribution. Recall that a random process y , where Y is the reconstruction alphabet and Y8 , Y × Y ···, is said to be jointly stationary with x81 if and only if, for every l ? N, the distribution of   does not depend on k, for k = 1,2,.... 
 The next theorem shows that, for such one-sided processes, joint-stationarity and causality may hold together only if y(k) is independent of xk1-1 when x(k) is given. 
 Theorem 2. If x81 and y81 are jointly stationary and y81 is causally related to x81 according to (3), then 
 Proof. If (25) does not hold for some k and if y81	and x81	are jointly stationary, then	N 
 does not hold, which corresponds to not satisfying (3) for k = 1, completing the proof. 
 To illustrate how restrictive condition (25) is, the next theorem shows that, for a Gaussian ?-th order Markovian stationary source x81 , causality and joint stationarity is possible only if x81 is i.i.d. (? = 0) or ? = 1. Recall that a random (vector or scalar valued) process x -th order Markovian if ? is the smallest non-negative integer such that 
 Theorem 3. Suppose x81	is a zero-mean Gaussian stationary process, and assume that, for some 
 Proof. Since xN1 and yN1 are jointly Gaussian and the latter depends causally upon the former, it holds that 
 for some lower triangular matrix A ? RN×N having entries ai,j , [A]i,j, i,j ? {1,...,N}. On the other hand, the fact that xN1 and yN1 are jointly stationary implies that KyN1 x1N and Kx1N are Toeplitz matrices. From (28), considering the entries on the first and second rows of KyN1 x1N and defining 
 a1,1[?0 ···?N-2] = a2,1[?1 ···?N-1] + a2,2[?0 ···?N-2] ??  a1,1 - a2,2 [?0 ···?N-2] = [?1 ···?N-1]. a2,1 | ,{z?	} 
 Therefore, ?k = ??k-1,k = 1,...,N -1, which for a Gaussian stationary sequence xN1 implies that 
 E . For Gaussian random variables the latter is equivalent to the Markov chains x , which defines a 1-st order Markovian process (if ? 6= 0) or an i.i.d. process (if ? = 0). This completes the proof. 
 In the next section we will see that if x81 is ?-th order Markovian, then it is possible to build a pair   causally related according to (3) such that (x8? ,y8? ) is stationary. Moreover, we will show in Theorem 4 below that the minimization associated with the causal IRDF can be restricted to such pairs. 
 In this section we show that for any ?-th order Markovian one-sided stationary source x81 the search for the causal IRDF (as defined in (2) and for a large class of distortion criteria) can be restricted to output sequences y81 causally related to the source, jointly stationary with it after ? samples, and such 
 Definition 2 (Set of quasi-jointly stationary process). The set of ?-QJS distributions Q? is composed of all joint distributions Px81 ,y81 of pairs of one-sided random processes   which satisfy 
 Notice that Q1 corresponds to the set of joint distributions associated with all jointly stationary one-sided process pairs. 
 One can define a distortion-feasible set for pairs of one-sided processes  , say (PD8), from the finite-length distortion-feasible sets {WDk,l}k=l?N, in more than one manner. A minimal condition we shall require for such definition is the following. 
 Assumption 1. The distortion-feasible set of distributions for pairs of one-sided processes PD8 satisfies the following: 
 If  , then x˜81	has the given probability distribution of the source process, say 
 If   is any given pair of one-sided processes, and there exists an infinite collection of increasing integers 1 = k1 < k2 < ··· such that, for all , 
 For any pair of sequences  , and if  , then the concatenated processes x¨ ,x˜(1),x˜(2), ,y˜(1),y˜(2),···} 
 Notice that if PD8 satisfies this assumption and if the integers ki in Definition 1 were restricted to be positive, then we would have WD ? PD8 (see Definition 1). However, the one-way implications in Assumption 1 allow PD8 to be larger than WD. 
 Definition 3 (Set of Causal Distributions). Define (Cn) as the set of all one-sided random n-sequences   which satisfy the causality constraint 
 The set of causally related one-sided process pairs (C8) is defined likewise but for one-sided processes 
 provided the limits exist. The “liminf” causal IRDF Rˆcit(D) coincides with  if in (17) one fixes k1 = 1 and lets k2 ? 8. By contrast, Rcit(D) differs from   in that the latter is associated with the (less general) distortion-feasible set WD (see Definition 1). 
 Since our main result will be stated with the assumption that Rcit(D) = Rˆcit(D), we develop next two sufficient conditions for such equality to hold. 
 We begin by stating a useful construction of a pair of processes from a finite-length sequence and some if the properties of the former. 
 Proposition 1. Let  , be given, with x?81 stationary and such that   satisfies the causality condition (3) for k = 1,2,... ,n. Build the processes   as follows: 
 For every l ? N0, choose the conditional distribution of y˜nln(l+1+1) given x˜  as 
 The second equality in (32b) together with the fact that x˜81 is stationary imply that   , and thus (33) follows. On the other hand, we have that 
 where the first inequality holds due to Proposition 3, in the Appendix (applying it successively to the 
 The fact that  , the definition of   and the stationarity condition (35) imply that  , for all l ? N0. The latter together with Assumption 1 
 On the other hand, (37) together with the fact that   satisfies (3) for k = 1,2,... ,n, 
 Invoking Proposition 4 with a,b,c,d according to the labeling in (40) and in (41), we readily obtain that (b,d) ?? c ?? a, implying 
 We now state a technical lemma which is akin to [6, Theorem 2] but for one-sided processes, the proof of which can be found in Appendix VI-A. 
 Lemma 2. Let  be a stationary one-sided source and suppose the distortion-feasible sets {WDk,l}k=l?N and PD8 satisfy Assumption 1 and the stationarity condition 
 Next, we propose a possible definition of PD8 general enough to encompass the asymptotic singleletter fidelity criteria described by (24). For that purpose, we need to define 
 Notice that with such construction, PD8 does not necessarily satisfy Assumption 1. Also, the distortion-feasible sets {WDk,l}k=l?N with the specific form given by (46) do not necessarily satisfy the stationarity condition (43). 
 This definition, based on the limit of a sequence of distortion functions, is clearly capable of representing the general asymptotic single-letter criteria of (24) while satisfying Assumption 1. Recall that, as shown in Lemma 1, it is not possible to do this with the distortion-feasible set WD from [6], given by Definition 1. In addition, the construction of PD8 provided by (47) allows for several specific criteria commonly found in the literature, such as the one utilized in [5] and in the definition of a rate-distortion achievable pair in [3, p. 306]. 
 We are now in the position to provide two independent conditions that are sufficient to ensure Rcit(D) = Rˆcit(D) (the proof is given in Appendix VI-A). 
 Lemma 3. Consider the same conditions given in the statement of Lemma 2. If, in addition, any of the two following conditions holds 
 With the above notions, we can state the main result of this section, akin to Theorem 1 but for one-sided processes and for the corresponding causality condition given by (3) (the proof is presented in Appendix VI-A): 
 Theorem 4. Let the source°x81 be a one-sided stationary ?-th order Markovian process and suppose that Rˆit(D) = Rcit(D), where Rcit(D) and Rˆit(D) are as defined in (30) and (31), respectively. Furthermore, suppose that the distortion-feasible sets PD8, {WDk,l}k=l?Z satisfy Assumption 1 and 1) The “shift-invariance” condition: 
 Then the minimization in the definition of Rcit(D) in (30) can be restricted to pairs of processes 
 (x81 ,y81 ) with distributions in Q? n PD8 n C8 which, in addition, satisfy (x?8 ,y?8 ) ? (PD8). N 
 Putting aside the obvious difference between Theorem 4 and Theorem 1 arising from the fact that the former considers as sources one-sided processes and the latter two-sided processes, it is worth drawing a parallel between these two theorems. The requirement of Assumption 1 in Theorem 4 is weaker than the requirement of WD to conform to Definition 1 in Theorem 1. Thus, Theorem 4 holds for a larger class of fidelity criteria. The assumption that   can be seen as the 
 equivalent of condition iv) in Theorem 1 translated to the setting of one-sided sequences. The same is true with condition 2) in Theorem 4 with respect to condition ii) in Theorem 1. However, no conditions are stated in [6] which suffice for condition iv) in Theorem 1 to hold. In contrast, we have provided Lemma 3, which gives two independent conditions under which   is satisfied. The other assumptions in Theorem 4 differ from those in Theorem 1. Condition 1) in Theorem 4 is weaker than condition v) in Theorem 1. Condition 3) in Theorem 4 is absent in Theorem 1, and is required in our proof as a consequence of the transient behavior arising from treating one-sided processes (see Theorem 2 in Section III). 
 Remark 2. Among the distortion criteria which satisfy the conditions of Theorem 4, we find the family of asymptotic single-letter constraints of (24) (by letting WDk,l = {Px?lk,y?lk : Px?kl = Pxlk,(l-k+ 
 a distortion-feasible set WD conforming to Definition 1, and hence it is not covered by Theorem 1. 
 As pointed out by Remark 1 in the Introduction, if now one supposes that °x81 is the positive-time part of a two-sided stationary process °x , then the fact that   does not guarantee 
 that y81 depends causally on °x81 . For the latter to hold in this situation, it is required that  satisfies (5). This implies that for this case, the definition of the causal IRDF as stated in (30) needs to be extended to 
 Notice that when the source lacks a negative-time part, (5) simplifies to (3), and thus (strong)(D) becomes equal to . 
 The above observations raise the question of whether Theorem 4 can be extended for the case in which the one-sided source is the positive-time part of a two-sided stationary process. This implies considering (strong)(D) instead of , or, equivalently, the strong causality constraint (5) instead of the short causality constraint (3). 
 It turns out that Theorem 4 can indeed be extended for this situation, thanks to the following proposition, the proof of which can be found in Section VI-A. 
 Proposition 2. Suppose that x81	is the positive-time part of a two-sided process x  and that 
 Then there exists (or, equivalently, one can construct) a one-sided random process y¯81	such that 
 Theorem 5 (Extension of Theorem (4)). Let the source °x81 be the positive-time part of a two-sided stationary ?-th order Markovian process°x . Under the same assumptions made in the statement of Theorem 4, the infimization in the definition of   can be restricted to pairs of processes 
 Proof. The only difference between the RHS of (30) and (51) is that they consider the causality constraints (3) and (5), respectively. First, notice that 
 and thus Rcit(strong)(D) = Rcit(D). On the other hand, from Theorem 4, the infimization yielding Rcit(D) can be carried out considering only pairs of processes   satisfying (3) and 
 (PD8 n Q?), (x8? ,y8? ) ? (PD8). But as a consequence of Proposition 2, for every such (x81 ,y81 ), there exists a process y¯81 such that   satisfies (5) and 
 This implies that the minimization associated with  (strong)(D) can be restricted to pairs  satisfying (5), (56) and (57) (proving the first claim of the theorem) and that (strong) . 
 The latter and the reverse inequality confirms that Rcit(strong) , concluding the proof. 
 The purpose of this section is to establish a correspondence between  (introduced in [5]). As we discuss next, drawing an appropriate comparison between these two causal IRDFs requires two modifications to the definition of  already described on page 4. 
 The first modification consists of extending  to account for arbitrary fidelity criteria embodied in an arbitrary distortion-feasible set PD8 ? P1,8. 
 The second modification is necessary in order to make   a tighter lower bound to the corresponding infimal operational data rate. To see why, it is necessary to recall how  lower bounds the operational data rate of encoding x81 and decoding it as y81 . For this purpose, let b(k) be the random binary sequence produced by the encoder from time 1 to k and let |b(k)| be the length of b(k) (in bits). Since the code must be uniquely decodable , the bit-string b(k) satisfies the Kraft inequality [3, § 5.1]. In general, b(k) can be generated by using not only xk1 but also x0-8, and thus 
 where (a) follows from [3, Theorem 5.3.1] and (b) is a consequence of the data-processing inequality [3, Theorem 2.8.1]. Thus,  lower bounds the operational data rate k-1 E[|b(k)|] as tightly 
 which is precisely the causality constraint for one-sided sources (3). But, as we have shown in theorems 2 and 3, such causality constraint is, in general, incompatible with the joint stationarity of 
 . As a consequence, since such joint-stationarity is required by , Markov chain (60) cannot hold. This means that when the causality constraint for a two-sided source established by (4) is 
 Following these observations, we propose here the following modified definition of . 
 Definition 4 (An Improved and More General Definition of  . For any given x  two-sided 
 stationary source, redefine the causal stationary IRDF introduced in [5, Definition 6] as 
 where (Q1) is the set of all pairs one-sided jointly stationary random processes, and (C-88) is the set of all pairs of random processes   which satisfy the causality constraint (4) . 
 We can now state the following corollary of Theorem 4, the proof of which can be found in Appendix VI-A: 
 One important consequence of this result stems from the fact that, for a ?-th order Gauss Markov stationary source and quadratic distortion,   can be found by solving a convex optimization problem over frequency-response magnitudes of linear-time invariant filters around an additive whiteGaussian noise (AWGN) channel [5, lemmas 3 and 5]. 
 The operational relevance of Corollary 1 is that when the latter AWGN channel is replaced by an entropy-coded subtractively-dithered scalar quantizer, one obtains a source-coding scheme whose operational rate exceeds  by at most 0.254 bits/sample when operating causally and by at most 1.254 bits/sample when operating with zero delay [5, section VI]. Thanks to Corollary 1, it turns out that the operational data rate of such scheme lies within the same bounds with respect to 
 We have shown that, in general, the causal information rate-distortion function (IRDF)  for one-sided stationary sources cannot be realized by a reconstruction which is jointly-stationary with the source. Nevertheless, if the source is ?-th order Markovian, then the search for the causal IRDF can be restricted to reconstructions which are jointly stationary with the source from the ?-th sample. This led us to prove that  actually coincides with  for a large class of distortion criteria. This reveals that for Gauss-Markov sources and quadratic distortion,  can be found by solving the convex optimization problem derived in [5]. It also implies that for the same source and distortion, a zero-delay average data rate exceeding   by not more than (approximately) 1.245 bits/sample is achievable with the scheme proposed in [5]. 
 Proof of Lemma 1. We will resort to a contradiction argument, and thus start by supposing that there exists (WD) = (AD). 
 Since ? is non-negative, there must exist a pair of random processes   and a value D = 0 
 From the definition of (AD), any other pair of processes   which distributes exactly as 
 everywhere except on a single positive index, say l ? N, in which Px?(l),y?(l) ? P1,1 and 
 Definition 1, there exists a pair of integers l1,l2 with l2 ? N such that l1 = l = l2 and 
 This, together with the fact that the sets {WDt,s}t=s?Z satisfy (23), implies that 
 where L , l2 -l1 +1. Hence, any pair of random processes   with pair-wise distributions given by 
 together with the collection of integers   satisfy the conditions of Definition 1, and thus  . However, 
 meaning that  . This contradicts the initial supposition that (WD) = (AD), completing the proof. 
 By the definition of inf, we have that ?o2 > 0,n = No1, there exists   such that 
 and build the processes   as in Proposition 1. The mutual information rate between x˜kn1	and 
 Since this inequality is satisfied for all o1,o2 > 0, it follows that , completing the 
 Proof of Lemma 3. Since the conditions of Lemma 2 are satisfied, we have that  . Therefore, it suffices to show that . 
 (WD1,n) for all n = N. Since all the latter holds for all n = max{No2,N}, we obtain 
 Since this inequality holds for all o1,o2 > 0, it follows that  , completing the first part of the proof. 
 We shall now prove that Assumption 2 and the continuity of   implies . The continuity assumption on   means that 
 satisfies limd?0 od = 0, for all D > 0. By the definition of inf, we have that, for every d > 0,o1 > 0, there exists a pair of processes   such that 
 Also, since   and from the definition of PD8 in (47), it follows that there exists a 
 Since this inequality holds for all d,o1,o2 > 0, and recalling that od ? 0 when d ? 0, it follows that Rˆit(D) = Rit(D), completing the proof. 
 Proof of Theorem 4. Since , it follows that for all o1 > 0, there exists a finite No1 such that 
 Thus, for all o2 > 0 and for all n = No1 there exists a pair of sequences  such that 
 The fact that x?n1 distributes as   allows one to define the stationary extension of x?n1 such that 
 Starting from   we build the processes   as in Proposition 1. From this construction 
 and the stationarity assumption on the distortion-feasible sets given by (43), we have that 
 Now suppose that t is a random variable uniformly distributed over {0,1,... ,n-1} and independent of x˜81 . Let n = ? and define the pair of processes 
 8A similar construction, for two-sided processes, was proposed in the proof of [6, Theorem 4] (seemingly for the first time), building upon [21]. The same idea was rediscovered in the proof of [22, Theorem 3.2]. Here we adapt it for the case of one-sided processes. 
 are jointly stationary. Thus x¯ -th order Markovian stationary. These facts imply, in view of theorems 2 and 3, that the pair   may not be causally related according to (3). However, since   satisfies (36) (see Proposition 1), we have that   does satisfy the 
 x¨ ,xˆ(2),... ,xˆ(? - 1),x¯(1),x¯(2),...}	(100a) y¨ ,yˆ(2),... ,yˆ(? - 1),y¯(1),y¯(2),...}	(100b) 
 The pair   further exhibits two important properties. First, Lemma 4 shows that the pairs of processes   are ?-QJS (i.e.  ). Second, as we show in Lemma 5, the mutual information rate of the pair processes   lower bounds , for all n ? N. Thus, for every n = max{No1,?}, 
 is guaranteed for every o1 > 0 and o2 > 0, it readily follows that the search for 
 the infimum on the RHS of (30) can be confined to such pairs, completing the proof. 
 Lemma 4. Let   be the concatenated processes defined in (100), which are built from 
 (see (95)) and   (see (98) and (99) and the text between these equations) and satisfy (102) and (101). Then  . N 
 Proof. By construction,   are jointly stationary. Thus, all that remains to prove is that   (see Definition 2). For this purpose, notice that for all i > ?, 
 where all the equalities labeled “(cr)” stem from the chain-rule of mutual information, (a) holds because I(a;b) = 0 , and (b) is a consequence of the fact that x¨ , and 
 Markov chain (99). The mutual information in the middle of (108) can be upper bounded as 
 where the equalities labeled (cr) are due to the chain rule of mutual information, (a) follows because mutual information is non-negative, and (b) holds because x¯  t implies x¨81 ?? t. 
 Using the definition of x¨81 ,y¨81	(see (100)) we have that, for the case i = n + ?, 
 are introduced so as to streamline the presentation of the following steps. The relations between all these variables is illustrated in Fig. 1. Notice that for these sequences the Markov chains (37) translate into 
 where (a) follows because mutual information is non-negative and (b) is due to (33) and the fact that 
 Figure 1. Schematic representation of the change of variables introduced in (115). Each dot represents one element in the sequences x˜81 and y˜81 , with time increasing from left to right. 
 Lemma 5. Let   the concatenated processes defined in (100), built from  and   and (99) and the text between these equations). Then 
 where (a) is due to the non-negativity of mutual information and (b) holds because t . But 
 (see (32)). Substituting this into (134) we obtain that for every n ? N and m ? N, 
 The proof is completed by taking the limit as m ? 8 and substituting (130) in it. 
 Proof of Proposition 2. From Fact 1, there exists y¯81	such that (53) holds and which satisfies 
 Applying Proposition 4 with a,b,c,d corresponding to the labels placed under the terms in (140) and (141), we obtain directly (54), completing the proof. 
 Proof of Corollary 1. We will first show that  and then that the reverse inequality is true as well. 
 can be confined to pairs of processes   which satisfy (5) and such that  (PD8)) and  . For each such pair, one can construct the pair of processes 
 where the last equality stems from the fact that  and recalling that   and the definition of (Q?) implies . In addition, the 
 Therefore, for every pair of processes which satisfies the constraints associated with , there exists another pair which satisfies the constraints in the definition of   and yields the same information rate. This proves that . 
 In order to show that , consider any pair   satisfying (4) and such that  . Construct the pair of processes   as 
 (the existence of such pair is guaranteed by the “first-samples condition” in the statement of Theorem 4). From Assumption 1, this construction yields 
 Applying Proposition 4 to (153) and (154) with variables in the proposition assigned according to the labels under (153) and (154), we obtain that 
 Proof of Proposition 3. The mutual information between a1,a2 and b1,b2 is given by	(165) 
 where all equalities labeled (cr) stem from the chain rule of mutual information. 
 Since mutual information is non-negative, it follows that I(a;b,d|c) = 0 if and only if both I(a;d|c) and I(a;b|c,d) are zero. The proof is completed by noting that the statement I(u;w|z) = 0 is equivalent to the Markov chain u ?? z ?? w. 
 ﻿ Abstract We prove of the recently quadratic rate distortion function subject to the constraint that the distortion is uncorrelated to the source . This result is based on shaped lattice quantization in the limit as the lattice dimension to infinity and for all positive . It turns out that this uncorrelated distortion can be causally . This feature , which in contrast to , is by causal transform . Moreover , we prove that by feedback noise shaping the uncorrelated distortion can be causally and with memoryless entropy . Whilst upon infinite dimensional , we prove that the rate loss in the finite dimensional case can be upper bounded by the space filling loss of the and , thus , is at most . bit dimension . 
 I . INTRODUCTION 
 rate distortion function for a stationary zero mean memory and under the fidelity criterion can be written in a parametric form the reverse water filling solution 
 a b where the power spectral density the distortion is given , if c 
 , otherwise . The water level is chosen such that the distortion constraint b is satisfied . 
 It is well known that in order to achieve in the quadratic case , the distortion must be independent of the output . This clearly that the distortion must be correlated to the source . 
 Interestingly , many well known source actually lead , by construction , to source uncorrelated . In particular , this is the case when the source coder the following two : a The linear if any achieve perfect reconstruction in the absence of quantization ; the quantization error is uncorrelated to the source . The first condition is typically satisfied by , transform and feedback . The second condition is met when subtractive and often when non subtractive dither are employed . Thus , any scheme , for example , quantization , to source uncorrelated . 
 An important fundamental question , which was raised by the in a recent paper , is : What is the impact on rate distortion function , when we further impose the constraint that the end to end distortion must be uncorrelated to the input 
 In , we the notion of , which is the quadratic rate distortion function subject to the constraint that the distortion is uncorrelated to the input . For a source , we defined as 
 R , I ;, 
 Y : E , 
  
 N , N where the notation the covariance matrix to the determinant . For zero mean stationary , we in that the above minimum in the limit when the following : 
 is the of the optimal distortion , which needs to be . Notice that here the parameter a akin to in does not represent a water level . Indeed , white , the of the optimal distortion for is not white , for all . 
 In the present paper we prove of by based on lattice quantization , which , in the limit as the dimension infinity , are able to achieve for any positive . We also show that can be causally , i . e ., that for all and for all positive one can build forward test that realize without non causal . This is contrary to the case of rate distortion function , where at least one of the 
 Other and between and are in . 
 of the forward test channel that needs to be non causal . To further illustrate the causality of , we present a causal transform architecture that it . We also show that the use of feedback noise shaping one to achieve with memoryless entropy . This a recent result by , and for . We conclude the paper by showing that , in all the , the rate loss with respect to when a finite dimensional can be upper bounded by the space filling loss of the . Thus , for any source with memory , by and scalar quantization , the scalar entropy conditioned to the dither of the output by at most . bit dimension . 
 . BACKGROUND ON LATTICE QUANTIZATION 
 A lattice is a lattice with subtractive dither , by entropy . The dither U V is uniformly distributed over a cell V of the lattice . Due to the dither , the quantization error is truly independent of the input . Furthermore , it was shown in that the rate of the , i . e . 
 , N 
 can be written as the mutual information between the input and the output of an additive noise channel E , where E the channel additive noise and is distributed as . More precisely , N I ; N I ; E and the quadratic distortion per dimension is given by N N EkE k . 
 It furthermore been shown that when is white there a sequence of lattice where the quantization error and therefore also the dither to be approximately distributed in the divergence sense for large . Specifically , let E have a probability distribution fE , and let be distributed with the same mean and covariance as E . Then limN N fE e e 
 with a convergence rate of log if the sequence is chosen appropriately . 
 In the next section we will be interested in the case where the dither is not necessarily white . By shaping the of a lattice whose dither is white , we also shape , a colored dither . This situation was considered in detail in from where we obtain the following lemma which was proven in but not put into a lemma . 
 Lemma : Let E U V be white , i . e . E is uniformly distributed over the cell V of the lattice 
 and . Furthermore , let , where 
 the shaped cell some invertible linear transformation . Denote the covariance of E by . Similarly , let , KEG covariance matrix KEG and let 
 , KEG where KEG . Then there a 
 sequence of shaped lattice such that 
 N fE e e O log . 
 Proof : The divergence is invariant to invertible since E E log . 
 Thus , fE e e e e fE e e for any . 
 . OF 
 The forward channel that is shown in Fig . . According to , all that is for the mutual information per dimension equal is with equal to the right hand side of b . 
 Z 
 X Y 
 Fig . : Forward test channel 
 In view of the asymptotic of lattice in Section , the of can be shown by the test channel of Fig . by an adequately shaped dimensional lattice and then . In order to establish this result , 
 the following lemma is . 
 Lemma : Let ,, and be mutually independent random . Let and be arbitrarily distributed , and the same mean and covariance as and , respectively . Then 
 I ; I ; . 
 Proof : 
 I ; 
 a 
 I ; , 
 where a from the well known result 
 h , see , e .., ,. . 
 We can now prove the of . 
 Theorem : For a an infinite length random vector with zero mean , is achievable . 
 Proof : Let be the sub vector the . For a fixed distortion , the average mutual information per dimension N I ; is when and are jointly and 
 K , 
 see . Let the dimensional shaped such that the dither is distributed as , with . It that the rate of the is given by 
 N I ; E . The rate loss due to to quantize is given by 
 N hI ; E 
 I ; E i 
 a 
 N fE e e , 
 where is the of the random vector , independent of E and , and the same first and second order statistics as E . In , inequality a directly from Lemma , since the use of subtractive dither the error E independent of . 
 To complete the proof , we invoke Lemma , which that the of as . 
 Remark : For zero mean stationary random , is by Theorem to be the complete input process . For this case , as shown in , the transform of the function of to the of b . 
 For vector , the of by Theorem from the concatenation of infinitely many consecutive . 
 Note that if one an infinite number of parallel scalar random , can be causally by Theorem from the th sample of each of the and entropy after . 
 The fact that can be causally is further in the following section . 
 . REALIZATION OF BY CAUSAL TRANSFORM 
 We will next show that for a random vector with positive definite covariance matrix , can be by causal transform , . A typical transform architecture is shown in Fig . . In this figure , is , a vector , independent of , with covariance matrix I . The system clearly the perfect reconstruction condition W . The reconstruction error is the random vector ,, and the is N , 
 Fig . : Transform coder . 
 be lower triangular , the transform coder in Fig . becomes causal , in the sense that ,..,, the th of U and can be determined just the the th element of . 
 To have N I ; , it is necessary and sufficient that 
 T T , 
 where the covariance matrix of the optimal distortion is 
 K . 
 Since is lower triangular , is the decomposition of , which always . Thus , can be by causal transform . 
 In practice , transform are by the vector channel by a 
 or several by entropy . The latter process is simplified if the are independent . When with subtractive dither , this can be shown to be equivalent to 
 N I U ; in the transform coder when the channel . Notice that , is invertible , the mutual information per dimension is also equal to . By the chain rule of mutual information we have 
  
 with equality the of are mutually independent . If is , this is equivalent to being diagonal . Clearly , this cannot be with the architecture shown in Fig . causal matrices while at the same time satisfying . However , it can be by error feedback , as we show next . 
 Consider the scheme shown in Fig . , where A is lower triangular and is strictly lower triangular . Again , a sufficient and necessary condition to have 
  
 Fig . : A causal transform scheme with error feedback . 
 N I ; is that , see , i . e ., 
  
 I I AT . 
 On the other hand , equality in is only if 
 I I , 
 for some diagonal positive . If we substitute the factorization into , we obtain I I , and thus 
 A I . 
 Substituting the above into we obtain 
 D I I 
 Thus , there exist A and . Substitution of into A AT , and log log A log . From and the fact that I it that A , and therefore 
  
 thus equality in . 
 We have seen that the use of error feedback one to make the average scalar mutual information between the input and output of each channel in the transform domain equal to . In the following section we show how this result can be extended to stationary . 
 V . BY NOISE SHAPING 
 In this section we show that , for any colored stationary stationary source and for any positive distortion , can be by noise shaping , and that is achievable memory less entropy . 
 A . Realization of by Noise Shaping 
 The fact that can be by the additive colored noise test channel of Fig . that could also be by an additive white noise channel in a noise shaping feedback loop , see Fig . . In this figure , is a stationary process with . The A and are . The channel is situated , where white noise , independent of , is added . The reconstructed by passing through the filter A , yielding the reconstruction error . 
  
 Fig . : Test channel built by the channel in a noise feedback loop . 
 The following theorem that , for this scheme , the scalar mutual information across the channel can actually equal . 
 Theorem : Consider the scheme in Fig . . Let , be independent stationary random . Suppose that the differential entropy rate of is bounded , and that is white . Then , for every , there exist causal and stable A , A and such that 
 I ; , where , . 
 Proof : Consider all possible of the A and such that the sequence is white , i . e ., such that . From Fig . , this is 
 the A and satisfy 
 . 
 On the other hand , since is , a necessary and sufficient condition in order to achieve is that 
 , 
 This 
 Substituting the latter and into , and after some algebra , we obtain 
  
 are bounded and positive for all ,, and that a bounded differential entropy rate of that 
 . From the Wiener criterion see also , e .., , this that , A and A can be chosen to be stable and causal . Furthermore , recall that for any fixed , the corresponding value of a is unique see , and thus fixed . Since the variance is also fixed , it that each frequency response magnitude that a can be associated to a unique value of . Since is strictly causal and stable , the minimum value of the variance is when 
 , 
  
 i . e ., if no outside the unit circle equivalently , if is minimum phase , see , e .., . If we choose in a a filter that , and then we take the logarithm and integrate both sides of a , we obtain 
  
 log p 
 W 
 p 
 log . 
 p a 
 p 
 where a been used . We then have that 
  
 where a from the of and , and from the fact that is independent of strictly causal . This the proof . Alternatively , 
 a 
 R I ; 
 A A 
 A A 
  
  
  
 e I ; , 
 In a , equality is the right hand side of a , i . e ., optimal . Equality because , which from b . The fact that is stationary been used in , wherein equality is is minimum phase , i . e ., if . Equality in if an only if the of are independent , which , from the of , is equivalent to . Finally , e from the fact that is independent of . Notice that the key to the proof of Theorem on knowing a the of the end to end distortion to realize . Indeed , one could also use this fact to realize by the in a feedback loop , and then following a reasoning similar to that in . 
 B . Through Feedback Quantization 
 In order to achieve by a instead of an channel , one would require the quantization to be . This cannot be with scalar . However , as we have seen in , lattice are able to yield quantization approximately as the lattice dimension to infinity . The sequential causal nature of the feedback architecture does not immediately allow for the possibility of vector . However , if several are to be simultaneously , we can overcome this difficulty by an idea in where the are in parallel by separate feedback . The feedback are operating independently of each other except that their scalar are by a single vector . If the number of parallel is large , then the vector that the marginal of the individual of the becomes approximately distributed . Thus , due to the within the vector , each feedback a sequence of i . i .. quantization . Furthermore , the effective rate per source is that of a high dimensional entropy constrained per dimension . 
 The fact that the scalar mutual information between and the mutual information rate between and in each of the parallel that can be by a memoryless entropy coder . 
 . RATE LOSS WITH FEEDBACK QUANTIZATION 
 The in that if a test channel an channel , then a source coder by the channel by a , finite dimensional lattice , would exhibit a rate close to . 
 The next theorem , whose proof the line of the given in , sec an upper bound on the rate loss in this case . 
 Theorem : Consider a source coder with a finite dimensional lattice . If when the by an channel the scalar mutual information across the channel , then the scalar entropy of the output by at most . bit dimension . 
 Proof : the noise of the channel , denote the channel input and output . From the of the theorem , we have that 
 I ; . 
 If we now replace the by a with subtractive dither , such that the quantization noise is with the same first and second order statistics as , then the end to end remains the same . The corresponding in the case , namely and , will also have the same second order statistics as . Thus , by Lemma we obtain 
 I ; . 
 Finally , from , Theorem , we have that I ; . Substitution of into this last equation the result . 
 . 
 We have proved the of by lattice quantization with subtractive dither . We have shown that can be causally , and that the use of feedback one to achieve by memoryless entropy . We also that the scalar entropy of the output when optimal finite dimensional lattice quantization by at most . dimension . 
 . 
  
 ﻿ We derive closed form for the statistics of the power gain as a function of frequency of microwave indoor . We obtain our within a framework that is general enough to be compatible with several popular channel , such as the channel model and those by the . . a and . . a task . As in all these , our channel description is based upon and with possibly mixed and random . Our consist of closed form for the second order statistics of the channel power frequency response , where statistical involve over ray and arrival times . These reveal that the of the spectral power gain between any two and to zero as the difference between these to infinity if and only if the cluster arrival rate goes to infinity . They also show that the variance to squared mean ratio of the narrow band power gain exactly the same behavior with respect to the center frequency . We then use these to obtain closed form for the variance and the second order moment of the aggregate channel power gain over any given interval of . This us to express the channel spectral diversity as a function of model and . In addition , we illustrate how these allow one to devise automatic cluster identification which , from empirical of the second order spectral statistics of the channel power gain , can confirm or deny the existence of in a given scenario . 
 Index Fading , channel model , statistical channel modeling 
 wireless channel allow one to predict the statistics of radio propagation over an ensemble of with similar . This is particularly useful in complex , heterogeneous , and time , such as office , residential , and industrial indoor . 
 One of the most popular for indoor wireless is that in , which as the basis for several other channel . Building upon it , the . . a task group accepted a channel model for indoor , and similar have been adopted for the . . a standard . Similar to the model , the . . a and . . a channel consist of a discrete time description of the impulse response of a wireless channel , in which are grouped into . 
 Some of the useful time domain delay statistics of wireless channel are the power delay profile , the average delay , and the delay , which , under suitable , allow one to determine spectral statistics such as coherence and average power gain . These , which are associated with second order statistics of the channel impulse or frequency response , have been extensively and in the literature . 
 Another set of stochastic of wireless , which received relatively less attention , is those derived from the second order statistics of the channel power frequency response i . e ., its squared frequency response magnitude , which is by . An early analysis of these statistics and their application can be found in , where it is that the transmission over two with the right frequency spacing may benefit from the negative correlation between each of the received . Simplified for the spatial and spectral of and for its power over a frequency interval are in , which are then via . Building upon this result and , it been possible to assess fading depth statistics and relate them to the and to some of the propagation environment , . 
 . Personal use is permitted , but republication redistribution permission . See : index . for more information . 
 Although the of the second order statistics of are helpful in the performance of wireless communication , closed form for these statistics further . First and naturally , these closed form can be substituted into other , thus one to relate them with performance , which are also in closed form . Second , explicit parametric will provide useful insight into the relationship between the performance of wireless communication and certain . In particular , the second order statistics of allow one to refine probabilistic drawn from first order statistics alone . Moreover , if the power received over some band is known to distribute according to some parametric probability density function , then the first and second order statistics of this power one to identify , in some , the of the distribution . 
 This it possible to calculate fade depth statistics from the second order statistics of . 
 In this regard , it was already noted in and that the fade depth as the width of . In relation to this question , and an information theoretic approach and measured data , it been shown in that the number of significant of the covariance matrix of the random channel impulse response and , hence , the diversity order of the channel scale approximately linearly with the . This increase in diversity , which to the reduction of the relative channel power variance as the , been to reach a saturation point , , . explicit for this channel power variance as a function of would allow one to predict the associated saturation and relate it to environmental . 
 Available closed form that allow one to analytically obtain the second order statistics of are rather limited . Although the analysis in and does obtain some intermediate closed form , the actual evaluation of these statistics is carried out by . In addition , one of the underpinning the in and is the between the and arrival times of , which is known to contradict channel , . Under the assumption of uncorrelated and arrival times , and considering uniformly distributed arrival times , closed form for the of have been derived in . Exact closed form for all the joint between and , for any and , as a function of the of the channel impulse response have been found in , assuming that component are independent jointly complex and that arrival times are fixed . However , although popular channel such as the model consider complex component , other amplitude have been in the literature see , e .., . Moreover , the conditional independence assumption in does not hold for channel where the impulse response , such as the and its several , . In a recent paper , analytical for the of the channel frequency response squared magnitude and the variance of the power over any frequency band have been derived for the . . a channel model , conditioned to fixed and given ray and cluster arrival times . The corresponding statistics over random arrival times were via . To the best of the knowledge , no closed form are available in the literature for second order unconditioned statistics of the spectral power gain of wireless channel such as the . . a and . . a , as well as for the other like . 
 In this paper , we derive closed form exact for the second order statistics of for a general class of of the channel model . This class , as special , those accepted by the . . a task group and the . . a channel modeling subgroup , , . Unlike , our include considering the randomness of both ray and arrival times . As in all like , our channel description is based upon and , with different power decay among and within . Our analysis framework is general enough to encompass model such as mixed within , as well as in which the first ray of each cluster statistics different from the other these two are considered in , , and . To this end , we first derive , in Section , a general , exact , and closed form expression for the of . This expression is determined by four , corresponding to the second and fourth order moment delay of the cluster and ray by their respective arrival . We use these to analyze the effect of a finite infinite number of , as well as the of only one cluster or , equivalently , no at all . We then use the former result to obtain , in Section , a expression for the variance of the channel power gain over any given interval of . Our predict a positive lower bound for the ratio between the variance of and its squared mean . This bound only as the arrival rate of in the model to infinity , which is a result that is confirmed via . Some preliminary on this topic were in . In addition , the allow one to devise an automatic cluster identification algorithm , which is capable of or the presence of in an ensemble of channel impulse . This algorithm is shown in an example in can be considered an alternative to other cluster identification , such as those in and . 
 Here , we formulate the wireless channel model framework to be throughout this paper . The idea is to establish a framework that is general enough to encompass most of the in the available literature . With that idea in mind , and before proceeding , we will first review some of the salient of several of the model , from which we will then define the set of that our will be based upon . 
 In all and of the model , the channel is by the following random impulse response , : 
 is the random arrival time of the path or ray in the cluster , Ti is the random arrival time of the cluster , and ti , is the random delay of the path in the cluster relative to Ti . Cluster and relative arrival times are ordered , i . e ., Ti i and ti , ti , i . By definition , the cluster with its first ray ; thus 
 Each random coefficient ai , in the amplitude of the path in cluster i . These random are formed as : 
 Each is a random variable the amplitude of the cluster , whereas each real valued random variable ai , the amplitude or gain of the component or ray within the cluster relative to that cluster amplitude . The meaning of the random pi , in on whether a complex or real , representation is used . For the former , pi , , where fi , are uniformly independent and identically distributed i . i .. over , , , . For the real representation , which is as more suitable for , pi , are binary random taking from , with equal probability . In both 
 To establish a framework to encompass the various of the like channel present in , , and and possible future , we will formulate now the least restrictive set of under which our analysis and are valid . To do so , the following three are necessary . 
 Definition Bounded Density Process : We will state that a sequence of random arrival times , with xi i , is bounded density with the following . 
 There a constant such that , for every that is sufficiently small 
 In other , condition in the given definition that the time just after an arrival at and , hence , the arrival density at , at most , on the value of . 
 Definition Sequence i . i .. Under Another Sequence : Let be a bounded density process , with x possibly being equal to . We will say that a sequence of random is i . i . under if the following two are satisfied . 
 Notice that condition in Definition to the conditional independence between and arrival times by all channel considered in , , , and , except the . . a model for industrial under in and . Similarly , taking as the cluster or relative ray and as the cluster or relative ray arrival times , condition all the amplitude considered in , , and , the in which the amplitude of the first ray or cluster a special form , as in body surface to body surface , . . and body surface to external , . . in . 
 We will need one more definition to treat arrival time differently from plain , such as mixed in . To this end , we introduce the following single and joint arrival density . 
 Definition Arrival Density : Let be a bounded density sequence of arrival times , with x possibly being equal to zero . For any , denote the number of in falling inside 
 where , for , . We define the single and joint arrival density of as 
 One case in which arrival times do not conform to a process is in the model in , which is further considered in and . In that case , within each cluster are as mixed ; in which case , the ray times are i . i .. with the following : 
 the given , we can now establish the scope of validity of our by of the following assumption . 
 Assumption : Cluster and ray arrival times and are random and as . 
 , ,..., a , pi ,, unless i and b , ai ,, unless i c 
 where probabilistic independence , and the chain notation a a 
 Ti is a bounded density sequence , with T that is possibly equal to zero , i . i . times exponentially distributed with exponent , and arrival density and ,. 
 ti , m is a bounded density sequence with ti , and arrival density and , for every i . 
 is i . i .. under , with order moment delay profile , as shown in a at the bottom of the page , for , . 
 is i . i .. upon , with order moment delay profile , as shown in b at the bottom of the page , where , for every i . 
 Notice that , in the in Assumption , and , , capture the moment delay between and within , respectively , without taking arrival into account . In turn , the single and joint arrival of are by and ,, whereas the single and joint relative arrival of are given by and ,, respectively . 
 The and in Assumption are satisfied for all the channel in , , and . Indeed , by choosing cluster and ray arrival times as , with each ai , conditioned to ti , being distributed with second moment g , and deterministic Ai with the form Ai b Ti ,, , we obtain the widely used model . A similar choice with distributed Ai and ai the model by the . . a task group excluding its large scale fading term . The decay of the by can be also chosen to match those that the . a standard for high in some scenario such as office and industrial . 
 and a is Mixed do not Give Rise to a Mixed Distribution : It is worth at this point that the mixed , which are by the conditional given in , do not give rise to what is known as a mixed distribution . More precisely , if the random exponent selection in place after each arrival , then the probability any unit length time interval , cannot be written as , where the probability mass function with parameter . The exact probability distribution been derived by the first author in a work currently in preparation . 
 Another assumption that will facilitate the forthcoming analysis is related to the number of and . Although in , as in , , and , the number of and the number of within each cluster are finite , we will consider infinitely many and per cluster . As acknowledged in , the latter choice is more realistic . Of course , this assumption that the decay fast enough with increasing delay . We will make this requirement precise by assuming that the conditional of the defined in a and b satisfy 
 with ai , and ti , as defined in Section A . In addition , as in , we will adopt a real model for the impulse response ; hence , the pi , in are i . i .. random taking the or with equal probability . This is done mainly to simplify the notation , and it is worth that all forthcoming also hold for the complex representation . All that is is that and Assumption hold . 
 A deterministic frequency dependent gain for each ray , which is a feature in , , and , can be easily incorporated to our model by simply multiplying by the squared magnitude of this gain . For ease of notation , and because the effects of this factor on the resulting statistics can be easily added afterward , we will not include it in our . 
 In the following and under Assumption , we will derive the exact closed form for the second order statistics of and of the channel power over any given frequency interval as a function of the moment delay b , b , g , and g . 
 Here , we will obtain closed form for the mean , the , and the coefficient of over any interval of angular . These rely on several technical , which can be found in the Appendix . 
 From and the zero mean and independence of the pi established in , it is easy to verify that 
 Thus , the value of is the same for all , being equal to the product of the average energy of the cluster Ai and the ray relative ai ,. 
 The following theorem , which is the main result of this paper , an exact closed form expression for , in of the arrival density , , , and and the moment delay b , b , g , and g defined in Assumption . 
 Theorem : . Define the effective moment delay profile 
 Proof : See the Appendix . Remark : Theorem that the variance of the channel power gain the same variation with as does its O , O with O . Since the overall behavior of the latter is to decrease as O , it that the variability of the channel power gain is smaller at higher . Notice that this reduction does not come from the frequency dependence affecting each component which is typical of real indoor propagation since we have not included it in our model . However , had this dependence been included , the variance to squared mean ratio of would still exhibit the decay with by Theorem . Instead , the origin of this behavior can be more intuitively understood by looking at in the Appendix and by that E is independent of . In , if we choose , then T T becomes the only frequency dependent term . Now , T is a sum of of the random . At , T turns into a sum of . By contrast , if we let increase so that the of the random extend smoothly over an interval that is several times than p , then the phases of the will be approximately distributed uniformly over , p . This will reduce each of the in the sum . Thus , as , and so does E . 
 Remark : If the effective moment delay and are smooth , then all the frequency dependent in vanish if to infinity while keeping , , leaving only 
 . This that the between two infinitely distant frequency is greater than zero , which , at first sight , may seem . Nevertheless , although this to be the first time that such behavior is proven to exist for like channel , it is consistent with the saturation of the channel diversity order as the , as in and . Further treatment of this phenomenon is in B and . As will be in the following , this nonzero asymptotic is due to that the arrival rate of is finite . 
 Infinitely Many : Suppose the arrival of and are scaled by and , respectively , to obtain arrival 
 Let us also suppose that this is done while the original so that the total impulse response power is , which one to scale g and b by and , respectively . This that g and b are scaled by and , respectively . Denote the resulting scaled moment profile by , and . 
 Substituting them into , the behavior of the involved in the frequency independent part of as cluster and ray going to infinity is given by the following : 
 c , if the cluster arrival rate to infinity and the right hand side of b is zero . With this , as , in the case in which have which , B B and if the first cluster randomly i . e ., if , then the of the simpler form 
 squared magnitude of the and the at and . 
 Only One Cluster No : Consider the case in which do not exhibit the presence of . This situation can be under the framework defined by Assumption by supposing that there is only one cluster with deterministic arrival time T and amplitude A , and such that its first ray at time t , . To ensure that there are no other , we may take E Ak . With these , we obtain , and , 
 Nonzero Asymptotic : As previously in Remark , Theorem that , for finite cluster arrival , the of , i . e ., , , to a positive value as i . e ., if O to , and not to zero . To the best of our knowledge , this is the first time that this result is derived or revealed , at least for an like channel model . A simple and intuitive explanation of this phenomenon is provided at the beginning of Section , in of the variance of the aggregate channel power gain over a given band . It is worth that such asymptotic behavior cannot be if one the of the complex frequency response , i . e ., by looking at the second order statistics of instead of those of . Indeed , by the independence a and d and then Lemma , it becomes easy to show that 
 This simple expression which we believe to be novel , that , if the moment delay of and within them are smooth , then lim E . 
 The correlation coefficient of , which is as , , is given by 
 It was already shown that , for any given smooth moment delay profile , the between two and O to zero as O if the cluster arrival density goes to infinity . We shall see in Section A that even when this arrival density is finite , , O if the moment delay change so that the number of with a significant amplitude to infinity . 
 Here , we will use Theorem to derive closed form for the mean and the variance of the total channel power over any given frequency band , i . e ., 
 The variance of the channel power over be directly from the of as 
 Before proceeding , it is worth that readily that , if , u is bounded , then 
 since E linearly with . This one to provide a simple and intuitive explanation for the fact that , u i . e ., the of does not vanish when the two , u are infinitely distant from one another . Recall from theorem that , when , the total impulse response power . Then , if the impulse response is amplitude by a finite number of cluster , it is clear that the ratio variance of impulse response power over the square of the impulse response power will not be zero . In view of , this will imply that . That is , the of between a pair of infinitely distant will not be zero whenever the total number of with random amplitude remains finite . As , this an alternative and more intuitive explanation to what was already in Section B . 
 We now return to the variance of . From , , can be written as 
 the change of O u , u and along diagonal over the square , ,, we obtain 
 We note that is a measure of the fluctuation of the received power over a wireless channel and that the latter its fade statistics . The fact that fade depth with channel been in the literature , both from empirical data see , e .., and from , . However , a closed form formula fade depth and the classical channel model used here have , to the best of our knowledge , not been to date . The derived allow us to directly deal with this issue , provided that the fading distribution can be fully determined from its second moment an assumption that was successfully applied in . The explicit dependence between in the following example . 
 Here , we will illustrate the application of the in and to the classical model . In particular , we will show that our analytical found in and accurately predict the of and the variance of the channel power gain over any given band . In addition , we will show how the presence of can be confirmed or by the statistics of and the derived in this paper . 
 In the model , the ai , are distributed conditioned to ti ,, and the Ai are deterministic exponentially . The of cluster and ray are given by 
 where the last equation since the fourth moment E x of a distributed random related to its second order moment as E x E x . Both cluster and relative ray times are exponentially i . i .. with and , respectively . In addition , the arrival times of the first cluster and the relative arrival time of the first ray in every cluster are , by definition , zero . Therefore , the effective moment profile defined in take the following form : 
 Fig . with dashed line the empirical of , of the channel model with four different of , which are in Table I . Each of these was after random of a channel impulse response . On the same plot , the theoretical of , by are solid , for each set of . It is shown that , in all , the data very tightly . 
 The set A correspond to those in the . . a . Notice that , for the of this set , as O is , the spectral by approximately when O . rad , and then at about , which is precisely the behavior determined by provided K is much than K and . From , the next corner separation frequency , beyond which , almost to diminish , place when , which in this case to rad , in agreement with what is shown in Fig . . The from those of the previous one in that the arrival are halved , whereas the decay are doubled . This the same and and the same ratio as the set A . Thus , the , K , and are also the same as those by the set A . Accordingly , the only difference with respect with the set A is that the corner are reduced by a factor of 
 Fig . . Spectral of as a function of the frequency rad for the model separation O around a central frequency with the four of shown in Table I . in dashed line are over channel . Each theoretical curve in solid line was . 
 its value in set A the corner frequency associated with K by , which is sufficiently higher than to have a small but noticeable effect see the slight bump in the and rad . With the of set , these two corner are two away from one another , with their presence becoming clearly visible in the corresponding plot in Fig . . Intuitively , the existence of two corner separation beyond which the spectral or to decrease can be associated with the temporal resolution associated with O . More precisely , when O is too small , all in the impulse response are added with roughly the same phase in the sum for O and for O , yielding a high correlation . As O is , a point is at which some within the impulse response contribute with different random phases in , which to the channel power gains . This reduction when O is large enough so that all within the impulse response are added with different phases , only when O to make also the within each cluster add with different random phases . 
 The variance of is directly by for an arbitrary frequency , which 
 By substituting this and into , the correlation coefficient the form in , shown at the bottom of the page . By substituting the of , K , and K into this expression and after some algebra , one that the correlation coefficient 
 Thus , extending what was found for the of in Section B , we see here that the lower asymptote of the spectral power correlation as O not only when but also when the product . 
 Moreover , for any , we have that , O if and only if . 
 When a sufficiently large number of of the impulse response of a wireless channel , the only model that can be directly are the ray arrival rate and the decay exponent . In contrast , the arrival rate and the decay exponent are not readily observable . Indeed , the very existence of is not unquestionably evident from the impulse response . 
 Here , based upon the , we propose a quantitative method for and . By doing this , it is possible to assert the presence of if the value is comparable to or smaller than or the absence of them if 
 As previously , the decay the arrival rate can be directly from as done in , e .., and . Additional for finding the two and can be by O , O at two or more different . For the channel model considered in this 
 If there were no , the impulse response would be as if there were only a single cluster , beginning at , with decay exponent . 
 In practice , it will be also necessary to identify the power scaling factor of the impulse response , namely , the value of b at . This can be easily done by taking the empirical mean of the squared magnitude of the first ray in the impulse response . 
 The left hand sides of these L and L can be directly from the . Denote these as and , respectively . In principle , one could express and in closed form as of L and L , and then evaluate for L and L . However , is in this case the solution to the following quadratic equation : 
 resulting from substituting into . Therefore , the approximate nature of and can turn a real positive root of into a complex valued or negative one . Moreover , even if one could measure L and L with infinite precision , there can be still two real valued of greater than . To overcome these , the following algorithm is . 
 for L and L . Denote these as ,...,, respectively , where , . From these , calculate 
 the , i , i ,...,. These will be the preliminary for . 
 Let xi , i ,...,, be the corresponding preliminary for . 
 Then , apply a least curve fitting optimization algorithm , with and as unknown , to match against the empirical of , at several . Use i and i as initial . Define the residual matching as i 
 This technique for and was applied to impulse for the channel model , with 
 Fig . . Left Typical impulse response for the channel model with the of top set E . , . , , and . and bottom set , . , . from the same cluster are shown with the same color . Top right Twelve of and , each from the of the impulse for each set of , which are by the method in Section . Bottom right of from and s range along them , the standard deviation of the empirical estimate of 
 two of . For the first of these set E , . , . , , and . . Notice that this , i . e ., the arrive at the same rate as the within them , making it very difficult to distinguish one cluster from another in the time domain . 
 It may be that the chosen and decay and are the same as those from the . . a used in Section . In turn , the product been chosen about ten times than that used in Section , so that the resulting dense train of it even harder to recognize the presence of . A typical channel impulse response for these is shown in Fig . top left . As , with the chosen , arrive so densely that it is virtually impossible to tell one from the other . After of independent of these impulse , of L and L were by . From the corresponding empirical of ,, which are for of evenly distributed from to rad , the and were the method , yielding the shown in the scatter plot of Fig . top right . The average error magnitude of these is less than of their true . More importantly , in all , the existence of is unambiguously revealed since all of are greater than . This more remarkable after that , in this case , the impulse are such that are almost totally , making it virtually impossible to distinguish one from the other . 
 The for a scenario in which there are no is shown in Fig . bottom . In this case , the used for the model : , . , and . A typical channel impulse response from these , as the one shown on Fig . bottom left , the same overall decay exponent and a similar net density of as in the previous case . The corresponding of and , which are by the algorithm here , are shown in the scatter plot in Fig . top right . Here , for of of the channel impulse response , the corresponding empirical of , for of evenly distributed over , rad were . Notice how , in all , the of are well below , unambiguously revealing the absence of . 
 The possibility of correctly the true of a channel , and in the case from the empirical estimate of ,, e ..,,, channel , will ultimately depend upon the accuracy of the latter estimate . To illustrate this dependence , the theoretical O , O is plotted in Fig . right bottom , for the parameter E and . The within five standard of the estimate above and below each of the O , O is shown as shaded . The standard deviation shown to what is when , is calculated from channel , which is the same number to obtain the shown in Fig . top right . Given the significant overlap of these and since the variance of , is proportional to , it is clear that , in this case , a reliable decision about which of the parameter better the data cannot be from much less than channel . 
 We end by that the parameter estimation algorithm been for its simplicity and with the purpose of the potential applicability of the derived in and . Therefore , the search for better estimation , which must certainly exist , goes beyond the scope of this paper . 
 The variance of the channel power over a band , rad is by substituting into , with the change of u and u as : 
 these , the change of , and the identity , we obtain , after some manipulation , that 
 The relative variability of with respect to the average total power better by the variance , i . e ., 
 and reveal several interesting about the channel power variance , which are in the following . 
 Except for the first term on the of , all the other grow above some frequency . It is a simple although long exercise of algebra to show that the frequency above which the growth rate all these is significantly than 
 W is given by , . . If , then the variance of the total power be well as 
 where we have used . Thus , in agreement with what was shown in B and , the variance of the power over an asymptotically infinite only when the number of significant to infinity . 
 The narrow band channel power variance P E P is from by O 
 grow proportional to W . It is straightforward but lengthy to show that the maximum value for 
 Thus , unless comparable to or than , no reduction in channel power gain variance is to be . 
 Notice also that , if there is a single cluster with deterministic amplitude and infinite duration i . e ., if we let and , the narrow band channel power variance should coincide with that of a channel in which case should distribute exponentially . Indeed , if we substitute the for K and K in , fix , and let , then it is easy to verify that E , which is precisely the variance to squared expectation ratio of an exponentially distributed random variable . 
 The in the dashed line in Fig . display the value of E for a band of at rad from of the model for each set of given in Table I . The corresponding of E by are plotted in the same figure with solid . It is shown that , in all , the theoretical and simulation are almost indistinguishable , confirming the accuracy of our . It is also shown that , in all , as from , from zero , the channel power variance remains almost constant until a certain value , near the threshold defined previously . Beyond that threshold , all the decay a saturation point is , in a way that is consistent with what was found in . 
 We have derived the general that characterize the second order statistics of the frequency response power gain in wireless indoor . Our are applicable to several well established channel in the literature , all based upon the model . In particular , the closed form formula here for the of the squared frequency response magnitude of the channel one to predict the variance of the aggregate channel power gain over any frequency band . This an approximation 
 Fig . . variance of the channel power gain over a at rad , for the model with the four of A ,,, given in Table I . are over channel . The theoretical curve was and . 
 to the diversity order of over narrow band in such . Our also allow one to obtain an upper bound for this measure of spectral diversity , and they show how and why this limit from a finite number of with a significant amplitude in the channel impulse response . In addition , the derived explicitly reveal how the statistical of the in the channel impulse response affect the spectral of the channel power gain . This one to use these spectral statistics to identify the presence or absence of . A simple procedure to accomplish this task been and its effectiveness been by it to of the channel model . 
 Proof of Theorem : For notational simplicity , we will temporarily adopt a single indexing nomenclature for the path arrival times ti ,. More precisely and with a slight abuse of notation , we define the infinite random set 
 . For our , it will not be necessary to define a between the i the index . Indeed , it will be sufficient to note that the double index matching condition i , , which i and is equivalent to the single index condition . 
 the single index notation , the first term on the of is from , which 
 where a from , and the last equality from In , a is a consequence of the independence the fact that all pi have zero mean see . Proceeding established in , whereas directly from Lemma . similarly , it is easy to show that each term in the summation Proceeding similarly with the frequency dependent function of in which one or more is not to any T , we obtain other index to zero . Thus , one must only consider the 
 Substituting these for T and T into , by the result and into , 
 th moment delay profile of , and the single and joint effective arrival and , are as in 
 Proof of Lemma : Let E ,, with as in Definition . Recall the of i and from . If x , then 
 where Proposition in the Appendix been used . The last term in the latter expression is the difference between the rightmost term in and the squared of . the effective moment delay profile defined in , the of can be written as 
 Lemma : Let be an random process , with x possibly being equal to zero , and let be a random sequence i . i .. under . Then , for any given N 
 where a from and in the Appendix and from and . In turn , was the fact that and are bounded for . 
 Finally , if x is random , it readily by Lemma in the Appendix that 
 Lemma : Let the arrival times be an sequence with x randomly distributed . Define function as in . Then 
 where the discrete random variable to the number of in , . Its expectation 
 Substituting this result into , thus the proof . Lemma : Let the arrival times distribute as in 
 Lemma , and define , as in . Assume that , is bounded for all . Then 
 By dividing by and taking the limit as , and then substituting and Lemma , it that 
 that , E and that the integrand in is symmetric , and proceeding with as in , we conclude that 
 The proof is upon substituting this result into . Proposition : Let , be as in Definition , and let : be a bounded function . Then 
 Proof : If x is random , then , is bounded , and the result trivially . Otherwise , x , in which case 
  
 ﻿We improve the existing achievable rate regions for causal and for zero-delay source coding of stationary Gaussian sources under an average mean squared error distortion measure. To begin with, we find a closed-form expression for the information-theoretic causal rate-distortion function (RDF) under such distortion measure, denoted by , for first-order Gauss–Markov processes. is a lower bound to the optimal performance theoretically attainable (OPTA) by any causal source code, namely . We show that, for Gaussian sources, the latter can also be upper bounded as bits/sample. In order to
 analyze for arbitrary zero-mean Gaussian stationary sources, we introduce , the information-theoretic causal RDF when the reconstruction error is jointly stationary with the source. Based upon , we derive three closed-form upper bounds to the additive rate loss defined as  , where   denotes Shannon’s RDF. Two of these bounds are strictly smaller than 0.5 bits/sample at all rates. These bounds differ from one another in their tightness and ease of evaluation; the tighter the bound, the more involved its evaluation. We then show that, for any source spectral density and any positive distortion  ,   can be realized by an additive white Gaussian noise channel surrounded by a unique set of causal pre-, post-, and feedback filters. We show that finding such filters constitutes a convex optimization problem. In order to solve the latter, we propose an iterative optimization procedure that yields the optimal filters and is guaranteed to converge to . Finally, by establishing a connection to feedback quantization, we design a causal and a zero-delay coding scheme which, for Gaussian sources, achieves an
 operational rate lower than	and
 bits/sample, respectively. This implies that the OPTA among all zero-delay source codes, denoted by , is upper bounded as bits/sample.
 IndexTerms—Causality, convex optimization, differential pulsecode modulation, entropy coded dithered quantization (ECDQ), noise-shaping, rate-distortion theory, sequential coding.
 I. INTRODUCTION
 I
 N zero-delay source coding, the reconstruction of each input sample must take place at the same time instant the corresponding input sample has been encoded. Zero-delay source coding is desirable in many applications, e.g., in real-time applications where one cannot afford to have large delays [1], or in systems involving feedback, in which the current input depends on the previous outputs [2]–[4]. A weaker notion closely related to the principle behind zero-delay codes is that of causal source coding, wherein the reproduction of the present source sample depends only on the present and past source samples but not on the future source samples [5], [6]. This notion does not preclude the use of noncausal entropy coding, and thus, it does not guarantee zero-delay reconstruction. Nevertheless, any zero-delay source code must also be causal.
 It is known that, in general, causal codes cannot achieve the rate-distortion function (RDF)   of the source, which is the optimal performance theoretically attainable (OPTA) in the absence of causality constraints [7]. However, it is in general not known how close to   one can get when restricting attention to the class of causal or zero-delay source codes, except, for causal codes, when dealing with memory-less sources [5], stationary sources at high resolution [6], or first-order Gauss–Markov sources under a per-sample mean squared error (MSE) distortion metric [3].
 For the case of memory-less sources, it was shown by Neuhoff and Gilbert that the optimum rate-distortion performance of causal source codes, say  , is achieved by time-sharing at most two memory-less scalar quantizers (followed by entropy coders) [5]. In this case, the rate loss due to causality was shown to be given by the space-filling loss of the quantizers, i.e., the loss is at most  ( ) bits/sample. For the case of Gaussian stationary sources with memory and MSE distortion, Gorbunov and Pinsker showed that the information-theoretic  causal RDF, here denoted by
 (to be defined formally in Section II) and which satisfies , tends to Shannon’s RDF as the distortion goes to zero [8], [9]. The possible gap between the OPTA of causal source codes and this information-theoretic causal RDF was not assessed. Since operational data rates are lower bounded by the mutual information between the source and its reconstruction, we also have that  .
 0018-9448/$31.00 © 2012 IEEE
 On the other hand, for arbitrary stationary sources with finite differential entropy and under high-resolution conditions, it was shown in [6] that the rate-loss of causal codes (i.e., the difference between their OPTA and Shannon’s RDF) is at most the space-filling loss of a uniform scalar quantizer. With the exception of memory-less sources and first-order Gauss–Markov sources, the “price” of causality at general rate regimes for other stationary sources remains an open problem. However, it is known that for any source, the mutual information rates across an additive white Gaussian noise (AWGN) channel and across a scalar entropy coded dithered quantization (ECDQ) channel do not exceed   by more than 0.5 and 0.754 bits/sample, respectively [10], [11]. This immediately yields the bounds  and	.
 In causal source coding, it is generally difficult to provide a constructive proof of achievability since Shannon’s random codebook construction, which relies upon jointly encoding long sequences of source symbols, is not directly applicable even in the case of memory-less sources. Thus, even if one could obtain an outer bound for the achievable region based on an information-theoretic RDF, finding the inner bound, i.e., the OPTA, would still remain being a challenge.
 There exist other results related to the information-theoretic causal RDF, in which achievability is not addressed. The minimum sum-rate necessary to sequentially block-encode and block-decode two scalar correlated random variables under a coupled fidelity criterion was studied in [12]. A closed-form expression for this minimum rate is given in [12, Th. 4] for the special case of a squared error distortion measure and a per-variable (as opposed to a sum or average) distortion constraint. In [2], the minimum rate for causally encoding and decoding source samples (under either a per-sample or average distortion constraints) was given the name sequential rate-distortion function (SRDF). Under a per-sample MSE distortion constraint  , it was also shown in [2, p. 187] that for a first-order Gauss–Markov source  , whereis a zero-mean white Gaussian process with variance , the information-theoretic SRDF   takes the form
 (1)
 for all .  No expressions are known for for higher order Gauss–Markov sources. Also, with the exception of memory-less Gaussian sources, , with its average MSE distortion constraint (weaker than a per-sample MSE constraint), has not been characterized.
 In this paper, we improve the existing inner and outer rate-distortion bounds for causal and for zero-delay source coding of zero-mean Gaussian stationary sources and average MSE distortion. We start by showing that, for any zero-mean Gaussian source with bounded differential entropy rate, the causal OPTA exceeds   by less than approximately 0.254 bits/sample. Then, we revisit the SRDF problem for first-order Gauss–Markov sources under a per-sample distortion constraint schedule and find the explicit expression for the corresponding RDF by means of an alternative, constructive derivation. This expression, which turns out to differ from the one found in [2, bottom of p. 186], allows us to show that for first-order
 Gauss–Markov sources, the information-theoretic causal RDF   for an average (as opposed to per-sample) distortion
 measure coincides with (1). In order to upper bound  for general Gaussian stationary sources, we introduce the information-theoretic causal RDF when the distortion is jointly stationary with the source and denote it by  . We then derive three closed-form upper bounding functions to the rate-loss
  , which can be applied to any stationary Gaussian random process. Two of these bounds are, at all rates, strictly tighter than the best previously known general bound of 0.5 bits/sample. Since, by definition,  , we have that
 (2)
 and thus all three bounding functions also upper bound the gap
 . As we shall see, equality would hold in  if could be realized by a test channel with distortion jointly stationary with the source, which seems a reasonable conjecture for stationary sources.
 We do not provide a closed-form expression for   (except for first-order Gauss–Markov sources), and thus the upper bound on the right-hand side (RHS) of (2) (the tightest bound discussed in this paper) is not evaluated analytically for the general case. However, we propose an iterative procedure that can be implemented numerically and which allows one to evaluate , for any source power spectral density (PSD) and
 , with any desired accuracy. This procedure is based upon the iterative optimization of causal pre-, post-, and feedback filters around an AWGN channel. A key result in this paper (and its second main contribution) is showing that such filter optimization problem is convex in the frequency responses of all the filters. This guarantees that the mutual information rate between source and reconstruction yielded by our iterative procedure converges monotonically to   as the number of iterations and the order of the filters tend to infinity. This equivalence between the solution to a convex filter design optimization problem and avoids the troublesome minimization over mutual informations, thus making it possible to actually compute in practice, for general Gaussian stationary sources. We then make the link between   and the OPTA of causal and zero-delay codes. More precisely, when the AWGN channel is replaced by a subtractively dithered uniform scalar quantizer (SDUSQ) followed by memory-less entropy coding, the filters obtained with the iterative procedure yield a causal source coding system whose operational rate is below   bits/sample. If the entropy coder in this system is restricted to encode quantized values individually (as opposed to long sequences of them), then this system achieves zero-delay operation with an operational rate belowbits/sample. This directly translates into an upper bound to the OPTA of zero-delay source codes, namely . To illustrate our results, we present an example for a zero-mean AR-1 and a zero-mean AR-2 Gaussian source, for which we evaluate the closed-form bounds and obtain an approximation of   numerically by applying the iterative procedure proposed herein.
 This paper is organized as follows. In Section II, we review some preliminary notions. We prove in Section III that the OPTA for Gaussian sources does not exceed the information-theoretic RDF by more than approximately 0.254 bits/sample. Section IV contains the derivation of a closed-form expression for   for first-order Gauss–Markov sources. In Section V we formally introduce and derive the three closed-form upper bounding functions for the information-theoretic rate-loss of causality. Section VI presents the iterative procedure to calculate , after presenting the proof of convexity that guarantees its convergence. The two examples are provided in Section VII. Finally, Section VIII draws conclusions. (Most of the proofs of our results are given in Sections IX–XV.)
 A. Notation
   denote, respectively, the set of real numbers and the set of nonnegative real numbers. and denote, respectively, the sets of integers and positive integers. We use nonitalic lower case letters, such as  , to denote scalar random variables, and boldface lowercase and uppercase letters to denote vectors and matrices, respectively. We use , , and   to denote the Moore–Penrose pseudoinverse, the column span, and the null space of the matrix , respectively. The expectation operator is denoted by . The notation refers to the variance of . The notation describes a one-sided random process, which may also be written simply as . We write to refer to the sequence . The PSD of a wide-sense stationary process is denoted by
 . Notice that	. For any two functions	,	, we write the standard squared norm and inner product as and	,
 respectively, where denotes complex conjugation. For one-sided random processes and , the term denotes the
 mutual information rate between and , provided the limit exists. Similarly, for a stationary random process
 ,	denotes the differential entropy rate of .
 II. PRELIMINARIES
 A source encoder–decoder (ED) pair encodes a source into binary symbols, from which a reconstruction is generated. The end-to-end effect
 of any ED pair can be described by a series of reproduction functions , such that, for every
 	 	(3)
 where we write as a short notation for . Following [5], we say that an ED pair is causal if and only if it satisfies the following definition [5].
 Definition 1 (Causal Source Coder): An ED pair is said to be causal if and only if its reproduction functions are such that
  
  
 It also follows from Definition 1 that an ED pair is causal if and only if the following Markov chain holds for every possible random input process  :
 	 	(4)
 It is worth noting that if the reproducing functions are random, then this equivalent causality constraint must require that (4) is satisfied for each realization of the reproducing functions
 .
 be the total number of bits that the decoder has received when it generates the output subsequence  . Define   as the random binary sequence that contains the bits that the decoder has received when   is generated. Notice that   is, in general, a function of all source samples, since the binary coding may be noncausal, i.e.,   may be generated only after the decoder has received enough bits to reproduce
 , with . We highlight the fact that even though may contain bits which depend on samples with , the random sequences   may still satisfy (4), i.e., the ED pair can still be causal. Notice also thatis a random variable, which depends on  , the functions , and on the manner in which the source is encoded into the binary sequence sent to the decoder.
 For further analysis, we define the average operational rate of an ED pair as [5]
 	 	(5)
 In the sequel, we focus only on the MSE as the distortion measure. Accordingly, we define the average distortion associated with an ED pair as
 	 	(6)
 The aforementioned notions allow us to define the operational causal RDF as follows:
 Definition2: Theoperationalcausal RDFforasource  is defined as [5]
  	(7)
  
 We note that the operational causal RDF defined previously corresponds to the OPTA of all causal ED pairs.
 In order to find a meaningful information-theoretical counter-
  
 Also, from the data-processing inequality [13], it follows immediately that
 	 	(9)
 where the last inequality turns into equality for a causal ED pair, since in that case (4) holds. Thus, combining (5), (8), and (9)
  
 (10) This lower bound motivates the study of an information-theoretic causal RDF, as defined in the following.
 Definition 3: The information-theoretic causal RDF for a source  , with respect to the average MSE distortion measure, is defined as
  
 where the infimum is over all processes such that and such that (4) holds.  
 The aforementioned definition is a special case of the nonanticipative epsilon-entropy introduced by Pinsker and Gorbunov, which was shown to converge to Shannon’s RDF, for Gaussian stationary sources and in the limit as the rate goes to infinity [8],
 [9].
 In the noncausal case, it is known that for any source and for any single-letter distortion measure, the OPTA equals the information-theoretic RDF [13]. Unfortunately, such a strong equivalence between the OPTA and the information-theoretic
 RDF does not seem to be possible in the causal case [i.e., for
  ]. (One exception is if one is to jointly and causally encode an asymptotically large number of parallel Gaussian sources,inwhichcase   canbeshowntocoincidewiththe OPTA of causal codes.) Nevertheless, as outlined in Section I, it is possible to obtain lower and upper bounds to the OPTA of causal codes from	. Indeed, and to begin with, since , it follows directly from (7) and (10) that
 	 	(11)
 The previous inequality in (11) is strict, in general, and becomes equality when the source is white or when the rate tends to infinity. Also, as it will be shown in Section III, for Gaussian sources, does not exceed by more than approx-
 imately 0.254 bits/sample, and thus an upper bound to  can be obtained from .
 For completeness, and for future reference, we recall that for any MSE distortion , the RDF for a stationary Gaussian source with PSD is equal to the associated informationtheoretic RDF, given by the “reverse water-filling” equations [7] (12a)
 (12b)
 Although, in general, it is not known by how much  exceeds  , for Gaussian stationary sources one can readily
  (13)
 can be realized causally.
 More generally, it is known that, for any source, the mutual information across an AWGN channel [which satisfies (4)] introducing noise with variance  , exceeds
 Shannon’sRDF   byat most0.5 bits/sample, see,e.g., [11]. Thus, we have
  
 	 	(15)
 Until now it has been an open question whether a bound tighter than (15) can be obtained for sources with memory and at general rate regimes [10]. In Sections IV–VI, we show that for Gaussian sources, this is indeed the case. But before focusing on upper bounds for  , its operational importance will be established by showing in the following section that, for Gaussian sources, the OPTA does not exceed   by more than approximately 0.254 bits/sample.
 III. UPPER BOUNDS TO  
 In this section, we show that, for any Gaussian source
 and, an upper bound to can be readily obtained from by adding (approximately) 0.254 bits per sample to . This result is first formally stated and proved for finite subsequences of any Gaussian source. Then, it is extended to Gaussian stationary processes. We start with two definitions.
 Definition 4: The causal information-theoretic RDF for a zero-mean Gaussian random vector of length is defined as
 (16)
 where the infimum is taken over all output vectors satisfying the causality constraint
  
  
 Definition 5: The operational causal RDF for a zero-mean Gaussian random vector of length   is defined as
 	 	(19)
  
 We will also need the following result [14, Lemma 1]:
 Lemma 1: Let    be two random vectors with zero mean and the same covariance matrix, i.e., , and having the same cross-covariance matrix with respect to , that is, . If
 and   are jointly Gaussian, and if	has any distribution, then
 	 	(20)
 If furthermore , then equality is achieved in (20) if and only if with and being jointly Gaussian.
  
 Notice that if one applies Lemma 1 to a reconstruction error with which the output sequence satisfies the causality constraint (4), then the Gaussian version of the same reconstruction error will also produce an output causally related with the input. To see this, let
  
 be the Cholesky factorization of the covariance matrix of the random vector , , where is a lower triangular matrix. This allows one to write as
 	 	(21)
 where   satisfy (4). Then, there exists a set of reproduction functions satisfying the conditions of Definition 1 which generate each partial vector  . Specifically, for any given , there exists a function , such that . From (21) and given that
 is lower triangular, we have that	is fully determined by
 and thus , for some function . From this and the fact that is independent of , we have that
 	 	(22)
 where denotes probabilistic independence. On the other hand, for each , , let be the minimum mean square error (MMSE) linear estimator of
 given . Then, adopting the notation for the -bytop-left corner submatrix of a matrix , we have that
  
  
 where   follows from (22) and all the subsequent equalities stem from (21) and from the fact that is lower triangular. Therefore, since the residual is uncorrelated to  , it holds that
  	(23)
 Now, since have the same second-order statistics as , it follows from (23) that
 	 	(24)
 which, recalling that is jointly Gaussian with , implies that satisfy (4) too.
 We are now in the position to state the first main result of this section.
 Lemma2: Foranyzero-meanGaussian randomvectorsource
  
 The proof of Lemma 2 is presented in Section IX.
 The result stated in Lemma 2 for Gaussian random vector sources is extended to Gaussian stationary processes in the following theorem (the second main result of this section).
 Theorem 1: For a zero-mean Gaussian stationary source , and
 (26)
  
 The proof of Theorem 1 can be found in Section X.
 The fact that   for Gaussian sources allows one to find upper bounds to the OPTA of causal codes by explicitly finding or upper bounding  . This is accomplished in the following sections.
 IV.   FOR FIRST-ORDER GAUSS–MARKOV PROCESSES
 In this section, we will find   when the source is a firstorder Gauss–Markov process. More precisely, we will show that the information-theoretic causal RDF  , which is associated with an average distortion constraint, coincides with the expression for the SRDF on the RHS of (1) obtained in [2] for a per-sample distortion constraint. To do so, and to provide also a constructive method of realizing the SRDF as well as  , we will start by stating an alternative derivation of the SRDF for scalar source sequences of length  . In this case, from its definition in [2, Def. 5.3.5, p. 147], the SRDF takes the following form:
 	 	(27)
 where the infimum is over all conditional distributions (of  given  ) satisfying the causality constraint (17) and the distortion schedule constraints
 	 	(28)
 Before proceeding, it will be convenient to introduce some additionalnotation.Foranyprocess ,wewrite , , to denote the random column vector and adopt the shorter notation . For any two random vectors ,
  , we define	,	.
 It was already stated in Lemma 1 that the reconstruction vector which realizes mutual information between a Gaussian source vector and for any given MSE distortion constraint, must be jointly Gaussian with the source. This holds, in particular, for a realization of the SRDF with distortion schedule  . In the next theorem, we will obtain an explicit expression for this RDF and prove that in its realization, the sample distortions
  
 Fig. 1. Recursive Procedure 1 at its th iteration. Starting from known covariance matrices   , their next partial rows and columns are found. The numbers indicate the step in the algorithm which reveals the corresponding part of the matrix.
 step   is responsible of revealing the partial rows and columns indicated by number   in the figure.
 The aforementioned results are formally stated in the following theorem, which also gives an exact expression for the SRDF of first-order Gauss–Markov sources. 
 the constraints  . Under the latter interpretation, nothing precludes one from choosing an arbitrarily large value for, say  , yielding an arbitrarily large value for the second term in the summation on the RHS of (33), which is, of course, inadequate.
 We are now in a position to find the expression for for first-order Gauss–Markov sources. This is done in the following theorem, whose proof is contained in Section XII.
 Theorem 3: For a stationary Gaussian process
  where
 The technique applied to prove Theorems 2 and 3 does not seem to be extensible to Gauss–Markov processes of order greater than 1. In the sequel, we will find upper bounds to   for arbitrary (any order) stationary Gaussian sources.
 V. CLOSED-FORM UPPER BOUNDS
 In order to upper bound the difference between and for arbitrary stationary Gaussian sources, we will start this section by defining an upper bounding function for , denoted by . Wewill then derive three closed-form upper bounding functions to the rate-loss , applicable to any Gaussian stationary process. Two of these bounds are
 strictly smaller than 0.5 bit/sample for all distortions
  .
 We begin with the following definition.
 Definition 6 (Causal Stationary RDF): For a stationary , the information-theoretic causal SRDF
 1) jointly stationary with the source;
 3) Markov chain (4) holds.
  
 Next, we derive three closed-form upper bounding functions to that are applicable to arbitrary zero-mean stationary Gaussian sources with finite differential entropy rate. This result is stated in the following theorem, proved in Section XIII:
 Theorem 4: Let be a zero-mean Gaussian stationary source with PSD with bounded differential entropy rate and variance . Let denote Shannon’s RDF for [given by (12)], and let denote the quadratic Gaussian RDF for source-uncorrelated distortions for the source defined in (13). Let denote the information-theoretic causal RDF (see Definition 3). Then, for all
 (36)
 are given in (37)–(39), respectively,
 shown at the bottom of the page, where
 (40)
 with being any nonnegative scalar with which (40) exists and such that .	 
 Notice that is independent of , being therefore numerically simpler to evaluate than the other bounding functions introduced in Theorem 4. However, as is decreased away from and approaches , becomes very loose. In fact, it can be seen from (110a) that for , the gap between and is actually upper bounded by , which is of course tighter than , but requires one to evaluate .
 It is easy to see that time-sharing between two causal realizationswithdistortions , andrates , yields an output process which satisfies causality with a rate-distortion pair corresponding to the linear combination of ,
 . Thus, in some cases, one could get a bound tighter than by considering the boundary of the convex hull of the region above and then subtracting . However, such bound would be much more involved to compute, since it requires to evaluate not only  , but also the already mentioned convex hull.
 It is also worth noting that the first term within the erator on the RHS of (39) becomes smaller when is reduced. This difference, which from Jensen’s inequality is
  
 Fig. 2. AWGN channel within a “perfect reconstruction” system followed by the causal denoising filter	.
 always nonnegative, could be taken as a measure of the “nonflatness” of the PSD of (especially when ). Indeed, as   approaches a white process, tends to zero.
 It can be seen from (36) that provides the tightest upper bound for the information-theoretic RDF among all bounds presented so far. Although it does not seem to be feasible to obtain a closed-form expression for , we show in the next section how to get arbitrarily close to it.
 VI. OBTAINING  
 In this section, we present an iterative procedure that allows one to calculate   with arbitrary accuracy, for any .
 In addition, we will see that this procedure yields a characterization of the filters in a dithered feedback quantizer [15] that achievean operationalrate which isupper boundedby   [bits/sample].
 A. Equivalent Problem
 To derive the results mentioned previously, we will work on a scheme consisting of an AWGN channel and a set of causal filters, as depicted in Fig. 2. In this scheme, the source   is Gaussian and stationary, with PSD  , and is assumed to
 have finite differential entropy rate. In Fig. 2, the noise  
 is a zero-mean Gaussian process with i.i.d. samples, independent of . Thus, between and lies the AWGN channel . The filter is stable and strictly causal, i.e., it has at least a one sample delay. The filters   are causal and stable. The idea, to be developed in the remainder of this section, is to first show that with the filters that minimize the variance of the reconstruction error for a fixed ratio  , the system of Fig. 2 attains a mutual information rate between source and reconstruction equal to  , with a reconstruction MSE equal to  . We will then show that finding such filters is a convex optimization problem, which naturally suggests an iterative procedure to solve it.
 In order to analyze the system in Fig. 2, and for notational convenience, we define
  
 	 	(42)
  
 Fig. 3. Equivalent block diagram depicting the output as the sum of
 and , where is an i.i.d. zero-mean Gaussian process independent of	.
 see Fig. 2. Therefore,   is the signal transfer function of the system.
 The perfect reconstruction condition (41) induces a division ofrolesinthesystem, whichwilllatertranslateintoa convenient parametrization of the optimization problem associated with it. On the one hand, because of (41), the net effect of the AWGN channel and the filters , , and is to introduce
 (colored) Gaussian stationary additive noise, namely , independent of the source. The PSD of this noise, , is given by
 	 	(43)
 The diagram in Fig. 3 shows how the signal transfer function and the noise transfer function act upon and to yield the output process.
 On the other hand, by looking at Fig. 2, one can see that plays also the role of a denoising filter, which can be utilized to reduce additive noise at the expense of introducing linear disacts upon the Gaussian stationary corrupted by additive Gaussian stationary noise
 On the RHS of (44), the first term is the variance of the additive, source independent, Gaussian noise. The second term corresponds to the error due to linear distortion, that is, from the deviation of   from a unit gain.
 Since we will be interested in minimizing , for any given and , the filters and in Fig. 2 are chosen so as to minimize in (44), while still satisfying (41). From the viewpoint of the subsystem comprised of the filters , , and   and the AWGN channel, acts as an error frequency weighting filter, see (43). Thus, for any and , the filters   that minimize are those characterized in [15, Prop. 1], by setting in [15, eq. (20b)] equal to  . With the minimizer filters in [15], the variance of the source-independent error term is given by
 	 	(45)
 On the other hand, the filter   needs to be strictly causal and stable. As a consequence, it holds that
  
 which follows from Jensen’s formula [16] (see also the Bode Integral Theorem in [17]).
 Thus, from (44) and (45), if one wishes to minimize the reconstruction MSE by choosing appropriate causal filters in the
 , and for any given	and the fre-
 (46a)
 where   denotes the space of all frequency responses that can be realized with causal filters.  
 Now, we can establish the equivalence between solving Op-
  , if the filters	,
  
 From the aforementioned lemma, whose proof can be found in Section XIV, one can find   either by solving the minimization in Definition 6 or by solving Optimization Problem 1. In the following, we will pursue the latter approach. As we shall see, our formulation of Optimization Problem 1 provides a convenient parametrization of its decision variables. In fact, it makes itpossibletoestablishthe convexityofthecostfunctional defined in (46a) with respect to the set of all causal frequency responses involved. That result can be obtained directly from the following key lemma, proved in Section XV:
 Lemma 4: Define the sets of functions
 We can now prove the convexity of Optimization Problem 1.
 Lemma 5: For all   and for all  , Optimization Problem 1 is convex.	 
 Proof: With the change of variables  in (47), we obtain  , see (44). With this, Optimization Problem 1 amounts to finding the functions and that
 	 	(48a)
 	 	(48b)
 where
 	 	(49)
 Clearly, the space of frequency responses associated with causal transfer functions   is a convex set. This implies that   is a convex set. In addition, is also a convex set, and from Lemma 4, is a convex functional. Therefore, the optimization problem stated in (48), and thus
 Optimization Problem 1, are convex. This completes the proof.
  
 B. Finding   Numerically
 Lemma 5 and the parametrization in Optimization Problem 1 allow one to define an iterative algorithm that, as will be shown later, yields the information-theoretic causal RDF. Such algorithm is embodied in iterative Procedure 2.
  
 Step 5: Return to step 3.
 Notice that after solving Step 3 in the first iteration of Procedure 2, the MSE is comprised of only additive noise independent of the source.4 Step 4 then reduces the MSE by attenuating source-independent noise at the expense of introducing linear distortion. Each step reduces the MSE until a local (or global) minimum of the MSE is obtained. Based upon the convexityofOptimization Problem 1, the following theorem, which is the main technical result in this section, guarantees convergence to the global minimum of the MSE, say  , for a given end-to-end mutual information. Since all the filters in Optimization Problem 1 are causal, the mutual information achieved at this global minimum is equal to  .
 Theorem 5 (Convergence of Iterative Procedure 2): Iterative Procedure 2 converges monotonically to the unique and that realize  . More precisely, letting denote the
 4Indeed, after solving Step 3 for the first time, the resulting rate is the quadratic Gaussian RDF for source uncorrelated distortionsintroduced in [14] [see also (14)].
  
 Fig. 4. Uniform scalar quantizer and dither signals , , forming an SDUSQ, replacing the AWGN channel of the system from Fig. 2.
  
 Fig. 5. (in bits/sample) and several upper bounding functions for   for zero-mean unit variance white Gaussian noise filtered through . The resulting source variance is 5.26.
 MSE obtained after the  th iteration of Iterative Procedure 2 aimed at a target rate  , we have that
  
 and
  
  
 Proof: The result follows directly from the fact that Optimization Problem 1 is strictly convex in and , which was shown in Lemma 4, and from Lemma 3.  
 The aforementioned theorem states that the stationary information-theoretic causal RDF can be obtained by using Iterative Procedure 2. In practice, this means that an approximation arbitrarily closeto   for a given   can be obtained if sufficient iterations of the procedure are carried out.
 The feasibility of running Iterative Procedure 2 depends on being able to solve each of the minimization sub-problems involved in steps 3 and 4. We next show how these subproblems can be solved.
 C. Solving Step 3
 If   is given, the minimization problem in Step 3 of Iterative Procedure 2 is equivalent to solving a feedback quantizer design problem with the constraint and with error weighting filter . Therefore, the solution to Step 3 is given in closed form by [15, eqs. (20), (29), and (31b)], where   in [15, eq. (20b)] is replaced by  . The latter
  
 Fig. 6. (in bits/sample) and several upper bounding functions for for zero-mean unit variance white Gaussian noise filtered through . The resulting source variance is 6.37.
 equations in [15] characterize the frequency response magnitudes of the optimal , and given . The existence of rational transfer functions , , and arbitrarily close (in an sense) to such frequency response magnitudes is also shown in [15].
 D. Solving Step 4
 Finding the causal frequency response that minimizes   for a given is equivalent to solving
 	 	(50)
 for a given  , where   is as defined in (49). Since  are convex, (50) is a convex optimization problem. As such, its global solution can always be found iteratively. In particular, if is constrained to be an th order finite impulse response (FIR) filter with impulse response  , such that   denotes the discrete-time
 Fourier transform, then
  
 is a convex functional. The latter follows directly from the convexity of and the linearity of . As a consequence, one can solve the minimization problem in Step 4, to any degree of accuracy, by minimizing over the values of the impulse response of , using standard convex optimization methods (see, e.g., [18]). This approach also has the benefit of being amenable to numerical computation.
 It is interesting to note that if the order of the denoising filter   were not a priori restricted, then, after Iterative Procedure
 2 has converged to  , the obtained   is the causal
  magnitude of	back–Leibler distance, see, e.g., [13, p. 254]. The inequality in
 Wiener filter (i.e., the MMSE causal estimator) for the noisy signal that comes out of the perfect reconstruction system that precedes  . Notice also that one can get the system in Fig. 7 to yield a realization of Shannon’s using Iterative Procedure 1 by simply allowing to be noncausal. This would yield a system equivalent to the one that was obtained analytically in [10]. An important observation is that one could not obtain a realization offrom such a system in one step by simply replacing (a noncausal Wiener filter) by the MMSE causal estimator (that is, a causal Wiener filter). To see viously matched filters , and would no longer be optimal for . One would then have to change  , and then again, and so on, thus having to carry out infinitely many recursive optimization steps. However, a causally truncated version of the non causal Wiener filter   that realizes Shannon’s RDF could be used as an alternative starting guess in Step 2 of the iterative procedure.
 E. Achieving   Bits/Sample Causally
 If the AWGN channel in the system of Fig. 2 is replaced by an SDUSQ, as shown in Fig. 4, then instead of the noise
  , we will have an i.i.d. process independent of  , whose samples are uniformly distributed over the quantization interval [19]. The dither signal, denoted by  , is an i.i.d. sequence of uniformly distributed random variables, independent of the source. Let be the quantized output of the SDUSQ. Denote the resulting input and the output to the quantizer, before adding and after subtracting the dither, respectively, as and , and let be the quantization noise introduced by the SDUSQ. Notice that the elements of are independent, both mutually and from the source . However, unlike and , the processes and are not Gaussian, since they contain samples of the uniformly distributed process . We then have the following.
 Theorem 6: If the scheme shown in Fig. 4 uses the filters yielded by Iterative Procedure 2, and if long sequences of the quantized output of this system are entropy coded conditioned to the dither values in a memoryless fashion, then an operational rate   satisfying
 	 	(51)
 the last line of (52) is strict since the distribution of   is not Gaussian.
 The result follows directly by combining (52) with Lemma 3 and Theorem 5.	 
 InviewofTheorem6,andsinceanyEDpairusinganSDUSQ and linear time-invariant filters yields a reconstruction error jointly stationary with the source, it follows that the operational rate-distortion performance of the feedback quantizer thus obtained is within  
 from the best performance achievable by any ED pair within this class.
 Remark 2: When the rate goes to infinity, so does . In that limiting case, the transfer function tends to unity, and it follows from [15] that the optimal filters asymptotically satisfy , ,
 .
 Moreover, when , the system of Fig. 4 achieves which, in this asymptotic regime, coincides with
 	, with	tending to	.
 F. Achieving	Bits/Sample With Zero Delay
 If the requirement of zero-delay, which is stronger than that of causality, was to be satisfied, then it would not be possible to apply entropy coding to long sequences of quantized samples. This would entail an excess bit-rate not greater than 1 bit per sample, see, e.g., [13, Sec. 5.4]. Consequently, we have the following result.
 Theorem 7: The OPTA of zero-delay codes, say  , can be upper bounded by the operational rate of the scheme of Fig. 4 when each quantized output value is entropy-coded independently, conditioned to the current dither value. Thus
  
 The 0.254 bits/sample in (53), commonly referred to as the “space-filling loss” of scalar quantization, can be reduced by using vector quantization [11], [20]. Vector quantization could be applied while preserving causality (and without introducing delay) if the samples of the source were  -dimensional vectors. This would also allow for the use of entropy coding over
  -dimensional vectors of quantized samples, which reduces the extra 1 bit/sample at the end of (53) to   bits/sample (see [13, Th. 5.4.2]).
 G. Additive Rate Loss of Causality Arises From Two Factors
 It is worth noting that Lemma 3and the aboveanalysis reveals an interesting fact: the rate loss due to causality for Gaussian sources with memory, i.e., the difference between the OPTA of causal codes and  , is upper bounded by the sum of two terms. The first term is 0.254 bits/sample, and results from the space filling loss associated with scalar quantization, as was also pointed out in [6] for the high resolution situation. This term is associated only with the encoder. For a scalar Gaussian stationary source, such excess rate can only be avoided by jointly quantizing blocks of consecutive source samples (vector quantization), i.e., by allowing for noncausal encoding (or by encoding several parallel sources). The second term can be attributed to the reduced denoising capabilities of causal filters, compared to those of noncausal (or smoothing) filters. The contribution of the causal filtering aspect to the total rate-loss is indeed  . This latter gap can also be associated with the performance loss of causal decoding.
 As a final remark, we note that the architecture of Fig. 2, which allowed us to pose the search of   as a convex optimization problem, is by no means the only scheme capable of achieving the upper bounds (52) and (53). For instance, it can be shown that the same performance can be attained removing either  or in the system of Fig. 2, provided an entropy coder with infinite memory is used. Indeed, the theoretical optimality (among causal codes) of the differential pulse-code modulationarchitecture,with predictivefeedbackand causalMMSE estimation at the decoding end, has been shown in a different setting [21].
 VII. EXAMPLE
 To illustrate the upper bounds presented in the previous sections, we here evaluate , , and , and calculate an approximation of via Iterative Procedure 2, for two Gaussian zero-mean AR-1 and AR-2 sources. These sources were generated by the recursion
 	 	(54)
 where the elements of the process   are i.i.d. zero-mean unit-variance Gaussian random variables.
 Iterative Procedure 2 was carried out by restricting   to be an eight-tap FIR filter. For each of the target rates considered, the procedure was stopped after four complete iterations.
 The first-order source (Source 1) was chosen by setting the values of the coefficients in (54) to be  . This amounts to zero-mean, unit variance white Gaussian noise filtered through the coloring transfer function  . The second-order source (Source 2) consisted of zero-mean, unit variance white Gaussian noise filtered through the coloring transfer function . The resulting upper bounds for Source 1 and Source 2 are shown in Figs. 5 and 6, respectively. As predicted by (103) and (39), all the upper bounds for derived in Section V converge to
 in the limit of both large and small distortions (i.e., when and	, respectively).
 For both sources, the gap between and is significantly smaller than 0.5 bits/sample, for all rates at which was evaluated. Indeed, this gap is smaller than 0.22
 bit/sample for both sources.
 For the first-order source, the magnitude of the coefficients of the FIR filter obtained decays rapidly with coefficient index. For example, when running five cycles of Iterative Procedure 2, using a tenth-order FIR filter for , for Source 1 at bits/sample, the obtained was
 Such fast decay of the impulse response of suggests that, at least for AR-1 sources, there is little to be gained by letting   be an FIR filter of larger order. (It is worth noting that, in the iterative procedure, the initial guess for is a unit scalar gain.) The frequency response magnitude of is plotted in Fig. 7, together with and the resulting frequency responsemagnitude after fouriterations onSource1 for a target rate of	.
 Notice that for Source 1, after four iterations of Iterative Procedure 1, the obtained values for are almost identical to
  , evaluated according to (35). This suggests that Iterative Procedure 2 has fast convergence. For example, when applying four iterations of Iterative Procedure 2 to Source 1 with a target rate of 0.2601 bits/sample, the distortions obtained after each iteration were 1.6565, 1.6026, 1.6023, and 1.6023, respectively. For the same source with a target rate of 0.0441 bits/sample, the distortion took the values 4.0152, 3.9783, 3.9783, and 3.9782 as the iterations proceeded. A similar behavior is observed for other target rates, and for other choices of   in (54) as well. Thus, at least for AR-1 sources, one gets close to the global optimum   after just three iterations.
 VIII. CONCLUSION
 In this paper, we have obtained expressions and upper bounds to the causal and zero-delay rate distortion function for Gaussian stationary sources and MSE as the distortion measure. We first showed that for Gaussian sources with bounded differential entropy rate, the causal OPTA does not exceed the information-theoretic RDF by more than approximately 0.254 bits/sample. After that, we derived an explicit expression for the information-theoretic RDF under per-sample MSE distortion constraints using a constructive method. This result was then utilized for obtaining a closed-form formula for the causal information-theoretic RDF   of first-order Gauss–Markov sources under an average MSE distortion constraint.
 We then derived three closed-form upper bounding functions to the difference between   and Shannon’s RDF. Two of these bounding functions are tighter than the previously best known bound of 0.5 bits/sample, at all rates. We also provided a tighter fourth upper bound to  , named  , that is constructive. More precisely, we provide a practical scheme that attains this bound, based on a noise-shaped predictive coder consisting of an AWGN channel surrounded by pre-, post-, and feedback filters. For a given source spectral density and desired distortion, the design of the filters is convex in their frequency responses. We proposed an iterative algorithm, which is guaranteed to converge to the optimal set of unique filters. Moreover, the mutual information obtained across the AWGN channel, converges monotonically to  . Thus, one avoids having to solve the more complicated minimization of the mutual information over all possible conditional distributions satisfying the distortion constraint. To achieve the upper bounds on the operational coding rates, one may simply replace the AWGN channel by a subtractively dithered scalar quantizer and using memoryless entropy coding conditioned to the dither values.
 IX. PROOF OF LEMMA 2
 We will first show that   can be realized by a vector AWGN channel between two square matrices. It was already established in Lemma 1 that an output   corresponds to a realization of   only if it is jointly Gaussian with the source
  . From this Gaussianity condition, the MMSE estimator of
 from	, say	, is given by
 	 	(55)
 where the inverse of   exists from the fact that   has bounded differential entropy. It is clear from (55) and the joint Gaussianity between and that the causality condition is satisfied if and only if the matrix
 	 	(56)
 On the other hand, the distortion constraint (18) can be expressed as
 (57)
 From the definition of	, for every	, there exists an
 output vector jointly Gaussian with such that and satisfy (56), (57) and
 (58)
 We will now describe a simple scheme which is capable of reproducing the joint statistics between and any given jointly Gaussian with   satisfying (56)–(58).
 Suppose is first multiplied by a matrix yielding the random vector . Then a vector with Gaussian i.i.d. entries with unit variance, independent from , say , is added to  , to yield the random vector . Finally, this result is multiplied by a matrix to yield the output
 	 	(59)
 On the other hand, the joint second-order statistics between  and   are fully characterized by the matrices
 (60)
 (61)
 It can be seen from these equations that all that is needed for the system described previously to reproduce any given pair of covariance matrices   is that the matrices and
 satisfy
 (62)
 (63)
  	 		
 		 	
 Thus,   can be chosen, for example, as the lower triangular matrix in a Cholesky factorization of  . With this, a tentative solution for could be obtained as , which would satisfy (62) if and only if. The latter holds if and only if (recall that is nonsingular since has bounded differential entropy). We will now show that this condition actually holds by using a contradiction argument. Suppose
 . Since , the former supposition is equivalent to . If this were the case, then there would exist such that and . The latter, combined with (63), would imply
 . One could then construct the scalar random variable	, which would have nonzero variance. The MSE of predicting	from	is given by
  
 From this, and in view of the fact that is Gaussian with nonzero variance, we conclude that would be unbounded. However, by construction, the Markov chain holds, and therefore by the Data Processing Inequality we would have that
 , implying that is unbounded too. This contradicts the assumption that is a realization of , leading to the conclusion that . There-
 fore, the choice
 	 	(64)
 is guaranteed to satisfy (62), and thus for every  , there exist matrices and which yield an output vector satisfying (56)–(58).
 On the other hand, we have that
 	 	(65)
 The first equality follows from the data-processing inequality and the fact that   is obtained deterministically from  . To prove that the second equality in (65) holds, we will prove that
  . We first have, from (64), that   , which combined with the identity  reveals immediately that (64) implies
 	 	(66)
 Second, we note that	and that	can be decomposed
 as
 	 	(67)
 where denotes the orthogonal projection operator onto a given subspace . Since and is orthogonal to the other two terms on the RHS of (67), we have that
  (68)
 where the last equality follows from the fact that   is Gaussian i.i.d., which implies that   is independent of the other two terms in the expression. On the other hand, from (67)
  
 Thus, we have (69)–(72), shown at the bottom of the page, where   comes from the data-processing inequality, and
 follows from the fact that and from (66). To complete the proof of the second equality in (65), we note that the data-processing inequality also yields .
 	Therefore, if	and	yield an output	such that
 	, then	.
 Finally,ifwekeepthe	and	satisfyingtheaforementioned conditions and replace the noise	by the vector of noise samples	with unit variance introduced by	independently operating SDUQS [11], with their outputs being jointly entropycoded conditioned to the dither, then the operational data rate would be upper bounded by [11]
 where is the output of the ECDQ channel. Since the distortion yielded by the SDUQs is the same as that obtained with the original Gaussian channel, we conclude that
  
 X. PROOF OF Theorem 1 We will start by showing that
 	 	(73)
 First, following exactly the same proof as in Lemma 6 in the Appendix, it is straightforward to show that
 	 	(74)
 Now, consider the following family of encoding/decoding schemes. For some positive integer , the entire source sequence is encoded in blocks of contiguous samples. Encoding and decoding of each block is independent of the encoding and decoding of any other block. As in the scheme described in the second part of the proof of Lemma 2, each source block is multiplied by the optimal   pre-processing matrix, the resulting block being encoded and decoded utilizing   parallel and independent SDUSQs, with their outputs jointly entropy coded conditioned to the dither values. When decoded, the result is then multiplied by the optimal post-processing matrix described in the proof of Lemma 2.
 For such an ED pair, and from (5), the operational rate after samples have been reconstructed is
 (75)
 where denotes rounding to the nearest larger integer (since the th sample is reconstructed only after blocks of length are decoded). On the other hand, since the variance of each reconstruction error sample cannot be larger than the variance ofthe source, we havethatthe averagedistortionassociated with the first samples is upper bounded as
 (76)
 where denotes rounding to the nearest smaller integer. Therefore, for any finite , the average distortion of this scheme equals when (i.e., when we consider the entire source process). Also, from (75) and (5), letting  , we conclude that
 	 	(77)
 for every finite . Our aim is to use this result to show that . Since (77) is valid only for
 finite values of , we must resort to analyzing the convergence of as . First of all, since   is bounded,
 (78)
 such
 that
 (79)
 Since
 XI. PROOF OF THEOREM 2
 From Lemma 1, for any given reconstruction-error covariance matrix, the mutual information is minimized if and only if the output is jointly Gaussian with the source. In addition, for any givenmutual information between   and a jointly Gaussian
 , the variance of every reconstruction error sample is minimized if and only if is the estimation error resulting from estimating from , that is, if and only if
 	 	(80)
 which for Gaussian vectors implies   are indepen-
 dent, and therefore
 	 	(81)
 Thus, hereafter we restrict the analysis to output processes jointly Gaussian with and causally related towhich also satisfy (80). For any such output process, say, , the following holds:
  
  .
 and thus equality holds in (83) if and only if the following Markov chain is satisfied:
 	 	(88)
 Finally, (85) and (86) follow because	satisfies (81) for all
 .
 	Thus, the mutual information	of every output	that
 is a candidate to constitute a realization of
 is lower bounded by the RHS of (87), which in turn depends only on the error variances associated with . We shall now see that this lower bound is minimized by a unique set of error variances, and then show that the resulting bound is achievable while having these error variances.  Revisiting (84)–(86), we have that
 With this, and since the RHS of (87) decreases when any error variance   increases, the minimum value of the RHS of (87) subject to the constraints
 	 	(90)
 is attained when these variances satisfy	, for
   [see (29)]. Therefore, for all outputs causally related to and jointly Gaussian with satisfying the distortion con-
 Now, we will show that for any distortion schedule , the output   yielded by the recursive algorithm of Procedure 1 is such that	equals the lower bound (91), thus being a realization of	.
 We will first demonstrate that satisfies the causality Markov chain
 	 	(92)
 and the conditions (80) (MMSE), and (88) (Source’s Past Independence) which are necessary and sufficient to attain equality in (91).
 Causality Condition (92): Let
 pose   satisfies causality. Then, since , it follows from (56) that the top-left square submatrix   is lower triangular, being
 given by
 	 	(93)
 Then, Step 2 of the algorithm is equivalent to
 	 	(94)
 	 	 
  		
 This means that the top entries in the th column of   depend only on the entries of above its th row. Recalling that , we conclude that is also lower triangular, and thus also satisfies causality. Notice that for any given andsatisfying causality up to sample , the vectoryielded by Step 2 is the only vector consistent with satisfying causality up to the  th sample.
 MMSECondition(80): Step1guaranteesthat(80) issatisfied for . Steps 3, 4 and 5 mean thatfor all. Therefore, the reconstruction vector yielded by the above algorithm satisfies (80) for all .
 Source’s Past Independence (88): Since all variables are jointly Gaussian, condition (88) is equivalent to
 From steps 1, 3 and 4 it follows that  
   . Substitution of this into (96) and
 the result into (95) leads directly to (30). Thus, (88) is satisfied for all  .
 Since the above algorithm yields an output which satisfies (92), (80), and (88), for all , this output attains equality in (91), thus being a realization of . Notice that once the distortions are given, each step in the recursive algorithm yields the only variances and covariances that satisfy (92), (80) and (88). Therefore, for any given distortion schedule , the latter algorithm yields the unique output that realizes . This completes the proof.	 
 XII. PROOF OF THEOREM 3
 Consider the first samples of input and output. The average distortion constraint here takes the form
 (97)
 Then, we have (98), shown at the bottom of the page, where the last inequality follows from Jensen’s inequality and the fact that is a convex function of  . Equality is achieved if and
 onlyifalldistortions	equalsomecommonvalueforall
 . Given that the RHS of (98) is minimized when constraint (97) is active (i.e., by making ), we can attain equality in (98) and minimize its RHS by picking
 (99)
 For this choice to be feasible, the distortion	must satisfy
 (89), which translates into the constraint
  (102)
 XIII. PROOF OF THEOREM 4
 The first inequality in (36) follows directly from definitions 3 and 6. For a plain AWGN channel with noise variance  , the mutual information between source and reconstruction is
  [14]
 In both cases the end-to-end distortion can be reduced by placing a scalar gain after the test channel. The optimal (minimum MSE) gain is  . The mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it. However, now the resulting end-to-end distortion is . Therefore, for a given end-to-end distortion , the distortion between the source and the signal before the optimal scalar gain is
  
 the distortion obtained in (12) when   is substituted by  , we find that
  
  
 (109)
 where (106) follows from (12), (104), and (105) and by noting that  , (108) stems from (104), and (109) follows from Jensen’s inequality. Notice that the RHS of (109) equals the first term on the RHS of (39).
 The middle term on the RHS of (39) follows directly from (15). Finally, for distortions close to  , a bound tighter than (109) can be obtained from (103a) as follows:
  
 (110b)
 which is precisely the third term on the RHS of (39). In the above, (110a) holds trivially since  , and
 (110b) follows from Jensen’s inequality. Therefore, equality holds in (110b) if and only if   is white. The validity of the chain of inequalities in (36) follows directly from (103) and (110). This completes the proof.  
 XIV. PROOF OF LEMMA 3
 The idea of the proof is to first show that if the distortion  
  
 (111)
 Immediately afterward we prove that, despite the distortion and causality constraints, the scheme in Fig. 2 has enough degrees of freedom to turn all the above inequalities into equalities. That means that if we are able to globally infimize  over the filters of the system while satisfying the distortion and causality constraints, then that infimum, say  , must satisfy
  .
 We now proceed to demonstrate the validity of (111) and to state the conditions under which equalities are achieved. The first equality in (111) follows from the fact that is a Gaussian i.i.d. process. Inequality stems from the following:
  
  
  
 (112)
 (113)
 (114)
 (115)
 (116)
 (117)
 (118)
 (119)
 where   is the signal at the output of , see Fig. 2. In the above, (112) follows from the fact that and are independent and from the fact that is strictly causal. As a consequence,   is independent of , for all . Inequality (113) holds from the property , with equality if and only if and are independent, i.e., if and only if is white. Similarly, (114) holds since the samples of are independent. By noting that is a linear combination of and , it follows immediately that is independent from upon knowledge of , which leads to (115). On the other hand, (116) stems from the fact that
 . Equality in (117) holds from the fact that, if is known, then can be obtained deterministically from
 , and vice-versa, see Fig. 2. Equality (118) follows from the fact that there exists no feedback from to , and thus the Markov chain holds. On the other hand,	, with equality if and only if is invertible for all frequencies for which . Finally, (119) follows directly from the
 Data Processing Inequality, with equality if and only if
 is invertible for all frequencies	for which	.
 Since is by definition an infimum, it follows that, for every , there exists an output process jointly Gaussian with , satisfying the causality and distortion constraints and such that	.
 Such output can be characterized by its noise PSD, say , and its signal transfer function, say , by using the model in Fig. 3.
 Therefore, all that is needed for the system in Fig. 1 to achieve
  
 chosen   in (121a) for simplicity and because, as we shall see next, we have enough degrees of freedom to do so without compromising rate/distortion performance. Solving the system of equations formed by (121a), (121c), and (121b), we obtain
  
 that their squared magnitudes equal their RHSs in (122). To do so, we will make use of the Paley-Wiener theorem (Theorem 8 in the Appendix).
 To begin with, we notice from Fig. 3, and since   is
  
  Substitution of the RHS of the second equation of (121c) into the above, together with the Paley–Wiener theorem, yields that there exists a causal, stable and minimum phase transfer func tion
   that satisfy (121), attaining equalities throughout and
  
  where
  
  
  
 all pairs   not satisfying (134). This completes the proof.	 
 APPENDIX
 Lemma 6: For any zero-mean Gaussian stationary source
  
 	 	(146)
 Proof: Suppose (146) does not hold, i.e., that
 	 	(147)
 for some . The definition of in (150) means that, , there exists such that
 	 	(148)
 Combining this inequality with (147), we arrive to
  
 (149)
 Since   can be chosen to be arbitrarily small, it can always be chosen so that  , which contradicts (147). Therefore
 (146) holds.	 
 Lemma 7: Let
 	 	(150)
 where lated to
 (151)
 Then, for any first-order Gauss–Markov source, the following holds:
 	 	(152)
  
 Proof: In Lemma 6 in the Appendix, it is shown that
 	 	(153)
 so	all	we	need	to	demonstrate	is	that
 	.	To	do	this,	we	simply	observe
 from Theorem 2 that if we construct an output process by using the recursive algorithm of that theorem, with the choice , for all , then this output process is such that equals .
 Therefore,	, concluding the proof.
 Proposition 1 (MMSE Column Correspondence): Let    be a Gaussian random vector source with covariance matrix  . A reconstruction Gaussian random vector   satisfies
 	 	(154)
 if and only if
 	 	(155)
  
 Proof: We have that
  
 (156) The proof is completed by noting that	if and only if	.
 Lemma 8 (MMSE Triangular Correspondence): Let
 , with , be a Gaussian random source vector with covariance matrix . A reconstruction Gaussian random vector satisfies
 	 	(157)
 if and only if
 	 	(158)
  
 Proof: Let us first introduce the notation , denoting the top-left submatrix of any given square matrix
 , with . From Proposition 1, it immediately follows that, for every
 	 	(159)
 which is equivalent to (158).	 
 Lemma 8 implies that, if the reconstruction is the output of a causal Wiener filter applied to the noisy source for some noise vector   [a condition equivalent to (157)], then and   have identical entries on and above their main diagonals.
 Paley–Wiener Theorem:
 Theorem 8 (see [22, p. 229]): Let   be a nonnegative function defined on  . There exists a unique stable, causal and minimum phase transfer function   such that   if and only if5
 	 	(160)
  
 and is such that , then
 (161)
  
 Proof: Let. From Jensen’s inequality and the fact that , we have
 5In [22, p. 229], it is stated that (160) is a sufficient condition for such a to exist. However, from [22, Note 2, p. 228] and the discrete-continuous equivalence in [22, p. 229], it follows that (160) is also necessary.
  
 ﻿ This paper with control system design subject to average data rate . By on , and a class of source , we establish lower and upper on the minimal average data rate to achieve a performance level . We also provide a specific source scheme , within the class , that is to achieve the desired performance level at average data below our upper bound . Our are based upon a recently framework to address control subject to average data rate . 
 Introduction 
 The study of control subject to communication recently received much attention in the control community see , e .., the in the special issue . Within this framework , a key question to the trade between control and communication . This paper on the interplay between average data rate in per sample and stationary performance in a class of control . 
 When stability is the sole control objective , the of guarantee that , for a noisy plant model and subject to mild on the noise statistics , it is possible to find causal , and such that the resulting closed loop system is mean square stable , if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant . This result a fundamental separation line between what is achievable in over digital and what is not , when the problem of interest is mean square stability see also the thorough discussion in the survey paper . 
 When performance subject to average data rate are sought , there are relatively available . There exist lower on the mean square norm of the plant state that make explicit the fact that , as the average data rate the absolute minimum for stability , the performance becomes arbitrarily poor when are present , . This irrespective of how the coder , and controller are chosen . Unfortunately , it is unclear whether or not these are tight in general . 
  
  
 A more general performance approach been in , . In those works , for separation and certainty equivalence have been in the context of quadratic stochastic for fully with data rate in the feedback path . If the a specific recursive structure , then certainty equivalence and a quasi separation principle hold . This result is interesting , but does not give a computable characterization of the optimal . A similar drawback is by the in . In that work , performance related are expressed in of the so sequential rate distortion function , which is very difficult to compute in general . For fully first order , an expression for the sequential rate distortion function . However , it is not clear from the in whether or not the sequential rate distortion function is tight see Section in . 
 Other works related to the performance of control subject to data rate are in and . The first work on noiseless state estimation subject to data rate under three different criteria . The case most relevant to this work an asymptotic in time quadratic criterion to measure the state reconstruction error . For such a measure , it is shown in that the bound established in is sufficient to achieve any asymptotic distortion level . This is , however , at the expense of arbitrarily large estimation for any given finite time . This feature of the solution the in too optimistic . On the other hand , non linear stochastic control over noisy , and a functional i . e ., not explicit characterization of the optimal control is . 
 In this paper , we focus on . Our main contribution is a characterization of upper and lower on the minimal average data rate that one to attain a given performance level as measured by the stationary variance of the plant output . To that end , we focus on a specific class of source that , as special , the studied in , . We also provide a specific source scheme , within the class , that is to achieve the desired performance level at average data below our upper bound . Instrumental to our is the characterization of the minimal signal to noise ratio that a closed loop performance level in a related class of . , . 
 The remainder of this paper is organized as : Section the problem of interest in this paper , and the class of considered source . Section , for the class of source in Section , a relationship between average data and an internal . Section on the interplay between and closed loop performance . These are then used in Section to present our main contribution . Section . 
 Notation : the set of real , the set of strictly positive real , for the magnitude absolute value of . is the set of 
 all proper and real rational transfer , all stable transfer in , and U all the , all the strictly proper transfer in 
 minimum phase transfer in . L and L are defined as usual , and the associated are by and , respectively . 
  
 Figure : control system . 
 Setup and Problem Definition 
 . The setup 
 This paper on the of Figure . In that figure , is a plant , u is the control input , is the plant output , an input disturbance . The feedback path in Figure an error free digital channel , and thus quantization becomes mandatory . This task is carried out by an whose output correspond to the binary . These are then back into real by a . It is clear that the and also embody a controller for the plant . As will become clear as we proceed , a distinction between the controller task and the task is rather artificial in our setup . 
 We introduce the following : 
 Assumption The strictly proper , free of unstable hidden , and no or on the unit circle . The initial state of the plant , and the disturbance , are such that 
 , is jointly second order ; is stationary and spectral factor U . In this paper we focus on source i . e ., of the following type : 
 Definition The source scheme of Figure is said to be linear if and only if its output u are related via 
 u , 
 a second order zero mean i . i .. sequence , is independent of and , are the transfer of with deterministic initial . 
 The class of linear the class of so independent and i . i .. in , . We acknowledge that it is a restricted class of . However , its simplicity one to actually address control system design subject to average data rate . 
 Any linear source scheme can be written as shown in Figure a , where E and are proper transfer . The design of E and is however non trivial . Indeed , it is easy to see that designing E and to an optimal control problem subject to sparsity . To the best of our knowledge , the only known sufficient see that allow one to pose such as convex are not satisfied in our case . 
 by the previous discussion , we introduce the alternative of a linear source scheme shown in Figure , where , E ,, are the transfer of with 
  
  
 Figure : Two alternative of a linear source scheme . 
  
 Figure : The of Figure when a linear source scheme is employed . 
 deterministic initial . In the scheme of Figure , is redundant . However , as will become clear below , this over of linear source one to construct a convex optimization problem that is equivalent to that of designing and E in Figure a . 
 When the linear source scheme of Figure is used in the of Figure , the feedback system of Figure . For future reference we define 
 : the loop of Figure internally stable and well . 
 Remark In the remainder of this paper , when we refer to a linear source scheme , it must be understood that we refer to the specific architecture of Figure , in 
 Definition . 
 . Average data rate 
 In this section we make a connection between the average data rate across a linear source scheme , and the stationary second order of the Figure . In 
  
  
 Figure : The link a linear source scheme . 
 order to do so , we first make the relationship between those and the channel in Figure explicit . 
 Without loss of generality , we assume that the link Figure is given by the scheme of Figure . In that figure , E ,, and are causal such that 
 s 
 s 
  
 where , are possibly nonlinear time deterministic , and SE , , SH correspond to side information that becomes available at time the side , side , and both at the and sides , respectively . The range of is assumed to be countable , and that of , to be a countable set of prefix free binary . The and are chosen so as to satisfy 
 H 
 for any , and any N . Condition explicit the fact that act as a transparent link between the output of E and the input Figure . Since we assume an error free digital channel , and , it that . 
 It is clear that , when one the scheme of Figure as the link a linear source scheme , one needs to focus on , such that the process 
 q 
 the in Definition . We also note that , since , the feedback link the side in Figure does not require a physical channel . To at the side , it to the side . 
 We denote the length of the symbol i by i , and define the average data rate across the considered source scheme as 
 R . 
 We will work under the following : 
 Assumption 
 a is independent of ,. 
 The Di is such that , i N , there another deterministic . such that i . e ., is invertible upon knowledge of 
 Assumption a is by the sensible requirement that the Figure only past and present , and side information not related to the message being sent , to construct its current output . On the other hand , if , for some and , Assumption does not hold , then one can define another set of that achieve a the same statistics as in the original situation , but at the expense of a lower average data rate . Accordingly , if one at , then one can focus , without loss of , on and satisfying Assumption . 
 . Problem definition 
 The main focus of this paper is on the minimal average data rate that one to attain a given performance level , as measured by the stationary variance of the plant output . With the above , we are now in a position to formally define the problem of interest as : Problem Consider the of Figure , suppose that Assumption , that the source scheme is linear , and that the scheme of Figure is used as the link . Denote by the minimum stationary variance is achievable in the of Figure when . Find , for a given performance level , , 
 , D 
 where is the stationary variance of , and the optimization is carried out with respect to : 
 • All causal Definition , Assumption ., by , such that the noise see is as 
 • All side information SE SH satisfying Assumption a . 
 • All ,,, E see . 
 Remark and the auxiliary definition of independent source scheme that , the in Definition , the considered in Problem ,,, E 
 Sis mean square stable . 
 In order to solve Problem , we will first establish a lower bound on the average data rate across a linear source scheme in of stationary second order of the see Figure . The existence of this bound Section , where we study the optimal design of linear source subject to . These are then used in Section to give both upper and lower on . 
 Bounding the Average Data Rate Across Linear Source 
 This section the of Section A in to show that , when a linear source scheme is used in the of Figure , the minimum average data rate across it subject to a performance constraint is bounded from below by a simple function of the minimum ratio between the stationary subject to the same performance constraint . To do so , we start by that the following : 
 Theorem Consider the of Figure where the source scheme is linear , and the link , given by the scheme of Figure . If ,,, E and and 
 R , 
 where I the directed mutual information rate , , and is the variance of . Moreover , equality in the and , is the stationary power spectral density of inequality if and only . 
 Proof : The first inequality immediately from Theorem in . The second inequality from Part of Lemma . in , and from the proof of Theorem in see also Theorem . 
 in . 
 Theorem a lower bound on the average data rate across a linear source scheme in of a simple function of the spectrum the equivalent noise variance . This key result , upon which the remainder of this paper is based upon , can be further simplified . Note that inequality 
 , 
 where is the stationary variance of , and is the of the linear source scheme . By finding the minimal subject to a performance constraint e .., an upper bound on the stationary variance of the plant output , one is also calculating an upper bound on the minimal value of the right hand side of , subject to the same performance constraint . If , in addition , the optimal solution to the former minimization problem is such that the gap between the left and right hand sides of the inequality in is arbitrarily small or can be made so without compromising , then , by virtue of Theorem , one would immediately get a lower bound on by the much simpler minimization problem . The following result that this is actually the case : 
 Lemma Consider the of Figure , where the source scheme is linear and a fixed noise source . Suppose that Assumption and define . If the choice ,,, E 
 H , C , F , E ,,, such that s , Fand , , then , for any arbitrarily that , f and , in addition , , there exist 
 a choice of , 
 . 
 Proof : The proof of this result goes along the of the proof of Theorem in . 
 Optimal Performance Subject to 
 by the discussion preceding Lemma , we will now focus on the following problem : 
 Problem Consider the of Figure where the source scheme is linear , and suppose that Assumption . Define , where pi is the unstable pole of . Find , for a given , , 
 q , 
 ,,, E , G 
 where all the are as defined before . 
 Remark It from Theorem in that to the minimal compatible with mean square stability in the of Figure . 
 We note that Problem is concerned with the best achievable performance , as measured by , that is achievable when an upper on the . As shown in Lemma below , this problem is equivalent to the problem of finding the minimal achievable subject to an upper bound on . Our momentary change of focus is only due to technical see Remark below . The next lemma necessary for the ,,, E to belong to : 
 Lemma Consider the of Figure where the source scheme is linear , and suppose that Assumption . Define 
  
 S , 
 and consider a factorization N , i . e ., consider . If ,,, EX ,, , , 
 X , D 
  
 , 
 where pi is the unstable pole of . 
 Proof : It is clear from Figure in open loop . be stable to ensure the internal stability of the loop . On the other hand , a possibly non linear and discontinuous , accordingly , must be strictly proper for the architecture to be 
 In Figure , denote the open loop transfer function u by , i . e ., 
 well the same conclusion to ,,, E must contain as non minimum phase all the unstable , be an admissible one degree of freedom controller . Therefore , must belong to H , E , . If 
  
 Lemma . . in . Denote all the unstable we have from the Bode Integral Theorem that , for , E , i , . 
 Since 
 . 
 To complete the proof , we note that is the closed loop transfer function from a disturbance at the input the output . By the , we thus conclude that must be as . 
 By virtue of Lemma , we are now ready to present a lower bound on the best achievable performance . To that end , we will make use of the following mild additional assumption : 
 Assumption In Problem , EH at the optimum . 
 If EH were identically zero at the optimum , then optimal performance would be by open loop . The where this are clearly of no interest in a control setting . 
 Theorem Consider the of Figure where the source scheme is linear , and suppose that and hold . Then , 
  
 , 
 where 
 , and , 
 and 
 , 
 where ,,, and pi are as in Lemma , 
 , where the zero of . Moreover , the optimization problem on the 
 right hand side of is convex i . e . is a convex function of its , and is a convex set . 
 Proof : Under our we have that , for any ,,, E and , and exist and are given by 
 , 
  
 and also that 
 A simple contradiction based argument that , since Assumption , the constraint in is active at the optimum . Hence , at the optimum , 
  
 and 
  
 , 
 where the first equality upon for with , and the inequality from the inequality and the fact that . If one and . 
 that , then the first part of the result immediately from , and Lemma 
 To complete the proof we need to show that the optimization problem on the right hand side of 
 Lemma in convex . Since is an function of , is a convex function of its . The proof is thus ., and both convex , it from 
 Theorem that one can calculate a lower bound on the best achievable performance subject to an constraint by a convex optimization problem . Convexity , among other , that the problem of finding the bound and also and such that , for any , 
 , , , 
 can be numerically standard . 
 Remark As an alternative to Problem , one can also consider the problem of finding the minimal subject to a performance constraint . By doing so , and by proceeding as in the proof of Theorem , one at an auxiliary optimization problem whose convexity we have not been able to prove yet . 
  
 We will next show that a solution of the optimization problem on the right hand side of actually a solution to Problem . To that end , we start by that the following : 
 Lemma Consider and satisfying , , . 
 . For any , there exist , such that and 
 . 
 . Consider ,, in Lemma , and define 
 , 
 z 
 den , 
  
  
 . 
 There , such that , for any , 
 . 
 Proof : 
 . Equation by the proof of Lemma in , and by the definition of 
 M . 
 . The that is an admissible one degree of freedom controller for . Thus , as all the unstable of . Accordingly , is stable , , and as the unstable plant only . Given the definition of , , we thus conclude that , and that , is also stable . Since , by assumption , is stable , we have that , L is analytic outside and on the unit circle and can thus be to any degree of accuracy 
 by a stable and proper , . 
 With the aid of Lemma we can state the main result of this section : 
 Theorem Consider the of Figure where the source scheme is linear , and recall the notation in Lemma . If and hold , then Moreover , for any > , there , , such that , satisfying the in Part of Lemma , and satisfying 
 , , as in with F 
 , satisfying , that the choice of 
  
  
  
 a , and , whilst . 
 Proof : Our first claim immediately from the second , which is proved below . 
 Since , E and , recall the definition of den , it that the choice of a well feedback loop . We next show that the choice of internal stability . To do so , we first note that the open loop transfer function u is given by HE . As such , there exist no unstable pole zero between . From the proof of Lemma , we know that is an admissible one degree of freedom controller for 
 and . Moreover , it is clear by construction see and that there are no unstable pole zero cancellation among any two transfer in the set , E , . The above imply that the closed loop is internally stable . 
 We next show that . Equation 
 . 
 Thus , since , dis stable since f , it that there a , is admissible , the in Part of Lemma . On the other hand , and is stable . As a consequence of the above , such that s q 
 in is finite and positive , as . 
 The fact that our choice of upon in , and that . 
 To complete the proof , it remains to show that the set of is such that , for any . By definition of , see , the choice for E in is such that there E L such that 
 . 
 Thus , 
  
 and the numerator of the second term on the right hand side of can be written as 
  
 where a , satisfying a , , is given by 
 , 
 In , the first equality upon and , the inequality from the inequality , and the last equality from the fact that our choice of 
 , and from our choice and thus , . By the above and the use the definition of and 
 we can write as 
 . 
 Now , note that v . The latter inequality and allow one to 
 rewrite , for sufficiently small 
 , 
 where , satisfying , , is given by 
 . 
 The result immediately from . Note that upon and , hence , to be chosen , once is chosen . 
 Theorem a characterization of the solution of Problem . In the next section , we will use this result to characterize on . 
 on the Minimal Average Data rate to Achieve a Given Performance Level 
 This section a characterization of the solution of Problem . In particular , we present both upper and lower on , and also a specific implementation of a linear source scheme that an average data rate that is to be below our upper bound . 
 We start by the following auxiliary result : 
 Lemma Consider the of Figure where the source scheme is linear , and suppose that and hold . Define 
 , 
 ,,, E , D 
 where all are as defined before . Then , . 
 Proof : By Problem one a set of such that and recall that Assumption that the constraint is active at the optimum . It is therefore immediate to see that , by definition , G is impossible . Assume now that and are the that minimize ,. Then , since the constraint in Problem is active at the optimum , we have that 
 , 
 which the of and . We thus see that . 
 Lemma that Problem is equivalent to the problem of finding in . That is , if some choice of the ,,, E and solve Problem and thus achieve a performance level and an , then the same optimize the subject to the constraint 
 . 
 We are now in a position to present the main result of this paper : 
 Theorem Consider the setup and of Problem . If , in addition , Assumption and , then . Moreover , there a linear source scheme such that , while satisfying for any . 
 Proof : Our first claim is immediate from and , and and . 
 We now prove our second claim . Use Theorem to find ,,, E Ho , and a noise variance so such that and for the desired value of . Consider an entropy as the link , i . e ., pick E ,, and in Figure such that 
 s , , , 
 , , , 
 where is a dither signal available at both the and sides accordingly , SE SH ,: i ; i to a uniform with step size , to the an entropy coder also loss less whose output symbol is chosen the conditional distribution of , given , and to the the entropy that is complementary to the entropy coder at the side . If is an i . i .. sequence , independent of ,, and uniformly distributed on 
 , , with so , then our claim from Corollary in . Theorem a lower bound on the minimal average data rate that is to achieve a given performance level , when linear source are employed to control . This lower bound is not tight , but the worst case gap is given by per sample 
 i . e ., . per sample . The first term of this gap to the divergence of the from , and due to the fact that generate uniform quantization noise 
 and not noise . The second term because practical entropy see proof of Theorem are not perfectly efficient , Chapter . A detailed discussion of these can be found in , . Since we constrain ourselves to a simple class of source , this gap inescapable but , in our view , it is a fair price to be given the simplicity of our approach . 
 A key aspect of our is that they are built upon the solution of an optimization problem . This is a key feature of our work , and one to easily provide average data rate in feedback , by standard control system design . 
 Remark If in Problem one the performance constraint , then the problem to the calculation of the minimal average data rate that is compatible with mean square stability , say 
 . By Theorem in , and proceeding as in the proof of Theorem , it that , for the setup and of the latter theorem , 
 , 
 where pi is unstable pole of . That is , independent source can achieve at average data that are at most per sample away from the absolute lower bound established in . 
  
 This paper studied control closed over noiseless digital . By on a class of source , we have established lower and upper on the minimal average data rate to achieve a performance level . Instrumental to our result was the characterization of the minimal that a given closed loop performance in a related control system . 
 Future work should focus on that include causal but otherwise unrestricted source . to the case are also under study by the . 
  
  
 ﻿The authors present an improved achievable rate region for two-pair bidirectional Gaussian relay networks based on successive compute-and-forward method. In these networks, one relay helps in the communication between two pairs of users. In their proposed scheme, the authors use nested lattice codes for encoding the messages at the users, and Gaussian random codes for the encoding at the relay. They use the successive compute-and-forward strategy to decode two integer linear combinations of the lattice codewords in the uplink, and successive interference cancellation for decoding the Gaussian codewords in the downlink. The downlink channel can be considered as a broadcast channel with two receiver groups, but within each group, a pair of users is considered as an additive white Gaussian noise channel (instead of a broadcast channel) because each node knows its own transmitted message. It is shown that for all channel gains of downlink channels and all channel gains of symmetric uplink channel pairs, the strategy achieves rates to within constant gaps of 1/2 and 3/4 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted models, respectively. These gaps are tighter than those previously obtained for this network, which have not exploited the successive compute-and-forward method. 
 Nowadays, the capacity of wireless networks is one of the main research topics in network information theory. As an interesting and important application, two-way communications via relays have been introduced, where the two-way relay channel (TRC) is used as a building block. Several coding strategies were proposed for the TRC in [1–3], by extending and combining the basic methods for the relay channel [4], such as decode-and-forward (DF), compress-and-forward (CF) and amplify-and-forward (AF). When using CF or AF, the effect of the noise remains on the signal transmitted by the relay. In contrast, by applying DF at the relay, the noise is completely removed from the signal. However, DF suffers from multiplexing loss [2]. 
 Compute-and-forward is another strategy proposed for the TRC which uses lattice codebooks. In this method, the receiver need not decode messages directly; instead, it computes a linear equation of the messages based on the fact that integer combinations of codewords also belong to the codebook [5–12]. In [5], an achievable scheme was proposed for the TRC based on nested lattice codes, which was shown to achieve within 1/2 bit from the cut-set bound, for all channel parameters. This strategy attains noise suppression without multiplexing loss. In [6], the capacity of the TRC with relay private messages has been achieved within constant gaps of 2/3 and 5/6 bit/s/Hz per user from the single-sided genie-aided upper bound for restricted and non-restricted channel models, respectively. 
 The extension of the TRC approaches to networks has been done in [8–10], where more than two users communicate via a relay in a multi-directional manner. The Gaussian multi-way relay channel (MRC) was studied in [8], where upper and lower bounds for the capacity were given. In this setting, users are divided into several clusters, where each user in a cluster has a single message intended to other users in the same cluster. A similar symmetric setup with one cluster and equal channel gains, noise variances and power constraints was considered in [9], where the sum capacity was obtained. In [10], a type of three-user Gaussian MRC was considered, where each user has two independent messages, each of them intended to one of the other users. 
 In [11], a compute-and-forward approach was proposed to exploit interference between the transmitters in an additive white Gaussian noise (AWGN) network. In their approach, relays are free to select integer coefficients that match the channel coefficients as closely as possible, thus reducing the effective noise and increasing the achievable rates. In [12], the successive compute-and-forward approach was presented, where after decoding a linear combination, the relay can combine it with its channel observation to obtain a new effective channel that can be better for decoding the next targeted linear combination. 
 In [13], the capacity regions of two-pair two-way relay networks (TPTRNs) for both linear shift deterministic and complex Gaussian channel models were obtained. In these networks, a relay node facilitates the communication between two pairs of users. It was shown that the cut-set upper bound is tight for the linear shift deterministic channel model. The strategy for the deterministic channel was translated to a specific superposition of lattice codes and random Gaussian codes at the source nodes and successive interference cancellation at the receiving nodes for the Gaussian network. It was shown that for all channel gains, that strategy achieves the cut-set upper bound to within 3 bit/s/Hz per user. In that approach, the lattice relations of [14] were exploited, so it was necessary for them to assume equal power constraints for lattice codebooks of one pair, and to use Gaussian codebooks for the excess part of the transmitted signal with extra power. However, the authors did not show that successive interference cancellation was actually feasible for lattice and Gaussian codewords together. 
 In this paper, we study two-pair two-way (bidirectional) relay networks, using the same model considered in [13], where one relay can help the communication between two pairs of users (see Fig. 1). For brevity, here we name it TPTRN. Here, we present a larger achievable rate region for a class of full-duplex Gaussian TPTRN by exploiting successive compute-and-forward method, which has the ability to efficiently decode multiple linear combinations of the signals. In contrast to the method proposed in 
 [13], which is based on the superposition of Gaussian random codes and lattice codes at the senders, and successive interference cancellation at the decoders, in our method, it is enough to consider only lattice codes for encoding at the users, and successive compute-and-forward for decoding at the relay in the uplink. In the downlink, Gaussian random codes are used for encoding at the relay, and successive interference cancellations are applied for decoding at the users. Moreover, in contrast to [8], we do not use timesharing and assume that all users can transmit simultaneously. It is shown that for all downlink channel gains and symmetric uplink channel pairs, the proposed method achieves within 1/2 and 3/4 bit/s/Hz per user of cut-set upper bound for restricted and non-restricted TPTRN, respectively, which are tighter than the gaps 2 and 3 bit/s/Hz per user obtained in [13]. 
 The remainder of this paper is organised as follows. In Section 2, the system model, the cut-set upper bound and some preliminaries about compute-and-forward strategies are reviewed. The achievable rate region for Gaussian TPTRN is characterised in Section 3. Capacity gap calculations and some simulations are presented in Sections 4 and 5, respectively. Some concluding remarks are provided in Section 6. 
 In this section, we review the system model for the Gaussian asymmetric TPTRN and its associated cut-set upper bound. We also review some concepts about lattice codes, and outline the successive CF strategy. The material of this section, presented for completeness, is a brief summary of the notions and results from [13–15], in which the interested reader can find a detailed exposition. Consider an asymmetric TPTRN as shown in Fig. 1, where nodes 1 and 2 (a pair of users) can communicate with each other via node r (a relay). There is no direct link between users. The same situation applies for nodes 3 and 4. The relay is assumed to be able to listen and transmit at the same time. We use a real-valued AWGN channel model for all channels in this network. Let log+(·) = max (log2(·), 0), C(x) = 1/2log2(1 + x), U = 1, 2, 3, 4, S = 1, 2, 3, 4, r and for each i ? U, let Oi denote the node on the other side of the relay which communicates with i. The forward and backward channels of each pair are assumed to be different. Channel gains are written on each link in Fig. 1 as hij, where i, j ? S and i ? j. 
 Node i ? U has messages miVi [ MiVi = {1, 2, ..., M} to be delivered to node Oi, where M [Z. Each user is assumed to choose its transmit message independently of the other users. The user at node i ? U encodes its message into a codeword xni = [xi1, xi2, ..., xin] [Rn using an encoding function xik = fik(miVi , yki -1), for k ? 1, …, n, where n is the codeblock length (in channel uses), and the messages of the relay node r are encoded into a codeword xnr = [xr1, xr2, ..., xrn] [Rn using an encoding function xrk = frk(ykr-1), for k ? 1, …, n, xik is a realisation of a real random variable Xik, satisfying the power constraint 1/n nk=1 E  P for i ? S, and yik -1 [yi1, yi2, ..., yi,k-1]alldenotes the previously received symbols at nodenodes transmit at the same time, =beginningi. Notethe that 
 transmission of each codeword simultaneously. In this setup, called non-restricted encoding, some dependency between the signals transmitted by different users can be introduced. In contrast, in restricted encoding, users apply the encoding functions xni = fi(miVi ), thus yielding independent transmitted signals for all i. 
 The rate RiVi is said to be achievable if, for any ? > 0 and for all n sufficiently large, there exists an (M, n) code with M = 2nRiVi such that the probability of error is less than ?. 
 The received signals at node i ? U and at the relay node r are given by the following equations, respectively 
 where zik is a realisation of independent and identically distributed real-valued Gaussian noise N˜ (0, 1) (i.e. with zero mean and unit variance). For each i ? U, hri and hir denote the downlink and uplink channel gains, respectively. Each node i ? U, uses a decoding function gi to decode mVii as mˆ Vii = gi(yni , miVi ), where yni = [yi1, yi2, ..., yin]. 
 The cut-set upper bound for the TPTRN was obtained in [13]. We rewrite a reduced and equivalent version of it for both restricted and non-restricted cases in the following theorem and corollary. For the proof and further details, the reader can refer to [13]. 
 Theorem 1: The capacity region of the non-restricted Gaussian TPTRN is upper bounded by 
 Corollary 1: The capacity region of the restricted Gaussian TPTRN is upper bounded by 
 RiVi + RkVk = min{C((|hir|2 + |hkr|2)P),	(6) C(max(|hrVi |2, |hrVk |2)P)},	i, k [ U, i =Vk 
 Remark 1: In [13, lemma 1], it is proved that (4) and (6) are within 1 bit/s/Hz per user of each other. The latter was shown by the 
 When the channel is complex valued, the capacity corresponds to 2C (·) so the previous inequalities yield a gap of 1 bit/s/Hz for two users or, equivalently, 1/2 bit/s/Hz per user. However, in our problem, we have assumed real-valued AWGN channels, for which the capacity is simply C(·). Thus, the gap between upper bounds for restricted and non-restricted channels would be 1/2 bit/s/Hz for two users or, equivalently, 1/4 bit/s/Hz per user. 
 Next, we review some preliminaries on lattice codes and the compute-and-forward approach. For more details see [11–15] and the references therein. 
 A nested lattice code is defined in terms of two n-dimensional lattices Lnf and Lnc, which form a lattice partition Lnf /Lnc, that is, Lnc #Lnf . (Lnf and Lnc are usually called fine and coarse lattices, respectively.) The nested lattice code is a lattice code which uses Lnf as codewords and the Voronoi region of Lnc as a shaping region which is denoted by Vc. For Lnf /Lnc, the set of coset leaders is defined as C = {Lnf mod Lnc} = Lfn > Vc. Lnc is chosen so that its average power per dimension is P, that is, the second moment per dimension of a random variable uniformly distributed over Vc is P. The coarse and fine lattices should be good for quantisation and channel coding, respectively (for definition of lattice goodness see [15]). The coding rate of the nested lattice code is defined as 
 In [12, Theorem 2], an achievable rate was proposed for real-valued AWGN networks based on the compute-and-forward strategy to decode one linear combination of codewords. In [12], the successive compute-and-forward method was proposed, where decoding more than one linear combination of codewords is desirable. We rephrase these theorems in the following theorem for the case of L transmitters and one receiver. 
 Theorem 2: For real-valued AWGN networks with L transmitters, one receiver and channel coefficient vectors h = [h1, h2, ..., hL] [ RL and equation coefficient vector a = [a1, a2, ..., aL] [ZL, the following computation rate is achievable 
 To successfully decode two linear combinations, for example, a, b [ZL, the following computation rate is achievable (see (11)) 
 In other words, the relay can first aim to obtain a linear combination that is easy to decode and then use it to create a better effective channel for decoding the second linear combination [12]. In Theorem 2, a and b are selected according to the design options where decoding different linear combinations of messages are desirable. In the next section, we present an application for that approach. 
 In this section, we present an achievable rate region for the Gaussian TPTRN. In our scheme, we consider restricted encoding functions. The obtained achievable rate region is also an achievable region for the non-restricted Gaussian TPTRN. 
 In the uplink, based on the fact that the TPTRN consists of two TRC pairs, we use the idea of successive interference cancellation for lattice codes introduced in [11] which was reviewed in the previous section. By considering appropriate values for a and b in (10) and (11), the linear function of the messages of each pair is decoded, respectively, at the relay. 
 We would like to mention that in [13] the superposition of Gaussian and lattice codes from four transmitters is received at the relay in the TPTRN. The relay then uses the successive interference cancellation method (originally thought for Gaussian codewords) for all codewords, regardless of the fact that the codewords are of different kinds, that is, they have used the equation of [14] for decoding the lattice codes of each pair but they consider the other codewords as interferences in the Gaussian rate equation. There is no proof for this performance and it does not seem to be a correct approach. 
 In contrast, we use the interference cancellation method that has been proposed for lattice codes in [11]. Moreover, in this method, nested lattice codes and dithered encoding have been used instead of lattice codes alone. The motivation for this is that the former method has been shown in [5] to yield the rate log+(1/2 + SNR) for equal power constraints which is higher than the one proposed in [14] which is log+(SNR). 
 In the downlink, we extend the method proposed for the TRC in [5] to the two pairs of TRCs present in the TPTRN. In this method, each pair of TRCs can be treated as an AWGN channel instead of a broadcast channel. This is due to the fact that each user of each pair knows its own signal, thus it can subtract this part from the received signal and can extract the message of the other user of that pair. This method yields higher rates than the method used in [13] which considers TPTRN as a broadcast channel with four receivers. The full strategy is explained next. 
 According to the described strategy, the transmit signal at user i ? U is given by xniVi, which denotes a dithered lattice code ensemble of nested lattice codewords (Lnj Vn) with size 2nRiVi , where Ln ,Ln1 ,Ln2 ,Ln3 ,Ln4. ?n should be good for quantisation and its second moment per dimension is P. {Lni }4i=1 should be good for channel coding. We consider the following uplink channel vector between users and the relay 
 The relay decodes two linear combinations of lattice code ensembles t = a1xn12 + a2xn21 and f = b3x34n + b4xn43, with the following parameters 
 The coefficient vectors a and b are decoded by successive compute-and-forward individually with transmitter rates as shown by the following equations 
 h2 = h21r + h22r + h32r + h24r hTb = b3h3r + b4h4r hTa = a1h1r + a2h2r aTh = hTa aTb = 0 
 By substituting the above relations in (10) and (11), (16) and (17) are obtained, respectively. The details are shown below 
 =2 log+??????????b32 +b24 -  P(a( a12 +1a2a2)(122r)(b3h2(h3r31rr+2b4hr4)rP)2)????? ????? 
 =12log+????(a12 +Pa(a221221)(1++a2222()(hb23r4+h13rh2-24rr)bP3)h2+4r)1(2ra+12h(2br 32-+a2bh241)r)2P????? 
 Remark 2: As stated in [11], the closer a = [a1 a2 a3 a4] and h = [h1r h2r h3r h4r] are, the higher the rate obtained by computing (10). However, in our problem, the two last elements of a are fixed to be zero, that is, (a3, a4) = (0, 0), so as shown in the simulation section, not necessarily those values of (a1, a2) which are closer to (h1r, h2r) result in higher rates. 
 Remark 3: Consider the case where each pair of TRCs in the TPTRN has symmetric uplink channel gains, that is, h1r = h2r and h3r = h4r. For this case, decoding a = [1100] and b = [0011] yields higher rates as shown by the following computations 
 More generally, let us consider the channel gain vector as h = [va1rh1r va2rh2r va3rh3r va4rh4r], where 0 =air = 1 for all i. It is possible to find 0 =afr, atr = 1, hfr and htr such that we have symmetric uplink channels for each pair of TRCs, that is, 
 . For this case, by assuming = [1100] and = [0011], (16) and (17) are simplified as 
 The relay maps two lattice codes t and f to Gaussian codewords {xnt }, {xnf } from codebooks of sizes {2nRrt } and {2nRrf }, respectively. The signal transmitted by the relay is given by 
 The two following cases of all different orders of downlink channel gains are sufficient for the analysis to be carried out. The other cases can be obtained and will lead to similar conclusions by changing the indices. 
 3.4.1 Case |hr2| = |hr1| = |hr4| = |hr3|: It is shown that, for any choice of channel gains, this can be done successfully as long as the following equations are satisfied. 
 Proof: User 2 first decodes xnf by treating xnt as noise. This can be done with arbitrarily small probability of error as long as (25) is satisfied. Then, the signal xnf is cancelled from the received signal and xnt is decoded. This can be done with arbitrarily small probability of error as long as (26) is satisfied, because by the same conclusion reached in [5] for TRC, in the downlink, each pair of TRCs in the TPTRN can be treated as an AWGN channel instead of a broadcast channel. As explained before, this is due to the fact that each user of each pair knows its own signal, thus it can subtract this part from the received signal and can extract the message of the other user of that pair. Hence, the TPTRN can be interpreted as a broadcast channel with two receiver groups (1, 2) and (3, 4) instead of a broadcast channel with four receivers, as was considered in [13].	? 
 Clearly, (25) and (27) are redundant in view of (29) and (30), respectively, because C(x/1 + x) is an increasing function of x. 
 3.4.2 Case |hr2| = |hr4| = |hr1| = |hr3|: It is shown that for any choice of channel gains, this can be done successfully as long as the following equations are satisfied. 
 In this section, we compute the capacity gaps between the achievable rates and the cut-set upper bound by using the described strategy. For the uplink, we obtain the capacity gaps by considering symmetric channel gains for each pair of TRCs. For the downlink, we consider the general case. 
 Lemma 1: (Uplink): For the TPTRN with symmetric uplink channel gains for each pair of TRCs, for any rate tuple r = rf, rt satisfying 
 codewords with rates {error probability for all channel gains.rij}i[U,j=Vi can be done with arbitrary small 
 Table 1 Optimised values for(a1, a2),(b3, b4),Rt andRf for each channel coefficient vector h 
 codewords with rates {error probability for all channel gains.rij}i[U,j=Vi can be done with arbitrary small 
 Fig. 2	Non-restricted and restricted cut-set upper bounds and achievable rates obtained for hUL = [8, 16, 1, 3] and different values for a and b 
 Remark 4: Note that if R > 1/2I, (I: unit vector), satisfies (5) and (6), then the rate tuple r = R- 1/2I satisfies the conditions of Lemmas 1 and 2. Thus, the capacity of the restricted Gaussian TPTRN with symmetric uplink channel pairs can be obtained within 1/2 bit/s/Hz per user of the cut-set upper bound. By comparing against (7), we see that the gap is <3/4 bit/s/Hz with respect to the cut-set upper bound for non-restricted Gaussian TPTRN. 
 In this section, we illustrate the performance of the proposed method for the TPTRN via simulations and discuss the results. All the rate units shown in the figures and the table are in bit/s/Hz. 
 Fig. 3 Cut-set upper bounds and achievable rates obtained for hUL = [12.5, Assuming P = 1, we consider several values for h as shown in 12.5, 2.5, 2.5] and (a1, a2) = (1, 1) and (b3, b4) = (1, 1) Table 1, and for each case, we compute the values of the decoding coefficients (a1, a2) and (b3, b4) which yield the highest rates Rt and Rf. As it is observed from the results, not necessarily decoding 
 and substituting (44) into (30), results in those values of (a1, a2) and (b3, b4) which are closer to (h1r, h2r) and (h3r, h4r) result in higher rates. The first row of the table 
 arf  | |P/ + h|r1 | shows the optimised values of (which are (1, 1) and (2, 1), respectively. Rows 2a1, a2) and (b3, b4)–5forof the tableh = [8751] hr4 P hr1 P show the results for h2r/h1r = 2 and h4r/h3r = 3. As it is seen, not necessarily decoding with (a1, a2) = (1, 2) and (b3, b4) = (1, 3) Other cases can be proved similarly. ? results in higher rates. Rows 6–9 of the table shows the results for 
 Fig. 4	Non-restricted and restricted cut-set upper bounds and achievable rates obtained for hDL = [7, 10, 4, 6] by our method and the method of [13] for the downlink communication 
 h2r/h1r = h4r/h3r = 2. It can be seen that, for most cases, decoding (a1, a2) = (1, 2) results in higher rates. Rows 10–13 of the table show the results for h1r = h2r and h3r = h4r, in this case, decoding with (a1, a2) = (1, 1) and (b3, b4) = (1, 1) results in higher rates. 
 Fig. 2 shows the cut-set upper bound and the achievable rates obtained for an uplink channel gain vector equal to hUL = [8, 16, 1, 3] and different values for a and b. It is seen that the choice (a1, a2) = (1, 2) and (b3, b4) = (1, 1) results in higher rates. This result is also shown at row 4 of the table. 
 Fig. 3 shows the cut-set upper bound and the achievable rates obtained for a symmetric uplink channel gain vector equal to hUL = [12.5, 12.5, 2.5, 2.5], and for (a1, a2) = (1, 1) and (b3, b4) = (1, 1), which yield the largest rate region. As it is seen from the figure, the distance between the achievable rate (Rt, Rf) = (1.80, 1.38) (the last row of Table 1) and the cut-set bound is <1 bit/ s/Hz, which is tighter than the gaps found in Lemma 1. 
 Assume the downlink channel gain vector to be hDL = [7, 10, 4, 6], and Pr = 1. Fig. 4 shows the cut-set bound, the achievable rates attained by the proposed method and the results of [13] for the downlink communication. As it is seen from the figure, for most cases, the proposed method yields gaps that are within those predicted by Lemma 2. 
 In this paper, we studied the capacity region of an asymmetric full-duplex Gaussian TPTRN with separated users (no direct link between users). We established an achievable rate for this network by using nested lattice codes for encoding at the users and Gaussian random codes for encoding at the relay. For decoding, we exploited successive compute-and-forward and successive interference cancellation at the relay and at the users, respectively. It was shown that, for the TPTRN with symmetric uplink channel pairs, the achievable rate is within constant gaps of 1/2 and 3/4 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted TPTRNs, respectively. In this paper, we have used real-valued AWGN channels. For complex-valued AWGN channels, we have 2C(x) instead of C(x) in the capacity expression, so the gaps would be twice the gaps obtained for real-valued channels, that is, 1 and 1.5 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted TPTRN, respectively. These gaps are one-half of those obtained in [13], that is, 2 and 3 bit/s/Hz per user for restricted and non-restricted TPTRNs, respectively. 
 ﻿The main purpose of this note is to show that in a realization   of the causal information
 rate-distortion function (IRDF) for a ?-th order Markovian source xn1, under a single letter sum distortion
 constraint, the smallest integer l for which y  holds is l = ?. This result is
 derived under the assumption that the sequences   have a joint probability density function.
 INTRODUCTION
 Consider the causal information rate-distortion function (IRDF) for a random source x ,
 defined as
 	 ,	(1)
 where the minimization is over all conditional PDFs  satisfying the distortion constraint
 	 	(2)
 and the causality Markov chains
 	y 	(3)
 If the infimum is achieved by some conditional distribution, the associated pair of sequences x  is called a realization of . Here we assume that such distribution exists and that the corresponding realization has a joint PDF. This assumption is satisfied if, for example, xn1 is Gaussian and ?(x,y) = (x - y)2.
 The first purpose of this note is to show that in a realization of the causal IRDF for a ?-th order Markovian source xn1, under the average distortion constraint (2), and supposing that in such realization the sequences have a joint PDF, it holds that
 	n	k	e 
 	fy	 	(4a)
 where fxn1 is the PDF of xn1 and
 	 	(4b)
 The expressions given in (4) are a special case of the ones given by [1, equations (16),(17),(18)] for abstract spaces, where their derivation is not included. The value of our first result resides in that
 We provide a proof for the validity of (4) (absent in [1]).
 In this proof, we pose the causal IRDF optimization problem with   as the decision variable (instead of the collection  as would be the case in [1] for probability measures having an associated PDF). Accordingly, we impose an explicit causality constraint on , instead of enforcing causality structurally by restricting   to be the product of  , as done
 in [1], [2].
 The second (and main) goal of this document is to note that from (4a) it is clear that
 	y 	(5)
 holds, and that
 	y 	(6)
 does not hold, except for ? = 1. Crucially, (6) does not become true by supposing that the joint PDF of x  is stationary, thus contradicting [2, Remark IV.5] and what is stated in the discussion paragraph at the end of [1, Section V].
 II. PROOF
 The causal IRDF under the above conditions is yielded by the solution to the following optimization problem:
 minimize:	 	(7a) subject to:	 	(7b)
 	 	(7c)
 	 	(7d)
 where the minimization is over the conditional PDF  . Notice that (7d) is an explicit causality constraint equivalent to (3).
 Let   be any conditional PDF, and define
  , 	(8)
  	(9)
  , 	(10)
  	(11)
 where o ? [0,1].
 Before writing the Lagrangian and taking its Gateaux differential, let us obtain the Gateaux differential
 of  in the direction , given by
 	 	(12)
 	 	(13)
 where
 	R , 	(14)
 (15)
 (16)
 	= 0	(17)
 On the other hand, for each i = 1,...,n, the causality constraint (7d) appears in the Lagrangian as
  (19)
 It will be convenient to manipulate this expression so as to give it a structure similar to the other terms in the Lagrangian. For this purpose, notice that
 (21)
 (22)
 (23)
 (24)
 (25)
 (26)
 where
  	(27) Substituting this into (20) we obtain
 (28)
 (29)
 We can now write the Lagrangian associated with optimization problem (7) as
 (30)
 (31)
 	 	(32)
 From the theory of Lagrangian optimization on vector spaces   is a solution to Optimization Problem (7) only if
  
 	 	(34)
 for every function gyn1 |xn1 as defined in (8), i.e., for every conditional PDF . This holds if and only
 if for every 
 	 	(35)
 	 	(36)
 The Lagrange multiplier function   must enforce the constraint (7b). Hence,
 e
 	 ,	(37)
 where
 	 	(38)
 Marginalizing over  we obtain
   e 
 (39)
 Using Bayes’ rule we can write
 	 	(40)
 (41)
 R e-s?( k,yk) Fk(x1,y1)dyk
 where
 	 , e 	(42)
 These functions can be written recursively as
 	Fn(y1n) = fyn1 (y1n)	(43a)
 	Fk(xn1,y1k) = e-(?¯k(xk1,y1k)-?k(x1n,y1k)) Z e-s?(xk+1,yk+1) Fk+1(x1n,y1k+1)dyk+1	(43b)
 In order attain causality in (41), the functions  must depend only on and . Since for each k, the function Fk+1 does not depend on terms   with i = k, the causality constraint
 is met if and only if we choose   in (43b) such that, for each k = 1,...,n
 	 	(44)
 for some function F?k.
 For k = n, the causality constraint is satisfied automatically since 
 (see (43a)).  Suppose now that (44) (i.e., causality) is satisfied for k + 1, for some k > n. In such case,
 one can replace  in (  and, defining
  ,
 write (44) as
 	 	(45)
 Multiplying both sides by  and integrating over xnk+1 we obtain
 (46)
 (47)
 (48) This yields that the recursion (43) takes the form
 (49)
 (50)
 If x -th order Markovian, then  , for all k = 1,...,n, in
 which case (50) becomes (4b). Substituting the latter into (44) and then in (41) yields (4a). Finally, from (4a), it follows that in a realization of the causal IRDF it must hold that
 	y 	(51)
 and that
 	y 	(52)
 does not hold, except for k = n. This completes the proof.	
 
 ﻿Fundamental limitations in networked control systems (NCS) have been researched for the last years with many stability results, from channel transmission minimal rate based on information theory, to minimal signal-to-noise (SNR). For the latter, it has been confirmed early on that the usual culprits increase the minimal SNR for stability, such as unstable plant poles, non-minimum phase (NMP) zeros, time delay, together with new ones such as communication channel bandwidth and colored transmission noise. In this work we specifically propose two approaches to completely avoid the effect of NMP zeros on the minimal SNR for stabilityindiscrete-time.The first approach stems from the observation that the controller that achieves the minimal SNR for a minimum phase plant in discretetime, has all its zeros located outside the unit circle (NMP zeros). Such observation allow then to characterize this set of NMP zeros as candidate NMP zeros of alternative plant models with the same unstable poles, which in turn will not increase the minimal SNR for stability above the expression imposed only by the plant unstable poles. Our second approach extends the set of possible NMP zeros that do not increase the SNR limitation to any location, by proposing the synthesis of a two-degree of freedom linear time invariant controller solution that achieves the minimal SNR for stability imposed only by the plant unstable poles. 
 Index Terms—Control over communications, Linear systems, Networked control systems, Optimal control. 
 ETWORKED control systems (NCS) have been a subject of focused research for at least the last two decades, with review articles available in [1], [2]. Results on fundamental limitations on stability in this NCS framework were reported for example in [3], [4]. NCS performance limitations have also been extended using information theory arguments to include disturbance rejection, reference tracking and multivariable systems, as in [5], [6]. 
 A different approach for NCS is, on the other hand, proposed in [7] where an LTI framework for stability of feedback loops, both in continuous-time and discrete-time, is introduced based on the channel model signal-to-noise ratio (SNR). The SNR fundamental limitation obtained in [7] is presented as an infimal lower bound on the channel 
 M. Derpich is with the Departamento de Ingenier´ia Electr´onica, Universidad T´ecnica Federico Santa Mar´ia, Chile. 
 A.J. Rojas is with the Departamento de Ingenier´ia El´ectrica, Universidad de Concepcio´n, Chile. Email: arojasn@udec.cl . 
 The authors thankfully acknowledge the support from CONICYT, through Basal Project FB0008. 
 SNR required for the stabilization of an unstable plant model by means of output feedback over a memoryless additive white Gaussian noise (AWGN) communication channel, such as the case depicted in Figure 1. More so, in 
 recent years it has been shown that for second order statistics, control problems over a power constrained erasure channel are equivalent to control problems subject to an SNR constraint [8], [9]. Furthermore, the SNR equivalence, in a second order sense, has been extended to include fading channels in [10]. As a result, the SNR approach has been able to capture the most important communication features and characterize their impact on fundamental limitations, as well as performance limitations, see also for example [8], [11]–[15]. More so, the SNR approach has also motivated a reinterpretation of the classic control feedback loop by means of the reference to disturbance ratio (RDR), [16], [17] , which quantifies performance as the trade-off between reference tracking and closed loop input disturbance rejection. 
 A common observation, remarked in particular in [7], is that the SNR limitation required for LTI output feedback stabilization of an unstable plant model with NMP zeros, is strictly greater than the one required for state feedback (emphasis added by the present authors). As an alternative the authors of [7] propose a linear time-varying alternative control scheme to eliminate the effect of the NMP zeros on the infimal SNR for stabilizability. 
 Our first contribution in this work is to explicitly characterize the one degree of freedom, see Figure 1, optimal controller that achieves the minimal SNR in discretetime. Furthermore we will characterize the zeros of such controller and prove that these zeros are all located outside the unit circle. To the best of our knowledge such a characterization is novel. The controller zeros outside the unit circle are then by definition NMP zeros. As a result, if the zeros of an alternative plant model (with the same unstable poles) belong to this specific set of NMP zeros, then both SNRs, output feedback and state feedback, will coincide thus avoiding the effect of NMP zeros over the SNR limitation. 
 Our second contribution extends the set of potential NMP zeros to an arbitrary set, by proposing and synthesizing a specific two-degree of freedom LTI controller solution. The use of a two-degree of freedom controller has been explored before in order to eliminate the effect of NMP zeros in different NCS setups (including the SNR approach), see for example [13], [18]–[22]. The contribution here instead is the explicit definition of a two-degree of freedom (DOF) controller to avoid the deleterious effect of arbitrary many NMP zeros, recovering the minimal SNR for stability imposed only by the plant unstable poles. A preliminary result for first order plant models was presented in [23]. 
 As a result of our contributions we update the common observation about minimal SNR and NMP zeros as: the SNR required for stabilization of an unstable plant model with NMP zeros, via output feedback, is greater or equal than the required for state feedback, in an LTI setting. 
 This paper is organized as follows: Section II states the fundamentals behind the minimal SNR in discretetime. Section III characterize the structure for the optimal controller that achieves the minimal SNR for an unstable plant model and analyzes the zero locations for this optimal controller. Section IV describes the synthesis of the two DOF LTI controller that avoids the effect of any arbitrary plant NMP zero. We conclude the exposition in Section V with final remarks and possible future research directions. 
 In this section we present the general plant model assumptions, and summarize the discrete-time SNR for stabilizability solution. 
 where |?i| > 1, ?i = 1,··· ,m are the m unstable and distinct plant model poles. The term Gs is the stable, minimum phase (MP), part of G. The relative degree of G is ng = 1 (the choice of ng = 1 is to avoid introducing a plant time delay that would further increase the later minimal SNR for stability). Channel model: we consider here the memoryless AWGN channel model, see for example Figure 1. This type of channel is characterized by the channel input power P > 0 and its channel additive noise process n. Channel additive noise process: The channel additive noise process n is a zero-mean i.i.d. white noise process with variance s2. 
 Setpoint assumptions: The setpoint signal r is assumed to be zero, that is we consider here only a regulation problem, not a setpoint tracking problem. 
 Consider the feedback loop in Figure 1 where the problem is to stabilize an unstable plant subject to a channel power constraint P. From Figure 1 we observe that the closed loop relationships between the channel input u and the exogenous signal n is given by u = -Tn, with T = CG/(1 + CG) the closed-loop complementary sensitivity function. The channel input is required to satisfy the power constraint P > kuk2Pow, for some predetermined input power level P > 0. We assume that the closed loop feedback system is stable, in the sense that for any distribution of initial conditions, the distribution of all closed loop signals in Figure 1 converges to a stationary distribution. Without loss of generality, we therefore consider the properties of the stationary distribution of the relevant signals. The power of the channel input signal then satisfies 
 . The channel input power constraint can be restated from this, as a constraint imposed on the channel SNR,  . We thus observe that for the setting in Figure 1, the SNR limitation is uniquely defined by the H2 norm of T. 
 We now present a novel lemma which establishes a lower bound to the channel SNR in discrete-time that exists when the closed-loop system is stable. This lemma also establishes sufficient and necessary conditions, stated in terms of the transfer function from n to v, Tvn, so that the channel SNR is equal to this lower bound. The latter is given only by the unstable poles of G and is universal in the sense that it holds for any choice of the blocks G and C in Figure 1. 
 Before proceeding, it is convenient to note that, since n is i.i.d., it holds that, for every k, n ? u, and thuskvk2Pow = s2 + kuk2Pow. This reveals that the channel SNR is given by 
 Tvn is an all-pass transfer function. Conditions (ii) and (iii) are equivalent to 
 Proof. Since the disturbances are independent of the channel noise, we have that 
 with equality in (a) iff v has zero mean, and equality in (b) iff n is the only exogenous signal in the system in which case v has zero mean. The external disturbance case is treated for example in [5], [6] for establishing performance limitations, however here we are interested in stabilizability limitations, thus the no external disturbance stated in condition (i). The channel output power is thus, kvk2Pow = kTvnk22s2 iff condition (i) holds. 
 If the closed-loop system is stable, then the sensitivity function Tvn is stable and there can be no unstable zeropole cancellations. Thus 
 where (c) is due to Jensen’s inequality and the fact that log(·) is a concave function, with equality if and only if |Tvn(ej?)| does not depend on r. On the other hand, (d) is due to Bode’s integral theorem (and the fact that the unstable poles of G, which cannot be cancelled, appear as zeros of Tvn), with equality if and only if the NMP zeros of Tvn are exactly the unstable poles of G [24, Theorem 3.4.4]. Combining this with (2) and (5), it follows that conditions (i), (ii) and (iii) in the proposed statement are necessary and sufficient for achieving equality in (3), and that the latter two are equivalent to (4), completing the proof. 
 It is worth emphasizing that Lemma 1 is valid regardless of whether there are NMP zeros in the plant model or its relative degree. It is well known, see [7], that the minimal SNR ?d coincides for a state feedback strategy and an output feedback strategy, as in Figure 1, if the plant model G is unstable, minimum phase with no time delay. As stated in [7] the presence of NMP zeros increases the infimal SNR to a value ?o such that ?o > ?d. We will show in the next section that, for a one-degree of freedom controller solution, this is not always true, and on occasions we can still have ?o = ?d, even if the plant model contains NMP zeros. 
 We next present a theorem which characterize the (unique) stabilizing controller which achieves the lower bound ?d. 
 Theorem 2. For the system shown in Figure 1, assuming G is unstable, minimum phase with relative degree ng = 1, it holds that . Furthermore, the infimum 
 SNR is reached by a unique controller Co ? K the set of stabilizing controllers, given by 
 Proof. We will show that, in this case, there exists a controller C ? K satisfying conditions (ii) and (iii) of Lemma 1 and that such controller is unique. For this purpose, let us define the coprime polynomials BN and BD, respectively, as the numerator and denominator of 
 Notice that Tvn is a stable all-pass transfer function if and only if all the zeros of BN are outside the unit circle. On the other hand, from (8), the controller can be expressed as 
 Being G minimum phase leaves no unstable zeros of G which C could cancel. In order to ensure that C does not cancel the unstable poles of G and that those poles become the only zeros of Tvn, one must choose  ?i). Since Tvn must have an all pass frequency response, the denominator of Tvn must be  ), automatically yielding a stable Tvn. 
 In summary, we have found that there exists a Tvn which meets conditions (ii) and (iii) of Lemma 1, and that it is unique. This proves  and that the corresponding minimal SNR stabilizing controller is the one defined in (7). 
 Theorem 2 illustrates how the unstable poles of the plant model G play a fundamental role in defining the minimum SNR of the communication channel. It is also easy to see that, for a stable plant, the minimal SNR is zero. 
 Remark 1 (Without disturbances, ?d is a minimum.). Theorem 2 shows that, in this case (one-degree of freedom, minimum phase, ng = 1 and without disturbances), one can reach an SNR equal to ?d and obtain a stable closed-loop system. Thus, the infimum SNR for stability is actually a minimum. However, it shall be noted that the SNRminimizing sensitivity function Tvn satisfies (4), and thus it is all-pass. This means that the channel noise (the only exogenous signal in the system) is not being mitigated at any particular frequency. 
 Notice also that the stability of the minimum SNR system guarantees that if one implements the controller Co and then perturbations are added, closed-loop stability is not compromised. However, any additional external disturbance will increase the variance of u (as well as that of the other signals in the system), increasing the SNR above ?d. 
 After the results presented in the previous sections it is natural to ask ourselves the following question: Can the lower bound ?d given by (3) be achieved if the plant has NMP zeros? We next show that if ng = 1, the answer is “yes”, for very special cases. In doing so, we will also provide some insight into the reasons why in general the answer is “no”. 
 Revisiting the reasoning in the proof of Theorem 2, let BN and BD be as in (8), so that Tvn = 1/(1 + CG) = BN/BD. In order to reach the lower bound in (3), condition (4) must hold, which implies that 
 for some complex ß such that  , and a set of complex roots  , with a correspondence between each root outside the unit circle and one of the unstable poles of G (from condition (ii) in Lemma 1). The required stability of Tvn implies that Tvn can only have stable poles and thus, in view of (10), only NMP zeros (i.e., m' = m and ai = ?i, 1 = 1,...,m). 
 and, from (9), a C whose zeros do not cancel the unstable poles of G (a requirement for C ? K). 
 We now lift the MP assumption and propose an unstable plant model that also contains NMP zeros G?(z) = 
 ), and to stress the difference with G, such an NMP unstable plant model is labelled G?. The term Gs? is the stable, minimum phase, part of G?. As for G the relative degree of G? is also one. From (9), the only way in which C does not cancel these zeros is that the latter are also zeros of (1/Tvn - 1). Clearly, this is not possible in general, since, as we have seen, satisfying conditions (ii) and (iii) in Lemma 1 demands that Tvn takes the specific form given by (11). 
 This shows that, in general, if the closed-loop system is stable and if G? indeed has NMP zeros, then the SNR cannot reach the lower bound in (3). At the same time, the above analysis raises the following question: Are there special cases in which the plant model has NMP zeros and yet the lower bound (3) is compatible with closed-loop stability? The following theorem gives an answer to this question. 
 In the systems shown in Figure 1, an SNR equal to the lower bound in (3) is reachable, while yielding a stable closed-loop system, if and only if ng = 1 and the NMP zeros {?i}qi=1 of G? constitute a subset of . 
 If the conditions required in (i) are met, then the unique stabilizing controller Co, which when replaced in Figure 1 yields an SNR equal to ?d, is given by 
 Proof. (i) As discussed above, the lower bound in (3) requires Tvn to take the form given in (11), which happens if and only if the C is given by (9) as C = (1/Tvn -1)/G. A construction of the optimal controller is also offered for example in [7, Theorem III.2] using the Youla parametrization approach, but such approach makes the characterization of the controllers zero locations obscure. Since (11) yields that (1/Tvn - 1) has relative degree equal to 1, it follows that when ng > 1 the required C has negative relative degree, which is not realizable causally. If ng = 1, this required controller is biproper, yields the stable Tvn given by (11) (thus satisfying conditions (ii) and (iii) in Lemma 1) and does not cancel the unstable poles of G. The only remaining condition for C to be a stabilizing controller is that it does not cancel the unstable zeros of G? either, which is equivalent to  being a subset of the roots of 1 - 1/Tvn. This proves claim (i). 
 (ii) When Tvn satisfies (11), the set  comprises the m-1 roots of 1-1/Tvn. These values can be studied using standard root-locus analysis, which characterizes the roots of the expression 1 + KL, as the scalar K is varied, for any given rational transfer function L [25]. Letting L = 1/Tvn, we will show that when K = -1, all the roots of 1 + KL lie outside the unit circle. 
 Since Tvn is as in (11), it follows that L has all its poles outside the unit circle and all its zeros inside the unit circle. Recall that the roots of 1 + KL continuously “migrate” from the poles of L (at K = 0) to the zeros of L (as K is decreased toward -8). Hence, for K sufficiently close to zero, the m zeros of 1 + KL lie outside the unit circle. 
 Suppose that 1 + KL = 0 for some z = ej?c when K = Kc, for some Kc < 0 and some ?c ? [-p,p]. 
 Then L(ej?c) = 1/Tvn(ej?c) is real and positive. Recalling that ] (see (10)), we conclude that  . Thus, as K is decreased from 0, all the roots which start outside and end inside the unit circle lie exactly on the unit circle only when 
 , which is strictly less than -1. Therefore, all the m - 1 zeros of 1 - 1/Tvn are non-minimum phase, proving claim (ii). 
 (iii) The fact that (ii) holds makes it feasible that C does not cancel the NMP zeros of G?, which happens if and only if these zeros are a subset of . In the latter case, the controller C must be given by (9), thus taking the form (13), which yields the SNR-minimizing Tvn given by (11). Therefore, this is the only controller which stabilizes the closed-loop system while satisfying the conditions for equality in (3). This completes the proof. 
 Example 1. Consider an NMP unstable plant with relative degree ng = 1 given by  , with 
 then the channel SNR can be set to ?d. For ?1 and ?2 real numbers, then ?1 further simplifies into   More so, the optimal controller that achieves ?d is then defined as 
 Thus, we have obtained a biproper controller without unstable zero/pole cancellations which yields a stable Tvn such that |Tvn(ej?)| = 2i=1 |?i|, ?? ? [-p,p], and thus kuk2Pow coincides with ?dQ· s2, and thus achieves the lower bound in (3). 
 Remark 2. Claim (ii) in Theorem 3 reveals a perhaps surprising fact: in the setup corresponding to Figure 1, when G is MP or its NMP zeros are a subset of , then the SNR-minimizing controller has at least m-q -1 non-minimum phase zeros. This may seem counterintuitive since, in general, introducing NMP zeros in the loop transfer function makes the closed-loop system harder to control (it limits the bandwidth of the complementary sensitivity function and makes it minimum phase) [26]. 
 Remark 3. Despite the fact that specific NMP plants satisfying the conditions of claim (ii) of Theorem 3 do exist, the latter theorem asserts that for almost all NMP plants with ng = 1, and for all plants with ng > 1, the minimum required SNR for stability in the scheme of Figure 1 is in general strictly greater than the lower bound ?d. Therefore the proposed result is theoretical in nature, since the plant model poles and zeros should not be assumed as design variables. This motivates the explicit definition of a 2 DOF controller, as discussed in the following section, for dealing with the practical aspect of plant model NMP zeros not necessarily at the locations z = ?i, whilst achieving the minimal SNR for stability imposed only by the plant unstable poles. 
 When the channel output is directly fed back, then an alternative two DOF controller solution that achieves ?d is possible, see Figure 2. 
 Theorem 4. The two DOF controller that achieves ?d when the unstable plant model G? contains arbitrary NMP zeros ?j with j = 1···q is given by 
 ,	(14) where for C1 the coefficients bi are the solution of the following set of m algebraic equations 
 and for C2 the coefficients cj are the solution to the next set of q algebraic equations 
 . Since we want to achieve ?d we therefore impose the need for C1G? - C2 = CoG, with Co as in (7) and G as in (1). We then further assume that C1 = 
 Since we require for stability that C2 ? RH8 (the space consisting of all proper and real rational stable transfer functions, [27]), the numerator of C2 must contain all the 
 m unstable poles of G, thus, for all l = 1,··· ,m. The simplest structure for C˜1 that satisfies these m interpolations is the one represented by  . The values bi’s are easily obtained by imposing the set of algebraic equations in (15), from which we then obtain C1 as in (14). To obtain C2 we follow a similar approach. From C1G? -C2 = CoG, evaluating at each NMP zero of G? we have that C2(?l?) = -Co(?l?)G(?l?), for l? = 1,··· ,q. To satisfy this q interpolation we propose C2 as in (14). Finally, to obtain the coefficients cj that achieve the given interpolations we then impose the algebraic equations in (16) and obtain C2 as in (14), which concludes this proof. 
 The remarkable implication of Theorem 4 is that we can effectively reduce the infimal SNR requirement, eliminating the increasing effect due to the presence of the NMP zeros, whilst remaining in an LTI setting. This is achieved by changing the plant model as seen by the channel input, through the second path that feeds the channel output through C2, see Figure 3. 
 Remark 4. The result from Theorem 4 also applies when the communication channel is located over the feedback path (between the plant and controller) and r = 0. 
 Fig. 3. Two DOF NCS LTI solution for Example 1, equivalent plant model configuration. 
 Example 2. Consider . We recognize then Gs? = 1z, m = 1 and q = 1. For one unstable pole ? we also have that  and . We then have from Theorem 4 that  , and . 
 (16) we have that . Therefore the two DOF controller that achieves ?d for the plant model G? studied in this example is given by 
 As a note, observe that if ? ? 0, then the C2 path opens up, and the plant model G? ? G. We then recover the standard infimal SNR controller solution for stabilizability of an unstable plant without NMP zeros. 
 In this work we have studied how to avoid the effect of NMP zeros on the minimal required SNR for output feedback stability. The first observation is that the optimal controller that achieves the minimal SNR for a minimum phase plant with no time delay, has all its zeros located outside the unit circle. This observation allow us then to characterize a set of specific NMP zeros that can be now located at the plant model, and would not increase the minimal SNR for stability above the expression imposed by only the plant unstable poles. We then used a two DOF controller structure to propose, for output feedback, the synthesis of such controllers that can retrieve the minimal SNR ?d, even when the unstable plant model has NMP zeros. Future research should consider extending the results for more complex communication channel models and replicate this study for the continuous-time setting. 
 ﻿ We characterize the rate distortion function for zero mean stationary under the fidelity criterion and subject to the additional constraint that the distortion is uncorrelated to the input . The solution is given by two coupled through a single scalar parameter . This a structure similar to the well known water filling solution without the uncorrelated distortion restriction . Our fully characterize the unique statistics of the optimal distortion . We also show that , for all positive , the minimum achievable rate subject to the constraint is strictly than that given by the un constrained rate distortion function . This gap with the distortion and to infinity and zero , respectively , as the distortion to zero and infinity . 
 Many source have the property that the end to end reconstruction error is uncorrelated with the source . We refer to such as uncorrelated distortion . As an example , consider a typical transform coder , as in Fig . . Here , a random vector is first by an analysis transform to yield U . Then U is , yielding the vector , U . The input signal is finally by , where is the synthesis transform , . If the quantization error is 
 uncorrelated to U , and if I , then it is easy to show uncorrelated to , thus yielding a coder . 
 More generally , any quantization scheme satisfying the following two a coder : a The error by the is uncorrelated to its input ; The linear if any before and after the perfect reconstruction in the absence of quantization . Property a is satisfied in many , e .. in high resolution or when a with dither either subtractive or non subtractive is employed . On the other hand , the condition Property is often sometimes implicitly in the design of filter , transform , , and feedback , . Thus , any source coder , for example , quantization , is a coder . The rate distortion performance of any coder can be to the underlying rate distortion function of the source , for a given distortion metric . One may question whether such a comparison is , in fact , fair . After all , the additional constraint that the end to end distortion is uncorrelated with the source is not upon . With this in mind , let denote the rate distortion function with the additional constraint that the end to end distortion is uncorrelated to the source . A formal definition of is given in Section . Clearly ,. However , to the best of the knowledge , the problem of not been formally before . Therefore , such as in which if any , and how can be , appear to be unanswered . 
 In this paper , we not only give conclusive to the above , but more importantly , we completely characterize for the quadratic case as a lower bound for the rate achievable under the uncorrelated distortion constraint . We show , in Section , that can be through a single scalar variable a . This is a result which the conventional water filling that describe . We characterize the unique optimal statistics that the reconstruction to have in order to achieve , for a given source . In particular , we show be . In addition , we recast the in a transform sense . More precisely , we show that if the quantization are , independent both mutually and from the source , then the Transform is optimal among all perfect reconstruction , at all . A comparative analysis between and is then in Section . There we show that is convex and monotonically decreasing in , and that , , converging in the limit as . 
 Furthermore , we show that , which is different from the well known result . 
 It is worth that our are not tied to any particular source architecture , but are general in the sense that any scheme in which the end to end distortion is uncorrelated with the source can do no better than . 
 Notation We use to represent random , a subscript when to one of its , i . e ., Xi is the i th element of the random vector . The expectation operator is by E . bold are used for matrices . The positive definite square root of a positive definite by . We write and for the determinant and the trace of a matrix , respectively . The probability density function 
 and covariance matrix of a random column respectively by and , where is the transpose of . We write , for the cross covariance matrix between two . The spectrum of a ... random function is by , 
 ,. The differential entropy and the differential entropy per dimension of an length random , respectively , by . a random process , the differential entropy rate of . 
 We use to refer , respectively , to the mutual information and the mutual information per dimension between two . random , the mutual information rate . We write a . e . for almost everywhere . 
 We begin by the definition of the quadratic rate distortion function under the constraint that the end to end distortion be uncorrelated with the source . Then , in Section . , we characterize this function for random , the case of stationary to Section . . 
 Definition . The uncorrelated quadratic rate distortion function for a random vector source is defined as 
 We now present one of the main of this paper , namely that , for vector , is given by two linked through a single scalar parameter . This the water filling that describe . The proof of this result , which is in Theorem , use of the following lemma . 
 Lemma . Let Let and be two random with zero mean and the same covariance matrix , i . e ., , and the same matrix with respect to , that is If jointly , and distribution , then 
 If furthermore , then equality is in , jointly . 
 where is the relative entropy or distance between the two probability . The equality a from the fact that log is a quadratic form , and from the fact that , The inequality from the 
 fact that , with equality . Thus , equality is y 
 Remark . We note that the above Lemma Lemma . in , by the requirement that and , to the requirement , 
 Theorem . Let the source be a zero mean random vector with positive definite covariance matrix Then 
 Proof . Let U denote the set of all length random uncorrelated to , and define the 
 With the above , can be written as where a directly from Lemma and where since the definition of see , that both and exist . 
 We now prove , by contradiction , that the minimizer of log I in , namely , is such that . For this purpose , suppose that , and let be the of be a random vector with covariance matrix 
 a strictly increasing function . Thus cannot be a minimizer of unless . 
 The minimizer of log subject to can be found a variational approach . More precisely , the covariance matrix of the minimizer must necessarily be such that the derivative of the 
 with respect to is zero at , for some , which is equivalent to the condition that the matrix differential , . the fact that log , for any positive definite matrix , the necessary condition be a minimizer the form 
 The fact that needs to be positive definite that and that it is infeasible to have a negative sign before the square root in . This , together with the change of variable , directly to , with a . On the other hand , the value of a must be such that the equality constraint is satisfied . From , and Lemma see appendix , this requirement is equivalent to , which . 
 Similarly , is by substituting into and then Lemma , which : 
 The uniqueness of a is easily by that the right hand side of is monotonically increasing with a . Since a is unique , it from that the covariance matrix of I ; is unique , the proof . 
 Transform Realization of : Closer examination of Lemma , when used in , that , for a source , can be by the transform architecture shown in Fig . . More precisely , an end to end distortion the optimal covariance matrix given by is by choosing the , where ,..., i . e ., the transform for , and by a random vector of quantization E with Interestingly , here 
 E is not a scaled identity matrix , as is usually the case in transform . This discrepancy from the approximation E cE , commonly used to link the variance of to the bit rate at which each th transform coefficient is . In this expression , is a constant that on the of the source and on the type of . The well known optimal bit allocation , e .., in , is based upon this formula , and thus the total bit rate . On the other hand , the optimal quantization by Theorem need to be , their being such that the end to end mutual information is . Thus , the difference in the optimal for in each case is due to the fact that . 
 The function defined in can be extended to random as : 
 Definition . The uncorrelated quadratic rate distortion function for a random defined as 
 The function for stationary random can be derived from the in Section . , by to random a covariance matrix , and then . More precisely , we have the following result : 
 Theorem . Let the a stationary random process with spectrum such that , a . e . on ,. Then 
 Y YN ,. It is known that ,, where and are 
 the of and , respectively see e .. , Theorem . . . This result , together with Lemma , in the Appendix , and the fact that , a . e . on ,, that , for all . We can then apply Theorem to each , , 
 and where the set of of . From , the optimal distortion covariance matrix for is 
 Remark . It is interesting to note that the the optimal feedback derived in achieve an end to end distortion whose spectrum is given precisely by . Furthermore , it is easy to show that such would achieve the function if the noise due to the scalar quantization within the feedback loop were white noise uncorrelated with the input . 
 The next theorem that strict and convexity with , but from in the asymptotic limit of large . 
 Theorem . For any random vector stationary random process with positive definite covariance matrix monotonically decreasing and convex . In addition ,. 
 Proof . We present here only the proof for the case of random . The proof for 
 stationary proceeds in an analogous fashion . : We have that 
 , provided that and exist and that the latter derivative is non zero . From , we obtain 
 proving that is a strictly decreasing function since a . Convexity : The fact that a monotonically with increasing , together with , imply that D D , and thus is convex . : It is clear from that and . Since , as can be seen from , monotonically with increasing a , a , , it that a , and that a . 
 Similarly , it from and the respect to a that , for fixed , a and a . We then have that and 
 Theorem . For any random vector stationary random process with positive definite covariance matrix with , a . e . on ,, the following 
 Proof . We present here only the proof for the case of random . The proof for stationary proceeds in an analogous fashion . Recall that for a random covariance matrix with one that , with equality . As a consequence , 
 Equality a is by substituting and into the right hand side of . The validity of then directly by taking the limit of the right hand side of as a . In order to prove i , we will show that , where the function is the inverse of and is distortion rate function . For this purpose , consider the random vector Y , where is a symmetric , positive definite matrix , as in Theorem . Notice that are not uncorrelated unless I . The mutual information per dimension between and 
 positive definite . We next show that for any and corresponding , choosing an optimal distortion is strictly smaller than 
 , the distortion associated with , with the covariance matrix in when . Since , we obtain , from Lemma , and after some algebraic manipulation , that 
 where the last inequality from the fact that . Finally , the fact that both and are monotonically decreasing , together with , i . This the proof . 
 that a , which that monotonically with increasing . On the other hand , for all , the ratio can be lower bounded by . It can be shown that this bound with a and thus well , tending to as a , which is in agreement with Theorem . 
 In this work we have completely , the quadratic rate distortion function subject to the constraint that the end to end distortion be uncorrelated with the source . We have further proved that this function convexity and with rate distortion function , but is positively bounded away from the latter , converging to only in the limit as the distortion to zero . We that the constraint the distortion to unboundedly grow as the rate to zero . We also the of for random and stationary random through transform and feedback quantization . 
 with , and where :, and , denote the th column and the th row 
 Q , respectively . If is analytic in a around each , for ,...,, then 
 Lemma Theorem . . in . Let A be an infinite matrix with entry ak on the th diagonal . Then the of A are in the interval , the essential and , respectively , of the function . Moreover , if finite and is any continuous function of 
 where the are the of the sub matrix A of A centered about the main diagonal of A . 
  
 ﻿ The present an achievable rate region for two pair bidirectional relay based on successive compute and forward method . In these , one relay in the communication between two of . In their scheme , the use lattice for the at the , and random for the at the relay . They use the successive compute and forward strategy to decode two integer linear of the lattice in the , and successive interference cancellation for the in the . The channel can be considered as a broadcast channel with two receiver , but within each group , a pair of is considered as an additive white noise channel instead of a broadcast channel because each node its own message . It is shown that for all channel gains of and all channel gains of symmetric channel , the strategy to within constant of and bit per user of the cut set upper bound for restricted and non restricted , respectively . These are than those previously for this network , which have not the successive compute and forward method . 
 Nowadays , the capacity of wireless is one of the main research in network information theory . As an interesting and important application , two way via have been , where the two way relay channel is used as a building block . Several were for the in , by extending and combining the basic for the relay channel , such as decode and forward , compress and forward and amplify and forward . When or , the effect of the noise remains on the signal by the relay . In contrast , by at the relay , the noise is completely removed from the signal . However , from loss . 
 Compute and forward is another strategy for the which lattice . In this method , the receiver need not decode directly ; instead , it a linear equation of the based on the fact that integer of also belong to the . In , an achievable scheme was for the based on lattice , which was shown to achieve within bit from the cut set bound , for all channel . This strategy noise suppression without loss . In , the capacity of the with relay private been within constant of and bit per user from the single sided genie upper bound for restricted and non restricted channel , respectively . 
 The extension of the to been done in , where more than two communicate via a relay in a directional manner . The way relay channel was studied in , where upper and lower for the capacity were given . In this setting , are divided into several , where each user in a cluster a single message intended to other in the same cluster . A similar symmetric setup with one cluster and equal channel gains , noise and power was considered in , where the sum capacity was . In , a type of three user was considered , where each user two independent , each of them intended to one of the other . 
 In , a compute and forward approach was to exploit interference between the in an additive white noise network . In their approach , are free to select integer that match the channel as closely as possible , thus reducing the effective noise and increasing the achievable . In , the successive compute and forward approach was , where after a linear combination , the relay can combine it with its channel observation to obtain a new effective channel that can be better for the next targeted linear combination . 
 In , the capacity of two pair two way relay for both linear shift deterministic and complex channel were . In these , a relay node the communication between two of . It was shown that the cut set upper bound is tight for the linear shift deterministic channel model . The strategy for the deterministic channel was to a specific superposition of lattice and random at the source and successive interference cancellation at the for the network . It was shown that for all channel gains , that strategy the cut set upper bound to within bit per user . In that approach , the lattice of were , so it was necessary for them to assume equal power for lattice of one pair , and to use for the excess part of the signal with extra power . However , the did not show that successive interference cancellation was actually feasible for lattice and together . 
 In this paper , we study two pair two way bidirectional relay , the same model considered in , where one relay can help the communication between two of see Fig . . For brevity , here we name it . Here , we present a achievable rate region for a class of full duplex by successive compute and forward method , which the ability to efficiently decode multiple linear of the . In contrast to the method in 
 , which is based on the superposition of random and lattice at the , and successive interference cancellation at the , in our method , it is enough to consider only lattice for at the , and successive compute and forward for at the relay in the . In the , random are used for at the relay , and successive interference are applied for at the . Moreover , in contrast to , we do not use and assume that all can transmit simultaneously . It is shown that for all channel gains and symmetric channel , the method within and bit per user of cut set upper bound for restricted and non restricted , respectively , which are than the and bit per user in . 
 The remainder of this paper is as . In Section , the system model , the cut set upper bound and some about compute and forward are . The achievable rate region for is in Section . Capacity gap and some are in and , respectively . Some concluding are provided in Section . 
 In this section , we review the system model for the asymmetric and its associated cut set upper bound . We also review some about lattice , and outline the successive strategy . The material of this section , for completeness , is a brief summary of the and from , in which the interested reader can find a detailed exposition . Consider an asymmetric as shown in Fig . , where and a pair of can communicate with each other via node a relay . There is no direct link between . The same situation for and . The relay is assumed to be able to listen and transmit at the same time . We use a real valued channel model for all in this network . Let log log , , log , U , , , , , , , , and for each i U , let denote the node on the other side of the relay which with i . The forward and backward of each pair are assumed to be different . Channel gains are written on each link in Fig . as , where i , and i . 
 Node i U , .., to be to node , where . Each user is assumed to choose its transmit message independently of the other . The user at node i U its message into a xi , xi .., an function , , for , …,, the length in channel , and the of the relay into a , .., an function , for , …,, is a of a real random variable , satisfying the power constraint i , and , .. the previously received at transmit at the same time , . that 
 transmission of each simultaneously . In this setup , non restricted , some dependency between the by different can be . In contrast , in restricted , apply the fi , thus yielding independent for all i . 
 The rate is said to be achievable if , for any and for large , there an , code with such that the probability of error is less than . 
 The received at node i U and at the relay given by the following , respectively 
 where is a of independent and identically distributed real valued noise , i . e . with zero mean and unit variance . For each i U , and denote the and channel gains , respectively . Each node i U , a function to decode as where , .., yin . 
 The cut set upper bound for the was in . We rewrite a reduced and equivalent version of it for both restricted and non restricted in the following theorem and corollary . For the proof and further , the reader can refer to . 
 Theorem : The capacity region of the non restricted is upper bounded by 
 Corollary : The capacity region of the restricted is upper bounded by 
 min , , , i , U , i 
 Remark : In , lemma , it is proved that and are within bit per user of each other . The latter was shown by the 
 When the channel is complex valued , the capacity to C so the previous yield a gap of bit for two or , equivalently , bit per user . However , in our problem , we have assumed real valued , for which the capacity is simply . Thus , the gap between upper for restricted and non restricted would be bit for two or , equivalently , bit per user . 
 Next , we review some on lattice and the compute and forward approach . For more see and the therein . 
 A lattice code is defined in of two dimensional and , which form a lattice partition , that is , . and are usually fine and coarse , respectively . The lattice code is a lattice code which as and the region of as a shaping region which is by . For , the set of coset is defined as . is chosen so that its average power per dimension is , that is , the second moment per dimension of a random variable uniformly distributed over is . The coarse and fine should be good for and channel , respectively for definition of lattice goodness see . The rate of the lattice code is defined as 
 In , Theorem , an achievable rate was for real valued based on the compute and forward strategy to decode one linear combination of . In , the successive compute and forward method was , where more than one linear combination of is desirable . We rephrase these in the following theorem for the case one receiver . 
 Theorem : For real valued with , one receiver and channel coefficient h , h .., and equation coefficient vector a a , a .., aL , the following computation rate is achievable 
 To successfully decode two linear , for example , a , , the following computation rate is achievable see 
 In other , the relay can first aim to obtain a linear combination that is easy to decode and then use it to create a better effective channel for the second linear combination . In Theorem , a selected according to the design where different linear of are desirable . In the next section , we present an application for that approach . 
 In this section , we present an achievable rate region for the . In our scheme , we consider restricted . The achievable rate region is also an achievable region for the non restricted . 
 In the , based on the fact that the of two , we use the idea of successive interference cancellation for lattice in which was in the previous section . By considering appropriate for a and , the linear function of the of each pair is , respectively , at the relay . 
 We would like to mention that in the superposition of and lattice from four is received at the relay in the . The relay then the successive interference cancellation method originally thought for for all , regardless of the fact that the are of different , that is , they have used the equation of for the lattice of each pair but they consider the other as in the rate equation . There is no proof for this performance and it does not seem to be a correct approach . 
 In contrast , we use the interference cancellation method that been for lattice in . Moreover , in this method , lattice and have been used instead of lattice alone . The motivation for this is that the former method been shown in to yield the rate log for equal power which is higher than the one in which is log . 
 In the , we extend the method for the in to the two of present in the . In this method , each pair of can be as an channel instead of a broadcast channel . This is due to the fact that each user of each pair its own signal , thus it can subtract this part from the received signal and can extract the message of the other user of that pair . This method higher than the method used in which as a broadcast channel with four . The full strategy is next . 
 According to the strategy , the transmit signal at user i U is given by , which a lattice code ensemble of lattice with size , where , , , , . should be good for and its second moment per dimension is . i should be good for channel . We consider the following channel vector between and the relay 
 The relay two linear of lattice code and , with the following 
 The coefficient a by successive compute and forward individually with transmitter as shown by the following 
 h 
 By substituting the above in and , and are , respectively . The are shown below 
 log b b a a aa r 
 log a Pa a a h r ra h P 
 Remark : As stated in , the closer a a a a a and are , the higher the rate by . However , in our problem , the two last of a are fixed to be zero , that is , a , a , , so as shown in the simulation section , not necessarily those of a , a which are closer to , result in higher . 
 Remark : Consider the case where each pair of in the symmetric channel gains , that is , and . For this case , a and higher as shown by the following 
 More generally , let us consider the channel gain vector as , where air for all i . It is possible to find , , and such that we have symmetric for each pair of , that is , 
 . For this case , by assuming and , and are simplified as 
 The relay two , from of sizes and , respectively . The signal by the relay is given by 
 The two following of all different of channel gains are sufficient for the analysis to be carried out . The other can be and will lead to similar by the indices . 
 . . Case : It is shown that , for any choice of channel gains , this can be done successfully as long as the following are satisfied . 
 Proof : User first by treating as noise . This can be done with arbitrarily small probability of error as long as is satisfied . Then , the signal is from the received signal and is . This can be done with arbitrarily small probability of error as long as is satisfied , because by the same conclusion in for , in the , each pair of in the can be as an channel instead of a broadcast channel . As before , this is due to the fact that each user of each pair its own signal , thus it can subtract this part from the received signal and can extract the message of the other user of that pair . Hence , the can be as a broadcast channel with two receiver , and , instead of a broadcast channel with four , as was considered in . 
 Clearly , and are redundant in view of and , respectively , because is an increasing function of . 
 . . Case : It is shown that for any choice of channel gains , this can be done successfully as long as the following are satisfied . 
 In this section , we compute the capacity between the achievable and the cut set upper bound by the strategy . For the , we obtain the capacity by considering symmetric channel gains for each pair of . For the , we consider the general case . 
 Lemma : : For the with symmetric channel gains for each pair of , for any rate , satisfying 
 with error probability for all channel gains . i U , can be done with arbitrary small 
 Table for a , a , b , b , for each channel coefficient vector h 
 with error probability for all channel gains . i U , can be done with arbitrary small 
 Fig . Non restricted and restricted cut set upper and achievable for , , , and different for a and b 
 Remark : Note that if I , I : unit vector , and , then the rate I the of and . Thus , the capacity of the restricted with symmetric channel can be within bit per user of the cut set upper bound . By against , we see that the gap is bit with respect to the cut set upper bound for non restricted . 
 In this section , we illustrate the performance of the method for the via and discuss the . All the rate shown in the and the table are in bit . 
 Fig . Cut set upper and achievable for . , Assuming , we consider several shown in . , . , . and a , a , and b , b , Table , and for each case , we compute the of the a , a and b , b which yield the highest and . As it is from the , not necessarily 
 and substituting into , in those of a , a and b , b which are closer to , and , result in higher . The first row of the table 
 r the of which are , and , , respectively . a , a and b , b the the for and . As it is seen , not necessarily with a , a , and b , b , Other can be proved similarly . in higher . of the table the for 
 Fig . Non restricted and restricted cut set upper and achievable for , , , by our method and the method of for the communication 
 . It can be seen that , for most , a , a , in higher . of the table show the for and , in this case , with a , a , and b , b , in higher . 
 Fig . the cut set upper bound and the achievable for an channel gain vector equal to , , , and different for a and . It is seen that the choice a , a , and b , b , in higher . This result is also shown at row of the table . 
 Fig . the cut set upper bound and the achievable for a symmetric channel gain vector equal to . , . , . , . , and for a , a , and b , b , , which yield the rate region . As it is seen from the figure , the distance between the achievable rate , . , . the last row of Table and the cut set bound is bit , which is than the found in Lemma . 
 Assume the channel gain vector to be , , , , and . Fig . the cut set bound , the achievable by the method and the of for the communication . As it is seen from the figure , for most , the method that are within those by Lemma . 
 In this paper , we studied the capacity region of an asymmetric full duplex with no direct link between . We established an achievable rate for this network by lattice for at the and random for at the relay . For , we successive compute and forward and successive interference cancellation at the relay and at the , respectively . It was shown that , for the with symmetric channel , the achievable rate is within constant of and bit per user of the cut set upper bound for restricted and non restricted , respectively . In this paper , we have used real valued . For complex valued , we have C instead of in the capacity expression , so the would be twice the for real valued , that is , and . bit per user of the cut set upper bound for restricted and non restricted , respectively . These are one half of those in , that is , and bit per user for restricted and non restricted , respectively . 
  
 ﻿We derive closed-form expressions for the secondorder statistics of the power gain (as a function of frequency) of wideband microwave indoor channels. We obtain our results within a framework that is general enough to be compatible with several popular channel models, such as the Saleh–Valenzuela (SV) channel model and those proposed by the IEEE 802.15.3a and IEEE 802.15.4a task groups. As in all these models, our channel description is based upon clusters and rays with (possibly mixed) Poisson arrivals and random amplitudes. Our results consist of closed-form expressions for the second-order statistics of the channel power frequency response, where statistical averages involve expectations over ray amplitudes and arrival times. These expressions reveal that the autocovariance of the spectral power gain between any two frequencies decreases and tends to zero as the difference between these frequencies tends to infinity if and only if the cluster arrival rate goes to infinity. They also show that the variance-to-squared-mean ratio of the narrow-band power gain exhibits exactly the same behavior with respect to the center frequency. We then use these results to obtain closed-form expressions for the variance and the second-order moment of the aggregate channel power gain over any given interval of frequencies. This allows us to express the channel spectral diversity as a function of model parameters and bandwidth. In addition, we illustrate how these equations allow one to devise automatic cluster identification algorithms which, from empirical estimates of the second-order spectral statistics of the channel power gain, can confirm or deny the existence of clusters in a given scenario. 
 Index Terms—Fading channels, Saleh–Valenzuela (SV) channel model, statistical channel modeling, ultrawideband channels. 
 TOCHASTIC wireless channel models allow one to predict the statistics of radio propagation conditions over an ensemble of scenarios with similar characteristics. This is particularly useful in complex, heterogeneous, and time-varying environments, such as office, residential, and industrial indoor scenarios. 
 One of the most popular models for indoor wireless channels is that proposed in [1], which has served as the basis for several other channel models. Building upon it, the IEEE 802.15.3a task group accepted a channel model [2] for ultrawideband indoor communications, and similar models have been adopted for the IEEE 802.15.4a standard [3]. Similar to the Saleh–Valenzuela (SV) model, the IEEE 802.15.3a and IEEE 802.15.4a channel models consist of a discrete-time description of the impulse response of a wireless channel, in which multipath components are grouped into clusters. 
 Some of the useful time-domain delay statistics of wireless channel models are the power delay profile (PDP), the average delay, and the RMS delay, which, under suitable assumptions, allow one to determine spectral statistics such as coherence bandwidth and average power gain. These parameters, which are associated with second-order statistics of the channel impulse or frequency response, have been extensively discussed and characterized in the literature [4]–[7]. 
 Another set of stochastic properties of wireless channels, which has received relatively less attention, is those derived from the second-order statistics of the channel power frequency response (i.e., its squared frequency response magnitude, which is denoted by |H(j?)|2). An early analysis of these statistics and their application can be found in [8], where it is argued that the transmission over two carriers with the right frequency spacing may benefit from the negative correlation between each of the received powers. Simplified expressions for the spatial and spectral autocorrelation coefficients of |H(j?)|2 and for its power over a frequency interval are presented in [9], which are then evaluated via simulations. Building upon this result and using simulations, it has been possible to assess fading depth statistics and relate them to the bandwidth and to some features of the propagation environment [10], [11]. 
 0018-9545 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 
 Although the simulations of the second-order statistics of |H(j?)|2 are helpful in assessing the performance of wireless communication systems, having closed-form expressions for these statistics provides further benefits. First (and naturally), these closed-form expressions can be substituted into other formulas, thus allowing one to relate them with performance indexes, which are also in closed form. Second, explicit parametric formulas will provide useful insight into the relationship between the performance of wireless communication systems and certain parameters. In particular, the second-order statistics of |H(j?)|2 allow one to refine probabilistic approximations drawn from first-order statistics alone. Moreover, if the power received over some band is known to distribute according to some parametric probability density function (pdf), then having the first- and second-order statistics of this power allows one to identify, in some cases, the parameters of the distribution [12]. 
 This makes it possible to calculate fade-depth statistics from the second-order statistics of |H(j?)|2. 
 In this regard, it was already noted in [9]–[11] and [13] that the fade depth decreases as the width of B increases. In relation to this question, and using an information-theoretic approach and measured data, it has been shown in [14] that the number of significant eigenvalues of the covariance matrix of the random channel impulse response and, hence, the diversity order of the channel scale approximately linearly with the bandwidth. This increase in diversity, which relates to the reduction of the relative channel power variance as the bandwidth increases, has been reported to reach a saturation point [12], [13], [15]. Having explicit expressions for this channel power variance as a function of bandwidth would allow one to predict the associated saturation bandwidth and relate it to environmental parameters. 
 Available closed-form results that allow one to analytically obtain the second-order statistics of |H(j?)|2 are rather limited. Although the analysis in [9] and [11] does obtain some intermediate closed-form expressions, the actual evaluation of these statistics is carried out by simulations. In addition, one of the simplifying assumptions underpinning the results in [9] and [11] is the uncorrelation between the amplitudes and arrival times of multipath components, which is known to contradict channel measurements [1], [16]. Under the assumption of uncorrelated amplitudes and arrival times, and considering uniformly distributed arrival times, closed-form expressions for the autocorrelation of |H(j?)| have been derived in [17]. Exact closed-form expressions for all the joint moments between |H(j?1)| and |H(j?2)|, for any frequencies ?1 and ?2, as a function of the PDP of the channel impulse response have been found in [18], assuming that multipath-component amplitudes are independent jointly complex Gaussian and that arrival times are fixed. However, although popular channel models such as the SV model [1] consider complex Gaussian multipath component amplitudes, other amplitude distributions have been reported in the literature (see, e.g., [2]). Moreover, the conditional independence assumption in [18] does not hold for channel models where the impulse response exhibits clusters, such as the SV and its several extensions [1]–[3], [16]. In a recent paper [12], analytical expressions for the autocorrelation of the channel-frequency-response squared magnitude |H(j?)|2 and the variance of the power over any frequency band have been derived for the IEEE 802.15.4a channel model, conditioned to fixed and given ray and cluster arrival times. The corresponding statistics over random arrival times were obtained via simulations. To the best of the authors’ knowledge, no closed-form expressions are available in the literature for second-order unconditioned statistics of the spectral power gain of wireless channel models such as the IEEE 802.15.3a and IEEE 802.15.4a, as well as for the other SV-like models. 
 In this paper, we derive closed-form exact expressions for the second-order statistics of |H(j?)|2 for a general class of extensions of the SV channel model. This class includes, as special cases, those accepted by the IEEE 802.15.3a task group and the IEEE 802.15.4a channel modeling subgroup [2], [3], [16]. Unlike [12], our expressions include expectations considering the randomness of both ray amplitudes and arrival times. As in all SV-like models, our channel description is based upon clusters and rays, with different power decay profiles among and within clusters. Our analysis framework is general enough to encompass model features such as mixed Poisson arrivals within clusters, as well as models in which the first ray of each cluster exhibits statistics different from the other rays (these two features are considered in [3], [16], and [19]). To this end, we first derive, in Section III, a general, exact, and closed-form expression for the autocovariance of |H(j?)|2. This expression is determined by four functions, corresponding to the second- and fourth-order moment delay profiles of the cluster and ray amplitudes multiplied by their respective arrival rates. We use these expressions to analyze the effect of having a finite/infinite number of clusters, as well as the implications of having only one cluster (or, equivalently, no clusters at all). We then use the former result to obtain, in Section IV, a closedform expression for the variance of the channel power gain PB integrated over any given interval of frequencies B. Our derivations predict a positive lower bound for the ratio between the variance of PB and its squared mean. This bound only vanishes as the arrival rate of clusters in the model tends to infinity, which is a result that is confirmed via simulations. (Some preliminary results on this topic were reported in [20].) In addition, the obtained equations allow one to devise an automatic cluster identification algorithm, which is capable of verifying or discarding the presence of clusters in an ensemble of channel impulse responses. This algorithm is shown in an example in Section V and can be considered an alternative to other cluster identification algorithms, such as those proposed in [21] and [22]. 
 Here, we formulate the wireless channel model framework to be utilized throughout this paper. The idea is to establish a framework that is general enough to encompass most of the SVlike models proposed in the available literature. With that idea in mind, and before proceeding, we will first review some of the salient features of several extensions of the SV model, from which we will then define the set of assumptions that our results will be based upon. 
 In all versions and extensions of the SV model, the channel is represented by the following random impulse response [1]–[3], [16]: 
 is the random arrival time of the mth path (or ray) in the ith cluster, Ti = 0 is the random arrival time of the ith cluster, and ti,m = 0 is the random delay of the mth path in the ith cluster relative to Ti.  Cluster and relative arrival times are ordered, i.e., Ti = Tj ?? i = j and ti,m = ti,n ?? m = n ?i. By definition, the cluster begins with its first ray; thus 
 Each random coefficient ai,m in (1) represents the amplitude of the mth path in cluster i. These random numbers are formed as follows: 
 Each  is a random variable representing the amplitude of the ith cluster, whereas each real-valued random variable ai,m = 0 denotes the amplitude (or gain) of the mth multipath component (or ray) within the ith cluster relative to that cluster’s amplitude. The meaning of the random numbers {pi,m} in (2) depends on whether a complex or real basebandi,m representation is used. For the former, pi,m = ejf , where {fi,m} are uniformly independent and identically distributed (i.i.d.) over [-p, p] [1], [3], [16]. For the real baseband representation, which is regarded as more suitable for ultrawideband applications, {pi,m} are binary random variables taking values from {-1,1} with equal probability [2]. In both cases 
 To establish a framework to encompass the various features of the SV-like channel models present in [1]–[4], [16], and [23] (and possible future ones), we will formulate now the least restrictive set of assumptions under which our analysis and results are valid. To do so, the following three definitions are necessary. 
 Definition 1 (Bounded-Density Markovian Process): We will state that a sequence of random arrival times  , with xi = xj ?? i = j, is bounded-density Markovian with the following conditions. 
 There exists a constant ? < 8 such that, for every ? > 0 that is sufficiently small 
 In other words, condition 1 in the given definition requires that the interarrival time just after an arrival at x (and, hence, the arrival density at x) depends, at most, on the value of x. 
 Definition 2 (Sequence i.i.d. Under Another Sequence): Let   be a bounded-density Markovian process, with x1 possibly being deterministically equal to 0. We will say that a sequence of random variables  is i.i.d under  if the following two conditions are satisfied. 
 Notice that condition 1 in Definition 2 corresponds to the conditional independence relationships between amplitudes and arrival times shared by all channel models considered in [1]–[4], [16], [23], and [24], except the 802.15.4a model for industrial environments under LOS in [3] and [16]. Similarly, taking {?k}k as the cluster (or relative ray) amplitudes and {xk}k as the cluster (or relative ray) arrival times, condition 2 encompasses all the amplitude distributions considered in [1]– [4], [16], and [23], including the cases in which the amplitude of the first ray or cluster takes a special form, as in CM3 (body surface to body surface, 3.1–10.6 GHz) and CM4 (body surface to external, 3.1–10.6 GHz) in [24]. 
 We will need one more definition to treat arrival-time distributions differently from plain Poisson processes, such as mixed Poisson arrivals [described in (6)]. To this end, we introduce the following single and joint arrival density functions. 
 Definition 3 (Arrival Density Functions): Let  be a bounded-density Markovian sequence of arrival times, with x1 possibly being equal to zero deterministically. For any ? = 0, denote the number of elements in  falling inside 
 where, for sets, | · | denotes cardinality. We define the single and joint arrival density functions of  as 
 One case in which arrival times do not conform to a Poisson process is in the model proposed in [19], which is further considered in [3] and [16]. In that case, rays within each cluster are described as having mixed Poisson arrivals; in which case, the ray interarrival times are i.i.d. with the following pdf: 
 Using the given definitions, we can now establish the scope of validity of our results by means of the following assumption. 
 Assumption 1: Cluster and ray arrival times and amplitudes are random and as follows. 
 ?n,m ? {1,...,Nr} (7a) pj,n ? pi,m, unless j = i and n = m (7b) aj,n ? ai,m, unless j = i (7c) 
 where ? denotes probabilistic independence, and the Markov chain notation a ? b ? c means “a and c are 
 {Ti} is a bounded-density Markovian sequence, with T1 that is possibly deterministically equal to zero, i.i.d interarrival times exponentially distributed with exponent ?, and arrival density functions ?¯T(·) and ?¯¯T(·,·). 
 {ti,m}8m=1 is a bounded-density Markovian sequence with ti,1 =   and arrival density functions ?¯t(·) and ?¯¯t(·,·) for every i ? N. 
 is i.i.d. under , with mth-order moment delay profile functions, as shown in (8a) at the bottom of the page, for m ? {1,2}. 
 is i.i.d. upon , with mth-order moment delay profile functions, as shown in (8b) at the bottom of the page, where m ? {1,2} for every i ? N. 
 Notice that, in the definitions in Assumption 1, functions bm(·) and gm(·), m = 1,2 capture the moment delay profiles between clusters and within clusters, respectively, without taking arrival rates into account. In turn, the single and joint arrival rates of clusters are represented by functions ?¯T(·) and ?¯¯T(·,·), whereas the single and joint relative arrival rates of rays are given by ?¯t(·) and ?¯¯t(·,·), respectively. 
 The requirements and suppositions in Assumption 1 are satisfied for all the channel models described in [1]–[4], [23], and [24]. Indeed, by choosing cluster and ray arrival times as Poisson processes, with each |ai,m| conditioned to ti,m being Rayleigh distributed with second moment g2(t) = exp(-t/?), and deterministic Ai’s with the form Ai = b0 exp(-Ti/G), G, ? > 0, we obtain the widely used SV model [1]. A similar choice with lognormally distributed amplitudes |Ai| and |ai| yields the model proposed by the IEEE 802.15.3a task group [2] (excluding its large-scale fading term). The decay profiles of the amplitudes [represented by the functions b and g of (8)] can be also chosen to match those that the IEEE 802-15.4a standard recommends for high frequencies in some scenario types (such as office and industrial scenarios) [16]. 
 0 deterministically and(8a) is randomRemark 1 (Mixed Poisson Arrivals do not Give Rise to a Mixed Poisson Distribution): It is worth mentioning at this point that the mixed Poisson arrivals, which are characterized by the conditional pdf given in (6), do not give rise to what is known as a mixed Poisson distribution [25]. More precisely, if the random exponent selection in (6) takes place after each arrival, then the probability of having n arrivals in any unit-length time interval [x, x + 1] cannot be written as ßpn(?1) + (1 - ß)pn(?2), where pn(?) denotes the Poisson probability mass function with parameter ?. (The exact probability distribution has been derived by the first author in a work currently in preparation.) 
 Another assumption that will facilitate the forthcoming analysis is related to the number of clusters and rays. Although in (1), as in [1], [2], and [16], the number of clusters Nc and the number of paths within each cluster Nr are finite, we will consider infinitely many clusters and rays per cluster. As acknowledged in [1], the latter choice is arguably more realistic. Of course, this assumption requires that the amplitudes decay fast enough with increasing delay. We will make this requirement precise by assuming that the conditional moments of the amplitudes defined in (8a) and (8b) satisfy 
 with ai,m and ti,m as defined in Section II-A. In addition, as in [2], we will adopt a real baseband model for the impulse response; hence, the coefficients pi,m in (2) are i.i.d. Bernoulli random variables taking the values 1 or -1 with equal probability. This is done mainly to simplify the notation, and it is worth mentioning that all forthcoming results also hold for the complex baseband representation. (All that is required is that (3) and Assumption 1 hold.) 
 A deterministic frequency-dependent gain for each ray, which is a feature discussed in [4], [16], and [19], can be easily incorporated to our model by simply multiplying |H(j?)|2 by the squared magnitude of this gain. For ease of notation, and because the effects of this factor on the resulting statistics can be easily added afterward, we will not include it in our expressions. 
 In the following and under Assumption 1, we will derive the exact closed-form expressions for the second-order statistics of |H(j?)|2 and of the channel power over any given frequency interval as a function of the moment delay functions b2, b4, g2, and g4. 
 Here, we will obtain closed-form expressions for the mean, the autocovariance, and the autocorrelation coefficient of |H(j?)|2 over any interval of angular frequencies. These results rely on several technical lemmas, which can be found in the Appendix. 
 From (10) and the zero-mean and independence properties of the polarities {pi} established in (7), it is easy to verify that 
 Thus, the expected value of |H(j?)|2 is the same for all frequencies, being equal to the product of the average energy of the cluster amplitudes {Ai} and the ray relative amplitudes {ai,m}. 
 The following theorem, which is the main result of this paper, provides an exact closed-form expression for c(?1, ?2) in terms of the arrival density functions ?¯T, ?¯¯T, ?¯t, and ?¯¯t and the moment delay functions b2, b4, g2, and g4 defined in Assumption 1. 
 Theorem 1: LetclustersandraysdistributeasinAssumption1. Define the effective moment delay profile functions 
 Proof: See the Appendix.	 Remark 2: Theorem 1 reveals that the variance of the channel power gain |H(j?)|2 exhibits the same variation with ? as does its autocovariance c(((T + O)/2),((T - O)/2)) with O. Since the overall behavior of the latter is to decrease as O grows, it follows that the variability of the channel power gain is smaller at higher frequencies. Notice that this reduction does not come from the frequency dependence affecting each multipath component (which is typical of real indoor propagation channels [26]) since we have not included it in our model. However, had this dependence been included, the variance-to-squared-mean ratio of |H(j?)|2 would still exhibit the decay with ? predicted by Theorem 1.  Instead, the origin of this behavior can be more intuitively understood by looking at (37) in the Appendix and by recalling that E[|H(?)|2] is independent of ?. In (37), if we choose ?2 = ?1 = ?, then T2(?1 + ?2) = T2(2?) becomes the only frequency-dependent term. Now, T2(2?) is a sum of expectations of the random variables  . At ? = 0, T2 turns into a sum of nonnegative terms. By contrast, if we let ? increase so that the pdfs of the random variables 2  extend smoothly over an interval that is several times larger than 2p, then the phases of the exponentials will be approximately distributed uniformly over [0,2p]. This will reduce each of the expectations in the sum. Thus, T (2?) decreases as ? grows, and so does E[| (?1)|2 (?2)|2]. 
 Remark 3: If the effective moment delay functions and Bm are “smooth,” then all the frequency-dependent terms in (15) vanish if ?1 - ?2 tends to infinity (while keeping ?1,?2 > 0), leaving only   - 
 . This implies that the autocovariance between two infinitely distant frequency values is greater than zero, which, at first sight, may seem counterintuitive. Nevertheless, although this seems to be the first time that such behavior is proven to exist for SV-like channel models, it is consistent with the saturation of the channel diversity order as the bandwidth increases, as reported in [13] and [15]. (Further treatment of this phenomenon is presented in Sections III-B3 and IV.) As will be discussed in the following, this nonzero asymptotic autocovariance is due to that the arrival rate of clusters is finite. 
 1) Infinitely Many Clusters: Suppose the arrival densities of rays and clusters are scaled by ? > 0 and µ > 0, respectively, to obtain arrival rates 
 Let us also suppose that this is done while preserving the original PDPs (so that the total impulse response power is maintained), which requires one to scale g2 and b2 by ?-1 and µ-1, respectively. This implies that functions g4 and b4 are scaled by ?-2 and µ-2, respectively. Denote the resulting scaled moment profile densities by , and ?. 
 Substituting them into (14), the behavior of the terms involved in the frequency-independent part of (15) as cluster and ray densities going to infinity is given by the following limits: 
 c(?1,?2) = 0 if the cluster arrival rate tends to infinity and the right-hand side (RHS) of (16b) is zero. With this, as µ ? 8, in the case in which clusters have Poisson arrivals (which implies ?(y, z) = B2(y)B2(z)) and if the first cluster arrives randomly (i.e., if q = 0), then the autocovariance of |H(j?)|2 takes the simpler form 
 squared magnitude Fourier transforms of the intercluster PDP and the intracluster PDP evaluated at ?1 - ?2 and ?1 + ?2. 
 Only One Cluster/No Clusters: Consider the case in which rays do not exhibit the presence of clusters. This situation can be analyzed under the framework defined by Assumption 1 by supposing that there is only one cluster with deterministic arrival time T1 = 0 and amplitude A1 = 1, and such that its first ray arrives deterministically at time t1,1 = 0. To ensure that there are no other clusters, we may take E[Ak|Tk = T] =  . With these choices, we obtain  1, and	 0, 
 Nonzero Asymptotic Autocovariance: As previously mentioned in Remark 3, Theorem 1 reveals that, for finite cluster arrival rates, the autocovariance of |H(j?)|, i.e., c(?1,?2), tends to a positive value as ?2 ? 8 (i.e., if O and T tend to 8), and not to zero. To the best of our knowledge, this is the first time that this result is derived or revealed, at least for an SV-like channel model. (A simple and intuitive explanation of this phenomenon is provided at the beginning of Section IV-B, in terms of the variance of the aggregate channel power gain over a given band). It is worth noting that such asymptotic behavior cannot be observed if one analyzes the autocovariance of the complex frequency response H(j?), i.e., by looking at the second-order statistics of H(j?) instead of those of |H(j?)|2. Indeed, by applying the independence relationships (7a) and (7d) and then Lemma 1, it becomes easy to show that 
 This simple expression (which we believe to be novel), implies that, if the moment delay profiles of clusters and rays within them are smooth, then lim|?1-?2|?8 E[H(j?1)H*(j?2)] = 0. 
 The correlation coefficient of |H(j?)|2, which is denoted as ?(?1,?2), is given by 
 It was already shown that, for any given smooth moment delay profile functions, the autocovariance between two frequencies ?1 and ?1 + O tends to zero as O ? 8 if the cluster arrival density goes to infinity. We shall see in Section V-A that even when this arrival density is finite, limO?8 ?(?1,?2 + O) = 0 if the moment delay profiles change so that the number of clusters with a significant amplitude tends to infinity. 
 Here, we will use Theorem 1 to derive closed-form expressions for the mean and the variance of the total channel power over any given frequency band, i.e., 
 The	variance	of	the	channel	power	over	band	B can be obtained directly from the autocovariance of |H(j?)|2 as  	(19) 
 Before proceeding, it is worth noting that (19) readily implies that, if c(?, u) is bounded, then 
 since E[PB] grows linearly with |B|. This allows one to provide a simple and intuitive explanation for the fact that c(?, u) (i.e., the autocovariance of |H(j?)|2) does not vanish when the two frequencies ?, u = 0 are infinitely distant from one another. Recall from Parseval’s theorem that, when |B| ? 8, PB equals the total impulse-response power. Then, if the impulse response is amplitude-modulated by a finite number of randomamplitude cluster envelopes, it is clear that the ratio variance of impulse-response power over the square of the expected impulse-response power will not be zero. In view of (20), this will imply that  0. That is, the autocovariance of |H(j?)|2 between a pair of infinitely distant frequencies will not be zero whenever the total number of clusters with random amplitude remains finite. As anticipated, this provides an alternative and more intuitive explanation to what was already discussed in Section III-B3. 
 We now return to characterizing the variance of PB. From (15), c(?1,?2) can be written as 
 Recalling the change of variables O = ? - u, T = ? + u and integrating along diagonal strips over the square [?l,?r] × [?l,?r], we obtain 
 We note that var(PB) is a measure of the fluctuation of the received power over a wireless channel and that the latter defines its fade statistics. The fact that fade depth decreases with channel bandwidth has been reported in the literature, both from empirical data (see, e.g., [13]) and from using simulations [12], [27]. However, a closed-form formula relating fade depth and the classical channel model parameters used here have, to the best of our knowledge, not been reported to date. The results derived earlier allow us to directly deal with this issue, provided that the fading distribution can be fully determined from its second moment (an assumption that was successfully applied in [12]). The explicit dependence between var(PB) and W is illustrated in the following example. 
 Here, we will illustrate the application of the results obtained in Sections III and IV to the “classical” SV model [1]. In particular, we will show that our analytical expressions found in Sections III and IV accurately predict the autocovariance of |H(j?)|2 and the variance of the channel power gain over any given band. In addition, we will show how the presence of clusters can be confirmed or discarded by using the secondorder statistics of |H(j?)|2 and the expressions derived in this paper. 
 In the SV model, the amplitudes |ai,m| are Rayleigh distributed (conditioned to ti,m), and the Ai’s are deterministic exponentially decaying. The PDPs of cluster and ray amplitudes are given by 
 where the last equation follows since the fourth moment E[x4] of a Rayleigh-distributed random variable x is related to its second-order moment as E[x4] = 2E[x2]2. Both cluster and relative ray interarrival times are exponentially i.i.d. with exponents ? and ?, respectively. In addition, the arrival times of the first cluster and the relative arrival time of the first ray in every cluster are, by definition, zero. Therefore, the effective moment profile functions defined in (14) take the following form: 
 Fig. 1 shows (with dashed-line curves) the empirical estimates of c(·,·) of the SV channel model with four different sets of parameters, which are specified in Table I. Each of these curves was obtained after 300000 simulated random realizations of a channel impulse response. On the same plot, the theoretical values of c(·,·) predicted by (28) are traced using solid lines, for each set of parameters. It is shown that, in all cases, (28) matches the simulated data very tightly. 
 The set A parameters correspond to those proposed in the IEEE802.15.3a CM3 [2]. Notice that, for the curves of this set, as O is increased, the spectral autocovariance drops by approximately 3 dB when O = 1/G = 7.14 × 107 rad/s, and then decays at about 10 dB/dec, which is precisely the behavior determined by (28) (provided K2 is much larger than K1 and K). From (28), the next corner separation frequency, beyond which c(·,·) almost ceases to diminish, takes place when  , which in this case corresponds to   rad/s, in agreement with what is shown in Fig. 1. The set B curves differ from those of the previous one in that the arrival rates are halved, whereas the decay exponents are doubled. This yields the same products ?G and ?? and the same ratio G/? as the set A. Thus, the constants K, K1?2, and K2G2 are also the same as those yielded by the set A parameters. Accordingly, the only difference with respect with the set A curves is that the corner frequencies are reduced by a factor of 
 Fig. 1. Spectral autocovariance of |H(j??)c|2=as a function of the frequency109 rad/s) for the SV model separation O (around a central frequency with the four sets of parameters shown in Table I. Simulated curves (in dashed line) are averages over 300000 channel realizations. Each theoretical curve (in solid line) was obtained using (28). 
 its value in set A increased the corner frequency 1/? associated with K1 by 4, which is sufficiently higher than 1/G to have a small but noticeable effect (see the slight “bump” in the set C curves between 2 × 108 and 7 × 108 rad/s. With the parameters of set D, these two corner frequencies are two decades away from one another, with their presence becoming clearly visible in the corresponding plot in Fig. 1. Intuitively, the existence of two corner separation frequencies beyond which the spectral autocovariance starts (or restarts) to decrease can be associated with the temporal resolution associated with O. More precisely, when O is too small, all rays in the impulse response are added with roughly the same phase in the sum (10) for ?c + O/2 and for ?c - O/2, yielding a high correlation. As O is increased, a point is reached   at which some clusters within the impulse response contribute with different random phases in (10), which begins to decorrelate the channel power gains. This reduction ceases when O is large enough so that all clusters within the impulse response are added with different phases, restarting only when O begins to make also the rays within each cluster add with different random phases  . 
 The variance of |H(j?)|2 is directly obtained by evaluating (28) for an arbitrary frequency ?/2, which yields 
 By substituting this and (28) into (18), the correlation coefficient takes the form in (29), shown at the bottom of the page. By substituting the definitions of K, K1, and K2 into this expression and after some algebra, one obtains that the correlation coefficient satisfies 
 Thus, extending what was found for the autocovariance of |H(j?)|2 in Section III-B1, we see here that the lower asymptote of the spectral power correlation as O ? 8 vanishes not only when ? ? 8 but also when the product ?G ? 8. 
 Moreover, for any ?1 ? R, we have that limO?8 ?(?1,?1 + O) = 0 if and only if ?G ? 8. 
 When analyzing a sufficiently large number of realizations of the impulse response of a wireless channel, the only SV model parameters that can be directly estimated are the ray arrival rate ? and the intercluster decay exponent G. In contrast, the intercluster arrival rate ? and the intracluster decay exponent ? are not readily observable. Indeed, the very existence of clusters is not unquestionably evident from the impulse-response realizations. 
 Here, based upon the results earlier, we propose a quantitative method for estimating ? and ?. By doing this, it is possible to assert the presence of clusters (if the value 1/? obtained is comparable to or smaller than G) or the absence of them (if 
 As previously mentioned, the intercluster decay profile G and the intracluster arrival rate ? can be estimated directly from measurements (as done in, e.g., [1] and [19]). Additional equations for finding the remaining two parameters (? and ?) can be generated by evaluating c(O,O) at two or more different frequencies.5 For the SV channel model considered in this 
 4If there were no clusters, the impulse response would be as if there were only a single cluster, beginning at t = 0, with decay exponent G. 
 5In practice, it will be also necessary to identify the power scaling factor of the impulse response, namely, the value of b2(T) at T = 0. This can be easily done by taking the empirical mean of the squared magnitude of the first ray in the impulse response. 
 The left-hand sides (LHSs) of these expressions (L1 and L2) can be directly estimated from the measurements. Denote these estimates as Lˆ1 and Lˆ2, respectively. In principle, one could express ?G and ?? in closed form as functions of L1 and L2, and then evaluate for L1 = Lˆ1 and L2 = Lˆ2. However, ?? is in this case the solution to the following quadratic equation: 
 resulting from substituting (30) into (31). Therefore, the approximate nature of Lˆ1 and Lˆ2 can turn a real positive root of (32) into a complex-valued (or negative) one. Moreover, even if one could measure L1 and L2 with infinite precision, there can be still two real-valued roots of (32) greater than 1. To overcome these difficulties, the following algorithm is proposed. 
 for L1 and L2). Denote these minimizers as y˜1,...,y˜p, respectively, where p ? {1,2}. From these, calculate 
 ? the nonnegative pseudominimizers yi = max{0,y˜i}, i = 1,...,p. These will be the preliminary candidates for ??. 
 Let xi = Lˆ1/(yi + 1) - 1, i = 1,...,p, be the corresponding preliminary candidates for ?G. 
 Then, apply a least squares curve-fitting optimization algorithm, with ? and ? as unknown values, to match (28) against the empirical estimates of c(·,·) at several frequencies. Use ?i and ?i as initial values. Define the obtained residual matching errors as i 
 This technique for estimating ?G and ?? was applied to simulated impulse responses for the SV channel model, with 
 Fig. 2. (Left) Typical impulse response for the SV channel model with the parameters of (top) set E (? = 1.1 ns-1,? = 1.1 ns-1,G = 14 ns, and ? = 7.9 ns) and (bottom) set F (? = 0,? = 16.94 ns-1,? = 14 ns). Multipath components from the same cluster are shown with the same color. (Top right) Twelve estimates of ?G and ??, each from the 4000 realizations of the impulse responses for each set of parameters, which are obtained by applying the method proposed in Section V. (Bottom right) Autocovariance of |H(j?)|2 from (28) and ±5s range along them, where s is the standard deviation of the empirical estimate of 
 two sets of parameters. For the first of these sets (set E), ? = 1.1 ns-1, ? = 1.1 ns-1, G = 14 ns, and ? = 7.9 ns. Notice that this makes ? = ?, i.e., the clusters arrive at the same rate as the multipath components within them, making it very difficult to distinguish one cluster from another in the time domain. 
 It may be noticed that the chosen intercluster and intracluster decay exponents (G and ?) are the same as those from the IEEE802.15.3a CM3 [2] used in Section V-B. In turn, the product ?? has been chosen about ten times larger than that used in Section V-B, so that the resulting dense train of rays makes it even harder to recognize the presence of clusters. A typical channel impulse response obtained for these parameters is shown in Fig. 2 (top left). As anticipated, with the chosen parameters, clusters arrive so densely that it is virtually impossible to tell one from the other. After simulating 12 sets of 4000 independent realizations of these impulse responses, estimates of L1 and L2 were obtained by averaging. From the corresponding 12 empirical estimates of c(0,?), which are obtained for 500 values of ? evenly distributed from 0 to 5 × 109 rad/s, the parameters ?G and ?? were estimated using the method described earlier, yielding the results shown in the scatter plot of Fig. 2 (top right). The average error magnitude of these estimates is less than 30% of their true values.  More importantly, in all cases, the existence of clusters is unambiguously revealed since all estimates of ?G are greater than 13. This seems more remarkable after recalling that, in this case, the impulse responses are such that clusters are almost totally overlapped, making it virtually impossible to distinguish one from the other. 
 The results for a scenario in which there are no clusters is shown in Fig. 2 (bottom). In this case, the set F parameters is used for the SV model: ? = 0, ? = 16.94 ns-1, and ? = 14 ns. A typical channel impulse response from these parameters, as the one shown on Fig. 2 (bottom left), has the same overall decay exponent and a similar net density of rays as in the previous case. The corresponding estimates of ?G and ??, which are obtained by applying the algorithm proposed here, are shown in the scatter plot in Fig. 2 (top right). Here, for 12 sets of 4000 realizations of the channel impulse response, the corresponding 12 empirical estimates of c(0,?) for 500 values of ? evenly distributed over [0,5 × 109] rad/s were obtained. Notice how, in all 100 realizations, the estimates of ?G are well below 3 × 10-2, unambiguously revealing the absence of clusters. 
 The possibility of correctly estimating the true parameters of a channel (?, G, ?, and ? in the SV case) from the empirical estimate of c(·,·), e.g., c˜(·,·), averaged from N independent channel realizations, will ultimately depend upon the accuracy of the latter estimate. To illustrate this dependence, the theoretical autocovariance c(?c + (O/2),?c - (O/2)) is plotted in Fig. 2 (right bottom), for the parameter sets E and F. The regions within five standard deviations of the estimate c˜ above and below each of the c(?c + (O/2),?c - (O/2)) curves is shown as shaded areas. The standard deviation shown corresponds to what is obtained when c˜(·,·) is calculated from N = 4000 channel realizations, which is the same number utilized to obtain the estimates shown in Fig. 2 (top right). Given the significant overlap of these regions and since the variance of c˜(·,·) is proportional to N-1, it is clear that, in this case, a reliable decision about which of the parameter sets better fits the data cannot be achieved from much less than 4000 channel realizations. 
 We end by noting that the parameter estimation algorithm described earlier has been proposed for its simplicity and with the purpose of illustrating the potential applicability of the results derived in Sections III and IV. Therefore, the search for better estimation algorithms, which must certainly exist, goes beyond the scope of this paper. 
 The variance of the channel power over a band B = [?l,?r] rad/s is obtained by substituting (27) into (23), with the change of variables v = ?l - u and v = ?r + u as follows: 
 Evaluating these integrals, using the change of variables ?c = (?l+?r)/2, and applying the identity arctan(x)+arctan(y) = arctan((x + y)/(1 - xy)), we obtain, after some manipulation, that var(PB) 
 The relative variability of PB with respect to the average total power over B is better captured by the normalized variance, i.e., 
 Equations (33) and (34) reveal several interesting aspects about the normalized channel power variance, which are discussed in the following. 
 Except for the first term on the RHS of (33), all the other terms grow subquadratically when W is above some frequency. It is a simple (although long) exercise of algebra to show that the frequency above which the growth rate with W of all these terms is significantly slower than 
 W2 is given by Wsat =? ?-1 max{1,0.5(1 + (2?c?)2)}. If , then the normalized variance of the total power over B can be well approximated as 
 where we have used (26). Thus, in agreement with what was shown in Sections III-B1 and IV-B, the normalized variance of the power over an asymptotically infinite bandwidth vanishes only when the number of significant clusters tends to infinity. 
 The narrow-band normalized channel power variance var(P0)/E[P0]2 is obtained from (34) by letting O ? 
 (33) grow proportional to W2. It is straightforward (but lengthy) to show that the maximum value for 
 Thus, unless the bandwidth W is comparable to or larger than Wflat, no reduction in normalized channel power gain variance is to be expected. 
 Notice also that, if there is a single cluster with deterministic amplitude and infinite duration (i.e., if we let G = 0 and ? ? 8), the narrow-band normalized channel power variance should coincide with that of a Rayleigh channel (in which case PB should distribute exponentially). Indeed, if we substitute the expressions for K1 and K2 in (35), fix G = 0, and let ? ? 8, then it is easy to verify that var(PB)/E[PB]2 ? 1, which is precisely the variance-to-squared-expectation ratio of an exponentially distributed random variable. 
 The curves in the dashed line in Fig. 3 display the value of var[PB]/E[PB]2 for a band of width W centered at 109 rad/s obtained from 300000 simulated realizations of the SV model for each set of parameters given in Table I. The corresponding values of var[PB]/E[PB]2 predicted by (34) are plotted in the same figure with solid lines. It is shown that, in all cases, the theoretical and simulation curves are almost indistinguishable, confirming the accuracy of our results. It is also shown that, in all sets, as expected from (34), when W is increased from zero, the normalized channel power variance remains almost constant until a certain value, near the threshold Wflat defined previously. Beyond that threshold, all the curves decay with W until a saturation point is reached, in a way that is consistent with what was found in [13]. 
 We have derived the general expressions that characterize the second-order statistics of the frequency response power gain in wireless indoor channels. Our results are applicable to several well-established channel models discussed in the literature, all based upon the SV model [1]. In particular, the closed-form formula obtained here for the autocovariance of the squared frequency response magnitude of the channel allows one to predict the variance of the aggregate channel power gain over any frequency band. This provides an approximation 
 Fig. 3. Normalized variance of the channel power gain over a band B of width W centered at 109 rad/s, for the SV model with the four sets of parameters (A, B, C, D) given in Table I. Simulated values are averages over 3000000 channel realizations. The theoretical curve was obtained using (33) and (34). 
 to the diversity order of wideband over narrow-band systems in such channels. Our results also allow one to obtain an upper bound for this measure of spectral diversity, and they show how (and why) this limit arises from having a finite number of clusters with a significant amplitude in the channel impulse response. In addition, the derived formulas explicitly reveal how the statistical properties of the clusters in the channel impulse response affect the spectral autocovariance of the channel power gain. This allows one to use these spectral statistics to identify the presence or absence of clusters. A simple procedure to accomplish this task has been proposed and its effectiveness has been verified by applying it to simulated realizations of the SV channel model. 
 Proof of Theorem 1: For notational simplicity, we will temporarily adopt a single indexing nomenclature for the path arrival times ti,m. More precisely and with a slight abuse of notation, we define the infinite random set 
 . For our purposes, it will not be necessary to define a mapping between the indexes i and m and the index . Indeed, it will be sufficient to note that the double-index matching condition (i, m) = (j, n) (which means i = j and m = n) is equivalent to the single-index condition . 
 Using the single-index notation, the first term on the RHS of (13) is obtained from (9), which yields 
 where (a) follows from (7), and the last equality stems from In (38), (a) is a consequence of the independence relationships the fact that all {pi} have zero mean [see (3)]. Proceeding established in (7), whereas (b) follows directly from Lemma 1. similarly, it is easy to show that each term in the summation Proceeding similarly with the frequency-dependent function of (36) in which one or more indexes is not matched to any T2(·), we obtain other index evaluates to zero. Thus, one must only consider the cases 
 Substituting these expressions for T1 and T2(·) into (37), followed by inserting the result and (12) into (13), yields 
 th moment delay profile of , and the single and joint effective arrival densities ?¯x(·) and ?¯¯x(·,·) are as in 
 Proof of Lemma 1: Let f(x) =? E[?km|xk = x], k ? S, with as in Definition 2. Recall the definitions of ?¯(i)(x) and from (5). If x1 = 0 deterministically, then 
 where Proposition 1 in the Appendix has been used. The last term in the latter expression is the difference between the rightmost term in (38) and the squared RHS of (12). Using the effective moment delay profile functions defined in (14), the autocovariance of |H(j?)|2 can be written as 
 Lemma 1: Let  be an incrementally Markov random process, with x1 possibly being deterministically equal to zero, and let  be a random sequence i.i.d. under  . Then, for any given m ? N 
 where (a) follows from Lemmas 3 and 4 (in the Appendix) and from (5) and (4). In turn, (b) was obtained using the fact that ?¯(2)(y) and f(y) are bounded for y = 0. 
 Finally, if x1 is random, it readily follows by applying Lemma 4 in the Appendix that 
 Lemma 3: Let the arrival times  be an incrementally Markov sequence with x1 randomly distributed. Define function  as in (4). Then 
 where the discrete random variable n?(x) corresponds to the number of arrivals in [x, x + ?). Its expectation satisfies 
 (43) Substituting this result into (41) yields (40), thus completing the proof.	 Lemma 4: Let the arrival times  distribute as in 
 Lemma 3, and define ?¯¯(y, z) as in (4). Assume that ?¯¯(y, z) is bounded for all . Then 
 By dividing by ? and taking the limit as ? ? 0, and then substituting (43) and applying Lemma 2, it follows that 
 Noting that Pr{n?(y) = 1,n?(y) = 1} = E[n?(y)n?(z)] and that the integrand in (44) is symmetric in y and z, and proceeding with (45) as in (42), we conclude that 
 The proof is completed upon substituting this result into (44).	 Proposition 1: Let ?¯¯x(y, z) be as in Definition 3, and let f : R ? R be a bounded function. Then 
 Proof: If x1 is random, then ?¯¯(y, z) is bounded, and the result follows trivially. Otherwise, x1 = 0 deterministically, in which case 
 ﻿In this paper we review the previously proposed methods for successive compute-and-forward and discuss about their incompleteness through some examples. Then we present a comprehensive formulation for multi-step successive interference cancellation problem through the asymmetric successive computeand-forward strategy. It is shown that the generalized formulation includes the previous approaches.
 I. INTRODUCTION
 Interference management in relay networks is one of the most important topics in wireless communications. Among the various methods proposed for relay networks named decodeand forward, compress-and-forward, amplify-and-forward, and compute-and-forward (see [1]-[2] and references therein), the later one has attracted interesting attention for the problem of interference harnessing in noisy relay networks, especially when it is more helpful to decode linear combinations of codewords rather than individually decoding them. In [2][10], several approaches have been presented for decoding integer/non-integer linear combinations of codewords in the networks with equal/unequal power constraints, and with symmetric/asymmetric channel gains conditions.
 In [2], a compute-and-forward strategy (CAF) has been proposed for Gaussian relay networks with equal power constraints and asymmetric channel gains based on nested lattice codes [3]. The receiver given sufficiently many linear combinations, can decode integer-linear combinations of transmitted codewords and solve for its desired messages. To obtain higher rates, receivers can recover those linear equations which are closer to the channel fading coefficients. This strategy simultaneously provides protection against noise and the opportunity to exploit interference for cooperative gains [2]. On the other hand, in [4], a scheme has been proposed for Gaussian relay networks with unequal power constraints and symmetric channel gains based on nested lattice codes and compute an achievable multi-cast rate within a constant gap to the capacity. There is one point here: in [2], the authors assumed equal power constraints and stated that unequal power constraints can be incorporated by scaling the channel coefficients appropriately. Here with an example we show that, when one does this, the results of [2] yields a smaller rate region than that found in [4].
 In [5] successive CAF strategy (SCAF) has been presented for the Gaussian relay network with the same assumption as in [2]. In this method after decoding a linear combination of transmitted codewords, the receiver can combine it with its channel observation to obtain a new effective channel which is better for decoding the next targeted linear combination.
 In [6] an asymmetric CAF has been proposed for MIMO networks with unequal power constraints and asymmetric channel gains based on nested lattice codes. In [7], Gaussian relay networks with unequal power constraints and asymmetric channel gains has been considered as a special case of the model presented in [?]. They also consider successful recovery of the individual messages at the final destination in addition to the relay recovery.
 In [8] and [9] a modified asymmetric CAF has been presented. This method utilizes scaling factors to decode integer linear combinations of transmitted codewords. The use of scaling factors in fact is equivalent to decoding non-integer linear combinations of lattice codewords and it allows different users to have different coding rates, so by appropriately adjusting those factors, different points on the boundary of the rate region can be achieved. Also, in [10] the idea of decoding non-integer linear combinations of lattice codewords has been presented. Their method can be considered as a special case of the methods of [8] and [9] as well.
 In this correspondence we first give a brief review of the previous works about SCAF. Moreover we discuss about their incompleteness of some methods through some examples. Then we present our generalized formulation for the successive interference cancellation problem in asymmetric Gaussian relay networks through the asymmetric successive computeand-forward strategy. More specifically, we extend the method of [7] to N-step SCAF and also we take advantage of scaling factors to reach to a comprehensive formulation for the asymmetric SCAF strategy. It is shown that the generalized formulation includes the previous approaches.
 978-1-5090-1922-9/16/$31.00 ©2016 IEEE
 The paper is organized as follows: In Section II the system model is defined. In Section III the strategies of [2][10] are reviewed and discussed. In Section IV our main results, the extended asymmetric successive compute-andforward approach for general Gaussian relay networks is introduced. Some concluding remarks are provided in Section
 V.
 Through the paper we use the following notations: C(x) =   denotes the
 set of integers   denotes the real-valued x rounded up to the nearest integer value. Vectors are shown with boldface letters.
 II. THE SYSTEM MODEL
 In this paper we consider a real Gaussian relay network with L transmitters and M relays. It is the same network as has been used in [2], but here we consider the assumption of unequal power constraints and asymmetric channel gains. Each relay in the network, indexed by m = [1;M], observes a noisy linear combination of the transmitted signals through
 the channel,
 L
 	y ,	(1)
 =1
 where hml ? R are the channel gains and zm is i.i.d. Gaussian noise, zm ~ N(0,In×n). Let hm = [hm1,··· ,hmL]T denote the vector of channel gains from the L users to the relay m. For all , each channel input x -length sequence subject to the average power constraint P, i.e.,  .
 The channel gains are assumed to be known and constant. In this document we only consider communication between the transmitter and the relays.
 III. REVIEW OF PREVIOUS WORKS
 In this section we review and compare the works of [2], [4]-[10] and discuss about their performances.
 A. Review of [2] and [5]
 In [2] and [5], achievable rate regions have been proposed for real-valued AWGN networks based on the successive compute-and-forward strategy. The combination of those results is rewritten in the following theorem.
 Theorem 1. For the defined system model with equal power constraints P and equation gain vectors am,bm ? ZL, where non-zero am = [am1,··· ,amL]T, bm = [bm1,··· ,bmL]T, with m ? [1;M], the following computation rates are achievable
  
 for user , where
 	 ,	(3)
 N2(am,bm) 
 As mentioned in [5], the interpretation of (4) is that the relay can first target a linear combination, am, that is easy to decode and then use it to create a better effective channel for decoding the second linear combination, bm.
 B.	Review of [4]
 In [4], a class of real Gaussian networks with one receiver and L users having unequal power constraints P1,··· ,PL and symmetric channel gains has been considered. Channel gains have been assumed to be one. An achievable rate region has been proposed based on the compute-and-forward strategy. It is rewritten in the following theorem,
 Theorem 2. Any rate less than
 	  ,	(5)
 is achievable for user  , with bounded error probability which goes to zero as n tends to infinity.
 C.	Comparison of [2] and [4]
 In [2] equal power constraints have been allocated to the users in the network. It has been declared in [2] that in the case of unequal power constraints, channel gains can be scaled in order to get equal power constraints. Next by the aid of two examples, we show that the rates that can be achieved by the method of [2] with the scaling of channel gains are smaller than those obtained by the method of [4].
 Example 1: Consider a network with M = 1, L = 2, unequal power constraint P1 = P2, and channel gains h1 = h2 = 1. Theorem 2 yields the following achievable rate for users :
  
 If we do channel gain scaling as , we get to equal power constraint P1 for both users. Substituting these values along with a = [1 1] (which maximizes
 N1(am)-1) in Theorem 1, results,
  -
 (6) is larger than (7), e.g., assume i = 1 in (6), then from the assumption P1 = P2, it is concluded that  .
 With another example, we show that the rate region of [5] with equal power constraints and asymmetric channel gains, is smaller than the one obtained from [4] by scaling the power constraints accordingly while leaving unit channel gains.
 Example 2: Consider a network with M = 1, L = 2, P1 = P2 = 1, and h1 = h2. Substituting them in Theorem 1 along with a = [1 1] results,
 	  .	(8)
 If we do scaling in order to obtain symmetric channel gains equal to 1, the scaled power constraints will be P1 = h21 and P2 = h22. With this, Theorem 2 yields the following rate for user i = 1,
  
 since (h22 + h21)/h21 < 2 subject to h1 = h2. So as before, it is seen that (9) results in a higher rate R1 in comparison with
 (8).
 Thus, the method of [5], scaling channel gains with power constraints, achieves smaller rate region than that obtained in [4] in the case of unequal power constraints.
 In [?] and [7], an asymmetric compute-and-forward is presented which solves the mentioned problem by appropriately defining fine and course lattices for each user. Also in [8], by the aid of scaling factors, the modified equation has been presented which can reach to the rate of [4] for the case of unequal power constraints.
 D. Review of [?] and [7]
 In [?] a Gaussian MIMO network has been considered which consists of L transmitters and a receiver with Nr antennas which observes an Nr×n dimensional channel output Y   Z, where h  is the channel gain between th transmitter and the receiver, Z is componentwise i.i.d. white noise with distribution N(0,1), and x  subject to the power constraint P. Let H = [h1,··· ,hL] and P = diag[P1,··· ,PL]. The theorem is as follows,
 Theorem 3. For the defined system model and equation coefficient vectors am ? ZL, with m ? [1;M] where am = [am1,··· ,amL]T, the following computation rate is
  [1;L],
   HTH) 
 In [7], a special form of (10) has been presented for the
 Gaussian relay network model as considered in [2] as follows,
 	 ,	(11)
 for user , where
 	 ,	(12)
 and P denotes the vector of power constraints of the users as P = [P1,P2,··· ,PL].
 In Section IV we state the extension of (12) to N-step asymmetric successive compute-and-forward by the same method as used in [5] and also we consider scaling factors as done in [8] and [9].
 E. Review of [8] and [9]
 In [8] and [9] by introducing real scaling factors ßi and decoding the resulting non-integer linear combination of codewords, one obtains additional flexibility for controlling the individual message rates of different users. Corresponding results of interest from [8] and [9] can be summed up in the following theorem,
 Theorem 4. Let ß1,··· ,ßL be L nonzero real numbers. For the defined system model in Section II with equal power constraint P and equation coefficient vectors am ? ZL, where am = [am1,··· ,amL]T, the computation rate tuple   is achievable if
  
 for all  where a˜m := [ß1am1,··· ,ßLamL].
 The main idea behind this result relies on the observation that the transmitted lattice codewords do not have to lie in the lattice which is used for encoding at the transmitters.
 As an example in [8], the Gaussian two-way relay channel (GTRC) has been considered where two users have unequal power constraints P1 and P2 and the channel gain vector to the relay is unity. The relay has power constraint PR. All noises are Gaussian with unit variance. They obtain the following achievable rate region for the GTRC, where the relay is assumed to decode a linear combination of user codewords tk of the form   where  ,
  
 where for any positive ß1, ß2, N˜(ß1:2) is equal to,
  
 Remark: (14) and (15) can be obtained by substituting hm =
 [vP1,vP2]/vP, and   for all  in (13). As it is seen, the previously mentioned problem for unequal power constraints is solved here by the aid of ß’s, but it is not obviously a straightforward substitution of channel vector and ß’s in the equation of the rate region. In the next section we propose a straightforward formulation which solves this problem.
 In [9], the multi-step successive interference cancellation strategy has been introduced for decoding more than one linear combination of codewords in multiple access networks. The method is based on the matrix algebra consider general equation gain vectors which is different from the method of [5] which is based on successive interference cancellation and consider orthogonal equation gain vectors.
 F. Review of [10]
 In [10] a network with M = 1 relay and L = 2 users has been considered. They’ve used the idea decoding the non-integer linear combination of codewords instead of using scaling factors and they have obtained critical points on the rate region where the rate of one user coincides the cut-set bound. For GTRC the method of [10] is actually a special case of [8] and [9] by considering  .
 IV. ASYMMETRIC SUCCESSIVE COMPUTE-AND-FORWARD
 In this section we propose a straightforward and comprehensive equation for multi-step asymmetric successive compute-and-forward strategy. We assume unequal power constraints and asymmetric channel gains.Our main result for N-step asymmetric successive compute-and-forward strategy is presented in the following theorem:
 Theorem 5. For the system model with unequal power constraint P1,··· ,PL, consider non-zero equation coefficient vectors ami ? ZL, where ami = [ami,1,··· ,ami,L]T and i ? [1;N], and let ß1,··· ,ßL be L nonzero real numbers. The computation rate tuple (R1,··· ,RL) is achievable where
  
 for all  is equal to
 	 ,	(17)
 for all i, and
  
 used in [5], [7], and [9]. See Appendix A.
 Corollary 1. For N = 1, i.e., single step decoding of integer linear combination of transmitted codewords, the rate of (16)-
 (17) reduces to
  
 for all .
 Remark 1: For P1 = P2 = ... = PL = P, we have  p p , and
  . Substituting these values in (22) results (13).
 Remark 2: One can see that by scaling channel gains with power constraints, i.e., substituting hm with hpm, and also substituting ß with   in (13), (22) is obtained. But in spite of the formal equivalence of (22) and (13) in the case of unequal power constraints, there is a difference between the conceptual basis of these equations. As explained in the proof of Theorem 5 in Appendix B, in the strategy of (13), the transmitted codewords of different users have equal powers independent of what the values of ß’s are, but in the strategy of (22), the users transmit their own real power values.
 Corollary 2. For N = 2, i.e., 2-step interference cancelation, the rate of (16)-(17) reduces to
  
 where
  ,	(24) N2(˜am1,˜am2) =	(25)
  .
 Remark 1: (Equal power constraints and asymmetric channel coefficients) By considering equal power constraints P1 = P2 = ··· = PL = P, (24) reduces to (13), and also by considering  for all , and ˜am2 = bm,
 (23)-(25) reduces to (3)-(4), and (11).
 Remark 2: (Unequal power constraints and symmetric channel coefficients) Assume  for all , and h = a = I1×L, then  .
 Substituting them in (23) and (24) results (5).
 As it is seen from the above remarks, our proposed formulation is capable of manipulating both unequal power constraints and asymmetric channel coefficients conditions without the aid of scaling factors.
 In the following corollary we obtain a more reduced form for (24) and (25) when L = 2 and M = 1, e.g., the network with one relay and less than three transmitters.
 Corollary 3. For the network with L = 2, M = 1, the rate of Theorem 5 for 2-step successive interference cancelation, i.e.
 N = 2, reduces to,
  
 for  where
 	 .	(27)
 Proof: See Appendix B.
 Remark: For L = 2, P1 = P2 = P, i.e., equal power constraints, a1 = [a1,a2] and a2 = [b1,b2], (26) and (27) reduces to [9, (11)-(13)].
 V. CONCLUSION
 In this paper we studied the asymmetric multi-step successive compute-and-forward scheme for interference management in Gaussian relay networks with unequal power constraints and asymmetric channel gains. We first reviewed the previous proposed methods for this scheme and discussed about their inefficiencies through some examples. Then we presented a comprehensive formulation for asymmetric multistep successive compute-and-forward scheme by taking advantage of all the benefits of the previous works together. It is shown that the proposed approach includes the previously proposed schemes.
 APPENDIX A PROOF OF THEOREM 5
 Proof:   Let ?1,...,?M be M nested lattice codes which are simultaneously good and ?cm denotes the coarsest lattice among them. The equivalent noise variance at the relays, which will be defined later in the proof, determine the order of this lattice chain. Larger noise variance corresponds to a coarser lattice. Additionally, L nested lattices are constructed such that   which are simultaneously “good” with second moments , where ß is a positive number. At each transmitter , the codebook  is constructed, where denotes the Voronoi region of  , and   denotes the number of the relay which results in lower achievable rates among the relays which should decode the message of transmitter . The transmitter , sends
 x ,
 where is a random vector uniformly distributed in  which is called as a dither. x is independent of t and uniformly distributed in , so it has average power P for all   
 Given L integers   and some real number am1, the decoder forms
 L
 ˜y 
 =1
 where
  ,
 Tm1 is independent of z˜m1.
 The receiver tries to retrieve Tm1 from ˜ym1 instead of recovering  , for all . Note that there is a one to one mapping between , so decoding 
 is equivalent to decoding  . For decoding, the receiver consider Euclidean lattice decoding which finds the closest lattice point ˜ym1 in   in which Tm1 lies, i.e.,
  . The probability of decoding error is given by
  ,
 so the lattice decoding of transmitter , is successful if R satisfies the following relation,
 	 ,	(29)
 for all , where   denotes the variance of the effective noise z˜m1 and it is obtained as follows:
  
 where
  
 is chosen to minimize  via the following computation,
  
 (am1)MMSE  
 Substituting (33) in (30) results,
 	 	(34)
 Substituting (34) in (29) results (16) for N = 1.
 The Nth step Successive Interference Cancellation:
 In the following, we obtain the rates for the successive decoding of other possible integer combinations of transmitted codewords. To decode the ith integer sum with coefficient vector ami, where i = [2;N], the relay removes the projection of  onto ym1 from ym1 as follows:
  
 where
 	(?mj)MMSE  .	(36)
 Then the relay forms a new effective channel observation
 L i 1
 y
  ,
 where
 	 .	(37)
 At last the relay decodes the linear combination with coefficient vector ˜ami by estimating
 	 .	(38)
 By repeating the same steps as shown in (28) and substituting  with with  , and am1 with ami, it can be shown that the following rate is achievable for all
 	,	(39)
 where,   is equal to
  
 The optimum values for  to get the global minimum are as follows,
  .
 Plugging back these values in (39) and (40) results (17).	 
 APPENDIX B PROOF OF COROLLARY 3
 First we introduce the following lemma which will be used in the proof of Corollary 3.
 Lemma 1. The following identity is satisfied for arbitrary n-dimensional real vectors a = [a1,a2,··· ,an], b = [b1,b2,··· ,bn] and c = [c1,c2,··· ,cn], with n = 2.
  (41) Proof: We prove the equivalent form of (41) which is,
  (42)
 By substituting the elements of the vectors a, b, and c in the right-hand side of (42), we have
 	The	number	of	terms	in  
 is	n2(n	-	1).	The	number	of	terms	in
 
 -	 i=1	j=1	k=1	i j k	. The number of terms
  . So the total number of terms in the first and second line of (43) are (3n2(n-1)-2n3 +2n) which is equal to n(n - 1)(n - 2) and is zero for n = 2.
 The number of terms in  
 is n2(n - 1), the number of terms in 2  i=1 j=1 k=1 i i j j k k is 2n3 - 2n when i = j = k is not satisfied, so the number of remaining terms in (43) is (3n2(n - 1) - 2n3 + 2n) which is equal to zero for n = 2.	 
 Proof of Corollary 3:
 Proof: We must show that (24) and (25) reduces to (26). It is easy to see that (24) is exactly the same as the first term of (26). Now we simplify (25) to obtain the second term of
 (26).
 According to Lemma 1, (44) reduces to,
 	 .	(45)
 Substituting (45) in (23) results the second term of (26).	 
 
 ﻿ Fundamental in control have been for the last with many stability , from channel transmission minimal rate based on information theory , to minimal signal to noise . For the latter , it been confirmed early on that the usual increase the minimal for stability , such as unstable plant , non minimum phase , time delay , together with new such as communication channel and colored transmission noise . In this work we specifically propose two to completely avoid the effect of on the minimal for time . The first approach from the observation that the controller that the minimal for a minimum phase plant in , all its outside the unit circle . Such observation allow then to characterize this set of as candidate of alternative plant with the same unstable , which in turn will not increase the minimal for stability above the expression only by the plant unstable . Our second approach the set of possible that do not increase the limitation to any location , by the synthesis of a two degree of freedom linear time invariant controller solution that the minimal for stability only by the plant unstable . 
 Index Control over , Linear , control , Optimal control . 
 control have been a subject of research for at least the last two , with review available in , . on fundamental on stability in this framework were for example in , . performance have also been extended information theory to include disturbance rejection , reference and , as in , . 
 A different approach for is , on the other hand , in where an framework for stability of feedback , both in continuous time and discrete time , is based on the channel model signal to noise ratio . The fundamental limitation in is as an lower bound on the channel 
 M . is with the de ´ ´ ,´ Mar ´ , Chile . 
 A .. is with the de ´ El ´ , de ´, Chile . : 
 The thankfully acknowledge the support from , through Basal Project . 
 for the stabilization of an unstable plant model by of output feedback over a memoryless additive white noise communication channel , such as the case in Figure . More so , in 
 recent it been shown that for second order statistics , control over a power constrained erasure channel are equivalent to control subject to an constraint , . Furthermore , the equivalence , in a second order sense , been extended to include fading in . As a result , the approach been able to capture the most important communication and characterize their impact on fundamental , as well as performance , see also for example , . More so , the approach also a reinterpretation of the classic control feedback loop by of the reference to disturbance ratio , , , which performance as the trade off between reference and closed loop input disturbance rejection . 
 A common observation , in particular in , is that the limitation for output feedback stabilization of an unstable plant model with , is strictly greater than the one for state feedback emphasis added by the present . As an alternative the of propose a linear time alternative control scheme to eliminate the effect of the on the for . 
 Our first contribution in this work is to explicitly characterize the one degree of freedom , see Figure , optimal controller that the minimal in . Furthermore we will characterize the of such controller and prove that these are all outside the unit circle . To the best of our knowledge such a characterization is novel . The controller outside the unit circle are then by definition . As a result , if the of an alternative plant model with the same unstable belong to this specific set of , then both , output feedback and state feedback , will coincide thus the effect of over the limitation . 
 Our second contribution the set of potential to an arbitrary set , by and a specific two degree of freedom controller solution . The use of a two degree of freedom controller been before in order to eliminate the effect of in different the approach , see for example , . The contribution here instead is the explicit definition of a two degree of freedom controller to avoid the deleterious effect of arbitrary many , the minimal for stability only by the plant unstable . A preliminary result for first order plant was in . 
 As a result of our we update the common observation about minimal and as : the for stabilization of an unstable plant model with , via output feedback , is greater or equal than the for state feedback , in an setting . 
 This paper is organized as : Section the behind the minimal in . Section characterize the structure for the optimal controller that the minimal for an unstable plant model and the zero for this optimal controller . Section the synthesis of the two controller that the effect of any arbitrary plant zero . We conclude the exposition in final and possible future research . 
 In this section we present the general plant model , and summarize the discrete time for solution . 
 where i , i are and distinct plant model . The term is the stable , minimum phase , part of . The relative degree the choice of is to avoid a plant time delay that would further increase the later minimal for stability . Channel model : we consider here the memoryless channel model , see for example Figure . This type of channel is by the channel input power and its channel additive noise process . Channel additive noise process : The channel additive noise a zero mean i . i .. white noise process with variance s . 
 : The assumed to be zero , that is we consider here only a regulation problem , not a problem . 
 Consider the feedback loop in Figure where the problem is to stabilize an unstable plant subject to a channel power constraint . From Figure we observe that the closed loop between the channel input u and the exogenous given by u , with the closed loop complementary sensitivity function . The channel input is to satisfy the power , for some input power level . We assume that the closed loop feedback system is stable , in the sense that for any distribution of initial , the distribution of all closed loop in Figure to a stationary distribution . Without loss of generality , we therefore consider the of the stationary distribution of the relevant . The power of the channel input signal then 
 . The channel input power constraint can be from this , as a constraint on the channel We thus observe that for the setting in Figure , the limitation is uniquely defined by the H norm of . 
 We now present a novel lemma which a lower bound to the channel in discrete time that when the closed loop system is stable . This lemma also sufficient and necessary , stated in of the transfer function so that the channel is equal to this lower bound . The latter is given only by the unstable is universal in the sense that it for any choice of Figure . 
 Before proceeding , it is convenient to note that , i . i .., it that , for every , u , and s . This that the channel is given by 
 is an all pass transfer function . and are equivalent to 
 Proof . Since the are independent of the channel noise , we have that 
 with equality in a zero mean , and equality in is the only exogenous signal in the system in which mean . The external disturbance case is for example in , for performance , however here we are interested in , thus the no external disturbance stated in condition i . The channel output power is thus , condition i . 
 If the closed loop system is stable , then the sensitivity function is stable and there can be no unstable . Thus 
 where is due to inequality and the fact that log is a concave function , with equality if and only if does not depend on . On the other hand , is due to Bode integral theorem and the fact that the unstable of , which cannot be , appear as of , with equality if and only if the of are exactly the unstable of , Theorem . . . Combining this with and , it that i , and in the statement are necessary and sufficient for equality in , and that the latter two are equivalent to , the proof . 
 It is worth that Lemma is valid regardless of whether there are in the plant model or its relative degree . It is well known , see , that the minimal for a state feedback strategy and an output feedback strategy , as in Figure , if the plant unstable , minimum phase with no time delay . As stated in the presence of the to a value o such that o . We will show in the next section that , for a one degree of freedom controller solution , this is not always true , and on we can still have o , even if the plant model . 
 We next present a theorem which characterize the unique controller which the lower bound . 
 Theorem . For the system shown in Figure , unstable , minimum phase with relative degree , it that . Furthermore , the 
 is by a unique controller the set of , given by 
 Proof . We will show that , in this case , there a and of Lemma and that such controller is unique . For this purpose , let us define the and , respectively , as the numerator and denominator of 
 Notice that is a stable all pass transfer function if and only if all the of are outside the unit circle . On the other hand , from , the controller can be expressed as 
 phase leaves no unstable cancel . In order to ensure not cancel the unstable that those become the only of , one must choose i . Since must have an all pass frequency response , the denominator of must be , automatically yielding a stable . 
 In summary , we have found that there a which and of Lemma , and that it is unique . This and that the corresponding minimal controller is the one defined in . 
 Theorem how the unstable of the plant a fundamental role in the minimum of the communication channel . It is also easy to see that , for a stable plant , the minimal is zero . 
 Remark Without , is a minimum .. Theorem that , in this case one degree of freedom , minimum phase , and without , one can reach an equal to and obtain a stable closed loop system . Thus , the for stability is actually a minimum . However , it shall be noted that the sensitivity function , and thus it is all pass . This that the channel noise the only exogenous signal in the system is not being at any particular frequency . 
 Notice also that the stability of the minimum system that if one the controller and then are added , closed loop stability is not . However , any additional external disturbance will increase the variance of u as well as that of the other in the system , increasing the above . 
 After the in the previous it is natural to ask ourselves the following question : Can the lower bound given by be if the plant We next show that if , the answer is yes , for very special . In doing so , we will also provide some insight into the why in general the answer is no . 
 the reasoning in the proof of Theorem , let and be as in , so that . In order to reach the lower bound in , condition must hold , which that 
 for some complex such that , and a set of complex , with a correspondence between each root outside the unit circle and one of the unstable of from condition in Lemma . The stability of that can only have stable and thus , in view of , only i . e ., and ai i , ,...,. 
 and , from , do not cancel the unstable of a requirement for . 
 We now lift the assumption and propose an unstable plant model that also 
 , and to stress the difference with , such an unstable plant model is . The term is the stable , minimum phase , part of . As relative degree of is also one . From , the only way in not cancel these is that the latter are also of . Clearly , this is not possible in general , since , as we have seen , satisfying and in Lemma that the specific form given by . 
 This that , in general , if the closed loop system is stable and if indeed , then the cannot reach the lower bound in . At the same time , the above analysis the following question : Are there special in which the plant model and yet the lower bound is compatible with closed loop stability The following theorem an answer to this question . 
 In the shown in Figure , an equal to the lower bound in is reachable , while yielding a stable closed loop system , if and only if and the i of constitute a subset of . 
 If the in i are met , then the unique controller , which when in Figure an equal to , is given by 
 Proof . i As above , the lower bound in to take the form given in , which if and only if given by as . A construction of the optimal controller is also for example in , Theorem . the approach , but such approach the characterization of the zero obscure . Since that relative degree equal to , it that when relative degree , which is not realizable causally . If , this controller is , the stable given by thus satisfying and in Lemma and does not cancel the unstable of . The only condition be a controller is that it does not cancel the unstable of either , which is equivalent to being a subset of the of . This claim i . 
 When , the set the of . These can be studied standard root locus analysis , which the of the expression , as the varied , for any given rational transfer function . , we will show that when , all the of lie outside the unit circle . 
 Since is as in , it its outside the unit circle and all its inside the unit circle . Recall that the of continuously migrate from the of at to the of toward . Hence , close to zero , lie outside the unit circle . 
 Suppose that for some when , for some and some ,. 
 Then is real and positive . that see , we conclude that . Thus , from , all the which start outside and end inside the unit circle lie exactly on the unit circle only when 
 , which is strictly less than . Therefore , all the of are non minimum phase , proving claim . 
 The fact that it feasible not cancel the of , which if and only if these are a subset of . In the latter case , the be given by , thus taking the form , which the given by . Therefore , this is the only controller which the closed loop system while satisfying the for equality in . This the proof . 
 Example . Consider an unstable plant with relative degree given by , with 
 then the channel can be set to . For and real , then further into More so , the optimal controller that is then defined as 
 Thus , we have a controller without unstable zero pole which a stable such that i i , ,, and thus with s , and thus the lower bound in . 
 Remark . Claim in Theorem a perhaps surprising fact : in the setup corresponding to Figure , or its are a subset of , then the controller at least non minimum phase . This may seem since , in general , in the loop transfer function the closed loop system harder to control it the of the complementary sensitivity function and it minimum phase . 
 Remark . Despite the fact that specific satisfying the of claim of Theorem do exist , the latter theorem that for almost all with , and for all with , the minimum for stability in the scheme of Figure is in general strictly greater than the lower bound . Therefore the result is theoretical in nature , since the plant model and should not be assumed as design . This the explicit definition of a controller , as in the following section , for dealing with the practical aspect of plant model not necessarily at the i , whilst the minimal for stability only by the plant unstable . 
 When the channel output is directly fed back , then an alternative two controller solution that is possible , see Figure . 
 Theorem . The two controller that when the unstable plant model arbitrary with is given by 
 , where for C the are the solution of the following set 
 and for C the are the solution to the next set 
 . Since we want to achieve we therefore impose the need for C CoG , with as in in . We then further assume that C 
 Since we require for stability that C the space of all proper and real rational stable transfer , , the numerator of C must contain all the 
 m unstable of , thus , for all . The structure for that the one by . The are easily by imposing the set of algebraic in , from which we then obtain C as in . To obtain C we follow a similar approach . From C CoG , at each zero of we have that C , for . To satisfy we propose C as in . Finally , to obtain the that achieve the given we then impose the algebraic in and obtain C as in , which this proof . 
 The remarkable implication of Theorem is that we can effectively reduce the requirement , the increasing effect due to the presence of the , whilst in an setting . This is by the plant model as seen by the channel input , through the second path that the channel output through C , see Figure . 
 Remark . The result from Theorem also when the communication channel is over the feedback path between the plant and controller and . 
 Fig . . Two solution for Example , equivalent plant model configuration . 
 Example . Consider . We recognize then z , and . For one unstable pole we also have that and . We then have from Theorem that , and . 
 we have that . Therefore the two controller that for the plant model studied in this example is given by 
 As a note , observe that if , then the C path up , and the plant model . We then recover the standard controller solution for of an unstable plant without . 
 In this work we have studied how to avoid the effect of on the minimal for output feedback stability . The first observation is that the optimal controller that the minimal for a minimum phase plant with no time delay , all its outside the unit circle . This observation allow us then to characterize a set of specific that can be now at the plant model , and would not increase the minimal for stability above the expression by only the plant unstable . We then used a two controller structure to propose , for output feedback , the synthesis of such that can retrieve the minimal , even when the unstable plant model . Future research should consider extending the for more complex communication channel and replicate this study for the continuous time setting . 
  
 ﻿ We present several novel and the mutual information and the directed information in with feedback . The internal within such are restricted only to be causal , but are to be non linear , stochastic and time . Moreover , the involved can be arbitrarily distributed . We bound the directed information between inside the feedback loop by the mutual information between inside and outside the feedback loop . This fundamental result an interesting interpretation as a law of conservation of information flow . Building upon it , we derive several novel and , which allow us to prove some 
 information under less restrictive . Finally , we establish new between directed inside a feedback loop . This a new and general data 
 inequality for with feedback . 
 INTRODUCTION 
 The notion of directed information by in the amount of information that causally from a given random and ordered sequence to another . For this reason , it increasingly found use in diverse , from the capacity of with feedback , the rate distortion function under causality , some of the fundamental in 
 control , causal in neural , to portfolio theory and hypothesis testing , to name a few . 
 The directed information from a random sequence to a random sequence is defined as 
 k 
 I , XI i ; xi , 
 i 
 where the notation xi the sequence , ,..., i . The causality inherent in this definition becomes evident when it with the mutual information between and , given by 
 . In the latter sum , what is the amount of information about the entire 
 sequence present in i , given the past . By contrast , in the conditional mutual in the sum of , only the past and current of are considered , that is , xi . Thus , I the amount of information causally from to . 
 There exist several the relationship between I and I ; . First , it is well known that I I ; , with equality if and only if is causally related to . A conservation law of mutual and directed information been found in , which that I I I ; , where the concatenation , ,.. . 
 Given its prominence in feedback , it is perhaps in these where the directed information becomes most important . For instance , the directed information been instrumental in the capacity of with feedback see , e .., , , and the therein , as well as the rate distortion function in feedback , , . 
 In this paper , our focus is on the and directed and mutual within feedback , as well as between directed different within the corresponding feedback loop . In order to discuss some of the related to this problem , it is convenient to consider the general feedback system shown in Fig . a . In this diagram , the S ,..., S represent possibly non linear and time causal such that the total delay of the loop is at least one sample . In the same figure ,,,, are exogenous random , or , which could represent , for example , any combination of random initial or side . We note that any of these exogenous , in combination with its corresponding deterministic Si , can also yield any desired stochastic causal . 
 For the simple case in which all the Si i are linear time invariant and stable , and assuming ,, , it was shown in that I does not depend on whether there is feedback from e to u or not . between mutual and directed in a less restricted setup , shown in Fig . , have been found in , . In that setting a system , is a strictly causal dynamic system vector state sequence , with x , being the random initial state in its state space representation . The external signal which could correspond to a disturbance is statistically independent of , the latter corresponding to , for example , side information or channel noise . Both are also statistically independent of x . The E , 
  
 a 
 Figure . a : The general system considered in this work . : A special case , corresponding to the closed loop system studied in . 
 D to an , a and a channel , respectively , all of which are causal . The to in a possibly time manner , i . e ., . Similarly , the concatenation of the , the channel and the , and to u as a possibly time dependent function u . Under these , the following fundamental result was shown in , Lemma . : 
 I x , ; I ; I x ; . 
 By further assuming in that Fig . is deterministic , the following chain naturally , 
 x , , 
 leading directly to 
 I x , ; I ; I x ; , 
 which is found in the proof of , Corollary . . The deterministic nature of crucial role in the proof of this result , since otherwise the chain does not hold , in general , due to the feedback from u to . 
 Notice that both and provide lower to the difference between two mutual , each of them a signal external to the loop such as x , to a signal internal to the loop such 
 as or . Instead , the inequality 
 I I ; , 
 which for the system in Fig . a and in , Theorem and later in , Lemma . . , the directed information between two internal and the mutual information between the second of these and an external sequence . A related bound , similar to but information and with the leftmost mutual information by the directed information from to which are two internal to the loop , been in , Lemma . : 
 , 
 with I , and I ; u , ; , provided 
 . This result on three : a that the memory less and a conditional invertibility property , a finite memory condition , and a fading memory condition , these two related to the see Fig . . It is worth that , as defined in , these the use of side information by the and or the possibility affected by random noise or a random internal state which is non observable please see for a detailed description of these . 
 The inequality recently been extended in , Theorem , for the case of discrete valued random and assuming ,,, as the following identity written in of the and setup shown in Fig . a : 
 . 
 q in Fig . a and with the additional assumption that , , it was also shown in , Theorem that 
 I I ; I ; I ; , 
 for the in which u i i i i . e ., when the concatenation of S and S to a node . In , and play important in the capacity of with noisy feedback . 
 To the best of our knowledge , , , , and are the only available in the literature which lower bound the difference between an internal to internal directed information and an external mutual information . There exist even in relation to between two directed only internal to the loop . To the best of our knowledge , the only inequality of this type in the literature is the one found in the proof of Theorem . of . The latter the form of a quasi data inequality for directed in closed loop , and that 
 I I , 
 provided , and if S is such that is a function of i . e ., if S is conditionally invertible i . In , 
 k 
 I , XI i ; xi , 
 i 
 to the causally conditioned directed information defined in . Inequality a crucial role , since it lower bounding the average data rate across a digital error free channel by a directed information . In , to a random dither signal in an entropy 
 . 
 In this paper , we derive a set of information and of internal or external to the loop in feedback . The first of these is an identity which , under an independence condition , can be as a law of conservation of information . The latter identity is the starting point for most of the which follow it . Among other , we extend and to the general setup in Fig . a , where none of the made in except causality needs to hold . Moreover , we will prove the validity of without assuming the conditional invertibility of S nor that ,. The latter result is one of four novel data derived in Section , each two directed valid for the system in Fig . a . The last of these is a complete closed loop counterpart of the traditional open loop inequality . 
 The remainder of this paper with a description of the under study and the extension of directed information to the case in which each of the in the loop may introduce an arbitrary , non negative delay i . e ., we do not allow for anticipation . The information and are in Section . For clarity of the exposition , all the are deferred to Section . A brief discussion of potential of our is in Section , which is by the in Section . 
 . 
 A . System Description 
 We begin by providing a formal description of the S ... S in Fig . a . Their are given by the possibly deterministic 
 e i S d i a i S d i , pi , b i S xi d i , si , c u i S d i d 
 where ,,, are exogenous random and the possibly time d , d , d , d , ,... are such that 
 d d d d , . 
 That is , the concatenation of S ,..., S a delay of at least one sample . For every i ,...,, i i , i . e ., i is a real random vector whose dimension is given by some function : ,.. . 
 The other ,,,,, u are defined likewise . 
 B . A Necessary Modification of the Definition of Directed Information 
 As stated in , the directed information as defined in is a more meaningful measure of 
 the flow of information between and than the conventional mutual information I ; 
 when there causal feedback . In particular , if and are 
 discrete valued , input and output , respectively , of a forward channel , and if there strictly causal , perfect feedback , so that i i a scenario in as part of an argument in favor of the directed information , then the mutual information becomes 
 I ; . 
 Thus , when strictly causal feedback is present , I ; to account for how much information about been to through the forward channel that between them . 
 It is important to note that , in as well as in many works concerned with , the forward channel is instantaneous , i . e ., it no delay . Therefore , if a feedback channel is , then this feedback channel must have a delay of at least one sample , as in the example above . However , when the system in Fig . a , we may need to evaluate the directed information between and which are , respectively , input and output of a strictly casual forward channel i . e ., with a delay of at least one sample , whose output is instantaneously fed back to its input . In such case , if one further perfect feedback and i i , then , in the same spirit as before , 
 . 
 As one can see , definition of directed information to be meaningful if instantaneous feedback is . 
 It is natural to solve this problem by that , in the latter example , the forward channel had a delay , say , greater than one sample . Therefore , if we are interested in measuring how much of the information in , not present in , was from xi through the forward channel , we should look at the mutual information I i ; xi , because only the input xi can have an influence on i . For this reason , we introduce the following , notion of directed information 
 Definition Directed Information with Forward Delay : In this paper , the directed information from 
 to through a forward channel with a non negative time delay of i is defined as 
 k 
 I , XI i ; xi i . 
 i 
 For a zero delay forward channel , the latter definition with . 
 Likewise , we adapt the definition of causally conditioned directed information to the definition 
 k 
 I , XI i ; xi d i , d i . 
 i 
 when the e , related according to . 
 Before finishing this section , it is convenient to recall the following identity a particular case of the chain rule of conditional mutual information , which will be extensively in the of our : 
 I a ,; I ; I a ;,. 
 . INFORMATION AND 
 A . Between Mutual and Directed 
 We begin by a fundamental result , which the directed information between two within a feedback loop , , to the mutual information between an external set of and : 
 Theorem : In the system shown in Fig . a , it that 
 , 
 with equality independent of ,,. N 
 This fundamental result , which for the in which ,, can be understood as a law of conservation of information flow , is in Fig . . For such , the information causally information flow from ,, to . When ,, are not independent of , part of the mutual information between ,, and corresponding to the term can be thought of as being through , thus the forward link . This an intuitive interpretation for . 
  
 Figure . The flow of information between exogenous ,, and the internal directed information from to when ,,. 
 Remark : Theorem that I is only a part of or at most equal to the information flow between all the exogenous entering the loop outside the link namely ,,, and 
 y . In particular , if ,, were deterministic , then I , regardless of the S ,..., S 
 and irrespective of the nature of . N 
 Remark : By ,. Then , Theorem , we recover , whenever ,,. Thus , , Theorem and , Lemma . . can be as 
 a corollary of Theorem . N 
 The following result an inequality I with the separate of information 
 I ; and . 
 Theorem : For the system shown in Fig . a , if ,, and , then 
 . 
 with equality if and only if the chain . 
 Theorem that , provided ,, , I is lower bounded by the sum of the individual from all the in any given partition of , to , provided these are mutually independent . Indeed , both and can be generalized for any appropriate choice of external and internal . More precisely , the set of all external in a feedback system . Let a and be two internal in the loop . Define Ta , as the set of exogenous which are to the loop at every subsystem Si that in the path going from a to . Thus , for any Ta if Ta , Ta we have that and become 
 I a I Ta , ; , 
 I a I ; I Ta , ; , 
 respectively . 
 To finish this section , we present a , non asymptotic version of inequality : 
 Theorem : In the system shown in Fig . a , if ,,, are mutually independent , then 
 . 
 N 
 As , Theorem can be seen as an extension of to the more general setup shown in Fig . 
 a , where the made in , Lemma . do not need to hold . In particular , x in Fig . correspond to S and in Fig . a , respectively , we see that inequality even E have dependent initial , or if the internal state not observable . 
 Theorem also an interpretation in of information . This can be in the diagram shown in Fig . , which the individual full turn around the entire feedback loop stemming from , and . Theorem that the sum of these individual is a lower bound for the directed information , provided ,,, are independent . 
  
 Figure . A representation of the three first information on the right hand side of . 
 B . Between Directed 
 This section three closed loop of the data inequality two directed , both between of internal to the loop . As already in Section I , to the best of our knowledge , the first inequality of this type to appear in the literature is the one in Theorem . in see . Recall that the latter result stated that I I , S to be such that is a deterministic function of and that ,. The following result another inequality which also two directed , namely , I and I , but only that ,,. 
 Theorem : For the closed loop system in Fig . , if ,, , then 
 I I . 
 N 
 Notice that Theorem does not be independent . This may seem counter intuitive upon loop between the link from e to . 
 The following theorem is an identity between two directed only internal . It can also be seen as a complement to Theorem , since it can be directly applied to establish the relationship between I and I . 
 Theorem : For the system shown in Fig . a , if , ,, then 
 . 
 with equality if , in addition ,. In the latter case , it that 
 . 
 N 
 Notice that , by additional independence upon the exogenous specifically ,, Theorem and , in particular , 
 I I , 
 which the inequality in , Theorem . stated above in . More precisely , does not require one of the directed and irrespective of the invertibility of the in the loop . 
 A closer counterpart of i . e ., of , Theorem . , I , is next . 
 Theorem : For the system shown in Fig . a , if , ,, then 
 . 
 where the equality † if , in addition , the chain 
  
 is satisfied for all i ,...,. N 
 Thus , provided , ,, that regardless of the invertibility of S , instead that , for all i ,...,, any statistical dependence between and si only in i . e ., that chain . 
 The derived so far relate directed either the same starting sequence or the same destination sequence . We finish this section with the following corollary , which directly by combining and and directed four different internal to the loop . 
 Corollary Full Closed Loop Directed Data Inequality : For the system shown in Fig . 
 a , if , , and , then 
 . 
 Equality in a if , in addition , i . e ., if ,,, are mutually independent . N 
 To the best of our knowledge , Corollary is the first result available in the literature providing a lower bound to the gap between two directed , four different inside the feedback loop . This result can be seen as the first full extension of the open loop traditional inequality , to arbitrary closed loop . Notice that there is no need to consider with more than four , since all external entering the loop between a given pair of internal can be as exogenous to a single equivalent deterministic . 
 . 
 We start with the proof of Theorem . 
 Proof of Theorem : It is clear from Fig . a and from that the relationship between ,,,, be by the diagram shown in Fig . . From this diagram and Lemma in the appendix it that independent of ,,, then the following chain : 
 y . 
 d d si 
 i i 
 Figure . Representation of the system of Fig . the dependency between ,,,, and . The dependency on i of the d i ,..., d i is for clarity . 
 the triad of exogenous by 
 , 
 we have the following 
 k 
 I XI i ; xi d i 
 i 
 k 
 i , xi d i ; i I i ; i xi d i , i i 
 i ; i I i ; i xi d i , i i a 
 k 
 XI i ; i XI ; i 
 i i b 
 I ; . c 
 k a 
 In the above , a from the fact that , if is known , then xi d i is a deterministic function of i . The resulting on the right hand side of a correspond to I I , and thereby proving the first part of the theorem , i . e ., the equality in . In turn , from the non negativity of mutual , turning into equality if ,,, as a direct consequence of the chain in . Finally , equality in if ,,, upon 
 . This that equality in is if ,,, the proof . 
 Proof of Theorem : Apply the chain rule identity to the of to obtain 
 . 
 Now , twice , one can express the term as : 
 I , ; I , ; , I , ; I , ; , 
  
 , 
 where the second equality since . The result then directly by combining with and . 
 Proof of Theorem : Since ,,, 
  
  
 , 
 where a is due to Theorem , from Theorem and the fact that , , and from the chain rule of mutual information . For the second term on the of the last equation , we have 
  
  
  
  
  
 k 
 I ; e I ; u e , 
 where a since , , and e stem from the chain rule of mutual information , 
 and is a consequence of the chain e which is due to the fact that S d Finally , is due to the chain , which because ,, as a consequence of Lemma in the appendix see also Fig . a . Substitution of into , thereby the proof . 
 Proof of Theorem : Since ,, , we can apply where now , the role of , and obtain 
 . 
 Now , we apply Theorem , which 
 , 
 the proof . 
 Proof of Theorem : Theorem , since , ,, 
 I I , ; . 
 For the other directed information , we have that 
  
  
  
  
  
  
  
  
 , 
 where a from Theorem , which also that equality is if and only if ,, . In turn , is due to the fact that is a deterministic function of . Equality if and only if , . Finally , from Lemma in the appendix , turns into equality if ,,. Substitution of into , the proof . 
 Proof of Theorem : We begin with the second part of the theorem , proving the validity of the equality † in . We have the following : 
 k 
 I XI i ; xi d i , 
 i 
 k 
 i i i d i ; i , I , pi ; i xi d i , , i 
 I ,, x 
 i 
 a k 
 XI , pi , xi d i ; i , i 
  
 i 
 k 
 , pi , ; i , I ; i , pi i i 
 k 
 i i i , i I ,,; i y 
 i 
 k 
 i i i , I ; i , i I ,; i y 
 i 
 k 
 i i i , 
 I ,; i y 
 i 
  
  
 i 
 where equality in a if and only if the chain si , pi for all i ,.. as a straightforward extension of Lemma . In our case , the latter chain since we are assuming , In turn , from the fact that , for all i ,.., xi d i is a function of , pi . To prove , we resort to and write 
  
 From the of the in , it can be seen that , given , the triad of random 
 is a deterministic function of at most . that and that see , it readily that , and thus each of the mutual 
 on the right hand side of is zero . To verify the validity of , we use and obtain 
 , 
 where now since , where the 
 last term in this chain of was shown to be zero in the proof of . Equality in e if 
 and only if , a chain which is satisfied in our case from the fact that , , and from Lemma . 
 Finally , since , we have that the chain of from to , from which we conclude that 
 . 
 this result into and Theorem we arrive at equality † in . 
 To prove the first equality the , it to notice that I to the sum on the right hand side of , from where we proceed as with the first part . This the proof of the theorem . 
 POTENTIAL 
 Information and , in particular , the data inequality , have a fundamental role in Information Theory and its . It is perhaps the lack of a similar body of associated with the directed information and with non asymptotic , causal information transmission which limited the extension of many important information theoretic and to feedback or causality , . Two such , already in this paper , are the understanding of the fundamental in control over noiseless digital , and causal rate distortion . In those , causality is of paramount relevance an thus the directed information , naturally , as the appropriate measure of information flow see , for example , , , , , and . We believe that our might help gaining into the fundamental trade underpinning those , and might also allow for the solution of open such as , for instance , the minimal average data rate that a given performance level an version of the latter paper , which extensively the derived here , is currently under preparation by the . On a different vein , directed mutual information a role akin to that of standard mutual information when channel feedback capacity see , e .., , and the therein . Our may also play a role in expanding the understanding of communication over used with feedback , particularly when in the analysis additional exogenous such as a random channel state , interference and , in general , any form of side information . Thus , we hope that the and in Section may help in extending such as dirty paper , , distributed source , , , , terminal , , and data encryption , to causal feedback . 
  
 In this paper , we have derived fundamental between mutual and directed in general discrete time with feedback . The first of these is an inequality between the directed information between to inside the feedback loop and the mutual information a subset of all the exogenous incoming . The latter result can be as a law of conservation of information for closed loop . Crucial to these was the repeated use of chain for conditional mutual information as well as the development of new . The proof do not rely upon of or , and the hold in very general non linear , time and stochastic with arbitrarily distributed . Indeed , the only restriction is that all within the system must be causal , and that their combined delay must be at least one sample . A new generalized data inequality was also proved , which is valid for directed within the loop . A key insight to be from this inequality was that the further apart the are in the loop , the lower is the directed information between them . This closely the behavior of mutual information in open loop , where it is well known that any independent of the can only reduce their mutual information . 
 APPENDIX 
 Lemma : In the system shown in Fig . , the exogenous , are mutually independent and 
 S , S are deterministic possibly time causal by 
 , for some . For this system , the following chain 
  
 Figure . Two arbitrary causal S , S in a feedback loop . The exogenous , are mutually 
 independent . 
 r . 
 Proof : Since and u are deterministic , it that 
 for every possible pair of the : S , and 
 : S , are also deterministic . Thus , and 
 . This that for every pair of , of appropriate 
 , 
  
 , , 
 , , n , , 
 a 
 , , 
 , 
 where a from the fact that . This the proof . 
  
 ﻿This paper derives novel results on the characterization of the the causal information ratedistortion function (IRDF) Rcit(D) for arbitrarily-distributed one-sided stationary ?-th order Markov source x . It is first shown that Gorbunov & Pinsker’s results on the stationarity of the realizations to the causal IRDF (stated for two-sided stationary sources) do not apply to the commonly used family of asymptotic average single-letter (AASL) distortion criteria. Moreover, we show that, in general, a reconstruction sequence cannot be both jointly stationary with a one-sided stationary source sequence and causally related to it. This implies that, in general, the causal IRDF for one-sided stationary sources cannot be realized by a stationary distribution. However, we prove that for an arbitrarily distributed one-sided stationary source and a large class of distortion criteria (including AASL), the search for   can be restricted to distributions which yield the output sequence y81 jointly stationary with the source after ? samples. Finally, we improve the definition of the stationary causal IRDF   previously introduced by Derpich and Østergaard for two-sided Markovian stationary sources and show that  for a two-sided source ...,x(-1),x(0),x(1),... equals Rcit(D) for the associated one-sided source x(1),x(2),.... This implies that, for the Gaussian quadratic case, the practical zero-delay encoder-decoder pairs proposed by Derpich & Østergaard for approaching  achieve an operational data rate which exceeds Rcit(D) by less than 1 + 0.5log2(2p e /12) ? 1.254 bits per sample.
 INTRODUCTION
 The information RDF (IRDF) Rit(D) for a given one-sided random source process x can be defined as the infimum of the mutual information rate [1, Section 8.2]
 		(1)
 between source and reconstruction y81 such that a given fidelity criterion does not exceed a distortion value D [1]–[3]. If one adds to this definition the restriction that the decoder output can only depend causally upon the source, one obtains what is known as the causal [4], [5], non-anticipative [6]–[8] or sequential IRDF [9]–[11]. All these are equivalent and will be denoted as Rcit(D), defined in terms
 of the mutual information] as
 where the infimum is taken over all joint distributions of y81 given x81 such that the causality Markov chains (which will be referred to as the short causality constraint)
 		(3)
 hold and which yield distortion not greater than D, for some fidelity criterion. Notice that, if one is given a two-sided random source process x8-8 = {...,x(-1),x(0),x(1),...} instead, and one is interested only in encoding and reconstructing the samples x81 , then the causality constraints may be stated as
 		(4)
 as done in [5]–[7]. This notion of causality will be referred to as the long causality constraint.
 The motivation for considering in this work one-sided instead of two-sided sequences (and thus (3) instead of (4)) arises from the aim of building encoder-decoder systems which operate with zero delay (the same motivation behind the causality constraint). To see this, notice that the causality constraint (4) for two-sided sources corresponds to the situation in which source samples in the infinite past exist and are available to the encoder. This may require an infinite delay before actually beginning to encode and decode. By contrast, the causality constraint (3) describes the case when the source is a one-sided process and yk1 depends only upon x 
 Remark 1. It is important to highlight at this point that even though the causality condition (3) can also be applied to a two-sided source process x , it would not ensure causality in that case. To see why, consider the situation in which x  is a binary i.i.d. source where each xk takes the values 1 or 0 with equal probability. Suppose y81 is built as y(k) = x-k ?x2k, where ? denotes the exclusive “OR” operator. It is easy to see that   satisfies (3), even though y81 depends non-causally on x81 .
 The above observation reveals that if the source is two sided but only the samples x81 are encoded and the decoded process is one-sided (y81 ), then one needs to impose instead the (more general)
 causality constraint
 		(5)
 which implies (3). Besides causality, these Markov chains guarantee that even if the source is a two-sided process, its encoding and reconstruction proceeds as if it were a one-sided process.
 Notice that (5) implies (3) and (4). For this reason, (5) will be referred to as the strong causality
 constraints.
 As we shall see in sections III and IV, this situation, where at time k the encoder can take only xk1 as input, entails significant challenges due to the unavoidable need to deal with transient phenomena.
 The operational significance of Rcit(D) stems from its relation to the causal operational RDF (ORDF), denoted as Rcop(D). The latter is defined as the infimum of the average data-rates which are achievable by a sequence of causal encoder-decoder functions [4], [5] yielding a distortion not greater than D. Characterizing Rcop(D) is important because every zero-delay source code (suitable for applications such as low-delay streaming [13] or networked control [14], [15]) must be causal.
 An IRDF is said to be achievable if it equals the ORDF under the same constraints [2], [3]. As far as the authors are aware, the achievability of Rcit(D) has not been demonstrated yet, for any source and distortion measure, and thus the gap between Rcop(D) and Rcit(D) is unknown in general. However, it is known that [5, Section II]
 		(6)
 and for Gaussian sources it is possible to construct causal codes with an operational data rate exceeding   by less than (approximately) 0.254 bits/sample (1.254 bits/sample for zero-delay codes), once the statistics which realize the latter are known [5]. This underlines the importance of studying the
 causal IRDF .
 To the best of the authors’ knowledge, no closed-form expressions are known for  , except when considering mean-squared-error (MSE) distortion and for Gaussian i.i.d. or Gaussian autoregressive (AR)-1 sources, either scalar [5, Section IV] or vector valued [16] . However, there exist various structural properties of the causal IRDF that have been found in literature when  admits (or is assumed to admit) a stationary realization.
 Indeed, the stationarity of the realizations of the causal IRDF has played a crucial role in simplifying the computation of   for Gaussian 1-st order Markovian sources and MSE distortion in [17]. It has also been a key implicit assumption in [10], and an explicit assumption in works such as [8] and [5]. In particular, for a stationary two-sided random source x , Definition 6] introduced the stationary causal IRDF
 		(7)
 where the infimum is taken over all distributions of y81 given x81 which yield a one-sided reconstruction processes y81 jointly stationary with x81 , satisfying (4) and an asymptotic average MSE distortion constraint on  . For the case of a Gaussian source, it was shown in [5] that an operational data-rate exceeding   by less than 1 + 0.5log2(2p e) ? 1.254 bits/sample was
 achievable using a entropy-coded subtractively dithered uniform quantizer (ECSDUQ) surrounded by linear time-invariant (LTI) filters operating in steady state. These examples illustrate the relevance of determining whether (or in which cases) the causal IRDF admits a stationary realization.
 To the best of our knowledge, the only work which has given an answer to this question in a general framework is [6]. Under a set of assumptions (discussed in Section II below), it is shown in [6, Theorem 4] that the search for the causal IRDF for a large class of two-sided sources and distortion criteria can be restricted to reconstructions which are jointly stationary with the source. Unfortunately, as we show in Section II-B, the assumptions on the fidelity criteria utilized in [6] leave out some common distortions (such as the family of asymptotic average single-letter fidelity criteria), and the statement of [6, Theorem 4] contains an assumption whose validity has to be proved. More importantly, the entire analysis of [6] is built for two-sided processes (using the causality constraint (4)), which opens the question of whether its results could apply to one-sided processes as well, with the causality constraint (3).
 In this paper we give an answer to these questions and use the results to prove some novel properties of the causal IRDF associated with the stationarity of its realizations. Specifically, our main contributions are the following:
 We show in Theorem 2 that if a pair of one-sided random processes x is jointly stationary, with the latter depending causally on the former according to (5) (but otherwise arbitrarily distributed), then it must also satisfy the Markov chains
 	 	(8)
 which is a fairly restrictive condition. In particular, as we show in Theorem 3, if x  are jointly Gaussian and y81 depends causally upon x81 , then joint stationarity implies x81 is an i.i.d. or 1st-order Markovian process. This stands in stark contrast with what was shown in [6] for two-sided stationary processes and constitutes a counterexample of what is stated in [18, Theorem III.6].
 Despite the above, we show in Theorem 4 that for any ?-th order Markovian one-sided stationary source x81 and a large class of distortion constraints, the search for the causal IRDF (as defined in (2)) can be restricted to output sequences causally related to the source and jointly stationary with it after ? samples, and such that  . We refer
 to such pairs of processes as being ?-quasi-jointly stationary (?-QJS) (this notion is formally introduced in Definition 2 below). A consequence of this result is that for any ?-th order twosided Markovian stationary source x  equals  for the corresponding one-sided
 stationary source x81 . The relevance of this finding is that for Gaussian stationary sources and asymptotic MSE distortion, an operational data rate exceeding  (and thus ) by less than approximately 0.254 bits/sample, when operating causally, and 1.254 bit/sample, in zero-delay operation, is achievable by using a scalar ECSDUQ as in [5].
 The remainder of this paper begins with Section II, in which the assumptions leading to [6, Theorem 4] are revisited and the limitations of that theorem are discussed. In Section III we prove that, in general, it is not possible to have two one-sided processes which are jointly stationary and, at the same time, satisfy the causality constraint (3). Section IV presents our main theorem (Theorem 4), which shows that the search for the causal IRDF for one-sided ?-th order Markovian stationary sources can be restricted to ?-QJS processes. Finally, Section V draws the main conclusions of this work. All proofs are presented in section VI (the Appendix), which also contains some technical lemmas required by these proofs.
 Notation: R denotes the real numbers, Z denotes the integers, N = Z+ is the set of natural numbers (positive integers),  and N0 , {0,1,...}. For every x ? R, the ceiling operator ?x? yields the smallest integer not less than x. We use non-italic letters for scalar random
 variables, such as x. Random sequences are denoted as x .
 For a random (one-sided) process x81 we will sometimes use the short-hand notation x wherever this meaning is clear from the context. When convenient, we write a random sequence  , as the column vector ykj , [y(j) y(j +1) ··· y(k)]T (the indices j and k are swapped so that the smallest index goes above the largest one, thus mimicking the usual index order in a column vector). The entry on the f-th row and k-th column of a matrix M is denoted as [M]f,k, with [M]jk being the sub-matrix of M containing its rows j to k, j = k.
 For a random element x in a given alphabet (set) X, we write B(X) to denote a sigma-algebra associated with X and Px : B(X) ? [0,1] to denote its probability distribution (or probability measure). We write x ~ y to describe the fact that y has the same probability distribution as x, and x ?? y to state that x and y are independent. We write the condition in which two random elements a,b are independent given a third random element c using the Markov chain notation a ?? c ?? b. If W is a set of probability distributions, then (W) denotes the set of all random elements whose probability distribution belongs to W. The expectation operator is denoted as E[·]. We write Xk as a shorthand for  . The mutual information between two random elements x ? X y ? Y is defined as [1, Lemma 7.14]
 	  ,	(9)
 where the supremum is over all quantizers q and r of X and Y, and Pq(x),r(y), Pq(x) and Pr(y), are the joint and marginal distributions of q(x) and r(y), respectively. If x,y have joint and marginal probability density functions (PDFs) fx,y, fx and fy, respectively, then [3]
 I(x;y) , E  .
 The conditional mutual information I(a;b|c) is defined via the chain-rule (cr) of mutual information I(a;b|c) , I(a;b,c) - I(a;c). The mutual information rate   between two processes
 x81	and y81	is defined as in (1). The variance of a real-valued random variable x is denoted as
  . The auto-correlation function of a random process x81 is denoted ?x(t,k) , E[x(k)x(k + t)], k = 1, t > -k.
 The following properties of the mutual information involving any random elements a,b,c will be utilized and referred to throughout this work:
 P1. I(a;b,c) = I(a;b), with equality if and only if I(a;c|b) = 0.
 P2. I(a;b,c) = I(a;b|c), with equality if and only if I(a;c) = 0.
 We will also make use of the following fact:
 Fact 1. Let a,b,c be three random elements with an arbitrary joint distribution. Then, there exists a random element ¯a (equivalently, a joint distribution P¯a,b,c) such that
 (¯a,b) ~ (a,b)	(10)
 ¯a ?? b ?? c	(11)
 REVISITING [6] AND ITS INAPPLICABILITY TO ONE-SIDED SOURCES
 In order to assess whether (or to what extent) [6, Theorem 4] could provide support to the stationarity assumptions made in, e.g. [8], [10], [18], [19], it is necessary to take a closer look at the assumptions made in [6] and the statement of its Theorem 4. For that purpose, the first part of this section is an exposition of the definitions and assumptions leading to [6, Theorem 4].2 The second part is an analysis which reveals the limitations of [6, Theorem 4] and its inapplicability to the case in which the source and reconstruction are one-sided processes. At the same time, this section also introduces definitions and part of the notation to be utilized in the remainder of this paper (for convenience, a summary of these is presented in Table I below).
 A. A Brief Review of [6]
 Throughout [6], the search in the infimizations associated with various types of “nonanticipatory” (i.e., causal) rate-distortion functions is stated over sets of joint probability distributions between source and reconstruction (as opposed to the usual definitions, in which the search is over conditional distributions, see (2) and [3, Chapter 10], [2]). Since the distribution of the source is given, it is required that for every k2 > k1 ? Z, all the joint distributions Pxkk21,ykk21 to be considered yield x  having the same (given) distribution of the source for the corresponding block, say P°xkk21. This requirement can be formalized as requiring that  , for a set of admissible joint
 distributions Pk1,k2 defined as
 	 ,	(12)
 where  and  are, respectively, the alphabets to which xkk21 and ykk21 belong. In [6], this admissibility requirement is embedded in the definition of the sets of distributions which meet the distortion constraint, described next.
 The fidelity criterion for every pair of integers3 k1 = k2 is expressed in [6] as requiring  to belong to a non-empty set of distributions (hereafter referred to as distortion-feasible set)  , a condition written as  . In this definition, the number D = 0 represents an
 admissible distortion level. Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation.
 As mentioned above, the admissibility requirement   is expressed in the distortionfeasible sets in [6, eqn. (2.1)]. The latter equation can be written as
  .	(13) 2
 We believe this re-exposition of [6] to be valuable in itself since on the one hand, it selects the minimal set of notions required to formulate and understand its Theorem 4, and on the other hand, it provides an arguably clearer presentation than the one found in [6] (an English translation from Russian), which is not easy to read due to its notation, some mathematical typos and the low resolution of its available digitized form. 3
 The analysis in [6] considered both discrete- and continuous-time processes, but here we only refer to the discrete-time scenario.
 In [6, eqs. (2.4) and (2.5)], the distortion-feasible sets are assumed to satisfy the “concatenation” condition
 	 .	(14)
 With this, [6, eqn. (2.9)] defined the “nonanticipatory epsilon entropy” of the set of distributions4   as
 	 ,	(15)
 where the infimum is taken over all pairs of random sequences   such that the causality Markov chains
 	x 	(16)
 are satisfied. Then [6, eq. (2.13)] defines the “nonanticipatory message generation rate” as
 	 	(17)
 (when the limit exists). An alternative “nonanticipatory message generation rate” is also considered in [6] by defining the set of distortion-admissible process distributions WD as follows:
 Definition 1. The set (WD) consists of all two-sided random process pairs (x8-8,y8-8) ? (WD) for which there exist integers ··· < k-1 < k0 < k1 < ··· such that limi?±8 ki = ±8 and
 	 .	(18)
 N
 With this, [6, eq. (2.12)] defines
 	 	(19)
 (when the limit exists), where the infimum is taken over all pairs of processes  satisfying the causality Markov chains
 	x .	(20)
 Notice that these Markov chains imply (4) and differ from the latter in that here the reconstruction y8-8 is a two-sided random process.
 Now assume that Xi = X and Yi = Y, for all i ? Z, for some alphabets X and Y. Define, for any given non-negative sequence  such that  , the distribution
 s2
 	Px¯kk21,y¯kk12(E) = X a(s)Pxkk21++ss,ykk21++ss(E),	k1 = k2,E ? B(X(k2-k1+1) × Y(k2-k1+1)).	(21)
 s=s1 4
 The actual term employed in [6] is “nonanticipatory epsilon entropy of the message” where the term “message” refers to the random ensembles in  .
 We can now re-state Theorem 4 in [6] as follows:
 Theorem 1 (Theorem 4 in [6]). Suppose that
 x  is stationary.
 Stationary distortion-feasible sets: For every   and   are
 identical sets. 
 The concatenation condition (14) holds.
 	-?	 
 HD0 = HD0 .
 For every set of non-negative numbers  such that  ,
 	 	(22)
 where the processes   are distributed according to (21).
 Then, the analysis of the lower bound in (19) can be confined to jointly stationary pairs of random
 processes   satisfying the causality constraint (20).	N
 For convenience, Table I presents a summary of the definitions and notation described so far, together with some which will be defined in the following sections.
 B. Analysis of Theorem 1 and its Inapplicability to One-Sided Sources
 We now discuss three limitations of Theorem 1 which are relevant when trying to establish whether the causal IRDF of a one-sided stationary source admits a stationary realization.
 Limitation 1: The first obvious limitation is that even if source and reconstruction are twosided processes, every distortion criterion which considers only their “positive-time” part cannot be expressed by a distortion-feasible set WD given by Definition 1 if the sets {WDl,j}l=j?Z satisfy condition ii) in Theorem 1. To see this, notice that if l = j < 1, then such distortion criterion (which neglects non-positive times) would require WDl,j to admit all joint probability distributions satisfying (13). Combining this with condition ii) in Theorem 1 yields that every set WDn,m = WDl,j with m - n = j - l, which amounts to imposing no restriction on the distortion at all.
 It is natural to think that such elemental shortcoming could be avoided by simply replacing condition ii) in Theorem 1 by a one-sided version of the form:
 	For every t1 ? Z, s,t2 ? N such that t1 = t2: WDt1+s,t2+s and WDt1,t2 are identical sets.	(23)
 Leaving aside the fact that this alternative condition is not sufficient for Theorem 1 to hold, it is worth pointing out that using (23), the commonly utilized family of asymptotic single-letter fidelity
 Table I
 SUMMARY OF THE MAIN SYMBOLS UTILIZED IN THIS PAPER.
 	1	1	 
 	  such that the associated marginal distribution 
 equals the given distribution of the source sequence x 	, i.e., 
  	Distortion-feasible set. The set of all joint distributions   which satisfy a given constraint given by D ?R (see comments before ( 
  	The set of all pairs of sequences   such that	. (See also the
 Notation subsection at the end of Section I.)
 PD8	Generic distortion-feasible set of probability distributions for pairs of one-sided processes
  . In this paper, we state some minimal conditions on PD8 in Assumption 1 and
 some additional structural properties in Assumption 2.
 Q?, ? = 1,2,...	The set of all joint distributions   of pairs of one-sided random processes
  	such	that	(x?8,y?8)	are	jointly	stationary	and	 
   (see Definition 2).
 (Cn), n = 1,2,... and	The sets of causally related one-sided pairs of n-sequences (see Definition 3).
 (C8)	The set of one-sided pairs of processes causally related according constraint (3) (see Definition 3).	to the short causality
  	The set of causal distributions for processes of the form  the long causality constraint (4) (see Definition 4).	. Such processes satisfy
 criteria [2] can not be expressed by a distortion-feasible set WD given by Definition 1, as the following lemma shows (its proof can be found in Appendix VI-A).
 Lemma 1. Let ? be any given distortion functional which takes as argument a joint distribution Px,y and yields a non-negative real value. Let (AD) be the set of all pairs of processes  where x  is stationary, with pair-wise distributions {Px(k),y(k)}k?Z which satisfy the asymptotic
 single-letter fidelity criterion
  	(24) Then, there doesn’t exist an infinite collection of distortion-feasible sets {WDk1,k2}k1=k2?Z satisfy-
 ing (23) such that the associated WD given by Definition 1 satisfies (WD) = (AD).	N
 Limitation 2: The second limitation associated with Theorem 1 is that its application requires
 	-?	 
 one to prove its condition iv), i.e., the unproven supposition that HD0 = HD0 , holds. The only work we are aware of which builds upon Theorem 1 is [18], and, accordingly, [18] provides [18, Theorem III.5], which states that a similar equality holds. Unfortunately, as shown in [20], the proof of [18, Theorem III.5] is flawed.
 We note that Lemma 3 in Section IV-A below provides two alternative sufficient conditions for an equality similar to   (but for one-sided processes) to hold.
 Limitation 3: The third limitation of Theorem 1 for its applicability to one-sided sources is the fact that the entire framework built in [6] is stated for two-sided processes (and, crucially, for the corresponding causality restriction given by Markov chain (20)). This difference cannot be simply neglected while expecting Theorem 1 to remain valid. Indeed, as we show in the next section (Theorem 2), a pair of random processes   can be jointly stationary and at the same time satisfy the causality Markov chain (3) only if y(k) is independent of x  when x(k) is given. Moreover, we prove that joint stationarity and causality are incompatible when the source is a ?-th order Markovian Gaussian one-sided process with ? > 1.
 CONDITIONS FOR JOINT STATIONARITY AND CAUSALITY TO HOLD TOGETHER
 In this section we address the question of whether there exists a one-sided reconstruction process y81 jointly stationary with a source x81 and which also satisfies the causality constraint (3).
 Each source random sample x(i) belongs to some given set (source alphabet) X and is allowed to have an arbitrary distribution. Recall that a random process y , where Y is the reconstruction alphabet and Y8 , Y × Y ···, is said to be jointly stationary with x81 if and only if, for every l ? N, the distribution of   does not depend on k, for k = 1,2,....
 The next theorem shows that, for such one-sided processes, joint-stationarity and causality may hold together only if y(k) is independent of xk1-1 when x(k) is given.
 Theorem 2. If x81 and y81 are jointly stationary and y81 is causally related to x81 according to (3), then
 	x .	(25)
 Proof. If (25) does not hold for some k and if y81	and x81	are jointly stationary, then	N
 	x 	(26)
 does not hold, which corresponds to not satisfying (3) for k = 1, completing the proof.	 
 To illustrate how restrictive condition (25) is, the next theorem shows that, for a Gaussian ?-th order Markovian stationary source x81 , causality and joint stationarity is possible only if x81 is i.i.d. (? = 0) or ? = 1. Recall that a random (vector or scalar valued) process x -th order Markovian if ? is the smallest non-negative integer such that
 	x 	(27)
 Theorem 3. Suppose x81	is a zero-mean Gaussian stationary process, and assume that, for some
   are jointly Gaussian and jointly stationary, with yN1 being causally related
 to xN1 according to (3). Then x -th order Markovian with ? = 1.	N
 Proof. Since xN1 and yN1 are jointly Gaussian and the latter depends causally upon the former, it holds that
 	KyN1 x1N , E AKx1N	(28)
 for some lower triangular matrix A ? RN×N having entries ai,j , [A]i,j, i,j ? {1,...,N}. On the other hand, the fact that xN1 and yN1 are jointly stationary implies that KyN1 x1N and Kx1N are Toeplitz matrices. From (28), considering the entries on the first and second rows of KyN1 x1N and defining
 	?k , E[x(1)x(1 + k)],	k = 0,1... ,N - 1,
 this Toeplitz condition implies that
 a1,1[?0 ···?N-2] = a2,1[?1 ···?N-1] + a2,2[?0 ···?N-2] ??  a1,1 - a2,2 [?0 ···?N-2] = [?1 ···?N-1]. a2,1 | ,{z?	}
 Therefore, ?k = ??k-1,k = 1,...,N -1, which for a Gaussian stationary sequence xN1 implies that
 E . For Gaussian random variables the latter is equivalent to the Markov chains x , which defines a 1-st order Markovian process (if ? 6= 0) or an i.i.d. process (if ? = 0). This completes the proof.  
 In the next section we will see that if x81 is ?-th order Markovian, then it is possible to build a pair   causally related according to (3) such that (x8? ,y8? ) is stationary. Moreover, we will show in Theorem 4 below that the minimization associated with the causal IRDF can be restricted to such pairs.
 THE SET OF QUASI-JOINTLY STATIONARY REALIZATIONS IS SUFFICIENT
 In this section we show that for any ?-th order Markovian one-sided stationary source x81 the search for the causal IRDF (as defined in (2) and for a large class of distortion criteria) can be restricted to output sequences y81 causally related to the source, jointly stationary with it after ? samples, and such
 that . We refer to such pairs of processes as being quasi-jointly stationary
 (?-QJS), and define the set which contains them as follows:
 Definition 2 (Set of quasi-jointly stationary process). The set of ?-QJS distributions Q? is composed of all joint distributions Px81 ,y81 of pairs of one-sided random processes   which satisfy
   are jointly stationary
  .
 N
 Notice that Q1 corresponds to the set of joint distributions associated with all jointly stationary one-sided process pairs.
 As in [6], we write   when the distribution of   belongs to the distortion-
 feasible set WD1,k, defined as in (13).
 One can define a distortion-feasible set for pairs of one-sided processes  , say (PD8), from the finite-length distortion-feasible sets {WDk,l}k=l?N, in more than one manner. A minimal condition we shall require for such definition is the following.
 Assumption 1. The distortion-feasible set of distributions for pairs of one-sided processes PD8 satisfies the following:
 If  , then x˜81	has the given probability distribution of the source process, say
  . That is, PD8 ? P1,8 (see (12)).
 If   is any given pair of one-sided processes, and there exists an infinite collection of increasing integers 1 = k1 < k2 < ··· such that, for all ,
 then  .
 For any pair of sequences  , and if  , then the concatenated processes x¨ ,x˜(1),x˜(2), ,y˜(1),y˜(2),···}
 satisfy  .
 N
 Notice that if PD8 satisfies this assumption and if the integers ki in Definition 1 were restricted to be positive, then we would have WD ? PD8 (see Definition 1). However, the one-way implications in Assumption 1 allow PD8 to be larger than WD.
 We now define the sets of causally related pairs of sequences and processes.
 Definition 3 (Set of Causal Distributions). Define (Cn) as the set of all one-sided random n-sequences   which satisfy the causality constraint
 	x 	(29)
 The set of causally related one-sided process pairs (C8) is defined likewise but for one-sided processes
   which satisfy the causality constraint (3).
 With the above minimal notions, one can define two causal IRDFs, namely
  , 	(30)  , .	(31)
 provided the limits exist. The “liminf” causal IRDF Rˆcit(D) coincides with  if in (17) one fixes k1 = 1 and lets k2 ? 8. By contrast, Rcit(D) differs from   in that the latter is associated with the (less general) distortion-feasible set WD (see Definition 1).
 Since our main result will be stated with the assumption that Rcit(D) = Rˆcit(D), we develop next two sufficient conditions for such equality to hold.
 A. Sufficient Conditions for Rcit(D) = Rˆcit(D)
 We begin by stating a useful construction of a pair of processes from a finite-length sequence and some if the properties of the former.  
 Proposition 1. Let  , be given, with x?81 stationary and such that   satisfies the causality condition (3) for k = 1,2,... ,n. Build the processes   as follows:
 Choose x˜81	with the same distribution of x?81 , i.e.,
 	x˜ .	(32a)
 For every l ? N0, choose the conditional distribution of y˜nln(l+1+1) given x˜  as
 	 .	(32b)
 Then
 	 .	(33)
 	 .	(34)
 Also:
 If  , Assumption 1 and
 ?k,l ? N,t ? N0 :	WDk,k+t = WDl,l+t,	(35)
 hold, then  .
 If x? -th order Markovian, then
 	x˜ .	(36)
 N
 Proof. The first equality in (32b) is equivalent to the Markov chains
 	y˜ 	(37)
 The second equality in (32b) together with the fact that x˜81 is stationary imply that   , and thus (33) follows. On the other hand, we have that
 l-1
 	ln1 I(x˜ln;y˜ln1 ) = ln1 XI(x˜nini++1n;y˜nini++1n) (=33) n1I(x?n1;y?1n),	(38)
 1 i=0
 where the first inequality holds due to Proposition 3, in the Appendix (applying it successively to the
 sequences  , which can be done because they satisfy (37)). This proves (34).
 The fact that  , the definition of   and the stationarity condition (35) imply that  , for all l ? N0. The latter together with Assumption 1
 implies that  .
 On the other hand, (37) together with the fact that   satisfies (3) for k = 1,2,... ,n,
 generalizes to
 	y˜ .	(39)
 The latter implies the Markov chains
 	 .	(40)
 	b	c,d	a
 Supposing x? -th order Markovian, it holds that
 .	(41) {dz	{cz	{az
 Invoking Proposition 4 with a,b,c,d according to the labeling in (40) and in (41), we readily obtain that (b,d) ?? c ?? a, implying
 	y˜ .	(42)
 Thus   satisfies the causality condition (36). This completes the proof.	 
 We now state a technical lemma which is akin to [6, Theorem 2] but for one-sided processes, the proof of which can be found in Appendix VI-A.
 Lemma 2. Let  be a stationary one-sided source and suppose the distortion-feasible sets {WDk,l}k=l?N and PD8 satisfy Assumption 1 and the stationarity condition
 	?k,l ? N,t ? N0 :	WDk,k+t = WDl,l+t.	(43)
 Then
 	Rcit(D) = Rˆcit(D).	(44)
 N
 Next, we propose a possible definition of PD8 general enough to encompass the asymptotic singleletter fidelity criteria described by (24). For that purpose, we need to define
 	 	(45)
 and require the distortion-feasible sets to satisfy the following assumption:
 Assumption 2. The distortion-feasible sets {WDk,l}k=l?N can be expressed as	
 WDk,l = nPxl,yl : ?k,l(Pxl,yl ) = Do	(46)
 	k	k	k	k
 for some non-negative distortion maps . Moreover, the distortion-feasible
 set for one-sided sequences, PD8, has the form
 	 	(47)
 N
 Notice that with such construction, PD8 does not necessarily satisfy Assumption 1. Also, the distortion-feasible sets {WDk,l}k=l?N with the specific form given by (46) do not necessarily satisfy the stationarity condition (43).
 This definition, based on the limit of a sequence of distortion functions, is clearly capable of representing the general asymptotic single-letter criteria of (24) while satisfying Assumption 1. Recall that, as shown in Lemma 1, it is not possible to do this with the distortion-feasible set WD from [6], given by Definition 1. In addition, the construction of PD8 provided by (47) allows for several specific criteria commonly found in the literature, such as the one utilized in [5] and in the definition of a rate-distortion achievable pair in [3, p. 306].
 We are now in the position to provide two independent conditions that are sufficient to ensure Rcit(D) = Rˆcit(D) (the proof is given in Appendix VI-A).
 Lemma 3. Consider the same conditions given in the statement of Lemma 2. If, in addition, any of the two following conditions holds
 For every D > 0, there exists N < 8 such that,
 	 	(48)
   is continuous and Assumption 2 holds,
 then
 	Rcit(D) = Rˆcit(D).	(49)
 N
 B. Main Result
 With the above notions, we can state the main result of this section, akin to Theorem 1 but for one-sided processes and for the corresponding causality condition given by (3) (the proof is presented in Appendix VI-A):
 Theorem 4. Let the source°x81 be a one-sided stationary ?-th order Markovian process and suppose that Rˆit(D) = Rcit(D), where Rcit(D) and Rˆit(D) are as defined in (30) and (31), respectively. Furthermore, suppose that the distortion-feasible sets PD8, {WDk,l}k=l?Z satisfy Assumption 1 and 1) The “shift-invariance” condition:
 	 .	(50)
 The stationarity condition given by (43).
 The “first-samples condition”: There exists a pair of random sequences  
 (C?-1) and such that  .
 Then the minimization in the definition of Rcit(D) in (30) can be restricted to pairs of processes
 (x81 ,y81 ) with distributions in Q? n PD8 n C8 which, in addition, satisfy (x?8 ,y?8 ) ? (PD8). N
 Putting aside the obvious difference between Theorem 4 and Theorem 1 arising from the fact that the former considers as sources one-sided processes and the latter two-sided processes, it is worth drawing a parallel between these two theorems. The requirement of Assumption 1 in Theorem 4 is weaker than the requirement of WD to conform to Definition 1 in Theorem 1. Thus, Theorem 4 holds for a larger class of fidelity criteria. The assumption that   can be seen as the
 equivalent of condition iv) in Theorem 1 translated to the setting of one-sided sequences. The same is true with condition 2) in Theorem 4 with respect to condition ii) in Theorem 1. However, no conditions are stated in [6] which suffice for condition iv) in Theorem 1 to hold. In contrast, we have provided Lemma 3, which gives two independent conditions under which   is satisfied. The other assumptions in Theorem 4 differ from those in Theorem 1. Condition 1) in Theorem 4 is weaker than condition v) in Theorem 1. Condition 3) in Theorem 4 is absent in Theorem 1, and is required in our proof as a consequence of the transient behavior arising from treating one-sided processes (see Theorem 2 in Section III).
 Remark 2. Among the distortion criteria which satisfy the conditions of Theorem 4, we find the family of asymptotic single-letter constraints of (24) (by letting WDk,l = {Px?lk,y?lk : Px?kl = Pxlk,(l-k+
 1) ). Recall that this class of distortion criteria cannot be expressed using
 a distortion-feasible set WD conforming to Definition 1, and hence it is not covered by Theorem 1.
 N
 As pointed out by Remark 1 in the Introduction, if now one supposes that °x81 is the positive-time part of a two-sided stationary process °x , then the fact that   does not guarantee
 that y81 depends causally on °x81 . For the latter to hold in this situation, it is required that  satisfies (5). This implies that for this case, the definition of the causal IRDF as stated in (30) needs to be extended to
 	Rcit(strong)(D) , 	 .	(51)
 (x8-8,y81 ) satisfies (5)
 (x81 ,y81 )?(PD8)
 Notice that when the source lacks a negative-time part, (5) simplifies to (3), and thus (strong)(D) becomes equal to .
 The above observations raise the question of whether Theorem 4 can be extended for the case in which the one-sided source is the positive-time part of a two-sided stationary process. This implies considering (strong)(D) instead of , or, equivalently, the strong causality constraint (5) instead of the short causality constraint (3).
 It turns out that Theorem 4 can indeed be extended for this situation, thanks to the following proposition, the proof of which can be found in Section VI-A.
 Proposition 2. Suppose that x81	is the positive-time part of a two-sided process x  and that
   satisfies the one-sided causality condition (3), i.e.,
 	x .	(52)
 Then there exists (or, equivalently, one can construct) a one-sided random process y¯81	such that
 	 	(53)
 	 	(54)
 N
 Thanks to Proposition 2, we have the following extension of Theorem 4.
 Theorem 5 (Extension of Theorem (4)). Let the source °x81 be the positive-time part of a two-sided stationary ?-th order Markovian process°x . Under the same assumptions made in the statement of Theorem 4, the infimization in the definition of   can be restricted to pairs of processes
   which satisfy (5) and such that   and  . Moreover,
  .	N
 Proof. The only difference between the RHS of (30) and (51) is that they consider the causality constraints (3) and (5), respectively. First, notice that
 	  satisfies (5)   satisfies (3)	(55)
 and thus Rcit(strong)(D) = Rcit(D). On the other hand, from Theorem 4, the infimization yielding Rcit(D) can be carried out considering only pairs of processes   satisfying (3) and  
 (PD8 n Q?), (x8? ,y8? ) ? (PD8). But as a consequence of Proposition 2, for every such (x81 ,y81 ), there exists a process y¯81 such that   satisfies (5) and
 	 ,	(56)
 	 ,	(57)
 	 .	(58)
 This implies that the minimization associated with  (strong)(D) can be restricted to pairs  satisfying (5), (56) and (57) (proving the first claim of the theorem) and that (strong) .
 The latter and the reverse inequality confirms that Rcit(strong) , concluding the proof.	 
 C. Correspondence Between Rcit(D) and 
 The purpose of this section is to establish a correspondence between  (introduced in [5]). As we discuss next, drawing an appropriate comparison between these two causal IRDFs requires two modifications to the definition of  already described on page 4.
 The first modification consists of extending  to account for arbitrary fidelity criteria embodied in an arbitrary distortion-feasible set PD8 ? P1,8.
 The second modification is necessary in order to make   a tighter lower bound to the corresponding infimal operational data rate. To see why, it is necessary to recall how  lower bounds the operational data rate of encoding x81 and decoding it as y81 . For this purpose, let b(k) be the random binary sequence produced by the encoder from time 1 to k and let |b(k)| be the length of b(k) (in bits). Since the code must be uniquely decodable , the bit-string b(k) satisfies the Kraft inequality [3, § 5.1]. In general, b(k) can be generated by using not only xk1 but also x0-8, and thus
 	a	b
 	E[,	(59)
 where (a) follows from [3, Theorem 5.3.1] and (b) is a consequence of the data-processing inequality [3, Theorem 2.8.1]. Thus,  lower bounds the operational data rate k-1 E[|b(k)|] as tightly
 as  if and only if
 	y .	(60)
 This Markov chain, combined with the causality constraint (4)
 	y ,	(61)
 implies from Proposition 4 (in the Appendix) that
 	y ,	(62)
 which is precisely the causality constraint for one-sided sources (3). But, as we have shown in theorems 2 and 3, such causality constraint is, in general, incompatible with the joint stationarity of
  . As a consequence, since such joint-stationarity is required by , Markov chain (60) cannot hold. This means that when the causality constraint for a two-sided source established by (4) is
 imposed,   is a tighter lower bound to the operational data rate than .
 Following these observations, we propose here the following modified definition of .
 Definition 4 (An Improved and More General Definition of  . For any given x  two-sided
 stationary source, redefine the causal stationary IRDF introduced in [5, Definition 6] as
 	,	inf	lim	limI(xl
 	8 8 8 8	8	1	n;y1n)	(63)
 (x-8,y1 ):(x1 ,y1 )?(PD )n(Q1) l?-8 n?8 n
 (x8-8,y81 )?(C-88)
 where (Q1) is the set of all pairs one-sided jointly stationary random processes, and (C-88) is the set of all pairs of random processes   which satisfy the causality constraint (4) .
 We can now state the following corollary of Theorem 4, the proof of which can be found in Appendix VI-A:
 Corollary 1. Under the same assumptions of Theorem 4, it holds that
 	 .	(64)
 N
 One important consequence of this result stems from the fact that, for a ?-th order Gauss Markov stationary source and quadratic distortion,   can be found by solving a convex optimization problem over frequency-response magnitudes of linear-time invariant filters around an additive whiteGaussian noise (AWGN) channel [5, lemmas 3 and 5].
 The operational relevance of Corollary 1 is that when the latter AWGN channel is replaced by an entropy-coded subtractively-dithered scalar quantizer, one obtains a source-coding scheme whose operational rate exceeds  by at most 0.254 bits/sample when operating causally and by at most 1.254 bits/sample when operating with zero delay [5, section VI]. Thanks to Corollary 1, it turns out that the operational data rate of such scheme lies within the same bounds with respect to  
 itself.
 CONCLUSIONS
 We have shown that, in general, the causal information rate-distortion function (IRDF)  for one-sided stationary sources cannot be realized by a reconstruction which is jointly-stationary with the source. Nevertheless, if the source is ?-th order Markovian, then the search for the causal IRDF can be restricted to reconstructions which are jointly stationary with the source from the ?-th sample. This led us to prove that  actually coincides with  for a large class of distortion criteria. This reveals that for Gauss-Markov sources and quadratic distortion,  can be found by solving the convex optimization problem derived in [5]. It also implies that for the same source and distortion, a zero-delay average data rate exceeding   by not more than (approximately) 1.245 bits/sample is achievable with the scheme proposed in [5].
 APPENDIX
 A. Proofs
 Proof of Lemma 1. We will resort to a contradiction argument, and thus start by supposing that there exists (WD) = (AD).
 Since ? is non-negative, there must exist a pair of random processes   and a value D = 0
 for which (24) holds with equality. Hence,  , and thus  .
 From the definition of (AD), any other pair of processes   which distributes exactly as
   everywhere except on a single positive index, say l ? N, in which Px?(l),y?(l) ? P1,1 and
 	?(Px?(l),y?(l)) = D + 1,	(65)
 will also belong to (AD) and therefore  . The latter means that, according to
 Definition 1, there exists a pair of integers l1,l2 with l2 ? N such that l1 = l = l2 and
 	 .	(66)
 This, together with the fact that the sets {WDt,s}t=s?Z satisfy (23), implies that
 	 ,	(67)
 where L , l2 -l1 +1. Hence, any pair of random processes   with pair-wise distributions given by
 	???Px(k),y(k),	k /? {t = l + mL : m ? N}
 	Px¨(k),y¨(k) =	(68)
 	??Px?(l),y?(l),	k ? {t = l + mL : m ? N}
 together with the collection of integers   satisfy the conditions of Definition 1, and thus  . However,
 	 	(69)
 meaning that  . This contradicts the initial supposition that (WD) = (AD), completing the proof.	 
 Proof of Lemma 2. From the definition of  we have that
 	 .	(70)
 By the definition of inf, we have that ?o2 > 0,n = No1, there exists   such that
 	 .	(71)
 Now consider the pair   such that
 	x? 	(72)
 	 	(73)
 and build the processes   as in Proposition 1. The mutual information rate between x˜kn1	and
 y˜kn1	can be upper bounded as
 	 .	(74)
 From (74) we also obtain that, for all i ? N,
 	 	(75)
 Recalling that the latter holds for all i ? N and that  , we obtain
 	 .	(76)
 Since this inequality is satisfied for all o1,o2 > 0, it follows that , completing the
 proof.	 
 Proof of Lemma 3. Since the conditions of Lemma 2 are satisfied, we have that  . Therefore, it suffices to show that .
 We will first show that ( . By the definition of   we have that
 	 .	(77)
 The latter means that for all o2 > 0, there exists a finite No2 such that
 	 .	(78)
 Also, since  , it follows from (48) that there exists a finite N such that  
 (WD1,n) for all n = N. Since all the latter holds for all n = max{No2,N}, we obtain
 	 .	(79)
 The latter is equivalent to
 	 .	(80)
 Since this inequality holds for all o1,o2 > 0, it follows that  , completing the first part of the proof.
 We shall now prove that Assumption 2 and the continuity of   implies . The continuity assumption on   means that
 	 	(81)
 satisfies limd?0 od = 0, for all D > 0. By the definition of inf, we have that, for every d > 0,o1 > 0, there exists a pair of processes   such that
 	 .	(82)
 The latter means that, for every o2 > 0, there exists a finite No2 such that,
 	 .	(83)
 Also, since   and from the definition of PD8 in (47), it follows that there exists a
 finite Nd such that
 	 	(84)
 Thus, ?d > 0,o2 > 0:
 	 	(85)
 Since this inequality holds for all d,o1,o2 > 0, and recalling that od ? 0 when d ? 0, it follows that Rˆit(D) = Rit(D), completing the proof.	 
 Proof of Theorem 4. Since , it follows that for all o1 > 0, there exists a finite No1 such that
 	 .	(86)
 Thus, for all o2 > 0 and for all n = No1 there exists a pair of sequences  such that
 	 .	(87)
 The fact that x?n1 distributes as   allows one to define the stationary extension of x?n1 such that
 	x? 	(88)
 	 .	(89)
 The latter, together with the fact that   implies that   satisfies (3) for k =
 1,2,... ,n.
 Starting from   we build the processes   as in Proposition 1. From this construction
 and the stationarity assumption on the distortion-feasible sets given by (43), we have that
 	 ,	(90)
 and, from (34), that
 	 .	(91)
 Then, for any m ? N, t ? {0,... ,n - 1}, if we define , we have
  	(92)
  .
 Dividing both sides by m we obtain	(93)
 	 .	(94)
 Now suppose that t is a random variable uniformly distributed over {0,1,... ,n-1} and independent of x˜81 . Let n = ? and define the pair of processes  
 x¯(k) , x˜(k + n + t),	k = -? + 2,-? + 3,...	(95a)
 y¯(k) , y˜(k + n + t),	k = 1,2,...	(95b)
 8A similar construction, for two-sided processes, was proposed in the proof of [6, Theorem 4] (seemingly for the first time), building upon [21]. The same idea was rediscovered in the proof of [22, Theorem 3.2]. Here we adapt it for the case of one-sided processes.
 It is easy to verify that x¯  (from the stationarity of x ), and that
   are jointly stationary. Thus x¯ -th order Markovian stationary. These facts imply, in view of theorems 2 and 3, that the pair   may not be causally related according to (3). However, since   satisfies (36) (see Proposition 1), we have that   does satisfy the
 causality Markov chains
 	x¯ .	(96)
 On the other hand, in view of (90) and thanks to Assumption 1, we have that  .
 Thus, from shift-invariance condition (50), it readily follows that
 	 .	(97)
 Now let
 	xˆ?1-1 , x¯0-?+2	(98)
 and build yˆ?1-1 such that   and the Markov chain
 	yˆ ,t)	(99)
 holds. According to the “first-samples condition” in the statement of Theorem  
 (WD1,?-1) n(C?-1) and . Now, concatenate   with   so as to
 obtain the pair of one-sided processes
 x¨ ,xˆ(2),... ,xˆ(? - 1),x¯(1),x¯(2),...}	(100a) y¨ ,yˆ(2),... ,yˆ(? - 1),y¯(1),y¯(2),...}	(100b)
 Since   and	 )), it follows from Assumption 1 that
 	  and  .	(101)
 On the other hand, (99) and (96) imply that x¨ , i.e,
 	 .	(102)
 The pair   further exhibits two important properties. First, Lemma 4 shows that the pairs of processes   are ?-QJS (i.e.  ). Second, as we show in Lemma 5, the mutual information rate of the pair processes   lower bounds , for all n ? N. Thus, for every n = max{No1,?},
  .
 Since the existence of a pair of processes such as   which also satisfy
   is guaranteed for every o1 > 0 and o2 > 0, it readily follows that the search for
 the infimum on the RHS of (30) can be confined to such pairs, completing the proof.	 
 Lemma 4. Let   be the concatenated processes defined in (100), which are built from
   (see (95)) and   (see (98) and (99) and the text between these equations) and satisfy (102) and (101). Then  . N
 Proof. By construction,   are jointly stationary. Thus, all that remains to prove is that   (see Definition 2). For this purpose, notice that for all i > ?,
 	 	(103)
 	 	(104)
 	 	(105)
 	 	(106)
 	 	(107)
 	 	(108)
 where all the equalities labeled “(cr)” stem from the chain-rule of mutual information, (a) holds because I(a;b) = 0 , and (b) is a consequence of the fact that x¨ , and
 Markov chain (99). The mutual information in the middle of (108) can be upper bounded as
 	 	(109)
 	 	(110)
 	 ,t) + 	(111)
 	 ,	(112)
 where the equalities labeled (cr) are due to the chain rule of mutual information, (a) follows because mutual information is non-negative, and (b) holds because x¯  t implies x¨81 ?? t.
 	Using the definition of x¨81 ,y¨81	(see (100)) we have that, for the case i = n + ?,
 	 ,t) =	 +1+t++1+t	i-?,t)	(113)
 	 +1	1	+1+t	,t)	(114)
 	= I(b23,b4;a1,a2 |a3,a4,t),	(115)
 where the random elements
 a1 , x˜n1	a2 , x˜nn++1t	(116a)
 a3 , x˜2nn+t+1	a4 , x˜n2n+1+t++1	i-?	(116b)
 	b23 , y˜ 	b4 , y˜n2n+1+t++1	i-?	(116c)
 are introduced so as to streamline the presentation of the following steps. The relations between all these variables is illustrated in Fig. 1. Notice that for these sequences the Markov chains (37) translate into
 b23 ??a2,a3 ?? a1,a4,b4	(117)
 b4 ??a4 ?? a1,a2,a3,b23
 With this, we can continue from (115) and deduce that	(118)
 	 ,t)	(119)
 (cr)
 = I(b23;a1,a2 |a3,a4,t) + I(b4;a1,a2 |b23,a3,a4,t)	(120)
 (118)
 = I(b23;a1,a2 |a3,a4,t)	(121)
 (cr)
 = I(b23;a1,a2,a3,a4 |t) - I(b23;a3,a4 |t)	(122)
 	 t)	(123)
 (cr)
 = I(b23;a2,a3 |t) + I(b23;a1,a4 |a2,a3,t)	(124)
 (117)
 = I(b23;a2,a3 |t)	(125)
  t)	(126)
  	(127)
 where (a) follows because mutual information is non-negative and (b) is due to (33) and the fact that
  .
 Substituting (127) into (112) and then the latter into (108), we arrive at
 	 .	(128)
 Dividing by i and taking the limit as i ? 8, we conclude that, for all n ? N,
 	 ,	(129)
 proving the claim that  .	 
  
 Figure 1. Schematic representation of the change of variables introduced in (115). Each dot represents one element in the sequences x˜81 and y˜81 , with time increasing from left to right.
 Lemma 5. Let   the concatenated processes defined in (100), built from  and   and (99) and the text between these equations). Then
  .
 N
 Proof. First, notice that (129), (95) and (100) imply
 	 .	(130)
 On the other hand
 	 	(131)
 	 ,t)	(132)
 	 ;t)	(133)
 	 ,	(134)
 where (a) is due to the non-negativity of mutual information and (b) holds because t . But
 	 t)	(135)
 	 ;y˜t+t+1m|t)	(136)
 	 ,	(137)
 where (a) follows from the fact that, for all 
 (see (32)). Substituting this into (134) we obtain that for every n ? N and m ? N,
 	 .	(138)
 The proof is completed by taking the limit as m ? 8 and substituting (130) in it.	 
 Proof of Proposition 2. From Fact 1, there exists y¯81	such that (53) holds and which satisfies
 	x .	(139)
 Combining (53) with (52) one obtains
 	 .	(140)
 	d	c
 On the other hand, (139) readily implies that	a
 	 .	(141)
 	{bz	c,d	a
 Applying Proposition 4 with a,b,c,d corresponding to the labels placed under the terms in (140) and (141), we obtain directly (54), completing the proof.  
 Proof of Corollary 1. We will first show that  and then that the reverse inequality is true as well.
 From Theorem  . Also, Theorem 5 states that the search for 
 can be confined to pairs of processes   which satisfy (5) and such that  (PD8)) and  . For each such pair, one can construct the pair of processes  
 as
 	x¨ 	(142a)
 	 .	(142b)
 This construction yields that
 	 	(143)
 	  (since  )	(144)
 	 ,	(145)
 where the last equality stems from the fact that  and recalling that   and the definition of (Q?) implies . In addition, the
 fact that   satisfies (5) implies that
 	 	(146)
 which in turn leads directly to
 	x¨ 	(147)
 and to
 	 .	(148)
 This leads to
 	 .	(149)
 Therefore, for every pair of processes which satisfies the constraints associated with , there exists another pair which satisfies the constraints in the definition of   and yields the same information rate. This proves that .
 In order to show that , consider any pair   satisfying (4) and such that  . Construct the pair of processes   as
 	 	(150a)
 	x? , x ,	(150b)
 and let the joint distribution of the pair   be such that  and
 	y? 	(151)
 (the existence of such pair is guaranteed by the “first-samples condition” in the statement of Theorem 4). From Assumption 1, this construction yields
 	 .	(152)
 The fact that   satisfies (4) translates into
 (153) b	{dz	c	{az
 The ?-th order Markovianity of x?	yields that
 	 	(154)
 	d	c	a
 Applying Proposition 4 to (153) and (154) with variables in the proposition assigned according to the labels under (153) and (154), we obtain that
 	y? 	(155)
 which combined with   yields  . In addition,
 	 	(156)
 	 	(157)
 	 	(158)
 	 .	(159)
 On the other hand,
 	 	(160)
 for all l < -? + 2. Thus
 (161)
 (162)
 Hence, for every pair   such that   there exists a pair
   satisfying (162). This readily implies that , completing
 the proof.	 
 B. Other Technical Results
 Proposition 3. For any random elements a1,a2,b1,b2 satisfying the Markov chains
 a2,b2 ?? a1 ?? b1	(163)
 a1,b1 ?? a2 ?? b2
 it holds that	(164)
 I(a1,a2;b1,b2) = I(a1;b1) + I(a2;b2) - I(b1;b2)
 Proof of Proposition 3. The mutual information between a1,a2 and b1,b2 is given by	(165)
 N
 (cr)
 I(a1,a2;b1,b2) = I(a1,a2;b1) + I(a1,a2;b2 |b1)	(166)
 (cr)
 = I(a1;b1) + I(a2;b1 |a1) + I(a1,a2;b2 |b1)	(167)
 (163)
 = I(a1;b1) + I(a1,a2;b2 |b1)	(168)
 (cr)
 = I(a1;b1) + I(a1,a2,b1;b2) - I(b1;b2)	(169)
 (cr)
 = I(a1;b1) + I(a2;b2) + I(a1,b1;b2 |a2) - I(b1;b2)	(170)
 (164)
 = I(a1;b1) + I(a2;b2) - I(b1;b2),	(171)
 where all equalities labeled (cr) stem from the chain rule of mutual information.	 
 Proposition 4. Let a,b,c,d. Then
 	{a ?? c ?? d	?	a ?? c,d ?? b}	??	a ?? c ?? (b,d).
 Proof. The chain rule of mutual information yields	(172)
 N
 I(a;b,d|c) = I(a;d|c) + I(a;b|c,d).	(173)
 Since mutual information is non-negative, it follows that I(a;b,d|c) = 0 if and only if both I(a;d|c) and I(a;b|c,d) are zero. The proof is completed by noting that the statement I(u;w|z) = 0 is equivalent to the Markov chain u ?? z ?? w.  
 ﻿This paper deals with the design of feedback quantizers to encode plant output measurements in networked control systems with data-rate constrained channels. Starting form a nominal design made under the assumption of transparent communication links, we show how to design a feedback quantizer so as to systematically reduce the impact of quantization on closed loop performance. To obtain our results, we model quantization errors as additive white noise with a signal-to-noise ratio constraint. As a byproduct, we obtain a simple characterization of the minimal quantizer signalto-noise ratio that allows one to design a feedback quantizers that guarantees stability. This bound depends only upon the plant and controller unstable poles. If the plant is strongly stabilizable, then the bound is consistent with the absolute minimal data-rate for stabilization obtained in previous work. 
 Standard control theory deals with situations where the communication links between plant and controller can be regarded as transparent (see, e.g., [22,55]). There exist, however, cases where the links in a control system are far from being transparent and may become bottlenecks in the achievable performance. Control systems where this happens are collectively referred to as Networked Control Systems (NCS’s) (see, e.g., [1,2,27,28] and the many references therein). Clearly, unless the channel characteristics are explicitly taken into account at the design stage, the performance of an NCS may be far from optimal and sometimes completely unsatisfactory. The main issues that need to be considered when dealing with NCS’s include data-rate constraints (i.e., quantization), data loss and random delays. A unifying framework for the treatment of the general NCS design problem does not exist. Nevertheless, there has been significant progress in the study of specific situations that focus on subproblems. For example, data-rate constraints have been studied in [40,42,45,48,60] and design strategies to deal with quantization have been proposed in, e.g., [21,66]. The issue of data loss has been studied in [33,49,51], among many others, and delays have been considered in, e.g., [32,43,58,62]. 
 In this paper we focus on linear time invariant (LTI) plant models, and concentrate on the effects of quantization on closed loop performance. Within this framework, a key result obtained in [40] relates the minimal data-rate which is necessary and sufficient to achieve stabilization of an unstable plant, to its poles in very simple way. This bound has been linked to information theoretic concepts where it has been given an interpretation akin to that of entropy (see, e.g., [37,41,48]). Furthermore, [7,8] established that the minimal data-rate for stabilization is sometimes consistent with minimal signalto-noise ratio requirements in standard one-degree-of-freedom control architectures that employ LTI controllers. More precisely, [7,8] showed that, if the plant is defined in discrete time, has relative degree one and is minimum phase, then a Gaussian memoryless channel having a signal-to-noise ratio equal to the lower bound derived in [7,8] would exhibit a channel capacity equal to the data-rate bound in [40]. 
 The results discussed above give absolute lower limits on the admissible channel data-rate which cannot be by-passed by any control law. It seems, however, quite difficult to obtain practical design guidelines from considerations such as those in [40,41,48]. This has motivated some researchers to move towards a simplified treatment of quantization. For example, [21] models quantization as a sector bound uncertainty and employs standard robust control tools. On the other hand, [66] uses a simple white noise model for quantization errors. The latter model for quantization has close connections to the signal processing literature, where it has been successfully used to design high performance quantization schemes (see, e.g., [4,25,30,36,50]). 
 In the present work we assume that a controller has already been designed under the assumption of transparent communication links. However, we subsequently extend the set-up by assuming that the control loop has to be implemented using a bit-rate limited channel in the plant to controller communication link. Thus, the plant output measurements have to be quantized prior to transmission. To that end, we borrow ideas from the signal processing literature and employ a feedback quantizer to encode the plant output (see, e.g., [30,50]). Using a fixed signal-to-noise ratio additive noise model for quantization errors, we show how to design the feedback quantizer so as to systematically reduce the impact of quantization on closed loop performance, as measured by the tracking error variance. We show via simulations that our approach gives very good results even for bit rates as low as one bit per sample. We also study stability properties for this linear model. As a byproduct, we obtain a simple characterization of the minimal quantizer signal-to-noise ratio that allows one to design a feedback coding system that guarantees stability. This result is expressed in terms of the plant and controller unstable poles only. For stable controllers, and regardless of the plant zeros or relative degree, our results suggest a minimal data-rate for stabilization that is consistent  with the bound in [40]. 
 The idea of designing coding schemes to embellish given controller designs is not new. For example, our previous work documented in [23] considers a coding scheme that turns out to be a special case of the one considered here. On the other hand, [53] considers the same coding architecture as the one studied in this paper, but the design procedure in [53] assumes that quantization effects are relatively small. The methodology used in the current paper does not require this assumption. Also, the stability analysis included in the current paper goes beyond the results of [23,53]. Another related line of work has been developed in [9,10,34]. The latter work presents a precise deterministic stability analysis when the coding system is constrained to be a ?-modulator (or variations thereof;  see, e.g., [30]), but does not address performance issues. Another recent publication closely related to the current paper is [38]. In that work, the authors propose a coding architecture similar to the one in this paper, but restrict the quantizers to have infinitely many levels and a prespecified quantization step. The latter assumptions are not needed here. Interestingly, the optimal coder in [38] (which focuses on minimizing a time domain functional) turns out to have a structure that is a special case of the architecture considered here. 
 The remainder of this paper is organized as follows: Section 2 presents the notation employed in the paper. Section 3 describes the NCS architecture of interest and derives a linear model that is suitable for analysis and synthesis using linear system theoretical tools. Section 4 studies stability properties of the linear model, while Section 5 presents the proposed design procedure. Section 6 documents a simulation study. Concluding remarks are included in Section 7. 
 We use standard vector space notation for signals, i.e., x denotes {x(k)}k?N0. We also use z as both the argument of the z-transform and as the forward shift operator, where the meaning is clear from the context. Given any matrix X, (X)H and (X)T denote conjugate transposition and transposition, respectively. Given any complex scalar x, |x| and ¯x denotes magnitude and complex conjugation, respectively. 
 The set of all discrete time real rational transfer functions is denoted by R. We define six subsets of R as follows: Rp contains all proper transfer functions, Rsp contains all strictly proper transfer functions, RH8 contains all stable and proper transfer functions, U8 contains all matrices in RH8 that have inverses in RH8, RH2 contains all stable and strictly proper transfer functions, RH?2 contains all transfer functions that have only poles outside the unit circle and are either proper or improper. For any A(z) ? R we define A(z)~ , A(z-1)T. We say that A(z) ? R is unitary if and only if A(z)~A(z) = I. We also define {A(z)}|z=8 , A(8) = limz?8 A(z). 
 Every A(z) ? R with no poles on the unit circle belongs to L2 in which case we define the 2-norm of A(z) via (see, e.g., [39]) 
 For each such A(z), we can always find   and A2(z) ? RH2 such that A(z) = A?(z) + 
 Any biproper n × 1 transfer matrix A(z) ? RH8 admits an inner-outer factorization of the form 
 where Ai(z) ? RH8 is unitary (i.e., Ai(z) is inner) and Ao(z) ? RH8 is a biproper scalar transfer function, that has no zeros in |z| > 1 (i.e., Ao(z) is scalar and outer). Moreover, if A(z) has no zeros on the unit circle, then Ao(z) ? U8 (see, e.g., [20]). 
 Given any wide sense stationary (wss) process x, we denote its power spectral density by Sx(ej?), its variance by sx2 and its standard deviation by sx. We note that if, in addition, x has an always positive rational spectrum, then we can always find a spectral factor ?x(z) ? U8 such that Sx(ej?) = ?x(ej?)?x(ej?)H, ?? ? [-p,p]. We also recall the well known fact that  (see, e.g., [56]). 
 In this paper, we will consider the NCS architecture depicted in Figure 1. In that figure, G(z) is the SISO plant model, C(z) is a SISO controller, y is the plant output, r is the reference signal, do 
 models output disturbances and dm corresponds to measurement noise. Unlike standard non-networked situations (see, e.g, [22,55]), the feedback path in Figure 1 comprises a communication channel and a (source) coding/decoding system (C and D).   The main focus of the current paper lies in designing this coding system, having performance in mind. To that end, we utilize as the performance assessment quantity the stationary variance of the tracking error e, defined via 
 Since our aim is to design coding systems, we will assume that the controller C(z) in Figure 1 has been already designed assuming transparent communication links.  The control loop formed by C(z) and G(z) when transparent communication links are in place (i.e., when ˆym = ym in Figure 1) will be referred to as the nominal loop (or nominal design). 
 The transfer function S(z) is the nominal loop sensitivity function and T(z) is the nominal loop complementary sensitivity function (see [22]). 
 Assumption 1 (Plant and nominal design) The plant model belongs to Rsp, whilst the controller, C(z), belongs to Rp, is non-zero and is such that the nominal loop is stable and well posed (in the standard sense; see, e.g., [22,55]). 
 The assumption that the nominal loop is stable and well defined is, of course, sensible in our context where the coding system is designed a-posteriori. We assume that G(z) is strictly proper for simplicity. In principle, this can be removed at the expense of additional technical care. On the other hand, the assumption of C(z) being non-zero discards non interesting situations, where the nominal loop is such that G(z) is left in open loop (i.e., uncontrolled). 
 Assumption 2 (Signals) The signals r, do and dm are mutually independent scalar zero mean wss processes, each having a rational power spectral density that, if not identically zero, admits a spectral factor in U8. 
 In this paper, we will focus on error free bit-rate limited channels. As a consequence, the input to the channel, i.e., h (see Figure 1), must be quantized prior to transmission. To that end, we will consider a standard feedback quantizer as depicted in Figure 2 (also known as a noise shaping quantizer; see, e.g., [30,50]). In that figure, A(z),B(z) and F(z) are filters in Rp that need to be designed and Q denotes a uniform quantizer (see, e.g., [25,30]), i.e., 
 where V is the quantizer dynamic range, ? , 2V (L - 1)-1 and L is the number quantization levels. 
 We recall that a quantizer is said to be overloaded if and only if the absolute value of its input is greater than its dynamic range, i.e., |v(k)| > V for some k ? N0. If the quantizer does not overload, then the quantization noise, defined via 
 As already mentioned in Section 3.1, we are interested in designing coding systems for pre-specified nominal designs. In this setting, it is natural to employ coding systems that, in the absence of channel artifacts, have unit transfer function. That is, we will utilize coding systems that achieve perfect reconstruction. In our case, the channel is assumed to be error-free and hence hˆ = h. As a consequence, it is straightforward to see from Figure 2 that 
 It follows from (7) that perfect reconstruction is tantamount to having B(z) = A(z)-1 for every z. On the other hand, in order to have a properly defined feedback loop around the quantizer it is necessary to have a strictly proper F(z) (see, e.g., Chapter 4 in [44]). We summarize the previous discussion as follows: 
 Constraint 1 (Structural constraints on the feedback quantizer) The feedback quantizer filters are such that B(z) = A(z)-1 and F(z) ? Rsp. 
 Quantization is a deterministic non-linear operation and hence, the exact analysis of quantized systems is difficult (see, e.g., [16,26,40,47]). It has thus become standard, particularly in the signal processing literature (see, e.g., [5,25,30,36,50,63]), to approximate quantization noise by an additive white noise source uncorrelated with the input of the quantizer. Here, we adopt this paradigm and assume the following: 
 Assumption 3 (Quantization noise model) The quantization noise signal q (defined in (6)) is a sequence of i.i.d. random variables uniformly distributed in  , and uncorrelated with w. 
 Note that we do not assume that the quantization noise is uncorrelated with v, which is certainly not the case since the quantization noise is fed-back to the input of the quantizer and, moreover, the coding system is inside the main feedback control loop. Instead, we adopt a milder assumption that requires only uncorrelatedness with the exogenous signals contained in w. We stress that the previous model is valid only if ? is small enough, the quantizer does not overload and v has a smooth probability density (see, e.g., [4]). These conditions usually do not hold in the case of quantizers that are embedded in feedback loops (see, e.g., the discussion regarding stand alone feedback quantizers in [24]). Nevertheless, one can make use of dithered quantizers (see, e.g., [25,65]) to render the model in Assumption 3 exact provided no overload occurs. Despite the above points, we will see in the simulation study included in Section 6, that, even if one employs a non-dithered uniform quantizer with as few as 2 levels, the predictions made using the simple model summarized in Assumption 3 are surprisingly accurate (see also simulation studies in [17,23,53]). 
 In order to guarantee that the quantizer does not overload, in principle one needs to consider infinite quantization levels (or assume that the quantizer input is deterministically bounded, which is seldom the case in a stochastic framework). In practice, it is standard to choose a dynamic range such that the probability of overload is negligible (see, e.g, [30]). Indeed, if v is wss and ß is any positive real, then one can always find a finite a such that choosing V = asv guarantees that the probability of overload is less than ß; a is called the quantizer loading factor.  With such a choice for the overloading factor, it is immediate to see that 
 where we have used the fact that, according to Assumption 3, . This justifies the following additional assumption: 
 Assumption 4 (Fixed signal-to-noise ratio) For a fixed number of quantization levels, the variance of the quantization noise is proportional to the variance of the signal being quantized, i.e., the quantizer has a fixed signal-to-noise ratio ? defined via 
 Assumption 4 is a key constraint. As mentioned before, it allows one to guarantee that the quantizer dynamic range is always properly scaled. In addition, it has a regularizing effect on the optimization based design of the coding system. Indeed, if this constraint were not in place (i.e., if q were assumed to have some prescribed statistics), then it would be optimal to choose F(z) = 0 and A(z)-1 = ² with ² ? 0. This is, of course, not a sensible choice since A(z) and sv2 grow unbounded when ² ? 0. 
 Remark 1 We would like to stress that, in some situations, quantizer overload may become the dominant quantization effect in feedback schemes. Indeed, quantizer overload may trigger limit cycle oscillations that are, of course, not predicted by the linear model for quantization introduced above (see, e.g., [19,44,47]). As implied by Assumptions 3 and 4, we assume in this paper that quantizer overload is infrequent enough and, accordingly, that it has no significative effect on overall closed loop performance. (Careful design of the quantizer loading factor may act as a safeguard against quantizer overload.) 
 Considering the model for quantization described above, together with the nominal loop description in Section 3.1, it is easy to derive the linear model shown in Figure 3 for the considered NCS. (Note that we have made the perfect reconstruction constraint explicit.) In Figure 3, q satisfies Assumptions 3 and 4, and r,do,dm satisfy Assumption 2. We will refer to this model as the linear model. It will be the basis of the remainder of this paper. 
 In this section we study stability properties of the linear model for the considered NCS derived in Section 3. In particular, we characterize all filters F(z) and A(z) that lead to stable linear models (in an appropriate sense) for a given quantizer signal-to-noise ratio ?. As a byproduct, we characterize the minimal quantizer signal to noise ratio ? that allows one to find F(z) and A(z) such that the resulting linear model is stable. 
 We begin by noting that, if x is a n-dimensional vector that contains the states of C(z), G(z), A(z), A(z)-1 and F(z) (see Figure 3), then the evolution of x can be described by a linear state space model: 
 where A,Bw,Bq,Cv,Dwv and Dqv are matrices of appropriate dimensions that depend on the particular realizations of C(z),G(z),A(z),A(z)-1 and F(z). Next, since we are considering a stochastic system, we need an appropriate notion of stability: 
 Definition 1 (Mean Square Stability [12,13,31]) The linear system in (10) is Mean Square Stable (MSS ) if and only if there exist a finite µx ? Rn and a finite Rx ? Rn×n, Rx = 0,  both not dependent on the initial state xo, such that 
 Theorem 1 (Conditions for Mean Square Stability) If Assumptions 1-4 hold, and xo is an independent random variable with finite mean and finite variance matrix, then the linear model in Figure 3 is MSS if and only if A(z) ? U8, F(z) ? RH2 and 
 Proof: Define Rw as the variance matrix of w. Since the spectral factor of w, ?w(z), belongs to RH8, we lose no generality if we restrict attention to the case where ?w(z)?w(z)~ = Rw = 0, for every z (i.e., if we assume that w is white noise). The general case employs the same arguments, but requires an augmented description of the system that has additional stable modes. 
 Consider the state space description of the system under study given by (10). Standard results allow one to conclude (see, e.g., Chapter 4 in [56]) that under our working assumptions 
 where µv(k) and Rv(k,k) are defined as µx(k) and Rx(k,k), but considering v instead of x. 
 • (?) If the NCS is MSS, then both µx and Rx = 0 are finite and unique. Therefore, (13) implies that A must be stable. On the other hand, we also see from (17) that limk?8 Rv(k,k), i.e., the stationary variance of v, say sv2, must be positive semi-definite, finite and unique. 
 Since the nominal loop is stable, a simple calculation shows that A being stable implies that both A(z) and A(z)-1 must be stable, and moreover, that F(z) is stable. Of course, both A(z) and A(z)-1 must be proper and, on the other hand, F(z) is constrained to be strictly proper (recall Constraint 1). Therefore, it follows that A(z) ? U8 and F(z) ? RH2. 
 If A is stable, then it is easy to see that the stationary variance of v satisfies (see also Section 
 where Twym(z) is defined in (30), and where we have used the fact that w is uncorrelated with q and is such that ?w(z) = Rw. Using the definition of ? in (18) yields 
 Therefore, we conclude that, provided A is stable,   being positive semi-definite, finite and unique is equivalent to (12). 
 • (?) Since the nominal loop is stable, F(z) ? RH2 and A(z) ? U8, we have that A is stable. Therefore, it follows from (13) that µx is finite, unique and well defined. 
 If (12) holds, then the facts deduced when proving the sufficiency part of this theorem imply that sv2 is positive semi-definite, unique and finite. Using the definition of ?, it then follows that sq2 is positive semi-definite, finite and unique. Therefore, Rwq is positive semi-definite, unique and finite and, since A is guaranteed to be stable (see above), then we have that the Lyapunov equation that describes the limiting value of Rx(k,k) in (14) admits a finite, unique and positive semi-definite solution (see, e.g., Section 21.1 in [67]). Therefore, we have proven that Rx is as required. This completes the proof. 
 The condition for MSS given in Theorem 1 is deceivingly simple. This is due to the fact that the nominal loop is assumed stable and we are focusing on coding systems that achieve perfect reconstruction (recall Section 3.2). It is relevant to note that (12) does not depend on A(z). Therefore, one can easily characterize the greatest lower bound on ? that allows one to guarantee MSS: 
 Theorem 2 (Minimal signal-to-noise ratio for MSS) If Assumptions 1-4 hold, and xo is an independent random variable with finite mean and finite variance matrix, then there exist filters A(z) and F(z) that allow one to guarantee MSS if and only if 
 Proof: It suffices to compute inf  (see Theorem 1). To that end, we employ the techniques described in detail in, e.g., [8,39,64]. We first note that F(z) ? RH2 ? Q(z) , zF(z) ? RH8. Define 
 where {pi}i?{1,···,n+p } denotes the set of non-minimum phase zeros of S(z) that lie strictly outside the unit circle (i.e., the unstable poles of G(z)C(z) outside the unit circle). It is clear that , is unitary, and is such that the transfer function ?S(z)S(z) belongs to RH8, is biproper and has as non-minimum phase zeros the zeros on the unit circle of S(z) (i.e., the poles on the unit circle of 
 = ||?S(z) - ?S(0)||22 + ||z?S(0) - z?S(z)S(z) + ?S(z)S(z)Q(z)||22 = ||?S(z) - ?S(0)||22 + ||?S(0) - ?S(8)||22 + 
 where we have used orthogonal decompositions in L2, the fact that both ?S(z) and z are unitary, the fact that Assumption 1 implies S(8) = 1, and basic properties of the 2-norm. By construction, z (?S(z)S(z) - ?S(8)) ? RH8 and ?S(z)S(z) is invertible in RH8 except for zeros on the unit circle. Elementary results (see, e.g., Chapter 6 in [64]) allow one to conclude from (22) that 
 Use of the Residue Theorem and some simple algebra yields the desired result.	¤¤¤ 
 Theorem 2 states a precise condition that the quantizer signal-to-noise ratio ? has to satisfy in order to be able to find a coding system that, when inserted in the feedback path of a stable nominal loop, guarantees the MSS of the resulting linear model. The bound on ? depends only on the unstable poles of G(z)C(z), i.e., on the unstable poles of the plant and controller. If the plant model is strongly stabilizable (i.e., can be stabilized using a stable controller; see, e.g., [18]), then employing a stable controller in the nominal loop allows one to find a feedback coder capable of stabilizing the resulting linear model if and only if 
 where {pGi}i?{1,···,nG} denotes the set of unstable poles of G(z). We note that the same conclusion applies if the controller is stable except for poles on the unit circle (e.g., controllers with integral action). 
 If we fix F(z) = 0, then ? must satisfy , which is a fixed constraint in our framework. If it were possible to redesign the controller under the constraint F(z) = 0, then one can use the results in [8] to establish that the admissible signal-to-noise ratio must satisfy 
 where ?G is non-negative and depends on the non-minimum phase zeros and on the relative degree of the plant model G(z) (?G = 0 if and only if G(z) is minimum phase and has relative degree equal to one). We thus conclude that the inclusion of the proposed coding system allows one to reduce the requirements on the quantizer signal-to-noise ratio, at least for strong stabilizable plants (regardless of the plant zeros or relative degree). This reduction may be very significative if, e.g., the plant has high relative degree. This is an important indication of the benefits that coding brings to networked control situations. A question that remains open, however, is whether or not there exist different coding architectures that allow one to recover (24) for any plant. 
 Remark 2 (Relationship to prior work) In [8] it is proved that (25) is the minimal signal-to-noise ratio that allows one to find one-degree-of-freedom controllers that stabilize a given LTI plant model over an additive noise channel with a power constraint. In a second step, the authors show that a Gaussian memoryless channel, with a signal-to-noise ratio ? that satisfies (25), would have a capacity Cap (see, e.g., [14]) that satisfies 
 where Rinf is the minimal data-rate which is necessary and sufficient to stabilize an LTI system over an error-free bit-rate limited channel [40]. Equality in (26) is achieved if and only if the plant is minimum phase and has a relative degree equal to one. These results suggest that signal-to-noise ratio requirements in LTI one degree-of-freedom control loops are, for a restricted class of plants, consistent with the minimal data-rate requirements of [40]. If the plant has non-minimum phase zeros, or has a relative degree larger than one, then ?G > 0 (see (25)). It thus follows that, in these situations, data-rate requirements suggested by signal-to-noise ratio considerations may be more demanding than those in [40]. 
 Our results can be applied to the channel model in [8] as well, provided error free feedback (with a unit delay) is available from the channel output to the channel input.  When doing so, it turns out that (24) is consistent (in the sense described above) with the minimal data-rate derived in [40]. Our results holds even if the plant model has arbitrary relative degree and arbitrary zeros, as long as G(z) is strongly stabilizable. We thus conclude that, within the LTI framework, the use of feedback coding is key to achieve (24) and, accordingly, key to make signal-to-noise ratio requirements consistent with the results in [40]. We stress that the issue of existence of feedback from the channel output to the channel input is inconsequential to the set-up used in [40] because the channel is error free, as in our case. (Note that the assumption of channel feedback has been explicitly made for NCS’s with stochastic channels (see, e.g., [3,59,61]) again recovering the results in [40]. If channel feedback is removed from the analysis of [59,61] then the minimal data-rates for stabilization obtained do not necessarily coincide with those in [40] (see Section VI in [60])). 
 In this section we go beyond stability and focus on how to actually design a feedback coding system that minimizes the impact that the communication channel has on closed loop performance, as measured by the steady state variance of the tracking error. 
 The purpose of this section is to define the performance goals of interest in a precise way. To that end, we consider the linear model in Figure 3. Straightforward analysis reveals that the tracking error obeys 
 Therefore, if the linear model is MSS and Assumptions 2 and 3 hold, then the stationary variance of e exists and is given by 
 where ?w(z) is a spectral factor of the power spectral density of w. Since Assumption 4 holds, sq2 is not a given constant; indeed, it depends on the variance of v. Proceeding as above (and using the same assumptions), it follows from Figure 3 that 
 We note that, since C(z) is assumed to be given, the choice of the coding parameters (i.e., A(z) and F(z)) affects only the second term in (33), which we denote as 
 Problem 1 (Main problem) Given a fixed ? ? (?inf,8), a controller C(z) and a plant G(z) that satisfy Assumption 1, and exogenous signals satisfying Assumption 2, find Jopt defined via 
 and filters A(z) and F(z) that achieve Jopt (or approximate Jopt arbitrarily well). 
 We note that all constraints in the formulation of Problem 1 stem from MSS considerations, as discussed in Theorems 1 and 2. We will use the term admissible A(z) (resp. admissible F(z)) to refer to a filter A(z) (resp. F(z)) that satisfies the constraints in Problem 1. 
 Problem 1 is non-trivial. Indeed, the much simpler problem of designing A(z) and F(z) so as to minimize the steady state variance of ˆym - ym, when G(z) = C(z) = 0 and dm = r = 0 has been only recently solved exactly (see [17]). This is quite surprising given the fact that feedback quantizers have been studied extensively (see, e.g., [30,50,57]). Unfortunately, the technique employed in [17] does not seem to yield an explicit characterization of the solution in the present situation. Instead of pursuing that line of reasoning here, we will derive an iterative approach that is guaranteed to yield performance that is arbitrarily close to optimum. 
 Before describing the proposed design procedure, we note that the following holds: 
 Fact 1 (Asymptotic behavior of Jopt) Assume that the conditions of Problem 1 hold. Then: 
 1.	If ? ? ?inf, then Jopt ? 8 (unless all exogenous signals have zero spectral density, in which case J(A(z)F(z)) = 0 for every admissible A(z) and F(z)). 
 1.	By definition of Jopt, we have that . Thus Jopt ? 8 unless either	 = 0. Since A(z) ? U8 and 
 2.	Fix A(z) ? U8 and F(z) ? RH2. In these conditions, ? ? 8 ? J ? 0 and hence, Jopt ? 0. 
 As a consequence of Fact 1, we will omit from our subsequent presentation an explicit analysis of the cases ? ? 8 or ? ? ?inf. (The reader can easily verify that the results below are consistent with Fact 1 by letting ? ? 8 or ? ? ?inf.) 
 We begin by showing how to choose A(z), when an admissible F(z) is given. To that end we define, for any given admissible F(z), 
 Theorem 3 (Optimal A(z) for a given F(z)) Assume that the conditions of Problem 1 hold and consider a fixed admissible F(z). If ?w(z) is not identically zero, then: 
 Proof: The definition of the 2-norm allows one to conclude that, for every X(z) ? R n L2, the following identities hold: 
 Both (38) and (39) follow using (40) and the Cauchy Schwartz inequality in (34) (note that (39) is always well defined if Assumption 1 holds, and ?w(z) is not identically zero). To complete the proof we note that (39) is a condition on the magnitude of the infimal filter A(z). Thus,  can always be approximated, to any desired degree of accuracy, by a rational filter in U8 as required. ¤¤¤ 
 Remark 3 Of course, assuming that ?w(z) is not identically zero does not hinder the generality of Theorem 3 (see Part 1 in Fact 1). 
 The characterization of ) given by Theorem 3, although explicit, is usually not satisfied by any transfer function in U8. This is due to the fact that, except in very special cases, the 4th root of the right hand side in (39) is irrational. Nevertheless, as mentioned in the proof of Theorem 3, it is always possible to find a filter in U8 that achieves a performance that is as close as desired to 
 Jopt1 (F(z)).  In practice, it is usually enough to consider reasonably low order filters to approximate  ) (see also [23]). 
 In this section we address the problem of choosing F(z) when an admissible A(z) is given. Consistent with the notation introduced before, 
 We begin by noting that Jopt2 (A(z)) can be written in a simpler form as follows: 
 Fact 2 (Equivalent formulation for Jopt2 (A(z))) Assume that the conditions of Problem 1 hold and consider a fixed A(z) ? U8. Then, 
 Proof: Using the definition of J and the fact that A(z) is fixed, it is immediate to see that 
 Define a new real variable, M, constrained to belong to [?inf,?). With this definition, elementary optimization results (see, e.g., Section 4.1.3 in [6]) allow one to write (45) as 
 where we have used the fact that, by definition of ?inf, J2(F(z)) = ?inf for any F(z) ? RH2. The result is now immediate.	¤¤¤ 
 Fact 2 is key to derive the main result in this section. Namely, a one parameter characterization for Jopt2 (A(z)) and the corresponding optimal F(z). Towards that goal, we begin by considering an auxiliary problem. Define the functional 
 Lemma 1 (Solution to auxiliary problem) Consider L² defined in (47) and suppose that Assumption 1 holds. 
 where m is the relative degree of G(z)C(z), {ci}i?{1,···,n+c } (resp. {pi}i?{1,···,n+p }) is the set of non-minimum phase zeros (resp. unstable poles) of G(z)C(z) that lie strictly outside the unit circle. 
 3.	If ² = 0 (resp. ² = 1), then the infimum in (48) is achievable in RH2, if and only if G(z)C(z) has no poles (resp. zeros) on the unit circle. 
 1. We will proceed as in the proof of Theorem 2 (see also [11]). As before, we define Q(z) ? RH8 via F(z) , z-1Q(z). It is easy to see from (22) that 
 Moreover, using the same procedure as in the aforementioned proof, it is also clear that 
 ¯¯¯¯T(z)A(z)-1 - T(z)A(z)-1z-1Q(z)¯¯¯¯22	¯¯¯¯z?T(z)T(z)A(z)-1 - z?T(z)T(z)A(z)-1Q(z)¯¯¯¯22 
 ¯¯¯¯z ¡? (z)T(z)A(z)-1 - ©?T(z)T(z)A(z)-1ª¯¯z=8¢ - ?T(z)T(z)A(z)-1Q(z)¯¯¯¯22 , (55) T where we have used the fact that the relative degree and non-minimum phase zeros of T(z) are the relative degree and non-minimum phase zeros of G(z)C(z), that ?T(z) and z are unitary, and that, since A(z) ? U8, ?T(z) is such that ?T(z)T(z)A(z)-1 belongs to RH8, is biproper and has as non minimum phase zeros the zeros on the unit circle of T(z) (i.e., the zeros on the unit circle of G(z)C(z)). From (54) and (55) it follows that 
 where P²,i(z) is an inner factor of P²(z) and P²,o(z) is the corresponding outer factor. We note that, since Assumption 1 holds, P²(z) has no zeros on the unit circle for ² ? (0,1). Thus, for those values of ², P²,o(z) ? U8. 
 Therefore, orthogonal decompositions as those employed before allow one to write 
 3.	The result follows upon noting that, by definition of ?T(z) and ?S(z), (?S(z)S(z))-1 (resp. (?T(z)T(z)A(z)-1)-1) belongs to RH8 if and only if G(z)C(z) has no poles on the unit circle (resp. zeros on the unit circle). 
 The characterization of F²(z) given in Lemma 1 plays an essential role in our subsequent discussion. It is worth mentioning that the only critical step when calculating F²(z) is the inner-outer factorization of P²(z). Since P²(z) has no zeros at infinity (i.e., P²(z) is biproper), this factorization can be made with the aid of standard algorithms (see, e.g., [20,46]). 
 The next theorem provides a characterization of the optimal F(z) in terms of F²(z). 
 Theorem 4 (Optimal F(z) for a fixed A(z)) Assume that the conditions of Problem 1 hold and consider a fixed A(z) ? U8. Then, 
 In (65), ²ˆ is defined as follows: If there does not exist ² ? (0,1] such that J2(F²?(z)) = ?, then ²ˆ= 1. Otherwise, ²ˆ= ²?, where ²? is the unique real in (0,1] such that J2(F²?(z)) = ?. 
 1. We first show how to solve an auxiliary problem related to the inner optimization problem in 
 The well-known KKT conditions for this problem (see, e.g., [6,35]) allow one to conclude that the optimal F(z), say Faux(z), (if it exists) is a critical point of ?1J1(F(z))+?2J2(F(z)), where ?1 + ?2 > 0, ?1,?2 = 0 and, moreover, ?2(J2(Faux(z)) - M) = 0. It is immediate to see that this is equivalent to saying that Faux(z) is a critical point of L² (see (47)), with ² ? [0,1] and (1 - ²)(J2(Faux(z)) - M) = 0. 
 L² is a strictly convex functional (and so are J1 and J2). Hence, it has an unique critical point given by F²(z) (see (48)). Moreover, the set of points in the J1 versus J2 plane defined by F²(z), when ² ranges from zero to one, is the set of Pareto optimal points of the multi-objective problem of minimizing simultaneously J1 and J2 (see, e.g., [6,15]). This set is a (strictly) convex and decreasing function when J1 is seen as function of J2. Clearly, the Pareto optimal point corresponding to ² = 1 (resp. ² = 0) is such that J1 is minimum (resp. J2 is minimum). Thus, by definition of Pareto optimal point, the minimum of J1, when J2 = M is achieved when J2 = M, provided J2(F1(z)) = M. If J2(F1(z)) < M, then the minimum J1 is achieved when J2 = J2(F1(z)). As a consequence, the optimal solution of the auxiliary problem is given by Faux(z) = F²M(z), where, provided J2(F1(z)) = M, ²M belongs to [0,1] and is such that J2(F²M(z)) = M (note that convexity ensures that, in this case, ²M is unique). On the other hand, if J2(F1(z)) < M, then ²M = 1. 
 2. We next show how to exploit the above reasoning to prove the result. We first note that, since F1(z) optimizes J1, it is of no use to consider values of M such that J2(F1(z)) < M (note also that M ? ?inf is optimal for the first optimization problem in (43) and, accordingly, constraining J2(F1(z)) to be greater than or equal to M does not impede the minimization of J). Thus, Fopt(z) satisfies 
 where ²M is guaranteed to exist in [0,1] (and to be unique). A key feature of this problem is that the Pareto optimal points of the auxiliary problem considered in Part 1 do not depend on M. Thus, varying M in [?inf,min{?,J2(F1(z))}] is equivalent to just varying ²M in [0,min{1,²?}] (if ²? [defined in the body of this Theorem] does not exist, then pick ²? = 1). It should be clear that the structure of the problem is such that M* ? (?inf,min{?,J2(F1(z))}). Thus, it suffices to consider ²M ? (0,min{1,²?}). As a consequence, the result follows. 
 Theorem 4 provides a one parameter characterization of the optimal F(z) and the corresponding minimal cost Jopt2 (A(z)), for any admissible A(z). The scalar parameter ²* can be found using any standard line search procedure and, as such, its calculation embodies no additional difficulties. This is reinforced by the fact that the search for ²* is made over (0,1) (and that ²* actually exists in (0,1)) which is precisely the range of values of ² for which F²(z) is always defined in RH2 (see Lemma 1). 
 In this section we show how to use the results in Sections 5.2 and 5.3 to design a feedback coding system in an iterative fashion. Of course, one can always choose to fix one of the coder filters (trivial choices are A(z) = 1 or F(z) = 0) and then use Theorem 3 or 4 to design the free parameter. Obviously, this choice will limit the achievable performance. To exploit the full potential of a feedback coding system we suggest that one uses an iterative algorithm such as the following: 
 Algorithm 1 (Iterative design procedure) For a given plant and controller satisfying Assumption 1, and a given ? > ?inf, proceed as follows: 
 –	Pick a tolerance ? > 0, a transfer function A0(z) ? U8 and a transfer function F0(z) ? RH2 that is admissible. 
 –	Set A(z) = A0(z), F(z) = F0(z). Set V (0) = J(A(z),F(z)), fix A(z) (or, alternatively, fix F(z)) and set k = 0. 
 –	If at the (k - 1)th iteration A(z) was fixed, then use Theorem 4 to obtain . Set   and V (k) = J(A(z),F(z)). Fix F(z). 
 –	If at the (k - 1)th iteration F(z) was fixed, then use Theorem 3 to obtain  . Set   and V (k) = J(A(z),F(z)). Fix A(z). 
 It should be clear that it is not certain that Algorithm 1 will converge to the global minimum of J. Nevertheless, it is easy to see that, by definition of  and , the algorithm reduces the value of J at each iteration. Therefore, Algorithm 1 converges, necessarily, to a local minimum. Thus, we suggest to use multiple starting points so as to find the global minimum. A procedure for getting a good starting point is mentioned below. 
 In general,  = 1 and  = 0. Thus, fixing A(z) or F(z) and optimally choosing the other filter, will obviously provide a coding system that enhances closed loop performance when compared with a non-coded networked situation.  It is also clear that the use of Algorithm 1 allows one to design coding systems that will always outperform coding systems that have been designed using the guidelines in our earlier work described in [23]. This is a consequence of the fact that [23] constrains F(z) to be identically zero. 
 A more interesting discussion arises if one compares the results in this paper with the results in [53]. 
 In the latter work, it is assumed that ? is sufficiently high so as to be able to approximate J in (34) by 
 a second stage, to choose F(z) as the minimizer of [53]. A problem with the above approach is that deciding, a priori, which ?’s are high enough seems to be impossible. In particular, since the procedure in [53] does not take the constraint  explicitly into account, the proposed choice for F(z) may be not admissible or may be such that J(A(z),F(z)) ? 8. (Needless to say, this drawback is explicitly avoided in the current paper.) It is also clear that choosing A(z) as in [53] and, then, using Theorem 4 to choose F(z) will always lead to a feedback coder that achieves a tracking error variance that is lower than the one achieved by the filters proposed in [53]. Of course, if the filters suggested in [53] are feasible, then they may provide a good starting point for Algorithm 1. 
 This section documents a design study that illustrates the results in this paper. We consider a very simple case that, nevertheless, will allow us to present the main features of our proposal. 
 The measurement noise and output disturbance are assumed zero, whilst the reference is considered to have a power spectral density with spectral factor 
 The quantizer loading factor is fixed at 4 in all cases,  and the number of quantization levels, L , 2b, ranges between L = 21 and L = 28. Since we do not consider the use of channel coding schemes (e.g., entropy coding; see [9,14]), b corresponds to the rate at which data is sent through the channel (in 
 Figure 4 shows the steady state tracking error variance se2 (see (33)) as a function of the number of iterations in Algorithm 1 for two representative values of the quantizer signal-to-noise ratio: ? = 1.6875 and ? = 9.1875, which correspond to b = 2 and b = 3, respectively. Cases 1 and 2 refer to iterations that start with A0(z) = 1 and F0(z) = 0. In Case 1 we initially fixed A(z), whereas in Case 2 we start fixing F(z). Case 3 refers to iterations that start with the choices suggested in [53]. We note that ? has to be greater than 4.42 in order for the proposal in [53] to be admissible. (Accordingly, we omitted 
 Figure 4: Tracking error as function of the number of iterations in Algorithm 1 (see text for details). 
 Case 3 in Figure 4 when ? = 1.6875.) It can be seen that rapid convergence of Algorithm 1 occurs and, more interestingly, that the limiting performance does not depend on the order in which the filters are calculated or on the initial condition. Thus, local minima related issues do not seem to play a role in this example. 
 In Figure 4 we have identified three points. The first of these (point (1)) refers to the performance achieved without coding (F(z) = 0 and A(z) = 1). The second (point (2)) refers to the performance achieved when employing the optimal coding system proposed in [23]. The third (point (3)) refers to the performance achieved using the approximately optimal filters described in [53]. 
 The results show that coding is, indeed, necessary to achieve the best possible loop performance. (Compare point (1) with, e.g., the value of se2 for 10 iterations.) It is also possible to see that use of Algorithm 1 yields coding systems that perform better than our previous proposals in [23,53], which is consistent with the discussion at the end of Section 5.4. (Compare points (2) and (3) with the limiting value for se2.) It is also interesting to mention that, for b > 4, the performance provided by the filters in [53] is substantially closer to the limiting value of se2 than the case shown in Figure 4. This suggest, as mentioned before, that the filters in [53], when feasible, provide good starting points for the iterative procedure proposed here. 
 We end this section by studying the behavior of the tracking error variance as a function of the channel bit rate b. The results are presented in Figure 5, where “Nominal performance” refers to the performance achieved by the nominal loop (without quantization), “No coding (empirical)” refers to simulated results  when no coding is employed (i.e., when A(z) = 1 and F(z) = 0), “Opt. coding (empirical)” refers to simulated results obtained with the filters suggested by Algorithm 1 (after 10 iterations), and “Opt. coding (analytical)” refers to the corresponding predictions made using the simplified noise model for quantization. One can see that, as expected, the effects of quantization vanish as b ? 8. Interestingly, the predictions made using our model turn out to be very accurate for every bit rate: indeed, for b = 3 the relative errors are of less than 1% and, for b ? {1,2}, the relative errors are around 8%. (We note that F(z) = 0 turns out to be non-admissible for b = 1. Accordingly, 
 This paper has presented a methodology to design feedback quantizers that encode plant output measurements in a networked control situation employing data-rate limited channels. Using a fixed signal-to-noise ratio additive noise model for quantization, we have shown how to iteratively design the parameters of a feedback coding system so as to minimize the impact of quantization on the closed loop tracking error. Our results show that feedback quantization schemes are beneficial when compared to simpler schemes documented in the literature. An interesting by-product of our results lies in the characterization of the smallest quantizer signal-to-noise ratio compatible with stabilization. We have shown that, for a given quantizer signal-to-noise ratio, the class of plants that are stabilizable when feedback coding is employed is significatively larger than the class of plants that are stabilizable when no coding is used. This result opens the door to investigating other LTI control and feedback coding architectures and the associated signal-to-noise ratio requirements. 
 A very interesting extension of the present work lies in addressing multiple-input multiple-output problems. In that case, it is worth exploring how networked architectures may help overcoming the well-known performance limitations that arise when constraining the structure of the controller (see, e.g., [29,52]). A second immediate extension lies in the problem of joint controller and coder design. The study of how to apply similar ideas to the case of channels prone to data loss is also interesting (see preliminary work in [54]). 
 Consider a set X and a function J defined on X ? Xˆ (and extensible to Xˆ). If infX?X J(X) exists and infX?X J(X) is achievable in X, i.e., if ? X* ? X such that J(X*) = infX?X J(X), then Xopt , arginfX?X J(X) = X*. On the contrary, if @ X* ? X such that J(X*) = infX?X J(X), then Xopt defined as above should be understood as Xopt = limn?8 Xn, where {Xn}n?N is a sequence in X (whose limit belongs to Xˆ) such that limn?8 J(Xn) ? infX?X J(X). Therefore, if we write Xopt = Xˆ and Xˆ 6? X, it is implicit that one can find a sequence {Xn}n?N as above. In these cases, it is clear that one can always pick an X ? X such that J(X) is as close to infX?X J(X) as desired. 
 ﻿We present an empirically based model for the gain of an indoor antenna array. This corresponds to the reparametrization of the Greenstein-Erceg model, as applied to 5 m–50 m narrowband outdoor-indoor links. Our model is applicable to a few adjacent tones in an OFDM femtocell system such as LTE. We find that as much as 14 out of 16 dB of antenna-gain are attainable even in rich indoor scattering conditions. At the same time, no larger fade margin is required when using the array vs. the omnidirectional antenna. To provide a reference for the observed results we consider a simple propagation model, which is analyzed theoretically and via simulation. This model is found to match our empirical results very well.
 Index Terms—Channel models, directive antennas, gain reduction factor, outdoor-indoor wireless links.
 I. INTRODUCTION
 D
 IRECTIVE antennas have been used to improve received signal power since the beginning of radiocommunications. However the specification of an antenna’s effectiveness in aspects such as improvement of signal strength and reduction of interference is typically specified for propagation settings devoid of obstructing or reflecting elements, i.e., under “free-space” conditions. The more recent appearance of cellular wireless service, results in propagation channels where the presence of obstacles is the rule rather than the exception.
 Relatively little has been published on the interaction between multipath propagation and the antenna gain characteristics. The decrement in achievable antenna gain due to local scattering elements was originally defined in [1] as the “Gain Reduction Factor” (GRF). This factor was characterized as a location dependent random variable and its statistical parameters were computed for a specific environment, namely suburban outdoor-to-outdoor links. The very fast growth in wireless traffic in urban environments has forced base-coverage ranges to those of femtocells [2], [3], for which the aforementioned results may no longer hold.
 While short-range outdoor-to-indoor links for static or nomadic users have been discussed [4]–[8], the effect of directional antennas on those links has received very little attention. In fact, most reported studies have been based on the use of omnidirectional antennas [4]–[6]. Moreover as stated in [9], path-loss prediction errors are much greater when using directional instead of omnidirectional antennas.
 Recent developments in antenna design including the use of metamaterials have made it practical to consider the application of steered arrays in relatively small devices, such as those used in wireless local area networks (WLANs) [10]–[13]. Such arrays will allow taking advantage of propagation conditions where the dominant arriving rays lie in an angular range that is much narrower than the array mainlobe [1]. To the best of our knowledge, there is no general model available to describe angular spread when indoor wireless service is provided from outdoor bases. The actual gains achievable with directional antennas in this setting therefore requires an empirical study similar to what was reported in [1] for suburban links. This is the main focus of the work we describe here. We provide the first statistical description of the GRF for this type of environment.
 The “Mean Effective Gain” (MEG), was proposed in [14] as a useful single parameter to describe the impact of the antenna characteristics on the link budget for multipath environments, primarily for mobile links. This is defined as the ratio between the (spatial) average power received by the antenna and the sum of the average powers that would have been received in the same environment by two isotropic antennas, vertically and horizontally polarized. A theoretical method is described in [14] to analyze the MEG of a mobile antenna. A general expression based on a statistical model of incident waves is proposed and empirical data was obtained at 900 MHz to validate the model. A statistical model for incident waves is also discussed [15], where model-derived and measured values of MEG are compared for LOS street-links at 2.6 GHz. An analysis of some fundamental properties of the MEG and the corresponding physical interpretations is presented in [16] for theoretical Rice/Rayleigh channel models. In contrast to the GRF, described as a random variable, the MEG is an average value.
 1536-1276 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
 An approach based on modeling deviations from the freespace gain pattern of an antenna as a result of multipath propagation is discussed in [17]–[19]. This is defined as the Effective Directivity Antenna Model (EDAM), which provides a bin-fitted stochastic model to correct the nominal antenna pattern. The empirically based model described in the above references was obtained for specific directive antennas in shortrange outdoor-outdoor and indoor-indoor links, using 802.11 transmitters and receivers. Simulations are carried out to show that the model provides more accurate path-loss prediction than the standards models when directive antennas are used. The results however do not focus on the reduction of available gain for beam-steered indoor antennas when users are served from outdoor bases, as we discuss here.
 The GRF originally defined in [1], was subsequently discussed in [19]–[21] but the corresponding statistical model is based on wideband empirical data collected only in suburban links [21], [22]. The approach is based on estimating the angleof-arrival-distribution of power density which differs from our methodology, as will be discussed in the next section.
 Short-range outdoor-indoor wireless channels are multipathrich links for which there is still a lack of empirical results [4], [6]. Several papers such as [4]–[8] and the references quoted therein have addressed the subject of path-loss characterization for settings that in some cases are similar to ours. However, links with directional antennas are only considered in [7] and in [8]. A comparison of maximum received power using directive and dipole antennas is presented in [7], but this work does not include a statistical characterization of the gain-difference between both antenna options. In [8], simulations and measurements of outdoor-indoor links that include directive antennas are presented, but this work does not include a comparison of received power for different antenna types.
 The main objective of this paper is to empirically evaluate the effectiveness of using steerable directional antennas at the indoor user terminal in typical short-range, fixed-wireless, outdoor-indoor service. We propose a statistical model for the GRF of narrowband links, applicable for instance to a group of tones within the coherence bandwidth in OFDM transmission. These results are important since antenna gain may compensate for the propagation loss disadvantage of indoor-outdoor (as compared to indoor-indoor) links, opening up a much wider application space for cellular service providers. The measurements were carried out at 3.5 GHz using a narrowband transmitter and a carefully calibrated power-measuring receiver. The frequency used is the same as in the WiMAX fixed wireless service [25]. A total of 179 different links were tested in the urban settings described in Section III. The links included changes in the relative position of outdoor and indoor terminals and variation in path lengths, ranging from 5 to 50 m.
 The remainder of this paper is organized as follows: Section II presents the theoretical background. Section III describes the measurement hardware and methodology. Section IV presents statistical models based on our empirical data. Finally, Section V provides the conclusions.
 II. CONCEPTUAL BACKGROUND
 For convenience we assume the outdoor base to be the transmitter and the indoor user the receiver. The receiver is connected to a directional antenna, capable of searching in azimuth for maximum power. The received power at the antenna terminals in a wireless link with properly aimed antennas can be expressed as [26]
 PtGtGr
 	Pr =	 ,	(1)
 Lf PL
 where Pt is the transmit power, Gt and Gr are the transmit and receive boresight antenna-gains in free space, Lf accounts for feeder cable losses between the antennas, the transmitter and the receiver and PL is the path-loss associated with the propagation environment. Expression (1) holds if the angular spread of the arriving wavefronts is much smaller than the antenna mainlobe [1], as may be the case for a base antenna placed above clutter. For an indoor antenna surrounded by clutter this directivity gain will in general no longer hold. While the antenna can be aimed in the direction of the strongest arrival, its radiation pattern will attenuate wavefronts arriving from other directions, which may be captured by a wider-beam antenna, in particular an omnidirectional element. Further, as in [1], we define the GRF as the decrease in gain advantage of a directive over an omnidirectional antenna, due to the presence of clutter. Let G0 dir and G0 omni be the nominal (free-space) antenna gains of a directive and an omnidirectional antenna respectively. We denote as Gc the actual in-clutter gain advantage of the directive antenna when aimed for maximum received power. Then,
 	Gc[dB] = G0 dir[dB]-G0 omni[dB]-GRF[dB]	(2)
 where Gc and GRF are location dependent random variables.
 To estimate GRF for a given link, we measure the received power using an omnidirectional (in azimuth) antenna. Let this power be Pr omni. For exactly the same antenna positions and transmit power, we repeat the measurement with the directive antenna, rotating it in search of the maximum power Pr dir. The dB difference in received power Pr dir[dBm]-Pr omni[dBm] is then the in-clutter gain advantage of the directive antenna, Gc[dB], at that spatial position.
 Then from (2) we can write the GRF as:
 GRF[dB] = (G0 dir[dB]-G0 omni[dB])
 	- (Pr dir[dBm]-Pr omni[dBm]).	(3)
 Since Pr dir[dBm] and Pr omni[dBm] are the position dependent random powers received by the antennas, the GRF is described by the difference between the spatial-fades of a directive and an omnidirectional antenna, both placed at the same position, adjusted by the difference in free-space gains.
 To establish a basis that can serve as a reference for comparison with our empirical results, we use a very simple model for our propagation environment, which nevertheless proved to be quite accurate, as we will see in Section IV. This model, described in [27] assumes that at each indoor measurement location a very large number of equal-power multipath wavefronts arrives at the receive antenna. These wavefronts are distributed uniformly over the complete azimuth angular range with phases distributed uniformly from 0 to 2p. In addition to these scattered paths, a single dominant wavefront arrives from one direction, as illustrated in Fig. 1.
  
 Fig. 1.	Spatial representation of propagation model.
 The average power ratio of the dominant wave over the total of the scattered components is the Ricean K-factor. We now consider a directional antenna rotated in azimuth. For simplicity we assume that the directive and the omnidirectional antennas have the same vertical gain and that the horizontal gain pattern for the directive antenna is constant within the mainlobe and zero elsewhere. If this antenna is pointed in a direction that does not include the dominant path, then the received voltage will be Rayleigh-distributed, provided that one can still assume that a large number of multipath components fall within the beamwidth. On the other hand, when the directive antenna’s mainlobe receives the dominant wavefront, then the received voltage will be Rice-distributed. However the K-factor for this
 distribution will be GG00omnidir times that characterizing the voltage received by the omnidirectional antenna. This is because the dominant wave is subject to the increased horizontal antenna gain of the directive antenna, while the scattered power is (on average) unchanged, as the gain increase is exactly offset by the reduction in number of the received diffuse components.
 With these considerations we can statistically describe the ratio of the power received using both types of antennas. For the omnidirectional antenna placed at random in the room the output voltage envelope vomni is a Rice-distributed random variable characterized by the local K-factor. We now consider that at the same position the directive antenna is rotated over N nonoverlapping positions in search of the maximum power, where N = 360/antenna beamwidth, with the antenna beamwidth expressed in degrees. Then the directive’s antenna output voltage vdir can be written as vdir = maxi=1···N[vi], where the collection of random variables {vi}i=1···N consists of N - 1 Rayleighdistributed random variables and one Rice-distributed random variable with a K-factor equal to K · GG00omnidir . It is reasonable to assume that these random variables are independent, since they arrive from non-overlapping angular sectors. From this we can obtain the statistical characterization of the ratio of powers between the directive and the omnidirectional antennas.
 As an example we will consider the limiting situation where K = 0. We note that in this case the directive antenna will only be able to choose among N Rayleigh-distributed random variables and will thus act as a N-branch selection combiner. This allows us to use well-known results. We characterize the statistics of the ratio PPr omnir dir , as it follows from (3) that this will determine the statistics of the GRF. Using the results derived for an N-branch selection combiner [30], we see that the Probability Density Function (PDF) of the output power Pr dir,
 PDFPr dir(x) is
 	PDFPr dir(x) = N(1-e-x)N-1e-x,	(4)
 where we have assumed that the average power for each direction is unity. This can be done without loss of generality since we will only consider power ratios. For the omnidirectional antenna the PDF of output power will then have an exponential distribution with unit average. We will also assume that this random variable is independent of the preceding ones.
 	PDFPr omni(x) = e-x.	(5)
 Given the statistics of Pr omni and Pr dir we now obtain the ratio distribution for ? = PPr omnir dir . Using the MATHEMATICAsoftware (version 9), we find that the PDF of ? for the Rayleigh case can be calculated as
 PDF xPDFPr dir(x)PDFPr omni(ux)dx
 ux
 	dx	(6)
 The above integral can in this case be obtained in closed form. Using again MATHEMATICA we obtained the PDF?(u) and from it the corresponding Cumulative Distribution Function (CDF)
 CDF?(u) = N 
 where G(·) is the gamma function.
 In the above example the directive antenna can only offer a diversity gain. The case where K is nonzero can be treated using the same procedure, using Ricean instead of Rayleigh distributions in (4) and (5). However the resulting integral corresponding to (6) does not yield a closed form expression and is thus best evaluated numerically. We note that when using the Ricean distributions these must not be normalized, since the model must consider that the average received powers are not equal as the K-factors vary with angular position. We will in general use ?K when referring to the (random) power ratio, given that the Ricean K-factor is equal to K. As the GRF is
 equal to PPr omnir dir multiplied by the ratio of nominal antenna gains, the statistics of the GRF are readily obtained from those of
 ?K = PPr omnir dir .
 We plot in Fig. 2 the CDF of the GRF for the cases K = 0,1,3,100 under the assumption N = 24. We have here again assumed that both type of antennas have the same gain in the
 vertical plane so that GG00omnidir = 24. The curves were calculated using (7) for the Rayleigh case, and numerical integration when K is nonzero. In addition we simulated the previously described model generating the corresponding random powers. The results were identical.
 We see that for small values of K, such as observed in our measurements, the model predicts that the GRF will be negative with non-zero probability i.e., the array gain advantage can appear to be higher than in free space. This corresponds to the condition where the omnidirectional antenna is subject to a large fade while the directive array placed at the same position can combat such a fade using angular selection diversity. Under the conditions we simulated, even for K-factors as low as 1, which implies a significant fade probability for omnidirectional
  
 Fig. 2. CDF of the simulated GRF for the cases K = 0, 1, 3, 100 under the assumption N = 24.
 reception, the directive antenna would almost in every case select the dominant wavefront direction and as a result only exhibit very shallow fades. Thus, we have a combination of classic (deterministic) beamforming gain as well as statistical reduction of fade margin, which can result in an overall gain higher than in free space.
 Throughout this work we consider that the beam-scanning of the directive antenna is only performed in azimuth and that the elevation beamwidth of the antennas is larger than the corresponding angular spread and hence the vertical gain is not reduced.
 Our approach to calculating the GRF is different from the one originally presented in [1] (see (1) in [1]), which is based on a transmission bandwidth much larger than the typical coherence bandwidth of the links considered. The use of the (real valued) angle-of-arrival distribution of power and the antenna power gains used in [1] is justified under such conditions due to the averaging effect over frequency. Instead, for short range links the coherence bandwidth is much larger, in the range of 10 MHz [28]. We thus here consider the case of a narrowband transmission, for example a range of OFDM carriers within the channel coherence bandwidth. Although the definition of the GRF is not dependent on the relation between the transmission and channel coherence bandwidths, in the narrowband case the approach used in [1] would not be practical. In fact, calculating the received power from the angle-of-arrival distribution of the electric field would require phase information, and the complex antenna voltage-gain. Instead we found it much simpler to directly measure received power with various antennas including an omnidirectional dipole used as reference and to apply (3).
 III. MEASUREMENT EQUIPMENT, SCENARIOS AND PROCEDURE
 The measurement system consisted of a 3.5 GHz continuous wave (CW) transmitter and a purpose built narrowband receiver coupled to a power meter. The outdoor base station (BS) transmitted 19 dBm into the antenna terminal of a
  
 Fig. 3.	Measurement location map.
 2.4 dBi gain dipole. The receiver bandwidth is 200 kHz. This is wide enough to capture any frequency dispersion that affects the CW transmission, induced for example by vegetation movement [29].
 The automated measurement procedure is essentially the same already described in [4], involving a laptop computer to control antenna position and the synchronized acquisition of power samples. For each antenna position 40 consecutive samples were obtained. These samples were then averaged to remove residual temporal fades, which were typically less than 0.5 dB. Before each measurement we performed calibration procedures to assure that the transmitter and the receiver were operating with their nominal powers and gains. In all field measurements the received power was at least 20 dB above the noise floor.
 A. Measurement Locations
 Measurements were carried out at 4 urban type settings in Santiago and Valparaiso, Chile. We selected placements where the base terminal could be positioned at a range of 5 m to 50 m from the indoor antenna. This space was in some cases occupied by gardens with shrubs and trees no higher than 5 m. Three locations were chosen in Santiago, one of them on the campus of Universidad Diego Portales and two in private houses. The fourth location was the campus of Universidad Santa Maria in Valparaiso, which offers a wide range of indoor options that includes offices and relatively large laboratories. A schematic of one of them is shown in Fig. 3 where we also specify the outdoor base and indoor subscriber unit (SU) locations. Fig. 4 shows the setup of the indoor measurement system for the analysis of both small-scale and GRF statistics.
  
 Fig. 4. Measurement configuration for the LOS case showing the on-axis rotating system and the 0.4 m rotating arm.
 At these scenarios we measured a total of 179 links that corresponded to two types of conditions, line-of-sight (LOS) through a window or non-line-of-sight (NLOS) when the direct path was blocked. This blockage included foliage of shrubs, small trees or a wall. The thickness of the foliated obstruction was typically in the range of 0.5 m to 1 m. The construction was of the brick and mortar type, with non-metalized windows. Window sizes varied in width in the range of 1.2 m to 3.2 m and in height from 1.8 m to 2.5 m. The minimum window area was 2 m2. All measurements were made by placing the outdoor antenna that simulates the wireless BS at 2.1 m height.
 B. Measurement of Path-Loss and Spatial Fade Statistics
 We firstly performed measurements aimed at fully characterizing our test locations. This involved connecting the receiver to the dipole antenna placed on a 0.4 m length rotating-arm moved stepwise in 6? azimuth increments, which implies a displacement of approximately   between successive antenna positions. The 60 power measurements obtained in a rotation at each placement were used to calculate the spatially averaged path-loss at that range and to generate the statistics of smallscale fades with respect to that average. The size of the region for the spatial average is well within the shadow-fade correlation distance reported in previous work [26], [31], [32]. In fact the smallest values mentioned in the literature, corresponding to indoor scenarios, are in the range of 1 to 2m [31]. This was repeated over a variety of positions in our test scenarios. The results allowed contrasting our statistical data with that of typical path-loss models. We note that these measurements were only aimed at characterizing the propagation environment and that spatial averaging was not used in obtaining the GRF, as will be described below.
 C. Measurement of the GRF
 To directly measure the values of the GRF as discussed in the previous section, we used a platform capable of rotating the antennas about their axes in 6? steps with no displacement other than rotation. Power samples were acquired as before at each angular position. Four types of antennas were used indoor, including an omnidirectional 2.4 dBi gain dipole used as reference and three different directional antennas. The first was a 10.2 dBi gain patch antenna with elevation and azimuth half power beamwidths of 50? and 60? respectively. The second was a 15.2 dBi gain horizontal linear array of four vertically polar-
  
 Fig. 5.	Normalized horizontal gain patterns for the directive antennas.
 ized patches with elevation and azimuth half power beamwidths of 50? and 15? respectively. The third was a 16.5 dBi gain 2 × 2 array of four vertically polarized patches with elevation and azimuth half power beamwidths of 25?. The corresponding normalized horizontal gain patterns for the directive antennas, measured in an anechoic chamber, are illustrated in Fig. 5. At each measurement location, the nominally omnidirectional antenna was rotated exactly as the directive ones to average out minor gain variations (less than +/-1 dB). Care was taken to place the 4 types of antennas in exactly the same position at the beginning of each rotation and this was repeated for a wide range of placements as will be described later. We recorded power samples for the full rotation, which in the case of the directional antennas allowed us to obtain the maximum signal strength as well as the direction of arrival of the diverse resolvable wavefronts. The computer-controlled automated measurement procedure is accurate to fractions of one degree and allowed us to verify that results were repeatable, i.e., that successive rotations at any given location would yield the same result.
 IV. MEASUREMENT RESULTS AND ANALYSIS
 This section shows the results of the measurements performed and the statistical data derived from them.
 A. Characterization of the Test Environment, Path-Loss
 We start by characterizing our measurement environment from a propagation point of view. To this effect we processed the data collected with the rotating arm in our scenarios to model average path-loss and small-scale fade statistics. The extensively used log-normal model [33] assumes that the spatially averaged path-loss (in dB) may be described as:
 	PLdB(d) = PLdB(d Xs,	(8)
 where d denotes distance between the BS and SU, PLdB(d0) is the free-space path-loss at a distance d0 in dB units, n is the
 TABLE I
 SUMMARY OF PATH-LOSS EXPONENT n AND STANDARD DEVIATION s FOR THE CASES LOS AND NLOS
  
 path-loss exponent and Xs is a zero-mean Gaussian distributed random variable (in dB) with standard deviation s.
 We applied this model to our data after averaging out smallscale fades over one rotation of the arm that holds the antenna. As conventionally done, we chose d0 = 1 m. Variations of the model in (8), using multiple slopes or choosing the intercept point PLdB(d0) different from the free-space path-loss did not provide a measurably better fit. In addition, we computed the CDF of the small-scale fades with respect to the local averages. As expected, we found that the fit to Ricean/Rayleigh distributions was very good. We show in Table I the bestfit model parameters. The observed K-factors were below 3.0 in 95% of the cases for both LOS and NLOS links attesting to a very rich multipath environment. It follows that our test environment is not characterized by large shadow-fade related power variations, particularly for the LOS case, but that it is rich in multipath-propagation, as expected for an indoor setting.
 B. Characterization of the Test Environment, Angular Spread
 Consistent with the above results, we found the indoor propagation environment to be rich in multipath over a broad angular range. The dominant energy arrives in general from the direction of the nearest window. Moreover, when pointing the antenna in the general direction of the dominant path, angular displacements in the range of a beamwidth typically resulted in a received power pattern that closely resembles that of the specific antenna used.
 This may be seen in Fig. 6, where we plotted received power vs. angle for the case of the 4 × 1 patch array in NLOS scenarios. To avoid a very cluttered figure we only show 10 realizations chosen at random among the total of 96 measurements. These were normalized to their maximum power and the angle at which this power was observed was set to zero. We also plot the antenna gain-pattern and the average values per angle obtained from all realizations. It can be seen that the dominant path is not resolvable into separate wavefronts by the antennas we used and that the angular spread of these dominant wavefronts is narrower than the beamwidths of the directive antennas. Other wavefronts were typically observed to arrive as wall reflections from angles larger than the beamwidths used, as seen in the above figure. This can be compared with simulations of the propagation model described in Section II, assuming the rotation of a directional antenna with the same gain-pattern as that used in our measurements. Since in this case our results are exclusively based on simulation, we chose to also include the randomness of the K-factor, which for our NLOS links was approximately uniformly distributed in the range 0 to 1.5. Thus in Fig. 7 we show 10 realizations for K ranging from 0.1 to 1.5 and the average of 100 simulations. As seen, the model results match the empirically observed behavior quite well, although in contrast with the simulations, the average power for the
  
 Fig. 6.	Measured received power vs. angle for NLOS cases.
  
 Fig. 7.	Model-simulated received power vs. angle for 0.1 = K = 1.5.
 measurements is not constant at angular positions outside the dominant direction. It may be concluded that in our scenarios, the angular distribution of the multipath components favors the direction of the dominant path.
 To further describe the angular characteristics of the arriving wavefronts, we compared the angle of arrival of the strongest signal, with the angle of the straight line connecting BS with SU. The CDF of this difference is shown in Fig. 8 for both the LOS and NLOS cases. As seen, for LOS links the dominant wavefront arrives on a close-to-direct path from the base. We found that for 90% of LOS cases the angular deviation was no larger than 7.5?, i.e., about half a beamwidth. Instead, for NLOS links the angular range for 90% of cases was 46.2?. The latter value is explained by the angular difference of between the direct path and that of the nearest window in our scenarios, which for 90% of placements was less than 40?. Alternatively, we found that when considering 90% of cases, aiming along a direct path resulted in a power reduction reaching up to of 3.5 dB for LOS and 12 dB for NLOS links.
  
 Fig. 8. CDF of the difference between the angle of arrival of the strongest signal and the angle of the straight path between BS and SU.
  
 Fig. 9.	CDF of the Gain Reduction Factor for the 2 × 2 array.
 C. Statistical Description of the GRF
 Our results show that, as previously observed in suburban settings, the GRF may be quite accurately modeled as a Gaussian random variable. Fig. 9 shows the cumulative distribution functions for the case of the 2 × 2 array in LOS and NLOS settings. In the same figure we also show the results obtained when using the model described in Section II, considering K-factors of 1.0 and 2.5 for NLOS and LOS respectively.
 Here, the model-based results need to take into account that the vertical gains of the omni and the directive antennas are not equal. As a result the increase in the K-factor that characterizes the directive antenna’s voltage only corresponds to the horizontal gain advantage over the omnidirectional antenna (or equivalently its horizontal beamwidth reduction), a factor of 14 for this case. On the other hand the total gains of each antenna were considered when calculating the powers they receive. As seen, despite the simplicity of the model, it predicts measurement results accurately, the error being negligible at the
 TABLE II
 SUMMARY OF MEAN µGRF AND STANDARD DEVIATION sGRF OF THE
 BEST-FIT GAUSSIAN MODEL FOR THE GAIN REDUCTION
 FACTOR FOR VARIOUS CASES
  
 TABLE III
 SUMMARY OF THE BEST-FIT VALUES FOR A AND B FOR
 THE MODEL OF µGRF IN (9)
  
 90% and 50% probability levels, while at 10% probability it is 2.6 dB for LOS and to 3.4 dB for NLOS. We have omitted the corresponding graphs for the remaining antennas, as they are quite similar. In contrast with [1], the observed GRFs exhibit a considerable likelihood of negative values i.e., the array gain advantage would appear to be higher than in free space. As already discussed, this happens at placements where the omniantenna is subject to a large fade, which the angular selection diversity gain of the directive antenna can significantly reduce. In [1] negative GRF values are not possible since the omniantenna small-scale fades have been averaged out in frequency.
 The CDF of the best-fit Gaussian random variables describing the GRFs for all cases is characterized by their mean µGRF and standard deviation sGRF in Table II.
 Although our propagation conditions are different, we fitted the model proposed in [1] to these average GRF values. The expression used is
 	µGRF  	(9)
 where ß is the half-power azimuth antenna beamwidth in degrees. The best-fit values for A and B are summarized in Table III. As observed in Fig. 10, the fit is quite good, although the actual average GRF values observed in our case are considerably lower than those measured in suburban settings.
 Furthermore, we measured an increasing average GRF with diminishing beamwidth, but this is much less than reported for the outdoor case. This may be due to the scalar integration of powers for the broadband case vs. vector addition of wavefronts in our narrowband case. Therefore in the wideband case narrowing the beamwidth monotonically decreases the total received power, whereas in our case excluding secondary reflected signals can in some cases be of benefit. Another manifestation of this phenomenon is the fact that we measured a significant fraction of negative GRF values, which contribute to lowering the average.
 We also found a relatively large standard deviation of the GRF with no statistically significant variation with beamwidth as compared to [1]. This is consistent with our observed angular spread, which is typically dominated by a strong component that is narrow compared with the directive-antenna beamwidths. Therefore when aiming these antennas towards
 Fig. 10.	Average Gain Reduction Factor versus antenna beamwidth.
 the strongest signal, there will be little difference among them resulting from the effect of secondary wavefronts. Equivalently, all directive antennas see the same strongly dominant planewavefront, from which they extract power in accordance with their respective effective areas. The variation of the GRF, which is similar for all directive antennas, may instead reflect the fact that in our narrowband measurement the omnidirectional antenna used as the common reference is subject to considerable Rice/Rayleigh spatial fading.
 We also study the possible correlation between the GRF and the shadow-plus-small-scale fade at the location, as measured by the dipole. It cannot be assumed a-priori that the GRF is a random variable independent of these fades. We calculated the total fades of the dipole with respect to the average path-loss regression (8) as well as the GRF for the same measurement point. The correlation coefficient between these random variables is then easily obtained. We found that for all antennas used and both LOS and NLOS cases, the correlations ranged between -0.4 and -0.5. The negative values observed in all cases reflects the fact that deeply faded locations for the dipole antenna will be those where the directive antennas can offer the greatest benefit (lowest GRF), due to angular diversity gains. The negative correlation between the spatial fades and the GRF, suggests that a directive array is capable of partially offsetting fades that would affect a dipole at the same position. We discuss this further in what follows.
 D. Achievable Power Gains
 To illustrate the actual gains that were achieved with directive antennas we plot in Figs. 11 and 12 the received power for our measurements with the 2 × 2 array and the dipole, considering the LOS and NLOS separately. We recall that for the dipole these measurements do not include spatial averaging, while for the directive antennas they represent the peak-received power over a rotation at the same measurement position of the dipole. This is consistent with a scenario where the user places the omni antenna at a random position, without searching to optimize
 Fig. 11. Received power measurements, LOS, transmitted power of 19 dBm. Straight and dashed lines represent the best-fit linear regression for a dipole and for a 2 × 2 array respectively.
  
 Fig. 12. Received power measurements, NLOS, transmitted power of 19 dBm. Straight and dashed lines represent the best-fit linear regression for a dipole and for a 2 × 2 array respectively.
 location and hence, small scale Rice/Rayleigh fade-margins must be included in the link budget.
 In these figures we also include the linear regressions for both types of antennas, considering an intercept at 1 m based on free-space propagation with the respective antennas. The least squares fit parameters are described in Table IV. As seen, the slopes are consistent with those obtained for pathloss when averaging out small-scale fades. In the LOS case, which is dominated by small-scale fades, the (negative) slope is marginally larger, as the lack of spatial averaging emphasizes deep fades when measuring power in dBm. The CDFs of the dB difference of received power, with respect to the regression lines, were in both cases very close to Gaussian. The difference between the two regression lines in the distance range where our measurements were concentrated is as expected almost identical to the nominal gain difference of the antennas after subtracting the average GRF.
 TABLE IV
 SUMMARY OF RECEIVED POWER EXPONENT n AND STANDARD DEVIATION s FOR THE CASES LOS AND NLOS
  
 We observe that the standard deviation of received power is about the same for the directive antennas and the dipole. While the antenna array may benefit from fade reduction due to diversity effects, this is more or less cancelled by the variability of the GRF. As a result, similar link fade-margins can be used for both antenna types.
 In summary, we found that indoor users able to search for the strongest signal with a directive antenna instead of an omnidirectional one, can achieve very significant receivepower increases. For example a nominally 16.5 dB gain antenna will in a NLOS indoor setting be able to provide an average 14.4 dB power gain. At the same time, the choice of a directive over an omni-antenna over does not increase the required fademargin. As already noted, part of the gain is attributable to classic beamforming and part to the angular diversity advantage of the directive antenna. As a reference we note that for a purely Rayleigh-fading environment, the average diversity gain of the 2 × 2 antenna, would have been only 5.1 dB, corresponding to a 14 branch selection combiner.
 V. CONCLUSION
 The results of an extensive measurement campaign, covering a variety of short-range outdoor-indoor links, show that significant power gains are available to indoor users using a steerable directive antenna. For the narrowband case that we considered, the antenna gains that should be used for link budget calculations are within 2.3 dB of the nominal, free-space gains (for example 14.4 instead of 16.5 dB for the 2 × 2 array). Moreover, while the directive antennas exhibit considerable randomness in the GRF, they also benefit from angular diversity and the combination of both effects resulted in fade statistics that are very similar to those of the omnidirectional antenna. Thus, the use of a directive antenna indoors implies an increase in average received power, without the need for increasing the fade margin.
 ACKNOWLEDGMENT
 The authors wish to acknowledge the help provided by Dr. Hector Carrasco in the design and calibration of the antennas used in this work.
 ﻿By focusing on a class of source coding schemes built around entropy coded dithered quantizers, we develop a framework to deal with average data-rate constraints in a tractable manner that combines ideas from both information and control theories. We focus on a situation where a noisy linear system has been designed assuming transparent feedback and, due to implementation constraints, a source coding scheme has to be deployed in the feedback path. We give a closed form expression for the minimal average data-rate required to achieve a given performance level, and also study the interplay between stability and average data-rates for the considered architecture.
 I.	INTRODUCTION
 Consider the networked control system (NCS) of Figure 1, where P is a given proper real rational transfer function, d is an exogenous signal, e is a signal related to closed loop performance, y can be measured, u is a manipulable input, and the channel is a noiseless digital channel. Partition P as
   ,
 where P22 is SISO and strictly proper, and both P12 and P21 are non-zero. The initial state xo of P is a second order random variable and d is a second order wss process with PSD Sd > 0.
 If no constraints but causality are imposed on the source encoder and decoder, then mild conditions guarantee that the NCS of Figure 1 is mean square stable (see definition in Section III) if and only if the average data-rate R of the coding scheme (i.e., across the channel) satisfies [1]
 	R > RNE  ,	(1)
 where pi is the ith unstable pole of P. When performance guarantees are sought, then much less is known. A causal rate-distortion inspired approach was followed in [2], but no achievable rate regions where established there.
 
  
 Fig. 1. NCS closed over a noiseless digital channel.
  
 	(a)	(b)
 Fig. 2. (a) Independent source coding scheme and (b) equivalent rewriting.
 II.	THE SOURCE CODING SCHEME
 The average data-rate of the source coding scheme in Figure 1 is defined via
 R ,
 where R(i) is the expected length (in nats) of sc(i).
 Theorem 1: Consider a causal source coding scheme inside a feedback loop, as in Figure 1. Under suitable assumptions (see [3]), R = I8(y ? u), where I8(y ? u) denotes the directed mutual information rate defined by Massey [5].  
 A. Independent source coding schemes
 We focus on a class of source coding schemes:
 Definition 1: The source coding scheme of Figure 1 is said to be independent iff the assumptions of Theorem 1 hold and the noise sequenceobeys n = Oq, where q is a second order zero-mean i.i.d. sequence, q is independent of (d,xo), and O is a stable and stably and causally invertible filter with deterministic initial state (see Figure 2(a)).  
 Any independent source coding scheme can be written as shown in Figure 2(b), where v and w are auxiliary signals, q is as in Definition 1, and A and F are auxiliary stable filters, and both A and 1-F are stably and causally invertible. Moreover: Theorem 2: If an independent coding schemes is written as in Figure 2(b), then I8(y ? u) = I8(v ? w).  
 The previous fact allows one to focus on I8(v ? w) instead of I8(y ? u). It is possible to establish upper bounds on I8(v ? w) that depend only on second order properties of w and v:
 Theorem 3: If in the NCS of Figure 1 the source coding scheme is independent, then1
  ,
 where Sw is the stationary PSD of  is the stationary variance of v, and  is the variance of q. Equality holds in the first inequality iff (d,xo,q) is Gaussian, whereas equality holds in the second inequality of iff  is constant a.e.  
 An additional key fact of independent source coding schemes is stated next (see also [7]):
 Theorem 4: For any given independent source coding scheme, there exists another independent source coding scheme, with the same noise color O and the same directed mutual information rate across it, such that the gap between the left and right hand sides of the second inequality of Theorem 3 can be made arbitrarily small.  
 B. Entropy Coded Dithered Quantizers (ECDQ)
 We consider ECDQs as defined in [8] with dither dh, input v and output w.
 Theorem 5: Consider the system of Figure 2(b) with A and F as before. If an ECDQ is used as the link between v and w, ? < 8 and the dither dh is i.i.d., independent of (xo,d), and uniformly distributed on (-?/2,?/2), then the system of Figure 2(b) becomes an independent source coding scheme and the entropy coder inside the ECDQ can be chosen so that
 R 
 If the entropy coder inside the ECDQ is memoryless, then it can be chosen so that
 R .
  
 III.	MEAN SQUARE STABILITY
 Definition 2: A system x(k + 1) = Mxx(k) + Mww(k), k ? N0, x(k) ? Rn, x(0) = xo, where xo is a second order random variable, Mx and Mw are constant matrices, and w is a second order wss process is said to be mean square stable (MSS) iff there exist finite µ ? Rn and a finite and positive semi-definite M ? Rn×n such that limk?8 E {x(k)} = µ,
  , regardless of the initial state
 The previous definition applies to the NCS of Figure 1, when the source coding scheme is independent.
 Theorem 6: Consider the setup and assumptions of Theorem 5. Then, irrespective of whether the entropy coder inside
 1D(x||y) denotes relative entropy [6].
 2
 the ECDQ has memory or not, the minimal average data-rate compatible with MSS, say RMSS, satisfies (cf. (1))
 RNE < RMSS  .
  
 The bound on RMSS provided by Theorem 6 is, by construction, achievable.
 IV.	PERFORMANCE GUARANTEES
 Defi 
 	12	21	 .
 Theorem 7: Consider the setup and assumptions of Theorem 5. If Dinf < D < 8, then, irrespective of whether the entropy coder inside the ECDQ has memory or not, the minimal average data-rate that allows one to attain a performance level se2 = D, say RD, obeys
 	RD ,	(2)
 where
  
 ?
 	1	p	Y	Y
 ?D = exp - ln??  2 + |S|2 + v	??d??? - 1,
 	?p	p	?D	?D
 and ?D is the unique positive real satisfying
  
 If D < Dinf then RD does not exist, whereas achieving D =
 Dinf requires an infinite average data-rate.	 
 The bound on RD provided by Theorem 6 is, by construction, achievable. To our knowledge, our result corresponds to the first closed form bound on the achievable average data-rate needed to attain a given performance level.
 We note that, if (xo,d) is Gaussian, then the bound on RD given in (2) is tight up to   nats/sample.
 ﻿This paper presents novel results on perfect reconstruction feedback quantizers (PRFQs), i.e., noise-shaping, predictive and sigma-delta A/D converters whose signal transfer function is unity. Our analysis of this class of converters is based upon an additive white noise model of quantization errors. Our key result is a formula that relates the minimum achievable MSE of such converters to the signal-to-noise ratio (SNR) of the scalar quantizer embedded in the feedback loop. This result allows us to obtain analytical expressions that characterize the corresponding optimal filters. We also show that, for a fixed SNR of the scalar quantizer, the end-to-end MSE of an optimal PRFQ which uses the optimal filters (which for this case turn out to be IIR)decreases exponentiallywith increasing oversampling ratio. Key departures from earlier work include the fact that fed back quantization noise is explicitly taken into account and that the order of the converter filters is not apriori restricted.
 Index Terms—Differential pulse code modulation, optimization, quantization, sigma-delta modulation, source coding.
 I. INTRODUCTION
 T
 HE term feedback quantizer (FQ) refers to a class of analog-to-digital converter (ADC) architectures wherein a scalar quantizer is placed within a linear feedback loop. Well-known examples of FQs include  -modulators, DPCM converters [1] and sigma-delta modulators [2]. The latter schemes have been very successfully applied in a number of areas, including audio compression [1], [3], oversampled A/D conversion [2], [4], subband coding [5], digital image half-toning [6], power conversion [7], and control over networks [8].
 Fig. 1 depicts a general FQ configuration. In this scheme,  may take the form of a nonuniform or a uniform quantizer [9], the latter being either dithered or undithered [10].
 The filters   in an FQ system allow one to exploit the predictability of the input signal so as to reduce the variance of  . When compared with simple PCM 
 
 Fig. 1. Feedback quantization system and frequency weighting filter.
 smaller quantization step. The error-feedback filter   opens the possibility of spectrally shaping the effect of quantization errorsontheoutput.Inthisway,onecanallocatemoreofthequantization noise in the frequency bands where it is less harmful from a user’s point of view. Accordingly, it is convenient to use a frequency weighted error criterion, via an error frequency weighting filter  , and to focus on the frequency weighted MSE (FWMSE) (see discussion in [3] and [11]).
 For the sake of generality, we consider the possible use of a clipper before . This device limits the value of the quantizer input signal so that if , and if
 , where is the saturation threshold of the clipper. Thisclippingtechniquecanbeusedtokeep fromoverloading, which is helpful in reducing limit-cycle oscillations (idle tones) in an FQ with high order filters, as proposed in [4]. On the other hand, if we chose   to be sufficiently large, then  , and the clipper has no effect on the system.
 If the characteristics of   and the spectral properties of the input signal   are known, then the design of an FQ converter that minimizes the variance of   amounts to choosing the filters
  .
 It is often desirable that a converter is transparent to the system in which it is inserted. This corresponds to the widespread paradigm in which the coding scheme adapts to the application that employs it, without need to modify the latter. A transparent converter is one whose signal transfer function (i.e., the transfer function from input   to output  ) is unity at the frequencies of interest. The design of such perfect reconstruction feedback quantizers (PRFQs) constitutes the main topic of the present work. PRFQs are characterized by the property that, in the absence of quantization effects, there is no frequency weighted reconstruction error, i.e.,. If we denote the power spectral density (PSD) of , then it can be seen from Fig. 1 that the latter holds if and only
 if
 1053-587X/$25.00 © 2008 IEEE
   (1)
 Thus, in the design of an optimal PRFQ converter, only two degrees of freedom are available: the filters   (or, alternatively,  ). 
 To the best of our knowledge, existing results on optimal filter design for PRFQ converters either consider finite order filters [2], [12], [13], assume (or require) that the variance of the signal   is much smaller than that of   [1], [4],
 [14], [15], or have a heuristic component in the optimization [2], [3],[13],[16]–[19].Theonlyexplicitanalyticalexpressionscurrently available for the optimal performance (and corresponding filter frequency responses) of a PRFQ converter are those given in [14]. However, the assumption of negligible fed back quantization errorsused in [14] makes these filters suboptimal. Indeed, as we will show in the sequel, there exist situations where the filters proposed in [14] yield large fed back quantization error, even when a fine step scalar quantizer is used. In these situations, not only is the main assumption in [14] violated, but also an FWMSE much larger than predicted can result due to excessive quantizer overload (see, e.g., [2] and [13]).
 In the present paper, we will show how to design optimal PRFQ converters. For this purpose, as in [12], [14], and [16]–[18], we model the scalar quantizer as a linear device that introduces additive white noise whose variance is proportional to that of the signal being quantized. A key departure from [14], however, is that we explicitly take into account fed back quantization noise in the feedback loop. Our main contributions are:
 i) We derive one-parameter equations that relate the minimum achievable frequency weighted MSE to the signal-to-noise ratio (SNR) of  ; ii) We show, within our model, that the frequency weighted MSE in an optimal PRFQ where the SNR of   is fixed decreases exponentially with oversampling ratio; and iii) We derive equations that characterize the optimal filters for a PRFQ. Our results can be applied to any given number of quantization levels, and to almost arbitrary input spectra and frequency weighting criteria.
 The remainder of this paper is organized as follows: In Section II, we present our analysis model for PRFQ converters. In Section III, we formulate the associated optimization problem. Section IV presents a one-parameter characterization of the solution. In Section V we discuss the main properties of an optimized PRFQ. The case of oversampled FQ is analyzed in Section VI. Section VII discusses the relationship to previous results and highlights the importance of taking account of fed back quantization noise. Section VIII presents simulation results. Section IX draws conclusions. (For ease of exposition, all proofs of our results are included in the Appendix.)
 A. Preliminaries and Notation
 We write “iff” as a short hand expression for “if and only if.” The sets of all complex-valued square integrable and absolutely integrable functions on are denoted by and  , respectively. Given , , we adopt the standard inner product	, where denotes complex conjugation. We denote the corresponding
 2-norm as	. We use   as the argument of the
 -transform. If is a transfer function, then we use the short hand notation to refer to the associated frequency response
 . If is a set, then we write “a.e. on  ” (almost everywhere on ) for “everywhere on , except on a zero Lebesgue measure subset of .” We use to denote the variance of a given wide sense stationary (w.s.s.) random process , having PSD . We recall that if has zero mean, then
 , where
 	is a frequency response satisfying	,
 	. For any functions	or	we write
 and	to denote the sets	and
 , respectively.
 To simplify notation, we introduce the operator , defined as follows:
 (2)
 where is any given function and denotes any arbitrary and positive bounded value. For later use, we also recall the following definition.
 	Definition	1	(Almost	Constant	Function):	A	function
  	(3)
  
 II. PRFQ CONVERTER MODEL
 In this section, we discuss some of the main aspects of feedback quantization. We also describe the analysis model and the constraints to be considered later in the search for the optimal filters.
 A. Feedback Quantizer Equations
 We begin by presenting the equations that describe the behavior of the PRFQ shown in Fig. 1.
 1) Quantization and Clipping Errors: From Fig. 1, the quantization error   is given by
 	 	(4)
 Every practical scalar quantizer has an associated constant  
 such that, if , then is said to be overloaded. When the quantizer is not overloaded, then is only granular quantization error, namely , which can be bounded as , , for some
 (see, e.g., [9]). For example, if is a symmetric, uniform, nondithered quantizer with levels and quantization interval , then one needs in order to obtain .
 In general, we can write
 	 	(5)
 where
  
 is the overload error. Clearly overload errors are bounded as
 , but they cannot be bounded by a
 constant unless	is bounded.
 As outlined in the introduction, the clipper in Fig. 1 can be used to keep   from overloading. For simplicity, we will only consider here two possibilities, namely, that  , or else
  . The former choice guarantees that   does not overload, since clipping error, defined as
 	 	(6)
 takes place instead. More precisely, if   we have that
  
 using clipping is that, unlike overload errors, clipping errors are not fed back into through . This helps to avoid large limit-cycle oscillations arising from the overload of  , see [4]. Since such oscillations are not part of the analysis model we will use, their occurrence could increase the FWMSE significantly above the value predicted by the model.
 Using the above definitions, and from Fig. 1, we can write
 	 	(7)
 which reveals that differs from by the sum of the quantization and clipping errors.
 2) Transfer Functions: From Fig. 1 and (7) we have that
  	(8a)  	(8b)
 	 	(8c)
 Notice thatthese equations are exactand require no assumptions on the signals involved. From (8b) one can see that corresponds to the signal transfer function (STF), from to , of the converter. Similarly, the product is the transfer function for quantization errors, usually referred to as the noise transfer function (NTF) of the converter . The term   will play a crucial role in the derivation of the optimal
 filters in Section IV.
 3) Stability: We say that a PRFQ is Bounded-Input-Bounded
 Output (BIBO) stable iff for any input sequence   satisfying allthesignalsinthe converterarebounded.
 has infinitely many quantization levels, then
 , and, thus, all the other signals in the converter are bounded. On the other hand, if , then can be written as
 (9)
 If the quantizer has a finite number of quantization levels, then   is bounded. If is stable and is minimum-phase, then it follows from (9) that is bounded. This, in turn, guarantees that   and all the other signals in the converter are bounded [see (4) and (8)]. Summarizing, if all the filters in Fig. 1 are stable, and if   has no zeros on or outside the unit circle, then the resulting PRFQ is BIBO stable.
 In addition, if and are stable, then the norm of their impulse responses, namely and , are bounded.
 Thus, if there exists a bounded	such that
 	,	,thenasufficientconditiontoensure
 	,	, is that	, where
 (10)
 Therefore, for a uniform quantizer with quantization interval  , it suffices to have   or more quantization levels in order to avoid clipping or overload errors.
 B. Assumptions
 The assumptions associated with our PRFQ model are described next.
 1) Input Spectrum and Frequency Weighting: The error weighting filter   in Fig. 1 models the impact that reconstruction errors have at each frequency. This “performance assessment” filter is application dependent, and is assumed to be stable and given. The input signal  is a zero-mean w.s.s. stochastic process  with known PSD and finite power, i.e.,  . In
 order to simplify our subsequent analysis, we shall further restrict and to satisfy the following:
 Assumption 1: The product is a piece-wise differentiable function having at most a finite number of discontinuities and satisfying , . In addition, is such that one  of the following conditions holds.
 There	exists	a	constant	such	that
 	, for all	, or
 such that . Furthermore, if denotes the set of noncontiguous and nonoverlapping intervals in such that
 , then, for every , such that	is	as
 .
 We note that the above is a rather weak constraint, since conditions i) and ii) include almost any product of practical or theoretical interest. In particular, condition i) covers all the cases where the product has no zeros on the unit circle. In turn, condition ii) is satisfied if is zero over any interval on   having nonzero measure, or if is rational and has zeros on the unit circle.
 2) The Quantizer: We shall focus our analysis on the effect that granular quantization errors have on the FWMSE. For this effect to closely represent the actual FWMSE, we need to assume the following:
 Assumption 2: The variances of overload and clipping errors are negligible, i.e.
 	 	(11a)
 or
 	 	(11b)
 In addition, and as stated in the introduction, we will adopt an additive white noise model for  . This model is widely used for the analysis and design of data converters (see, e.g., [1]–[5], [12]–[14], [16]–[18], and [20]–[22]), being usually described as follows.
 Assumption 3: The sequence of quantization noise   is a zero-mean w.s.s. random process, uncor-
 related with the input of the PRFQ, and having constant PSD
  
 where   is the variance of  .	 
 The above additive white noise model, although not exact, is, in general a good approximation when a signal with a smooth probability density function (pdf) is quantized with many levels and negligible overload (in the sense of Assumption 2 ), see, e.g., [2]. The model can be made exact, even for few quantization levels, by utilizing a uniform scalar quantizer with either subtractive or nonsubtractive dither6, provided quantizer overload does not occur, see [10]. As discussed before, one way to achieve this is to use a quantizer with a sufficiently large number of quantization levels, so as to satisfy (10). In this case, if the quantization interval is   and the dither sequence  
 whitens  , makes uncorrelated to when is not overloaded and is bounded as , then any number of levels greater than or equal to will make Assumption 3 hold exactly. If a smaller number of quantization levels are employed so that  , then the use of dither with the same characteristics as before, together with clipping (i.e., setting  ), will also make   satisfy Assumption 3 exactly.
 Assumption 3 allows one to write the variance of  as
 	 	(12)
 seeFig.1.Thisequationdescribestheeffectof on through the feedback path. However, if the scalar quantizer has a finite and fixed number of quantization levels, then another link between these two variances needs to be considered. In order to model this relationship, we will use the fixed SNR model employed in, e.g., [12], [14], [16], [17], and [21].
 Assumption 4: For a fixed number of quantization levels, the variance of quantization errors is proportional to the variance of the signal being quantized, i.e., there exists   such that
 (13)
  
 If no clipping is used (i.e., if ), then corresponds exactly to the SNR of . If , then is a good a approximation of the SNR of when (11b) in Assumption 2 holds. In our model, is assumed fixed and given. Strictly speaking,   depends on the pdf of , on the number of quantization levels of , and on how quantization thresholds and levels are distributed along the dynamic range of  . In practice, for a given number of quantization levels,   should be chosen such that the dynamic range of   is used efficiently, whilst en-
 6Here and in the sequel, we assume the dither is such that n is white and uncorrelated with x when Q is not overloaded.
 suring a low probability of quantizer overload or clipping. For example,for the often cited uniform quantizer with levelsand loading factor  equal to 4 we obtain(assuming that   has a uniform pdf and neglecting overload errors). We note that for large  , and provided overload errors are negligible, a quadratic relationship between and holds for most types of scalar quantizers (see, e.g., [9]). This is indeed the well-known rule of “6 [dB] reduction of quantization noise variance per additional bit of quantizer resolution.”
 In the sequel, we refer to the model of PRFQ determined by Assumptions 2, 3, and 4 as The Linear Model. Summarizing, the Linear Model is exact if the PRFQ uses a dithered quantizer having enough quantization levels to avoid overload. If not enough quantization levels are available and dither is used jointly with clipping, then the model is exact in predicting the effects of granular quantization errors, and is a good approximation in predicting the total FWMSE if Assumption 2 also holds. If the scalar quantizer is undithered, has a small quantization interval (relative to  ) and enough quantization levels to avoid overload, then the Linear Model can be expected to yield a good approximation of the total FWMSE. Perhaps surprisingly, the Linear Model turns out to predict with remarkable accuracy the FWMSE of an optimal PRFQ when few quantization levels and clipping are used with a loading factor big enough to satisfy Assumption 2, even without dither, and even for a 1-bit quantizer. This can be observed from the simulation results presented in Section VIII.
 C. Optimization Constraints
 The filters   in Fig. 1 are design choices.
 We shall restrict the search for the optimal filters to those satisfying the following constraint.
 Constraint 1:
 and	satisfy (1).
 and	are stable.
 is stable and strictly causal (i.e.,  
 As foreshadowed in Section I, the first constraint enforces perfect reconstruction. As discussed in Section II-A-3, the stability constraints on   are a necessary condition for the converter to be BIBO stable. The additional requirement on  , namely strict causality, is needed for the feedback loop in Fig. 1 to be well defined (see, e.g., [2, ch. 4]). Notice that we will not a priori require   to have zeros only inside the open unit disk. Instead, we will show that the latter property arises naturally from the solution of the design optimization problem.
 An additional constraint on   arises from the value of  , as explained next. The ratio between the variances of and
 imposed by the feedback can be obtained by dividing (12) by  , yielding
 (14)
 One can see from the above that if , then any pre-filter or scaling of the quantization intervals of will yield   , thus, making large overload (or clipping) inevitable. This would increaseoveralldistortion and, if no clipping is used, may lead to large limit-cycle oscillations. We, thus, conclude that the use of feedback imposes the following constraint. Constraint 2:
 If the above constraint is met, then	can be found by substituting (13) into (14). This gives
 (15)
 III. OPTIMAL PRFQ DESIGN
 Given the model described in the previous section, we can now evaluate the quantity that we aim to minimize, namely, the frequency weighted mean squared error (FWMSE). From (8c), and Assumptions 2 and 3, it follows that the FWMSE is given by
  . Thus, in view of (15),the minimization
 of the FWMSE in the Linear Model can be stated as follows. Optimization Problem 1: For given  , and for given and   satisfying Assumption 1, find the frequency responses , and   satisfying Constraints 1 and 2 that minimize
 	 	(16)
 The following proposition allows us to further reduce the number of unknowns in (16) by characterizing the optimal
  to  mizes
 Itisconvenienttorewrite(17)morecompactlybyintroducing the following change of variables:
  ,
 proof of Proposition 1 in the Appendix ), Constraint 2 is satisfied iff . In addition, a stable and strictly causal
 (i.e., one satisfying Constraint 1) always leads to a function	, see (20), which satisfies 
 	 	(22)
 This result follows directly from Jensen’s formula [23] (see also the Bode Integral Theorem in, e.g., [24]).
 On the other hand, as we shall see in Section IV, if Assumption 1 holds, then the optimal within the set of functions described by (22) and the requirement turns out to be piece-wise differentiable on , has at most a finite number of discontinuity points, and satisfies
 (23a)
 (23b)
 Under these conditions, it is always possible to find a stable and strictly causal filter such that approximates   arbitrarily well on , as stated in the following lemma.
 	Lemma 1: Suppose that	is piece-wise differentiable on
  , that it has at most a finite number of discontinuity points and that it satisfies (23). Then, for every , there
 exists a (finite order) rational, strictly proper and stable such that  .
 Using the above results, Optimization Problem 1 can be restated as follows.
 Optimization Problem 2: For given and known   and for   satisfying Assumption 1, find
  characterizes the optimal feedback filter, say  , via (20) (see also Lemma 1). In the following section, we will show how to solve this optimization problem.
 IV. SOLUTION OF THE PRFQ OPTIMIZATION PROBLEM
 It would be desirable to provide an explicit analytical solution to Optimization Problem 2. Unfortunately, and as will become apparent in the discussion later, developing a closed form solution, for arbitrary functions , appears infeasible. Nevertheless, we can provide a one-parameter characterization of the optimal function in (24) as follows.
 Theorem 1: For any given satisfying Assumption 1, and for any , the function in (24) belongs to the one-parameter family of functions , where
 (25a)
 and
 (25b)
 Here,	, is the lower bound of feasible	’s, and	, if it exists, is the unique scalar such that
 	. If such a scalar does not exist, then we choose	.
 Note that the above result provides an explicit analytic expression for , once the optimal , defined as
 	 	(26)
 has been found, i.e.,. Expression (25a) also gives
 insight into the structure of
 Theorem 1 can be used to develop an efficient algorithm to solve Optimization Problem 2. The key point is that substitution of (25a) into (21) changes the search space from the infinite-dimensional set   to the real interval  . More precisely, Optimization Problem 2 is turned into the simpler problem of finding the minimizer of the single variable nonconvex scalar function
 (27)
 	We will show next that the global minimizer of	, i.e.,
  , (and hence the solution of Optimization Problem 2) is unique. Furthermore,   can be obtained by finding the root of a scalar, convex, and monotonically decreasing function.
  
 Theorem3: Theright-hand side(RHS) of (28) is aconvex and
  
 Moreover, it follows from Theorems 2 and 3 that, for any satisfying Assumption 1, and for any  , the global minimizer of (27) exists and is unique . In addition, these results guarantee that   can be easily found by solving (28), via, for example, the bisection algorithm [25], or any other convex optimization method [26].
 We can now express and the minimum achievable FWMSE, namely , in terms of , and . Indeed, combining (28) and (25a) with (21) yields (after some algebraic simplification) that
  
 fies the conditions of Lemma 1.
 It can be seen from (31a) that   is a monotonically increasing function of. In view of Theorem 3, this implies that, as expected, is monotonically decreasing with increasing  . As a consequence, the converse of Optimization Problem 1, namely, finding the optimal filters and minimum required SNR of   for a given target distortion, can be solved by using (28) and (31). Moreover, since the RHS of (31a) is a concave, monotonically increasing function of  , this parameter can be easily found by using standard iterative algorithms, as in the original optimization problem.
  
 It is also interesting to note that (28) and (31a), which relate and via the parameter  , have a structure akin to the well-known reverse water-filling equations (see, e.g., [27, pp. 108–123], and [28]). The latter characterize the rate-distortion function for Gaussian sources.
 To summarize, we have given an explicit analytic expression for the optimal and , once has been determined. Furthermore, we have shown that the parameter   always exists, is unique, and can be easily found using simple numerical methods.
 In the following sections, we will provide additional insight into the consequences of theseresults, as well as into some properties of optimal PRFQs,
 V. PROPERTIES OF OPTIMAL PRFQ
 In the sequel, we say that a PRFQ is optimal or optimized if its filters	,	satisfy (19) for negligibly small values of and	, and	is such that  , a. e. on	, with	as defined by (24).
 A. The Effect of the SNR of  
 It follows from Theorems 2 and 3 that, for any given  satisfying Assumption 1,   in (25a) describes the family of all noise shaping characteristics that are optimal for some  .
 As we will show, adjusting   from 0 to   (equivalently,  from   to 0) allows one to undergo a smooth progression from “full” noise-shaping to no noise-shaping, in an optimal manner. An example of this progression is shown in Fig. 2. Note in this figure how   (solid lines) approaches a unit transfer function as   (the quantizer SNR for which   in the figure), becomes smaller (and gets larger). It can also be observed that approaches the inverse of   is increased.
 Such asymptotic convergence does indeed take place in general, as the following theorem shows:
 Theorem 4: For any   satisfying Assumption 1, the functions   defined in (25a) converge uniformly to
 	 	(32)
 as  . Similarly, for any function satisfying condition i) in Assumption 1, the functions defined in (25a) converge uniformly to
 	 	(33)
 as	 in (32) corresponds to the choice of no feedback
 ( ), which reduces the PRFQ to a PCM converter. In view of (30), this no-noise shaping scenario is asymptotically optimal as  . In turn,   defined in (33) corresponds to the full whitening feedback filters proposed in [1], [14], [15]. From (29) and (33),   is optimal iff  . See also the discussion in Section VII.
 B. Signal Spectra
 1) The Output of the Quantizer: By looking at Fig. 1 and using Assumption 3, we find that the PSD of in an optimized PRFQ is given by
 	,	. Applying (18) to the latter result
 yields
  
 (34)
 Comparing (15) and (16), it is easy to see that
 	.	If	satisfies	(18),	then	we	have
 	. With the choice	, and using
 (31) and (28), we conclude that the variance of the quantization noise in an optimized PRFQ is given by 
  
 (25a) has been used. Substitution of (49a) and (28) into this expression leads to
  
 4, the output of the quantizer in an optimized PRFQ is white. This suggests that near optimal coding of the quantizer output can be achieved with a memory-less entropy coder.
 2) The Frequency Weighted Reconstruction Error: The PSD of the frequency weighted reconstruction error is given by tion of (18) into the above yields	.
 Applying (35) to the latter, we obtain
  
 (37) Thus, we conclude that the frequency weighted quantization error in an optimized PRFQ is not white. This fact stands in stark contrast to the conclusions reached when the FQ filters are optimized without the perfect reconstruction constraint (1), see, e.g., [22]. It also differs from the result obtained when the feedback filter is optimized ignoring fed back quantization error, as in [14] and [15]. Note that, as   is made larger,   not only becomes smaller, but its PSD asymptotically approaches11 a constant function over the frequencies
  .
 VI. OVERSAMPLED FEEDBACK QUANTIZATION
 It is well known that oversampling (i.e., sampling a bandlimited continuous-time signal at a frequency above its Nyquist rate) allows one to achievea smaller MSE error for a given,fixed number of quantization levels. For instance, the MSE of simple scalar quantization (without feedback) is known to decrease as  , see [29], where is the oversampling ratio, given by the order of the feedback filter (see also recent work in [20]). From a rate-distortion viewpoint, the inversely polynomial error decay of this error estimate is ”too slow” to compensate for the increase in the overall bit-rate due to oversampling (which is proportional to ). To be more precise, let us consider a scalar quantizer with quantization levels, where   denotes the quantization resolution in bits per sample. If the additional bitrate caused by oversampling was utilized instead to increase  , then the MSE would decay as  , i.e., exponentially12.
 A faster decay of the MSE of oversampled FQ with   can be achieved by selecting a different feedback filter (with possibly different order) for each oversampling ratio. An example of such a family (of 1-bit   converters) was given in [31]. Here, the continuous-time reconstruction error can be uniformly bounded by   is independent of . This bound guarantees an MSE that decays with as , which is faster than any inverse polynomial, but still far from
 11Substitution of (49a) into (37) yields
 g(!) +
 
 	S (!) = ( 	=4)[2g(!)=(	+ g(!))]:
 Thus, S (!) <	=4 for all ! 2 [ ;], and S (!) !	=4 as	!
 0 , 8! such that g(!) > 0.
 12Strictly speaking, this only holds for signals whose pdfs have finite support. Indeed, it has been shown that for several infinite support pdfs, the MSE of uniform quantization decreases asymptotically with b not faster than (ln2) , where a > 0 is a constant independent of b, see [30].
 exponential. Based on this result, the family of 1-bit converters reported in [32] achieve an MSE that is , i.e., exponentiallydecayingwith increasing . Notably, the results in [31] and [32] were obtained using an exact, deterministic model of quantization.
 We will next show that, within the Linear Model, if the optimal infinite order filters characterized in Section IV are used for each value of , then one can achieve an exponential decay of   with the oversampling ratio, provided   is kept constant. If the input sequence is obtained from sampling a band-limited analog signal, oversampling would cause   [defined in (20)] to vary with . To capture this effect, we replace
   by the family of functions	, defined as
 if
 (38) if	.
 In (38), denotes the square root of the PSD of the frequency weighted input without oversampling, and . Notice that , that is, the total power of (in units of variance per sample), remains constant for all . This ensures a uniform comparison basis for the distortion figures.
  We can now make explicit the dependence of
 (40)
 corresponds to the output-SNR of . Interestingly, it is possible to establish a precise “exchange” formula for and . Indeed, in terms of minimal achievable distortion, the effect of increasing oversamplingis equivalent to an exponential increase in the output-SNR of . This is shown in the next theorem:
 Theorem 5: Under the Linear Model described in Section II-B, for any function , and for any  , the minimum achievable FWMSE satisfies:
 (41)
  
 If we assume that depends exponentially on the number of bits per sample, then Theorem 5 suggests an FWMSE that decays exponentially with , provided the Linear Model holds and that optimal filters , and (characterized by (18), (25a) and (28)) are employed for each . The following simple example illustrates this idea:
 Example: (Flat Weighted Input Spectrum) Consider an input
 signal	and a weighting filter	such that
 is constant , without oversampling. For this setup, the optimal for our model of PRFQ is (
  ), i.e., a PCM converter. From (21), the minimum FWMSE without oversampling (i.e., with ) becomes
 where . To analyze oversampling behavior of in this case, we apply Theorem 5 to the above expression.
 This gives that	, and, thus
 (42)
 for all . Note that, to achieve (42), needs to be synthesized according to (31b) and (20). Therefore, for this example, the MSE of an optimized PRFQ with fixed exhibits an exponential decay with the oversampling ratio (since, by definition,  ).
 	If we further assume	to depend on the number of bits per
 sample	as	(which would correspond to  
 being a uniform quantizer with many levels and operating with a loading factor of 4), then (42) becomes
 (43)
 .This
 posing that Assumptions 3 and 4 hold, we obtain from (43) that is lower and upper bounded by terms proportional to
 . For loading factor values of 6, 10, and 20, the exponent in the latter expression changes to  , respectively.
 The next theorem shows that the exponential decay of the FWMSE obtained in the example above can be extended to arbitrary (band-limited) input signals and frequency weighting criteria.
 	Theorem 6: For any	and function   satisfying
 Assumption 1, the following holds:
 (44)
 where	denotes the optimal	for	.
 Thus, under the Linear Model, we have that the FWMSE of an optimized PRFQ decays exponentially with .
 Remark1: WerecallthatTheorem6isexactwithintheLinear Model described in Section II-B. Here it is convenient to present some further observations regarding the validity of that model when the oversampling ratio tends to infinity, for different implementations of a PRFQ.
 As already mentioned in Section II-B, if   is bounded and a sufficiently large number of quantization levels to avoid overload is used together with dither, then the Linear Model is exact. Nevertheless, there is no guarantee that the number of necessary quantization levels to avoid overload remains constant as   increases. If such number increases with , then can only be kept constant by increasing the number of quantization levels in the quantizer.
 If the number of quantization levels is insufficient to avoid clipping/overload errors, and if dither and clipping are used with a fixed loading factor, then there exists a certain finite value of   beyond which Assumption 2 is violated. This arises from the fact that, for any fixed loading factor, the effect of clipping errors in the output does not decay with  , thus, becoming the dominant component in the FWMSE for sufficiently high oversampling ratios. Further reduction of the FWMSE would then require one to balance clipping and granular quantization errors by increasing the loading factor. If the number of quantization levels is fixed, this would necessarily reduce the value of  , clearly increasing the component of the FWMSE due to granular quantization errors . Nevertheless, if clipping and dither are used (with  ), then the Linear Model and Theorem 6 is exact in describing the FWMSE due to granular quantization errors.  
 VII. THE IMPORTANCE OF TAKING ACCOUNT OF FED BACK QUANTIZATION NOISE
 If one tried to optimize the filters of a PRFQ neglecting fed back quantization noise, i.e., by trying to minimize
   (compare to (16)), then one would
 obtain a (sub optimal) feedback filter, namely  , which
 satisfies
  
 (45a)
 where
 	 	(45b)
 provided   [see (87) in the proof of
 Theorem 1]. This corresponds to the result obtained in [14], whichwasrestrictedtothecaseswhere .Forthecase
 , the noise transfer function magnitude is also equivalent to that derived in [15]. The latter is optimal in the sense of minimizing the ratio , but not in the sense of minimizing for a fixed quantizer SNR .
 As shown in Theorem 4, , in general, does approach  as . One can then expect to be near optimal in situations where , see (16). The latter is often satisfied at high bit-rates (i.e., when many quantization levels are available). However, for any given number of quantization levels, it is easy to find practical situations where is such that is comparable to (or greater than) . More precisely, from (22), and recalling that (see Appendix B), one can show that, if over a set of frequencies in with measure , where is some positive
 scalar, then
 (46)
 This means that a large	is obtained for any product  
 whose magnitude becomes significantly small (in relative terms) over certain frequency bands. (An example is included in Section VIII.) A direct consequence is that, for these cases, and in view of (16), trying to match  to will yield a performance far from optimal, also increasing the risk of incurring large limit-cycle oscillations if no clipping is employed (see, e.g., [2] and [13]).
 The (possibly unbounded) increase of  
 approaches   was already observed in [12]. Several heuristic solutions have been proposed since then (see, e.g., [2], [3], [13], [15], [17], and [18]). In contrast to these approaches, the method derived in the present paper allows one to characterize the true optimal filters, by explicitly taking into account   in the cost functional to be minimized [see (16)]. Our method not only guarantees that  , but also yields the actual optimal filters. Our proposal also has the advantage of being applicable to arbitrary input spectra and frequency weighting functions, regardless of how small the quantizer SNR
   may be, within the scope of validity of the Linear Model.
 VIII. SIMULATION STUDY
 To illustrate our results, we have designed the filters of a PRFQ aimed at digitally encoding audio signals in a psychoacousticallyoptimalmanner.Thedetailsofthesimulationmodel, as well as the results of both the simulations and the numerical optimizations are given later.
 A. Simulation Setup
 The PSD of audio signals was modeled as unit-variance zero mean white Gaussian noise filtered through
 . The mag-
 nitude of the frequency response of is depicted in Fig. 3 (solid line). The frequency weighting filter considered had a frequency response magnitude which approximated the psychoacoustic curve derived in [3, Table 1], thus, modeling the sensitivity of human hearing to noise . The corresponding frequency response is plotted with dotted line in Fig. 3 (the sampling frequency is 44.1 [kHz]). The resulting  
 for these   is also shown in the same figure (dashed line). For this choice of  , and in view of (46), one could expect the norm of a full whitening feedback filter to be very large. This is indeed the case:  . Thus, the suboptimal feedback filter characterized by (45) requires the use of a scalar quantizer with at least 18 bits in order to become feasible (see Constraint 2).
 In the simulations, was chosen to be a uniform mid-rise quantizer with quantization interval . Several values were considered for the simulations, calculated as
 , where   and where
 denotes the loading factor. Two different loading factors were considered: 4 and 6. The latter choice yields a slightly lower   than the usual loading factor of 4. However, this regime has the benefit of making overload errors smaller and more infrequent. As the simulation results will show, for our choices of and , this more conservative loading factor yields lower
 overall distortion when   takes values above 6 bits per sample.
  
 Fig. 3. Frequency response magnitudes for	(z) (solid line), P(z) (dotted line) and g(!) = j	(e	)P(e	)j (dashed line).
 For each   (and corresponding two values for  , one for each loading factor), the filters of the converter were designed according to the following:
 The parameter	was calculated by numerically solving
 (28).
 The optimal , and were obtained via (31b) and (18).
 These functions were then approximated  with rational IIR transfer functions , (of order 7) and  
 (of order 15).
 An appropriate value for the parameter in (18) was chosen via , see (35), assuming
 	(recall that	for all the simulations).
 	This ensures that	.
 For each combination of and , the resulting PRFQ converter was simulated utilizing two different architectures.
 Nonoverloading	: This scheme is as depicted in Fig.
 1, with	having (virtually) infinitely many levels. Thus, for all	(neither clipping nor overload er-
 rors occur).
 Overloading and Clipped : Here, has levels, which yields a scalar quantizer with a finite input dynamic range . As a consequence, any value would overload (if ) or produce clipping error (if ). To avoid large limit-cycle oscillations, this variant was simulated using clipping (i.e.,
 ).
 Each simulation with the nonoverloading PRFQ comprised 100,000 samples. For the overloading converter, five 100,000 samples simulations were performed for each combination of
 and .
 B. Results
 The results of the numerical optimizations and the simulations are discussed next.
  
 Fig. 4. Frequency weighted MSE for b 2 f1;...;16g.
 Comparison Between   and the Rate-Distortion Function: The information theoretic lower bound (see [28]) for the FWMSE associated with the given source   and filter   is plotted in Fig. 4 (solid line). This corresponds to Shannon’s quadratic frequency weighted Distortion-Rate function when . As the bit-rate is increased, the gap between and this absolute lower bound decreases to approximately 7.5 [dB] for and 11 [dB] for
 , at . This difference can be attributed to the rate-distortion inefficiency of the uniform scalar quantizer . On the other hand, the larger performance gap observed at lower bit-rates can be attributed to the perfect reconstruction constraint.  Recall that, at low bit rates, the achievement of Shannon’s rate-distortion function demands the suppression of relatively less significant bands of the PSD of the input signal (see, e.g., [27] and [28]). This linear distortion, which a PRFQ cannot achieve, is more severe at lower bit-rates. Thus, the performance gap increases as   is reduced.
 Nonoverloading  : The FWMSE of this converter variant is presented in four of the plots in Fig. 4, with labels beginning with “  opt. PRFQ, Nonoverloading.” These differ in the loading factor, and in the meaning of   in each case. For the plots whose labels do not have the ending “E.C.” (entropy coding),   is simply the number utilized to generate the value   for which the filters were optimized. The plots whose labels end in “E.C.” correspond to the same simulations, but for each point the value of   is the scalar entropy of the quantized output of the converter. It can be seen in Fig. 4 that the FWMSE obtained for the nonoverloading without entropy coding is remarkably close to the theoretical value predicted by (31a). More importantly, even for bit-rates as small as , each observed ratio   deviates from its nominal value of by less than 2%. (For the extreme situation , the observed   was slightly lower than predicted, while was 55% higher than 1/12 due to the highly nonuniform pdf of the resulting sequence  .) It can also be seen that the scalar entropy of the quantized output of the PRFQ in these cases is very close to Shannon’s   function for a given distortion. This agrees with the observation that the output of   in an optimized PRFQ is white, see the comment at the end of Section V-B-1. The difference between these quantities is bigger for lower values of  , for the same reason discussed in Section VIII-B-1.
 Overloading  : For the overloading PRFQ using an  of 4, the FWMSE diminished along with the corresponding  
 for	. However, the measured FWMSE varied
 , staying several dB higher than   over
 that range of bit-rates. This performance degradation can be attributed to clipping errors. The fact that overload errors become noticeable only for high bit rates (many quantization levels) might seem, at first, surprising. However, this phenomenon can be easily explained by noting that the size of the tails of the pdf of   that fall outside the dynamic range of remains approximately constant in relation to for all . (This is a direct consequence of the loading factor rule.) In contrast, granular (nonoverloading) quantization error is proportional to
   (which is held constant in the simulations). Therefore, the ratio between clipping and granular quantization errors grows approximately as   and clipping errors become dominant for sufficiently high bit-rates.
 Because of the reduced occurrence (and magnitude) of clipping errors, the optimized PRFQ with overloading  
 and   exhibits an FWMSE smaller than that of its counterpart with  . Furthermore, this more conservative loading factor allows the converter to perform almost exactly as predicted by our analytical expression for
  . 
 4) Comparison With PCM: The theoretical FWMSE of a
 PCM A/D converter, denoted by	, can be found from
 (16) by making	and	, which gives
 . For the chosen input PSD and frequency weighting filter, and calculating as , the value of varies with as shown in Fig. 4 (dotted line). As seen in this figure, the gap between and , for each value of , gets smaller as the bit-rate decreases. This agrees with the fact that the optimal PRFQ approaches a PCM converter as  , see Section V-A. It can also be seen in Fig. 4 that the optimized PRFQ with overloading and   exhibits an improvement of 32 [dB] over PCM at  . Equivalently, in order to obtain the same FWMSE as that of PCM at 16 bits, the PRFQ converter with   requires less than 12 bits. At lower bit-rates, the improvement of the optimal PRFQ overPCM is also significant. For example,the overloading PRFQ with   has a lower FWMSE than the PCM converter with  , thus, achieving a data rate compression of 50% (see Fig. 4).
 IX. CONCLUSION
 This paper has studied perfect reconstruction feedback quantizers based on an additive white noise model for quantization errors. We have derived results that relate the minimum frequency weighted MSE and the SNR of the scalar quantizer embedded in the converter. We have also provided closed form expressions for the optimal frequency responses of the filters in the converter and have derived several properties of optimal PRFQs. In particular, we have shown that the optimal frequency response magnitudes of the filters are unique, that the frequency weighted errors of an optimal PRFQ are nonwhite, and that consecutive samples of the output sequence of the scalar quantizer are uncorrelated. We have also shown that, within our model, thefrequencyweightedMSEofanoptimal,oversampledPRFQ, decreases exponentially with oversampling ratio.
 APPENDIX A PRELIMINARY RESULTS
 The following preliminary results are necessary to prove the theorems stated in the previous sections. We begin by introducing the following definition.
 Definition 2 (Similarly/Oppositely Functionally Related):
 We say that two functions   are similarly functionally related iff there exists a monotonically increasing function such that , for all , and write . Similarly, if there exists a monotonically decreasing function such that , for all
 , we say that and are oppositely functionally related, and write .	 
 	Theorem 7:   If	are similarly function-
 ally related, then
 	 	(47)
 If and are oppositely functionally related, then the inequality in (47) is reversed. In either case, equality is achieved iff   (and therefore  ) is almost constant.  
 Proof: We will examine the difference between the RHS and LHS in (47). We obtain
  
  manipulation. In order to show (49d), we define the functions
  
 Each of the terms above can be upper bounded as follows:
  
 (59b)
 Inequality	is due to (57) and to the fact that	.
 Inequality	stems from (58). Inequality	follows from
 (56), while	follows from the fact that	,
 	. Inequality	stems from (59), while	follows
  
 follows from the fact that
 (57)
 	, see (18), and from	. Inequality
 follows from the fact that
 (58a)
 (58b)
 which is readily obtained from (18) and (19). Inequality follows from
  
 is clearly biproper, stable, minimum-phase and such that
  
 We now proceed to upper bound the last term in the above inequality. From (63) and (64), we have that
  
 stable and minimum phase, we obtain
  
 It then follows from (68) that
  
 Substituting the latter into (66), we obtain
 	 	(69)
 where the last inequality stems from (62) and (63). Substitution of (69) into (65) yields
 (70)
 Since is bounded, and from (23b), it follows from (70) that for any , one can always choose sufficiently large
 (bounded) values for	[see (62)] and   [see (63)] so that  
 and are small enough to yield  . This completes the proof.	 
 C. Proof of Theorem 1
 Denote the squared norm of [see (24)] via, and define the set of all the having the same norm as
 by. Define
 	 	(71)
 It is easy to show21 that must belong to . From this, and since	, it follows that
 	. Minimization of	sub-
 ject to	can be stated as the following problem :
 (72)
 (73)
 (74)
 The problem described by (72)–(74) falls within the category of isoperimetrical problems, well known in variational calculus (see, e.g., [35] and [36]). The standard solution of these problems is based upon the fact that any   that extremizes   [see (72)] needs to satisfy
  ,
 of f.
 a.e. on	, where the scalars
 (78)
 are such that the constraints in (74) are met.
 We note that for the trivial case in which is almost constant (see Definition 1),   is also almost constant. Applying this to constraint i) in (74) yields that, for this case, is such that
 . Thus, the remainder of the proof addresses only the cases in which	is not almost constant.
 In order to find , we will next discard the possible solutions of (77) which do not correspond to global minimizers of  in . The unique remaining function, which is obtained with and in (77), will characterize the solution of Optimization Problem 2.
 The Case : Fore this case, substitution of (77) into (74) yields that needs to satisfy
 (79)
 so that can be obtained explicitly from . Note that cannot be zero in the above expression, otherwise would be undefined. From this, the feasible  sign combinations for , the sign before the square root, and   in (77) are
  
 We will next show that only option c) characterizes the optimum.
 Discarding Option a): We show next that any solution obtained by applying option a) in (77), say  , yields a greater
  
   follows by applying Theorem 7 to the numerator of(80), together with (48) and the fact that . Both inequalities are strict since is not almost constant (see Theorem 7 and Definition 1).
 On the other hand . From the above, it follows that [ee (21)], discarding, for all non
 A.E. flat  , the global optimality of the solutions associated to Option a).
 Discarding Option b): The candidate solutions are now characterized by options b) and c) only. Applying (49a) to (77) and (79), these solutions take the form
  
 Combining this result with (84), and considering   to be not almost constant, we obtain
  , i.e., the optimal noise shaping fre-
 quency response magnitude in the absence of fed back quantization noise [recall (45)]. This is not surprising, since taking   amounts to removing constraint i) (which restricts the
 power gain of fed back quantization noise, see Fig. 1).
 We will discard this option and its associated solution showing that
 24
 converges
 (88)
 (89)
 (90)
 stated in can not be the
 (91)
 (92)
 (93)
   follows directly from (84). This completes the proof.	 
 D. Proof of Theorem 2
 Since the functions , are continuously differentiable , so is . We, therefore, have that if
 (94)
 24Or else, if we extend the support of the function ln( ) by defining ln(0) =
 1, then we obtain = 0. This would imply f (!) = 0 for all ! such that g(!) > 0. Thus, since f must belong to C , the integral of lnf (!) over the remaining frequencies needs to be infinite. Since ln(x) < x;8x 2, this implies that kf k = 1 (infeasible) and D(f ) = 1.
 then  , the minimizer of	, needs to satisfy
 (95)
 We will first elaborate upon (95) to derive (28). Then we will prove that (94) holds.
  
 where and are as defined in (48). Application of the identity, [which follows from (48) and (49a)] to the numerator on the RHS of (96) yields
 (28).
 Now we will prove (94).
 The Sign of : Since , this limit needs to be analyzed for two possible scenarios, depending on whether or not is positive.
 The Case For this case, , so we need to prove that . From Proposition
 3 it follows that the first condition in Assumption 1
 (	) must necessarily hold in order to obtain . Thus, and its first derivatives are continuous. Therefore, in view of (93), we get
 .
 The Case	. For this case, we need to prove that
 . Rewrite (91) as
 (97)
 	From (92), it easy to see that	,	.
 	On the other hand, from (49d), and given that	and
 	, we conclude that	is bounded. Thus,	is
 bounded. From this, and recalling that
 , it is clear from (97) that there a value for	greater than under which	is small enough to render negative. Therefore,	.
 	The Sign of	: Substitution of (92) and
 (83) into (91) yields
  
  
 with
 (99a)
 (99b)
 (99c)
 (99d)
 Direct application of Theorem 7 to (98) allows one to conclude
 that
  
 are continuous functions, we have from (100) that
  (101)
 It follows directly from the last equation that the sign of
  
 see (99). From (49d), the LHS of (102) is lower bounded as
  
 E. Proof of Theorem 3
  
 wherein (49a) has been used. This proves the second claim in Theorem 3.
 Convexity: Differentiation of (103) yields
  
 RHS of (28) is a convex function, proving the first claim of the theorem.
 Limits: In order to show that the limits (29) and (30) in Theorem 3 hold, we write as
 (104)
 We will first prove the validity of . Clearly, if for all (condition i) of Assumption 1), then the RHS of the above equation tends to as
 . If this wasn’t the case, then the second condition of Assumption 1) must be satisfied, and therefore the conditions of
 Proposition 3 are met. Applying Proposition 3 and the fact that
 [see (81)], it follows that tends to   as . This proves the validity of (29).
 In order to show that [i.e., (30)] holds, we first note from (104) that for all . On the
 other hand, it follows from (49d) that
 (105)
 where	. Since	(as required by
 Assumption 1), the RHS of (105) clearly tends to 1 as  .
 Therefore,	.	 
 F. Proof of Theorem 4
 In view of Theorem 3, it suffices to proof the limits for  
  , respectively. The uniform convergence of
 From (28) and (38) we have
 H. Proof of Theorem 6 Applying (49c) to (37) one can write Since	is monotonically decreasing (see Theorem
 3), it follows that decreases with increasing . Since , this leads directly to
 	, where	is
 independent of	. Applying Theorem 5 to both sides of
 the latter inequality, we obtain
 	. Since	corresponds
 to the minimum FWMSE for a constant , by virtue of (42) we have that	. Substitution of this into the last inequality yields	.
 This completes the proof.
 
 ﻿Based on measurements for a street-canyon-type femto-cell we compare the downlink spectral efficiencies of various intra-cell interference mitigation systems. This includes Zero-Forcing (ZF), Regularized Zero-Forcing (RZF) and DirtyPaper Coding (DPC). As a reference for comparison we consider subsectorization and TDMA. Out-of-cell interference, treated as additive Gaussian noise, is included in two modes: a) all cells transmit simultaneously and b) neighboring cells transmit in alternated time slots. We find that ZF, RZF and DPC can offer increases in spectral efficiency over subsectorization and TDMA by factors of around 4 under interference-limited conditions. This contrasts with estimated gains of only around 2 when using a standard 3GPP mode instead of our measured data. This difference is due to the greater inter-cell isolation that characterizes our street-canyon-type test environment. We find that RZF achieves over 80% of the DPC’s spectral efficiency under virtually all operating conditions. 
 Index Terms—MIMO, multiuser, outdoor-to-indoor, femto-cell, inter-cell interference, spectral efficiency 
 EMTO-CELLS [1] are being deployed to address the explosive demand for higher capacity in wireless systems, allowing an increase in user density at locations where macrocells cannot cope with the expected traffic load. Spectral efficiencies can be further improved using some of the techniques we discuss below. 
 Multiple-Input Multiple-Output (MIMO) transmission techniques have received great attention. However, inter-user interference results in a low Signal-to-Interference-plus-NoiseRatio (SINR) and corresponding performance loss. This has led to the proposal of multiuser MIMO schemes (MU-MIMO), where coordinated transmission reduces or cancels intra-cell interference [2]–[5]. In multi-cell systems, inter-cell interference imposes additional limits on performance. Many schemes to reduce or avoid this interference have been proposed. For example, “almost blank subframes” (ABS) can avoid inter-cell interference [6] when a small cell (pico-cell or femto-cell) is located within a macro-cell to address a local traffic hotspot. 
 Massive MIMO [7], [8] uses a large number of antennas at the Base Station (BS) to mitigate intra-cell and intercell interference, with a considerable increase in complexity and higher backbone overhead to carry inter-cell Channel State Information (CSI). A far more elaborate scheme is the coordinated multipoint transmission, alternatively known as Network MIMO [9], where neighboring BSs cooperatively encode and decode messages for multiple simultaneous users. Although, in theory, such a scheme can produce very large increases in throughput, only limited gains have been achieved so far under realistics conditions. In [10] it is shown, based on simulations, that Network MIMO achieves no gains with respect to an ABS-based scheme if the larger overheads of the former are considered. 
 In this work we present an empirically based evaluation of pre-coding schemes for single antenna terminals, i.e., Multiple-Input Single-Output (MISO), concentrating on the case where only intra-cell CSI is available. 
 We discuss below relevant results. Pre-coding schemes can be classified into nonlinear or linear systems. Among the nonlinear schemes, Dirty-Paper Coding (DPC) [2] is a valuable capacity-achieving bound [11], [12], even though no practical implementations have been proposed. Simpler, suboptimal linear pre-coding schemes include Zero-Forcing (ZF) and Regularized Zero-Forcing (RZF). In ZF, intra-cell interference is eliminated by choosing a pre-coding matrix that diagonalizes the channel gain matrix and then allocates the user powers via the water-filling algorithm [13]. It has been shown that ZF can asymptotically achieve the theoretical sumcapacity limit when considering the case of a large number of users [14]. ZF performs poorly at low Signal-to-NoiseRatios (SNR) [4], when spectral efficiencies are noise- rather than interference-limited. This effect becomes particularly significant for ill-conditioned channel matrices. RZF [4] has been proposed to resolve this problem. In this scheme the matrix to be inverted includes noise-dependent terms that, at low SNR, compensate for the eigenvalue disparity of the channel matrix. 
 Simpler transmission schemes that need little or no CSI at the transmitter include subsectorization and Time-Division Multiple Access (TDMA). In subsectorization [15], intra-cell interference is reduced by separating the coverage sector into smaller subsectors through the use of directive antennas. The only feedback required is that resulting from the association of a user terminal to a subsector. Such a system, with no inter- 
 Fig. 1. Example of measured scenario for (a) the intra-cell campaign and (b) the inter-cell campaign. 
 user nulling capability, will be limited by intra-cell interference at high transmit power. 
 In TDMA, the transmission frame is divided into equalduration time slots, and each user is served at full power during one of those time slots. The capacity gain of DPC over TDMA for MIMO channels was reported in [16], but without including inter-cell interference. The bounds presented in that work are used as reference for our results. 
 The above techniques are especially important when outdoor femto-cells are intended to serve indoor users, yet these scenarios have only been studied relatively recently. Results on empirically-based outdoor-to-indoor channel characterizations are presented in [17]–[24], but these were not formulated in the context of MU-MIMO. Only a limited number of studies [25]– [29] include empirical data in the evaluation of multiuser MU-MIMO. In [25] ZFs spectral efficiency is evaluated for a distributed transmission system in an indoor environment, while [28] uses a similar setup (Network MIMO) to compare the spectral efficiency of several techniques, finding that ZF and DPC achieve about a three-fold increase in per user data rates over non-coordinated transmission and frequency division. In [26] the rate distribution among users is evaluated for an indoor MU-MIMO scenario when using time, frequency and spatial multiplexing. In [27] the aggregate capacity in a MU-MIMO system using ZF, RZF and DPC is presented and compared with the sum of individual capacities of various single-user MIMO channels in outdoor and indoor scenarios. Our work is related to the above studies but includes inter-cell interference and actual intra-cell channel measurements. 
 We evaluate the spectral efficiency of several MU-MISO techniques in outdoor-to-indoor scenarios with femto-cells placed along the streets, below the rooftop serving the buildings of one city block. This differs from the often-used assumption of hexagonal cells which is unrealistic in our deployment scenario. Pre-coding matrices are computed using measured intra-cell channels while out-of-cell interference is included as additive Gaussian noise, as discussed in [30], with power calculated using a measurement based model. We treat out-of-cell interference mitigation through two simple approaches. The mosat basic considers time-coordinated frames across femto-cells. We also include a simplified version of the method proposed in [8] where inter-cell interference is mitigated by adding BS antennas. In particular, we evaluated the achievable spectral efficiencies of DPC, ZF, RZF and subsectorization with 4 subsectors. The BS is assumed to serve 4 single-antenna users simultaneously. Transmitted power is varied to cover noise- and interference- limited conditions. Our measured outdoor-to-indoor scenario, described in the next section, differs from those previously reported for MU-MIMO: 
 In the following sections we describe the measurement setup and the statistical properties of our test environment. A summary of the various transmission strategies is included in Section IV. The empirical results and their comparison with those based on various channel models are presented in Section V, leading to the conclusions of our work. 
 Channel measurements were done at Universidad Tecnica´ Federico Santa Mar´ia’s (UTFSM) campus in Valpara´iso, Chile, and Universidad Diego Portales’ (UDP) campus in Santiago, Chile. Two measurement campaigns were carried out: one aimed at obtaining the channel matrices for the cell being served by the BS and another aimed at estimating the interference generated by neighboring femto-cells. 
 To obtain the channel matrices within the cell, we used a linear array to emulate an outdoor BS. It consists of 12 omnidirectional vertically polarized coaxial antennas antennas spaced half a wavelength apart, with a reflective backplane. With the backplane, each antenna has a maximum gain of 6 dB and a 120? half-power beamwidth. This array was placed along the street at a perpendicular distance from the building wall ranging from 10 to 25 m. The antenna corresponding to the possible Subscriber Unit (SU) positions was a single 1 dBi gain vertically polarized omnidirectional coaxial dipole antenna mounted on a 40-cm long swivel arm. This antenna was rotated step-wise in 6? increments to obtain local statistics. A 19.2 dBm, 3.5 GHz Continuous Wave (CW) signal is transmitted through this antenna. For convenience, we measured the propagation channels using uplink transmission. 
 SU placements  were chosen inside buildings in rooms with outside facing windows sized 1.5 m by 2.5 m, separated from the outdoor array by distances varying between 18 and 53 meters. The BS array antennas were located at 2.1 meters above ground level, i.e. a height that would allow easy installation by a service provider aiming to illuminate homes or buildings that are across the street. The indoor SU antenna was placed at desktop height on a first floor, i.e, at roughly the same height as the BS array. The perpendicular distance from the exterior wall to the SU varies from 1 to 7 m. The SU placement may be consistent with an access point installed in a street-side room, which can also act as an indoor wireless router. 
 A schematic representation of one measured scenario is shown in Fig. 1(a), where the points inside rooms are the positions of the SU and the bar indicates the position of the BS array. Placing the BS array at various positions along the direction of the street allowed measuring channels to users at angular positions of up to 60? off array boresight. The buildings are made of reinforced concrete and the nonmetallized glass windows have steel and aluminum frames. The measurements were carried out during the day in the absence of pedestrian traffic, to ensure static channel conditions. A shadow antenna (not used in measurements) is included at each end of the BS array to preserve the impedance due to mutual coupling in all the receiving antennas. A reference antenna in line with the others, at a distance of about 65 cm from its closest shadow antenna, allows measuring the relative phases of the array antennas. A system of switches connects any one of three sets of four antennas to a 4-channel receiver, while the reference signal is permanently connected to the remaining channel. The receiver downconverts the carrier frequency to 10 kHz and these signals are fed to a 4-channel data-acquisition system. A computer is utilized to automatically control the positions of the swivel arm, the switching and the data acquisition. GPS disciplined oscillators synchronize the transmitter and the 4-channel receiver for phase coherence. Fig. 2 shows a schematic of the measurement system. The signals are sampled for 250 ms at a rate of 
 50,000 samples/s per channel (200,000 samples/s total). This interval is partitioned into 25 contiguous sub-intervals of 10 ms per position of the swivel arm. Using a Fast Fourier Transform (FFT) algorithm, a value of magnitude and phase is obtained per sub-interval. Due to exact synchronization of sampling intervals, the data sequences correspond to an integer number of periods of the 4 sinewave outputs, allowing the FFT processing without windowing. The existence of 25 consecutive measurements for any transmit/receive combination of antennas allows verification of proper equipment operation and the reduction of residual measurement noise through averaging. We measured a total of 400 combinations of placements of the outdoor and indoor antennas, i.e. 400 60-by-12 matrices. 
 As already mentioned, in our study we have not considered joint pre-coding among BSs. Instead we treat the out-of-cell interference as noise. To estimate this interference power we measured path-losses over a wide range of positions since it is not a-priori obvious which will cause significant interference. The sounding system was basically the same of the first campaign, except that at the BS end we used a single antenna, since only the scalar path-loss is required. We proceeded as follows. Over distances extending from 15 m to 500 m we measured the outdoor-to-outdoor path-loss. This included same-street measurements as well as “around-the-corner” measurements in the Manhattan grid type street canyons. For a subset of these measurements we also determined the path-loss from the BS to the indoor location that matches the outdoor position as shown in Fig. 1 (b).This allowed us to model the “outdoor-toindoor penetration loss”. We describe this in further detail in the next section. We found that the outdoor-to-indoor pathloss statistics can be modeled quite accurately by adding a “penetration loss” to the corresponding outdoor-to-outdoor path-loss, as reported in [31]. 
 The empirically based channel data allowed us to obtain the channel gain statistics and models described in the following section. From these we also assessed and compared the achievable spectral efficiencies for the various systems. These results are subsequently compared to those that would be obtained with various models. This is described in detail in Section V. 
 In this section we statistically characterize the propagation channel for the chosen environment. As will be seen, our results are consistent with those reported in the literature for similar settings, which implies that we have chosen scenarios that are representative for urban femto-cells. 
 To model this loss we used both intra-cell and inter-cell measurements to compare averaged received power (in dBm) for locations separated by the exterior wall. The average power was obtained from the 60 measurements of the swivel arm, and thus excludes the small-scale fades. The results are shown in 
 Fig. 3. (a) Measured penetration loss and proposed fit vs. 3GPP model. (b) Outdoor-to-indoor path-loss at long ranges. 
 Fig. 3. Fig. 3 (a) shows the dB difference in measured outdoor and indoor powers as a function of the incidence angle ? defined in Fig. 1. For comparison we show the mean penetration loss of the 3GPP model. We adjusted a modified version of the 3GPP model given by (1) to our data, using a Minimum Mean Square Error (MMSE) criterion to obtain the best-fit parameters for the average penetration loss ?(?). These are listed in Table I 
 where Xs is a zero-mean random variable, which we found to match a Gaussian distribution. Its standard deviation was found to be angle-dependent, with values of 4.5 dB for intracell links growing to 7.2 dB as ? exceeds ?H. The latter angles correspond to those of inter-cell interference links in our street canyon geometry. For intra-cell links the average penetration loss is essentially constant at 6.4 dB, much less than predicted by the 3GPP model. In Fig. 3 (b) we show measured pathlosses for outdoor-to-outdoor and outdoor-to-indoor links at ranges relevant for inter-cell interference. As will be discussed in subsection C, for ranges up to 100 m the average outdoorto-outdoor path loss for a linear fit model has a slope only slightly larger than that of free-space links. The outdoor-toindoor average path-loss was found to have the same slope, but as shown, its values are offset by an average penetration loss of 23.5 dB, consistent with (1). 
 Intra-cell links were in general Line Of Sight (LOS) through a window, with minor obstructions in the path. This is often referred to as Obstructed Line Of Sight (OLOS) [32]. The large-scale path-loss is calculated at each transmit/receive placement, by again averaging the received power of the 60 positions of the swivel arm. Using these values we fit a classical log-distance model [33] to the path-loss PL in dB as: 
 where PL0 is the path-loss at the reference distance d0 = 1 m, ?1 is the path-loss exponent and Xs is the shadow fading term, a zero-mean Gaussian random variable with standard deviation s. It is assumed that . The value of ?1 is found through a MMSE regression fit of the 400 empirical values of path-loss. The values of PL0, ?1 and s thus obtained are shown in Table II. We tested various other alternatives. This included the free-space path-loss model (i.e. ?) plus the average intra-cell penetration loss from (1), a two-slope model and an exponential model based on IEEE802.16j Type-F BRTto-BRT [34].The improvement of the fit in terms of MMSE is negligible. Fig. 3 illustrate the comparison. In the same figure we also included for reference the line that corresponds to the free-space path-loss and that predicted by the 3GPP model for the outdoor-to-indoor case in urban micro-cells with a Manhattan-grid layout [35]. As will be discussed later, we found that the smaller intra-cell penetration loss with respect to the 3GPP model had a significant impact on the achievable spectral efficiencies. We observed a small proportion (less than 19%) of path-losses that are lower than those in freespace. This is likely due to ducting effects in our street canyon environment, particularly at shallow incidence angles. 
 Based on our inter-cell measurement campaign, we formulated an outdoor-to-indoor street canyon path-loss model in order to evaluate the inter-cell interference from neighboring femto-cells. We firstly found that a two-slope model accurately 
 Fig. 4. Single slope fit and exponential fit for the measured intra-cell pathloss in function of the distance, in logarithmic scale. For comparison, free space and 3GPP models are shown. 
 where the PLout is the outdoor-to-outdoor path-loss, dBP is the “break point distance” defining the change in slope and PLBP is the corresponding path-loss. The outdoor-to-indoor inter-cell path-losses were found to be well described by the sum of the average penetration and path-losses from (1) and (3), respectively, plus a shadow fade term Xs with 7.2 dB standard deviation. We note that for the angles of incidence that will characterize most inter-cell interference, the average penetration loss is 23.5 dB. This is consistent with what has been reported in the literature [36]. Table II summarizes the estimated values of the model parameters. 
 Through the same outdoor-to-outdoor street canyon measurements we found that Non Line of Sight (NLOS) aroundthe-corner links show a path-loss that is at least 15 dB above that of LOS links for the same distance, which is consistent with [35]. Thus, the interference generated by around-thecorner femto-cells is negligible compared to that from neighboring same-street bases. For this reason, the former are not considered in the performance analysis of Section V. Our intercell path-loss model does not differ very significantly from previously reported results [35] and we therefore omit further details. 
 The small-scale fading statistics for the intra-cell links, from any BS array element to the SU, were obtained from the 60 complex channel gain measurements corresponding to a rotation of the swivel arm, normalized to their local spatial average. As expected [1] for LOS links rich in multi-path components, we found that a Ricean distribution provides a good fit for the channel gain envelope statistics. The corresponding K-factor was estimated using the two-moment method [37]. The average percentage of (absolute value) error in the fit of the empirical and model Cumulative Distribution 
 Functions (CDF) was 6.7%. Higher errors were obtained when fitting other commonly used small-scale fade distributions (Nakagami, Weibull). To account for possible differences due to the separation of the 12 array elements, we estimated the fade statistics per element and subsequently averaged the individual K-factors. These were found to be similar, the differences being explained by the limited data set size. Following [38], [39], we adjusted a log-normal distribution to the observed K-factor statistics, i.e. Krice ~ Lognormal(µ,s), with its estimated parameters shown in Table III. 
 The correlation between the powers received by the different array elements of the serving BS was calculated using all intracell measurements. To this effect we normalized each 60-by12 channel matrix by its Frobenius norm, in order to combine the data corresponding to locations with different large-scale path-losses. We initially adjusted a single-exponential model to the empirically-obtained values as done in [40], but found that a double exponential model provided a much better fit, which significantly improved the accuracy in the prediction of the spectral efficiencies. For example, for the ZF case, the estimation error of the capacity with the single-exponential model is 7%, which drops to 1% when using a doubleexponential. The general expression for both cases is given by (4) 
 where ?j is the correlation between antennas separated by a spacing of j, i.e. j = 1 corresponds to adjoining antennas and j = M-1, with M being the number of antennas in the array. The corresponding model parameters A, B, r1 and r2 were obtained by a least squares fit, and are listed in Table IV. 
 with hLOS being the LOS component of the matrix, hLOS = exp(j2pd?msin?), where d? is the distance between adjoining antennas expressed in wavelengths ?, m = [0,1,2,...,M - 1] is a vector and ? is the steering angle of the LOS signal with respect to the boresight of the array. In our simulation-based results this angle was generated as a random variable with a uniform distribution over +/-60? to cover the range of possible user positions. hNLOS is a vector of circularly-symmetric zero-mean complex Gaussian random variables with unit variance, correlated following the procedure described in [42]. To this effect we used a 
 For the inter-cell propagation model all links are NLOS, due to the shallow angle of incidence between the neighboring BS and the indoor SU. We found that the small-scale fades in a neighborhood around the SU antenna are very close to Rayleigh distributed. This matches the expected results in scenarios without a dominant component [1]. The estimated K-factor using the two-moment method was essentially zero. Empirical CDFs were found on average to differ from a Rayleigh distribution by no more than 10% (absolute value). However as will be discussed in greater detail in Section V, including small-scale fades in the interference calculation was found to have little effect on the resulting spectral efficiencies. We confirmed this for several cases and subsequently omitted small-scale fades, which very considerably simplifies the calculations. 
 When comparing our measurement-based results with the above model we proceed as follows: 
 Step 1: Choose a distance d and a LOS angle theta from the BS to the SU. The distance must be chosen in the range from 15 to 55 meters, and the angles in the range within +/60? with respect to the boresight of the BS array. Also choose the number of antennas M of the BS. 
 Step 2: Calculate the shadow fading and path-loss PL using (2) and the intra-cell parameters of Table II. 
 Step 3: Generate Krice using a log-normal distributed random variable with parameters as in Table III. 
 Step 4: Generate the correlation matrix using (4) and the parameters of Table IV. 
 Step 5: Calculate hNLOS by generating a M-by-1 matrix of i.i.d. circularly-symmetric zero-mean complex Gaussian random variables with unit variance and correlate them using the correlation matrix and the procedure detailed in [42]. 
 Step 6: Generate the hLOS matrix using the chosen angle ? and the appropriate value of d?. 
 Step 7: Calculate h. Add the previously generated path-loss according to generate the channel matrix hsim to 
 After generating a MISO channel matrix, the interference generated by the neighboring BS femto-cells to that SU is estimated by the following steps: 
 Step 8: Determine the distance dneigh between the neighboring BS and the street position directly across from the indoor SU. 
 Step 9: Choose the Sum-Power Constraint (SPC) and an azimuthal gain pattern G for the neighboring BS in the direction of the street position of the previous step. 
 Step 10: Calculate the shadow fading and path-loss to the indoor SU using (1) and (3) with the inter-cell parameters from Table II, the penetration loss parameters from Table I and the distance calculated in Step 8. 
 where G(?neigh) = sin(??neigh)/??neigh models the gain of the antenna array, ? is a factor to adjust the width of the lobe and ?neigh is the direct path angle to the street position across from the indoor SU,  . In order to mantain the same parameters as in our measurements, the 3 dB lobe width is set at 120?, with ? = 1.32. P is the total transmitted power and PL(dneigh) is the path-loss between the femto-cell and the user, which is dependent on the distance d as described by (3). 
 We compute the per-user spectral efficiency (aggregate spectral efficiency/number of users) for several techniques involving a MIMO broadcast (MIMO-BC) channel in the downlink. We assume a narrowband transmission by the BSs array equipped with M antennas. There are K SUs receiving the signal each with a single-antenna. Thus, there exists an M-by-1 MISO channel, characterized by the column vector hk ? CM×1, between the transmitter and the k-th user, with k = 1,...,K. The MIMO-BC is formed by aggregating the K channel row vectors into a channel matrix H of dimension 
 where the random vector x ? CM is the transmitted signal, n ? CK is the random noise at the receiver and v ? CK is the total inter-cell interference reaching the receiver. The transmitted signal is expressed as 
 where u is an independent zero-mean complex Gaussian random vector, W ? CM×K is a linear weigth matrix and T : CK ? CK is a non-linear transformation in the case of DPC, and the identity matrix in the case of ZF and RZF. 
 In all the tested techniques, a SPC is imposed on the transmitted power as done in [14]. This is written as: 
 where Tr[·] is the trace of the matrix, wi is the i-th column of W, P = 0 is the maximum power and pi is 
 with ui being the i-th element of u and [·]*, the complex conjugate operator. The power allocated to each user then is pk||wk||2, for k = 1,...,K. 
 For our comparison we chose a receiver noise variance sn2 set at -101 dBm, which corresponds to a bandwidth of 2 MHz and a receiver noise-figure of 10 dB. As already discussed, the interference from neighboring cells is treated as additive 
 is the sum of the interference powers from all neighboring cells. The procedure for the calculation of  will be described in the next Section. We note that as the transmit power grows,   will do so in the same proportion, which will lead to the saturation of the achievable spectral efficiencies. For our analysis we assume a 4-antenna base and up to 4 users. The channel matrices will thus be 4-by-4, i.e. M = K = 4. When encoding, the BS has knowledge of the equivalent noise N at each SU. If more antennas were available at the SU, treating interference as noise would be pessimistic in that it ignores the possibility of exploiting the interference signal structure in a multiuser detection (MUD) type receiver [43]. We now briefly describe the various downlink transmit strategies considered. 
 This scheme is included as baseline against which to assess the advantages of the multiuser schemes described below. Each user is served during one of the L = K time slots with power P. We note that in this scheme we are only separating users into time slots within a cell, cell coordination being treated separately. The TDMA achievable spectral efficiency is therefore 
 In [16] a lower bound for TDMA spectral efficiency is proposed, which is tight for MISO channels. This bound is achieved when the best user is served all the time with power P 
 This is the capacity achieving technique and it is to be considered as an upper bound of actual performance, given that its implementation is complex and to date there is no cost-effective algorithm to implement it. 
 In DPC, there is no a-priori choice for the matrix W, and full CSI is required at the transmitter. The pre-coding algorithm uses this knowledge to avoid, for the k-th user, the (known) interference produced by the signals intended to the users 1,2,...,k - 1 (namely, the “previous” users). In doing so, the block represented by WTu generates a transmit vector x being the sum of K independent random vectors , as K 
 where x¯i ,wiTu is the information bearing vector intended to the i-th user. Defining the covariance matrices Si , 
 , the maximization problem for the DPC spectral efficiency is as stated in [44]: 
 An efficient numerical method to find the solution to this problem is proposed in [44], exploiting the duality between the MIMO-BC and the MIMO multiple access channel. 
 This pre-coding scheme chooses the weight matrix W to cancel the interference in the downlink. Thus, W is, in general, the pseudo-inverse of the channel matrix (which is assumed full rank): 
 In particular, when K = M and H is non-singular, W is W = H-1. The spectral efficiency is therefore calculated as [45] 
 Powers are allocated to each user weighting the columns of W using the water-filling solution [45]. 
 Although the simplicity in the selection of the matrix W makes this scheme attractive, ZF presents poor performance if the channel matrix is ill-conditioned. In these cases, usually one or a few eigenvalues of the matrix are very large compared to the rest. At low transmit powers, where spectral efficiencies are limited by noise rather than by interference, the waterfilling solution (which maximizes sum rate) may result in some users being allocated a zero data rate [28]. 
 RZF was proposed to correct the problem suffered by ZF in presence of ill-conditioned H matrices. A regularization factor is introduced in the pseudo-inverse of H, which aims to normalize the eigenvalues of the matrix, allocate power to all users and avoid noise enhancement [46]. In this case, W is chosen as 
 with a being the regularization factor. In [4] it is shown that the optimum a for a large number of users K is 
 with ? = P/N. Despite not being the optimal value for a finite K, for simplicity we will use this value of a. As W does not exactly invert the H matrix, intra-cell interference will now affect users. Thus, the spectral efficiency is calculated as 
 In (19), the term Pj6=i pj||hHi wj||2 represents the total interference reaching the i-th user. Powers are allocated by water-filling, using an iterative algorithm that updates the powers allocated in each iteration considering the intra-cell interference generated. 
 In this scheme the sector is divided into Q smaller angular subsectors of the same width. In practice, this can be done using a directive antenna per subsector. To simulate the effect of using various types of directive antennas in the subsectors we took advantage of our channel measurements with the 12antenna array. This allowed us to generate antenna patterns based on using between 4 and 12 dipole elements. We note that this involves using steering vectors with appropriate weights, which, however, remain fixed, i.e. which cannot be adaptively aimed according to user positions. We define the antenna weight matrix as D. The available power is distributed equally between subsectors and it is assumed that one user is associated with each of them. Then pi = P/K, i = 1,...,K. The spectral efficiency of subsectorization is thus calculated as 
 In this Section we present the main results of our work. The general conditions under which we compared the various systems are discussed in sub-section A. In subsection B we present the achievable spectral efficiencies based on the measured data. This includes subsectorization, discussed in B.1 and the various intra-cell interference avoidance systems discussed in B.2. Finally, in subsection C we evaluate the spectral efficiencies using channel matrices generated by various simulation models. This includes 3GPP models, a Rayleigh i.i.d. model for intra-cell small-scale fades and our own model described in Section III. These results are compared with those obtained in Subsection B.2. 
 Using our empirical data and models we now evaluate the per-user spectral efficiency (aggregate spectral efficiency/number of users) of DPC, ZF, RZF, subsectorization and TDMA for a 5-femto-cell system disposed along a street, as shown in Fig. 5. As a basis for comparison we considered BSs with 4 antennas. In all cases we have considered perfect CSI knowledge of intra-cell channels at the serving BS and a sum power constraint (SPC) with a range of realistic values for the transmit power. In order to establish a baseline for our comparison, we also calculate, as in [16], the upper bound for DPC capacity, the spectral efficiency for TDMA with equal time allocation to users and the achievable TDMA spectral efficiency when the sum rate is optimized. To avoid ambiguities, we will henceforth use the term “intra-cell TDMA” when referring to the system where the time division multiple access occurs only within the cell, in contrast to the case where the cells coordinate among each other, which we denominate time reuse. 
 Although our treatment focuses on intra-cell interference cancellation, for RZF we have additionally included, based on [8], the case where extra BS antennas allow mitigation of inter-cell interference. We chose a very simple scheme in which the BSs have 12 antennas and the extra degrees of freedom are used to steer nulls towards the 8 users being served in the 2 adjacent cells. Given our street-canyon geometry this is likely to eliminate the strongest sources of interference. We treat 2 cases. Firstly we obtain the achievable performance when full CSI is available across 3 contiguous cells. We call this system RZF-FCSI. Alternatively, we consider the case where the information exchanged between cells is limited to channel-correlation matrices. Although this is less effective, it will not require such frequent updating, putting less burden on the backhaul network [8]. We denominate this as RZF-PCSI. 
 The 5-femto-cell system arranged along a street, as shown in Fig. 4 considers BSs separated by 100 m. Each femto-cell BS is assumed to be placed close to the building facade in the middle of the block and provides service to the users across the street. This closely matches our measurement scenario. The spectral efficiencies are evaluated for the users in the central femto-cell. We found that adding more interfering cells in the system did not impact our results. 
 Fig. 5. Topology scheme of the system. Each femto-cells BS (red bars) points towards the city block across the street. 
 The evaluation of the achievable rates will consider two approaches: no coordination between femto-cells (reuse 1), and a coordination scheme that involves time slots (reuse 2), where adjacent femto-cells transmit in alternate time slots to eliminate the strongest source of inter-cell interference. For the case of reuse 2, the achievable rates described in Section IV are appropriately scaled. 
 At each scenario such as depicted in Fig. 1 we measured using at least 1 placement of the BS array and 9 placements of the rotating dipole that corresponds to possible SU positions. Each such combination yields a 60-by-12 channel matrix, where each row corresponds to a possible user position. Since our work considers a 4-antenna BS array, multiple choices of adjoining antennas in the 12-antenna array can be selected. Out of this ensemble we used a subset of 800 realizations for each BS placement to obtain the statistical description of the channel spectral efficiencies. We found that further increases in the number of realizations resulted in negligible changes of the results. Consistent with 3GPP Release 10 (LTEAdvanced) [47], Rev 10, we grouped users, dividing the 120? coverage region into 4 30? subsectors. We further assume that each subsector is illuminated by a pilot-beam of a different frequency generated using the 4 BS antennas. The corresponding phase shifts are chosen so that in free space the beams would be steered in the direction of the center of the corresponding subsector. A Dolph-Chebyshev [48] power distribution with 14 dB sidelobe-level suppression was assumed. This choice was based on the fact that this level of sidelobe suppression yields good results for the subsectorization case, as will be described below. Using these 4 beams and the measured channel gains corresponding to each user position, we associated the users to the subsector that provides them with the highest power, thus creating 4 disjoint groups. One user is randomly chosen from each group, leading to one possible realization of the 4-by-4 channel matrix. There are however scenarios where no user associates with a subsector, because their positions are such that all receive more power from one of the other subsectors. When this happens, the number of users K is limited to the number of occupied subsectors. Given the extensive set of available measurements, this was found to occur only rarely. In such cases we considered a K-by-4 channel matrix. 
 The per-user spectral efficiency of the techniques is obtained for each K-by-4 matrix, for a given SPC dividing the sumcapacity by the number of users. The results from all scenarios were then aggregated to generate CDFs of spectral efficiency for all systems. The total number of channel matrices used in obtaining our statistical results was 19,200 (24 BS placements x 800 realizations per BS placement). 
 In the case of intra-cell TDMA, the transmitted power is the same as the SPC of the multiuser techniques and the aggregate spectral efficiency obtained from (10) is divided by the number of users, to obtain the per-user spectral efficiency. 
 We found that the interference from neighboring cells, treated as additive Gaussian noise, could be included using only the scalar path-loss model described in Section III, i.e. neglecting the effect of the small-scale fades. This very considerably simplified and shortened the calculations. To validate this approach, we firstly calculated for a subset of cases, the spectral efficiencies for DPC, ZF and RZF with the small-scale fades included. For each BS placement we considered 160 randomly chosen channel realizations. Including the fades for ZF and RZF is straightforward as it only requires modifying (17) and (20) to take into account that the interference is dependent on the small-scale fade affecting each user. For DPC including small-scale fades is considerably more complex and leads to much longer calculations. Following the reasoning in [49] (Chapter 12.3, page 447), it is necessary to obtain the covariance matrices of the neighboring cells performing DPC in order to calculate the inter-cell interference they generate (as described in Section IV.B). These covariance matrices are obtained using a method proposed in [44]. Then, the received power from one neighboring cell is calculated as 
 where hneigh is the channel vector between the neighboring cell and the user in the central cell and Sineigh is the covariance matrix for the i-th user in the neighboring cell. We found that for ZF and RZF, neglecting small-scale fades resulted on average in overestimating the spectral efficiency by less than 8%. For the case of DPC his difference was less than 2%. Based on this finding we used the simplified approach that ignores small-scale fades and calculated the equivalent noise power due to interference  as: 
 where Pneighi is the interference power due to the i-th neighboring cell, calculated as in (6), i.e. omitting small-scale fades. 
 1) Optimization of Subsectorization for the Single-Cell Case: We evaluate as a reference the spectral efficiencies of subsectorization when considering only intra-cell interference. As will be seen later, for subsectorization this type of interference completely overshadows that from neighboring cells. Since this dominant intra-cell interference depends on the shape of the radiation patterns that cover the cell, our initial objective is to evaluate this aspect. We found that generating 4 subsectors with only 4 omnidirectional antennas is far from optimum and thus we evaluated other options that allowed more flexibility in selecting beamwidth and sidelobe 
 Fig. 6. Per-user spectral efficiency of subsectorization vs. -3 dB beamwidth, for different number of antennas in the array, at 50% availability. 
 suppression. Both beamwidth and sidelobe suppression will affect the interference that the SU is exposed to. Narrowing the beam pattern will eventually lead to the sector not being fully covered by the mainlobe and at the same time the appearance of poorly suppressed sidelobes, which will contribute to the interference in the neighboring subsectors. At the other extreme, very large suppression of sidelobes results in an increased beamwidth which may invade the neighboring subsectors. For our measurement scenarios, we found that a sidelobe suppression in excess of 25 dB produced virtually no further benefits with regard to the reduction of interference. The reason for this is that at a certain point the energy scattered from the mainlobe of a subsector into the adjoining region dominates over that due to the sidelobes. It follows from the above that for any antenna array there will be an optimum beamwidth. To observe this effect, we evaluated subsectorization using between 4 and 12 omnidirectional antenna elements to synthesize radiation patterns with sidelobe suppression levels ranging from 5 dB to 80 dB. The transmitted power was set at 20 dBm, a power level that assured interference-limited operation. 
 Fig. 6 shows the results. We have plotted the median per user spectral efficiency for various choices of directive antennas. In this and in subsequent results we will refer to the spectral efficiency available to X% of users as the “spectral efficiency at X% availability”. We note that the independent variable chosen for Fig. 6 is the -3 dB antenna array beamwidth, which is actually a result of the choice of sidelobe suppression and number of antenna elements used in synthesizing the directive array. As can be seen, using more antennas to synthesize the beams allows the achievement of higher spectral efficiencies as it adds degrees of freedom in the reduction of interference. In Table V we show results for the median spectral efficiency that can be obtained in our scenarios, using the optimum choice of beamwidth for various types of directive arrays. We include the relevant parameters that characterize the antenna arrays in free space and the attenuation of the mainlobe at the sector border. As seen, under interference limited conditions the mainlobe width must be made much narrower than the sector width to achieve the best possible spectral efficiency. 
 2) Spectral Efficiency of Intra-Cell Interference-Reduction Systems: We now evaluate the per-user spectral efficiency of the interference-reduction techniques in the non-coordinated 
 femto-cell system, varying the power limit from -30 dBm to 30 dBm in steps of 5 dB. From the CDFs of the spectral efficiencies we then obtain the spectral efficiencies available to 90% and 50% of users. Fig. 7 shows the results. For reference we include subsectorization (referred to as SubS) and intra-cell TDMA systems, both considering inter-cell interference. For subsectorization we used the best possible radiation patterns for both 4 and 12 antenna systems as previously presented in Table V. When the BS is limited to 4 antennas, the theoretical capacity aciheving DPC upper bounds all other techniques at any power level and any availability. However, ZF and RZF can achieve an important proportion of DPCs spectral efficiency. In fact, ZF reaches 88% and 95% at 30 dBm for 90% and 50% availability respectively, while RZF achieves 92% and 95%. The improvement of RZF over ZF, particularly at 90% availability, is attributable to RZF dealing effectively with poorly conditioned channel matrices and high intercell interference. As we will see later, these results differ significantly from those obtained through simulation using a 3GPP model with default parameters. Our results predict spectral efficiencies much higher than those obtained from that model. 
 The comparison with the case where the BSs are assumed to have 12 antennas (RZF-FCSI and RZF-PCSI) is shown in Fig. 8. We here repeated the results of RZF and DPC using 4 antennas to contrast them with RZF using 12 antennas. For the latter we considered systems with and without inter-cell interference mitigation. Improved performance at the expense of added transmit channels is of course present in the noiselimited region. However the more significant gains occur as the transmit power reaches levels where the spectral efficiency of the systems without inter-cell interference cancellation approach saturation (around 10 dBm). Full CSI exchange across cells (RZF-FCSI) will of course be best and in our case requires a relatively modest amount of information exchange. Using only the channel correlation matrices, which may require less frequent CSI updating, offers more limited benefits. 
 Subsectorization shows a very clear saturation for much lower powers than the interference- cancelling systems. The reason for this is that with growing transmit power, for subsectorization intra-cell interference limits spectral efficiency before the effect of neighboring cells sets in. This is illustrated by the fact that the saturation levels for these systems match the best values presented in Fig. 6, where no outof-cell interference was included. As a result of this intracell interference, subsectorization will only achieve spectral efficiencies comparable to the interference-cancelling schemes in the noise limited case, i.e., for low transmit powers. In 
 Fig. 7. Per-user spectral efficiency vs. power constraint of various techniques for a) 90% availability and b) 50% availability. 
 Fig. 8. Per-user spectral efficiency vs. power constraint of inter-cell interference mitigation techniques for 50% availability. RZF with 4 and 12 antennas and DPC with 4 antennas are shown for comparison. 
 the saturation region it performs no better than intra-cell TDMA, which is particularly simple to implement. In this region, subsectorization achieves around 25% of RZFs spectral efficiency and 23% of DPCs spectral efficiency for 90% and 50% availability. 
 As seen in Fig. 7, the DPC bound proposed in [16] considerably overestimates its actual spectral efficiency, which is 65% and 71% of the bound at 90% and 50% availability, respectively. The spectral efficiency gain of DPC over intracell TDMA is characterized in [16] without taking into account inter-cell interference. It is shown that this gain tends to one for low SNR and is bounded by min(M,K) = 4 for high SNR. This gain is shown for our data in Fig. 9 for 90%, 50% and 10% availability as a function of the SPC. We have plotted the spectral efficiency gains for our multi-cell system and, for comparison with the results in [16], for the case when the neighboring cells are turned off, referred to as “single-cell”. As seen, the DPC spectral efficiency is always above that of intra-cell TDMA, even for low powers. For high power, the gain asymptotically approaches the bound in the single-cell case, but saturates well below this value for the multi-cell case. 
 Since inter-cell interference limits the asymptotic spectral efficiency, we compared the performance of the above reuse 1 system with the reuse 2 system, considering only the case where the BSs have 4 antennas. Fig. 10 shows this comparison 
 Fig. 9. Spectal efficiency gain of DPC over intra-cell TDMA for 90%, 50% and 10% availability in the single-cell and multi-cell scenarios, in function of the SPC. 
 for DPC, RZF and 4-antenna subsectorization. Under our conditions, reuse 2 performs worse than reuse 1 for 90% and 50% availability. In the subsectorization case, which is intra-cell interference limited, reuse 2 results in a net loss of about half the spectral efficiency at all powers. It was found that for our data reuse 2 only presents an advantage over reuse 1 for 96% availability and above in the case of DPC. The reason for the advantage at high availability is that in such cases the spectral efficiency is limited by a small number of poorly placed users, subject to particularly high inter-cell interference. In contrast, for the rest of availabilities, the increased SINR of reuse 2 will not offset the disadvantage of transmitting only half the time. Similar results have been found for hexagonal macro cellular systems with techniques such as frequency reuse, fractional frequency reuse etc., where it is also found that coordination essentially helps those few users located at the boundaries between cells experiencing the strongest interference [50]. The proportion of such locations is even smaller in our linear layout. 
 We also evaluated the effect of the inter-cell interference on the spectral efficiency of the users most exposed to it. For this purpose we grouped the users into “edge users” (placed in the external 30? subsectors) and “central users” (placed in the 2 central subsectors). We compare their per-user spectral efficiencies by plotting the Complementary CDF (CCDF) for RZF at 30 dBm of SPC. We evaluated this for reuse 1 and reuse 2. The results are shown in Fig. 11. As seen, in both 
 Fig. 10.	Comparison between reuse 1 and reuse 2 transmissions for the per-user spectral efficiency vs. power constraint of DPC, RZF and 4-antenna 
 Fig. 11. Comparison of CCDFs of the per user spectral efficiency of RZF for 30 dBm, for edge users (red) and center users (black) when using reuse 1 and reuse 2. 
 cases reuse 1 is on average a better option than reuse 2. We illustrate this effect for RZF, as due to the form in which spectral efficiencies are obtained for DPC, it is not possible to obtain the individual per-user values separately. 
 In order to test the sensibility of the results to the level of inter-cell interference, we repeated the above calculations assuming penetration losses of 20 dB and 16.5 dB at values of ? exceeding 73.7?. As expected, this results in lower spectral efficiencies and the interference-limited zone starting at a slightly lower transmit power. We show this for DPC and RZF in Fig. 12. In particular, for a 20 dB maximum penetration loss at 90% availability, DPC and RZF come within 10% of the saturation spectral efficiency at powers 3.2 dB and 2.4 dB lower than in the 23.5 dB penetration loss cases. The corresponding values for 16.5 dB penetration loss are 5.9 dB and 5.5 dB. Not surprisingly, the benefit of using reuse 2 increases as the penetration loss diminishes. In fact we found that at 90% availability, reuse 2 will perform on par with reuse 1 in the saturation region when the penetration loss is 20 dB. At the same availability, when the penetration loss is 16.5 dB, the spectral efficiencies of reuse 2 at saturation are around 10% higher than those of reuse 1 for both DPC and RZF. However, reuse 2 is still worse than reuse 1 at 50% availability. 
 Finally we compared our empirically based results with those that are obtained using 4 different simulation models. This includes: the model based on our empirical data as discussed in Section III, a model based on our observed pathloss (see Eq. (2) and (3)) combined with small-scale Rayleigh i.i.d. fades and two models based on the 3GPP standards. We compare the per-user spectral efficiency at 50% availability vs. power constraint for DPC and subsectorization under the same conditions used to obtain the results shown in Fig. 7. The simulated results are obtained replicating the empirical setup but using the various channel models to generate the gain matrices. 20 placements for the BS array were simulated, each with a distance between the BS and the center of the room determined by a uniform random distribution in the range from 15 to 60 meters. For each simulated BS placement a set of 4 users (one per subsector) is created, all within a range of +/2 meters with respect to the previously chosen distance. 800 realizations are evaluated per BS array placement. The results are shown in Fig. 13. 
 The 3GPP model considered is described in detail in [35]. We started by evaluating the 3GPP model with the outdoorto-indoor urban microcell default parameters. We used these parameters for both the cell being served and for the interfering neighboring cells. This is denoted as the “3GPP default” model in Fig. 13. As seen, this did not yield a good prediction, as it underestimates spectral efficiency by a large margin (around 40% of empirical spectral efficiencies). We thus introduce a modified version that we refer to as “modified 3GPP”, which better fits our measured results. This differs from the default model in that for the central femto-cell (i.e. the cell being served) we use the 3GPP outdoor-to-outdoor LOS model to calculate the path-loss, which we combine with outdoor-toindoor small-scale default parameters. For the neighboring femto-cells we use as before the outdoor-to-indoor urban microcell scenario as formulated in the model. This choice was motivated by our empirical data. As discussed in Section III,the observed intra-cell path-loss for the outdoor-to-indoor scenario was close to that of a LOS link, but at the same time the K-factors were quite small, typically below one. Thus 
 Fig. 12. Effect of maximum penetration loss on the per-user spectral efficiency of DPC and RZF for 90% availability. 
 despite an “excess path loss” that is much smaller than that of the 3GPP model, the scenario is still characterized by very rich multipath propagation. The resulting modified 3GPP model achieves a better prediction with an error of 18% for DPC. 
 The two models based on our empirical data result as would be expected in the best match and provide added insight into the effect of the channel. The model that only uses empirically based path-loss combined with Rayleigh i.i.d. small-scale fades overestimates spectral efficiency, but the error is in general not very large. For DPC, it is 7% in the saturation region. Our model including the empiricallybased small-scale fades slightly underestimates the spectral efficiencies. We found that the difference between both models is essentially due to the inclusion of channel correlation. In fact, when omitting this correlation in our proposed model, the resulting spectral efficiencies become very close to those of the Rayleigh i.i.d. model. This is consistent with what was hypothesized in this regard in [51]. 
 We have evaluated the advantage of intra-cell interference canceling systems by comparing their spectral efficiencies with that of subsectorization, which only mitigates interference using fixed antenna beams. We found that for a typical femtocell scenario, characterized by a street canyon type setting, the spectral efficiency gains over subsectorization can reach factors between 3 and 5, the exact value depending on the specific system and the percentage of users being considered. Subsectorization is limited by intra-cell-interference and in fact does not perform better than using intra-cell TDMA. Among the intra-cell interference canceling systems, RZF can achieve more than 80% DPC’s spectral efficiency. Interference from neighboring cells limits the performance, but we found that attempting to improve this by alternating the transmission of contiguous cells results in general in a net loss of throughput. Considerable gains are achievable if the interference of the closest neighboring cells is mitigated, but at the expense of added transmission channels and extra CSI overhead. Our assumed service environment yields significantly higher spectral efficiencies (by a factor of 2 or larger) than predicted by the standard 3GPP model. This is because we observed that same-cell links had significantly less path-loss than predicted 
 Fig. 13. Per-user spectal efficiency versus power constraint comparison using empirical and simulated data for 50% availability. 
 by the 3GPP model when the SU is placed at indoor locations free of significant blockage to the BS (for example close to an outdoor facing window). The resulting advantage in spectral efficiency suggests that choosing an adequate position for the SU in combination with indoor repeating using WiFi, will provide considerable gains over attempting to directly serve indoor users at all positions. 
 ﻿We characterize the rate-distortion function for zero-mean stationary Gaussian sources under the MSE fidelity criterion and subject to the additional constraint that the distortion is uncorrelated to the input. The solution is given by two equations coupled through a single scalar parameter. This has a structure similar to the well known water-filling solution obtained without the uncorrelated distortion restriction. Our results fully characterize the unique statistics of the optimal distortion. We also show that, for all positive distortions, the minimum achievable rate subject to the uncorrelation constraint is strictly larger than that given by the un-constrained rate-distortion function. This gap increases with the distortion and tends to infinity and zero, respectively, as the distortion tends to zero and infinity.
 Introduction
 Many lossy source coding schemes have the property that the end-to-end reconstruction error is uncorrelated with the source. We refer to such schemes as uncorrelated distortion (UD) coders. As an example, consider a typical transform coder, as depicted in Fig. 1. Here, a random vector X ? RN is first transformed by an analysis transform T ? RN×N to yield U = TX. Then U is quantized, yielding the vector, Uˆ = Q(U). The input signal is finally approximated by Y = T˜Uˆ, where T˜ ? RN×N is the synthesis transform, cf. [1,2]. If the quantization error  is
 T˜	Y
 	
 T
 	X	U   Uˆ
 Figure 1: Transform coder.
 uncorrelated to U, and if TT˜ = I, then it is easy to show that Y - X is uncorrelated to X, thus yielding a UD coder.
 More generally, any quantization scheme satisfying the following two properties constitutes a UD coder: a) The error introduced by the quantizer is uncorrelated to its input; b) The linear processing (if any) before and after the quantizer yields perfect reconstruction (PR) in the absence of quantization errors. Property a) is satisfied in many cases, e.g. in high-resolution coding [3] or when a quantizer with dither (either subtractive [4] or non-subtractive [5]) is employed. On the other hand, the PR condition (Property b)) is often imposed (sometimes implicitly) in the design of filter banks [6], transform coders [1,2], and feedback quantizers [7,8]. Thus, any PR source coder using, for example, subtractively dithered quantization, is a UD coder. The rate-distortion performance of any UD coder can be compared to the underlying Shannon’s rate-distortion function R(D) of the source, for a given distortion metric. One may question whether such a comparison is, in fact, fair. After all, the additional constraint that the end-to-end distortion is uncorrelated with the source is not imposed upon R(D). With this in mind, let R?(D) denote the rate distortion function with the additional constraint that the end-to-end distortion is uncorrelated to the source. (A formal definition of R?(D) is given in Section 2). Clearly, R?(D)=R(D).  However, to the best of the authors’ knowledge, the problem of characterizing R?(D) has not been formally addressed before. Therefore, questions such as in which cases (if any) R?(D) equals R(D), and how R?(D) can be achieved, appear to be unanswered.
 In this paper, we not only give conclusive answers to the above questions, but more importantly, we completely characterize R?(D) for the quadratic Gaussian case2 as a lower bound for the rate achievable under the uncorrelated distortion constraint . We show, in Section 2, that R?(D) can be parameterized through a single scalar variable a > 0. This is a result which parallels the conventional water-filling equations that describe R(D). We characterize the unique optimal statistics that the reconstruction error Y-X needs to have in order to achieve R?(D), for a given Gaussian source X. In particular, we show that Y-X must be Gaussian. In addition, we recast the results in a transform coding sense. More precisely, we show that if the quantization errors are Gaussian, independent both mutually and from the source, then the Karhunen-Loève Transform (KLT) is optimal among all perfect reconstruction transforms, at all rates.  A comparative analysis between R?(D) and R(D) is then presented in Section 3. There we show that R?(D) is convex and monotonically decreasing in D, and that R?(D) >R(D),?D >0, converging in the limit as R ? 8.
 Furthermore, we show that R?(D)?0?D ?8, which is different from the well known result R(D)= 0?D=sX2 . 
 It is worth emphasizing that our results are not tied to any particular source coding architecture, but are general in the sense that any coding scheme in which the end-to-end distortion is uncorrelated with the source can do no better than R?(D).
 Notation We use uppercase letters to represent random vectors, adding a subscript when referring to one of its elements, i.e., Xi is the i-th element of the random vector X. The expectation operator is denoted by E[·]. Uppercase bold letters are used for matrices. The positive-definite square root of a positive-definite matrix M is denoted by  . We write |M| and tr(M) for the determinant and the trace of a matrix M, respectively. The probability density function
 (PDF) and covariance matrix of a random (column) vector X are denoted respectively by fX and K , where XT is the transpose of X. We write KX,Y for the cross-covariance matrix between two random vectors X and Y . The spectrum of a w.s.s. random process Z with autocorrelation function  is denoted by ,
 ?? ? [-p,p]. The differential entropy and the differential entropy per dimension of an N-length random vector X are denoted, respectively, by  . When X is a random process,   denotes the differential entropy rate of X.
 We use  to refer, respectively, to the mutual information and the mutual information per dimension between two random vectors X and Y . When X and Y are random processes ,  denotes the mutual information rate between X and Y . We write a.e. for “almost everywhere”.
 Rate-Distortion Function with Uncorrelated Distortion
 We begin by formalizing the definition of the quadratic rate-distortion function under the constraint that the end-to-end distortion be uncorrelated with the source. Then, in Section 2.1, we characterize this function for Gaussian random vectors, deferring the case of Gaussian stationary processes to Section 2.2.
 Definition 1. The uncorrelated quadratic rate-distortion function R?(D) for a random vector (source) X ? RN is defined as
 	R?(D)  E	T	1minY X	X Y	I¯(X;Y ),	(1)
 Y : [X(Y -X) ]=0, N tr(K - )=D,|K - |>0
 where Y is an N-length random vector.
 R?(D) for Gaussian Random Vector Sources
 We now present one of the main results of this paper, namely that, for Gaussian vector sources, R?(D) is given by two equations linked through a single scalar parameter. This resembles the “water-filling” equations that describe R(D). The proof of this result, which is presented in Theorem 1, makes use of the following lemma.
 Lemma 1. Let X ? RN ~ N(0,KX). Let Z ? RN and ZG ? RN be two random vectors with zero mean and the same covariance matrix, i.e., KZ = KZG, and having the same crosscovariance matrix with respect to X, that is, KX,Z = KX,ZG. If ZG and X are jointly Gaussian, and if Z has any distribution, then
 	I(X;X + Z) = I(X;X + ZG).	(2)
 If furthermore |KX+Z| = |KX+ZG| > 0, then equality is achieved in (2) iff Z ~ N(0,KZ) with Z and X being jointly Gaussian.
 Proof. Define . Then
 I(X;X + Z) - I(X;X + ZG) = h(X|YG) - h(X|Y ) = h(ZG|YG) - h(Z|Y )
  y  y
   dzdy
 	 	(3)
 where  is the relative entropy (or Kullback-Leibler distance) between the two probability density functions f and g. The equality (a) follows from the fact that log(fZG|YG(z|y)) is a quadratic form of z and y, and from the fact that KZ,Y = KZG,YG. The inequality follows from the
 fact thatfor all y  such that fY , with equality iff f = g. Thus, equality is achieved iff|	fZ|G|YG| =y = fGZ||Y =y0
 	(y) > 0. The proof is completed by noting that KX+Z = KX+Z	>
 implies fY (y) > 0 for all y ? RN.	 
 Remark 1. We note that the above Lemma generalizes Lemma II.2 in [12], by relaxing the requirement that fZ|X = fZ and fZG|X = fZG, to the requirement KX,Z = KX,ZG.
 We are now in a position to present the main result of this section:
 Theorem 1. Let the source X ? RN be a zero mean Gaussian random vector with positive-definite covariance matrix KX, having eigenvalues . Then
 For any positive D,
 	 ,	(4)
 where the scalar parameter a ? R+ is such that
 	 .	(5)
 For each D > 0, the value of a that satisfies (5) is unique.
 Let	> 0. Then I¯(X;Y ) =
 	(	)
 	K ,	(6)
 and where a satisfies (5).
 Proof. Let U denote the set of all N-length random vectors uncorrelated to X, and define the sets
 GD ? BD ? U as
  
 With the above definitions, (1) can be written as where (a) follows directly from Lemma 1 and where	 holds since the definition of BD (see (7)), guarantees that both h¯(X + Z) and h¯(Z) exist.
 We now prove, by contradiction, that the minimizer of log|K-Z1KX +I| in GD, namely Z, is such that tr . For this purpose, suppose that b  , and let  be the eigenvalues of  be a Gaussian random vector with covariance matrix
 K . We then have that tr , and that
 since b >   is a strictly increasing function. Thus 	cannot be a minimizer of   unless tr .
 The minimizer of log(|KX + KX|/|KZ|) subject to tr(KZ)/N = D can be found using a variational approach. More precisely, the covariance matrix of the minimizer, KZ, must necessarily be such that the derivative of the Lagrangian
 	 	(10)
 with respect to KZ is zero at K , for some ß ? R, which is equivalent to the condition that the matrix differential ?L(KZ) = 0, ??KZ. Using the fact that ? log|M| = tr(M-1?M), for any positive definite matrix M, the necessary condition for Z to be a minimizer takes the form
 	?L??(KZ)		  Z	Z= tr (KX-+ KZ)-1???K Z - trK±Z-?1?KZ  + ßtr(?KZ) = 0,??KZ
 K =K tr(KX + KZ )-1 K-Z1 + ßI ?KZ = 0, ?KZ
 	??	1-K-Z1 + ßI=0	KZ =	21KX2 + ß4KX - 21KX.	(11)
 (KX +KZ )-
 The fact that KZ needs to be positive definite implies that ß > 0 and that it is infeasible to have a negative sign before the square root in (11). This, together with the change of variable , leads directly to (6), with a > 0. On the other hand, the value of a must be such that the equality constraint tr  is satisfied. From (11), and applying Lemma 2 (see appendix), this requirement is equivalent to  , which proves (5).
 Similarly, (4) is obtained by substituting (11) into (8) and then applying Lemma 2, which yields:
   .
 The uniqueness of a is easily verified by noting that the right hand side of (5) is monotonically increasing with a. Since a = 1/ß is unique, it follows from (11) that the covariance matrix of  = argminZ?BD I¯(X;X + Z) is unique , completing the proof.  
 Transform Coding Realization of R?(D): Closer examination of Lemma 2, when used in (6), suggests that, for a Gaussian source X, R?(D) can be achieved by the transform coding architecture shown in Fig. 1. More precisely, an end-to-end distortion having the optimal covariance matrix KZ given by (6) is obtained by choosing the transform T such that T?T T = KX, where ? diag(?1,...,?N) (i.e., the KLT transform for X), and by having a Gaussian random vector of quantization errors E with  Interestingly, here
 E[EET ] is not a scaled identity matrix, as is usually the case in KLT transform coding, cf. [13]. This discrepancy arises from the approximation E[Ek2] = cE[Uk2]2-2bk, commonly used to link the variance of Ek to the bit-rate bk at which each k-th transform coefficient is quantized. In this expression, c>0 is a constant that depends on the PDF of the source and on the type of quantizer. The well known optimal bit allocation analyzed, e.g., in [13], is based upon this formula, and thus minimizes the total bit-rate . On the other hand, the optimal quantization errors Ek implied by Theorem 1 need to be Gaussian, their variances being such that the end-to-end mutual information  is minimized.  Thus, the difference in the optimal values for  obtained in each case is due to the fact that .
 R?(D) for Gaussian Stationary Random Processes
 The R?(D) function defined in (1) can be extended to random processes as follows:
 Definition 2. The uncorrelated quadratic rate-distortion function R?(D) for a random process X is defined as
 	R?(D) =	min	I¯(X;Y ),	(12)
 Y :E[X(Y -X)T]=0,limN?8 N1 tr(KY -X)=D,limN?8|KY -X|1/N>0
 where Y is a random process.
 The R?(D) function for stationary Gaussian random processes can be derived from the results obtained in Section 2.1, by restricting to random vectors X ? RN having a Toeplitz covariance matrix, and then letting N ? 8. More precisely, we have the following result:
 Theorem 2. Let the source X be a Gaussian stationary random process with spectrum SX(?) such that SX(?) > 0, a.e. on [-p,p]. Then
 For any(13)
 -
 where the scalar parameter a ? R+ is such that
 (14)
 -
 For each D > 0, the value of a that satisfies (14) is unique.
 Let Y satisfy E[X(Y - X)T ] = 0, N1 limN?8 tr(KY -X) = D, limN?8 |KY -X|N1 > 0.
 Then   is a Gaussian stationary random process with spectrum
 	 ,	a.e. on [-p,p].	(15)
 Proof. Define, from the random processes  
 [Y1 ··· YN]T , N ? N. It is known that ??(	) = ??(	+1),?N ? N, where ??(	) and ??(	+1) are
 the smallest eigenvalues of KX(N) and KX(N+1), respectively (see e.g. [14, Theorem 4.3.8]). This result, together with Lemma 3, in the Appendix, and the fact that SX(?) > 0, a.e. on [-p,p], implies that |KX(N)| > 0, for all N ? N. We can then apply Theorem 1 to each X(N), ?N ? N, obtaining
 	N	 
  
 	R?(N)(D)   N1 log?k(N) + a(N) + ?k(N) vaN ,	?N ? N,	(16)
 k=1
 where a(N) satisfies
 	1	 
 	D = 2	N	?(kN)2 + ?(kN)a(N) - ?(kN),	?N ? N,	(17)
 	N	k=1
 and where  denotes the set of eigenvalues of KX(N). From (14), the optimal distortion covariance matrix for X(N) is
 	K .	(18)
 Direct application of Lemma 3 (see the Appendix) to (16) and (17) yields
  
 wherein  is the only scalar that satisfies
 	 .	(20)
 Similarly, applying Lemma 3 to (18) we obtain (15). This completes the proof.	 
 Remark 2. It is interesting to note that the equations characterizing the optimal UD feedback converters derived in [8] achieve an end-to-end distortion whose spectrum is given precisely by (15). Furthermore, it is easy to show that such converters would achieve the R?(D) function if the noise due to the scalar quantization within the feedback loop were white Gaussian noise uncorrelated with the input.
 Comparison with R(D)
 The next theorem shows that R?(D) shares strict monotonicity and convexity with R(D), but deviates from R(D) in the asymptotic limit of large distortions.
 Theorem 3. For any Gaussian random vector (stationary random process) X with positive definite covariance matrix Kis monotonically decreasing and convex. In addition,.
 Proof. We present here only the proof for the case of Gaussian random vectors. The proof for
 Gaussian stationary processes proceeds in an analogous fashion.	Monotonicity: We have that
  , provided that   and   exist and that the latter derivative is non-zero. From (4), we obtain
 	 .	(21)
 On the other hand, from (5),
   ,
 and thus
 	,	(22)
 proving that R?(·) is a strictly decreasing function (since a > 0). Convexity: The fact that a grows monotonically with increasing D, together with (22), imply that  ? ? D1 > D2, and thus R?(·) is convex. Limits: It is clear from (4) that lima?8 R? = 0 and lima?0 R? = 8. Since, as can be seen from (21), R? decreases monotonically with increasing a, ?a ? (0,8), it follows that R? ? 0 ?? a ? 8, and that R? ? 8 ?? a ? 0.
 Similarly, it follows from (5) and the monotonicity of D with respect to a that, for fixed , D ? 8 ?? a ? 8 and D ? 0 ?? a ? 0. We then have that D? 8 ?? R? ? 0 and
 D ?0 ?? R? ? 8, completing the proof.	 
 We next show that R?(D)>R(D) for all D>0, converging asymptotically as D ? 0.
 Theorem 4. For any Gaussian random vector (stationary random process) X with positive definite covariance matrix KX (with SX(?) > 0,a.e. on [-p,p]), the following holds
 	(i) R?(D) - R(D) > 0,	?D > 0;	 .	(23)
 Proof. We present here only the proof for the case of Gaussian random vectors. The proof for Gaussian stationary processes proceeds in an analogous fashion. Recall that for a Gaussian random vector X having positive covariance matrix with eigenvalues  one has that R(D) =  , with equality iff . As a consequence,
  
 Equality (a) is obtained by substituting (4) and (5) into the right hand side of (24). The validity of (23)-(ii) then follows directly by taking the limit of the right hand side of (25) as a ? 0. In order to prove (23)-(i), we will show that D?(R) > D(R), where the function D?(·) is the inverse of R?(·) and D(R) is Shannon’s distortion-rate function. For this purpose, consider the random vector Y  W , where W ? RN×N is a symmetric, positive-definite matrix, and Z is as in Theorem 1-(iii). Notice that are not uncorrelated unless W = I. The mutual information per dimension between and
 h¯(WY )-
 positive definite . We next show that for any D (and corresponding KZ), choosing an optimal matrix W yields a Y  whose distortion   is strictly smaller than
 D. It is easy to show that tr  is minimized by choosing W  , where W 
   is the Wiener filter (matrix) for . From this equation, we define the function
  (KW , describing the distortion associated with , with the covariance matrix of Z as in (6) when R? = R. Since , we obtain, from applying Lemma 2, and after some algebraic manipulation, that
 	 ,	(26)
 where the last inequality follows from the fact that . Finally, the fact that both R(D) and R?(D) are monotonically decreasing functions, together with (26), implies (23)-(i). This completes the proof.	 
 The summation term in (25) describes exactly the rate loss  for all
  . For D within this range, it can be shown, using Chebyshev’s sum inequality,
 that ?RL(D)/?a > 0, which implies that RL(D) increases monotonically with increasing D. On the other hand, for all D > 0, the ratio D?(R)/D(R) can be lower bounded by (26). It can be shown that this bound increases with a (and thus with D as well), tending to 8 as a ? 8, which is in agreement with Theorem 3.
 Concluding Remarks
 In this work we have completely characterized R?(D), the quadratic Gaussian rate-distortion function subject to the constraint that the end-to-end distortion be uncorrelated with the source. We have further proved that this function shares convexity and monotonicity with Shannon’s rate-distortion function R(D), but R?(D) is positively bounded away from the latter, converging to R(D) only in the limit as the distortion tends to zero. We showed that the uncorrelation constraint causes the distortion to unboundedly grow as the rate tends to zero. We also discussed the achievability of R?(D) for random vectors and stationary random processes through transform coding and feedback quantization architectures.
 Appendix
 Lemma 2 (Adapted from Corollary 11.1.2 in [15]). Let
 A = Qdiag ,
 with Q ? CN×N, and where Q:,k and Qk,  denote the k-th column and the k-th row of Q and
 Q-1, respectively. If f(·) is analytic in a neighbourhood around each ?k, for k = 1,...,N, then
 	f(A) = Qdiag .	(27)
 Lemma 3 ( Theorem 4.5.2 in [16] ). Let A8 be an infinite Toeplitz matrix with entry ak ? R on the k-th diagonal. Then the eigenvalues of A8 are contained in the interval m = ? = M, where m and M denote the essential infimum and supremum, respectively, of the function f(?)   . Moreover, if both m and M are finite and G(?) is any continuous function of
 ? ? [m,M], then
 	 	(28)
 where the ?(N) are the eigenvalues of the sub-matrix A(N) ? RN×N of A8 centered about the main diagonal of A8.
 ﻿ This paper novel on the joint problem of sampling , reconstruction and quantization of . literature on this topic exclusively with band limited in form . Our key departure from is that we deal with continuous time reconstruction of not necessarily band limited . Our approach and from optimal data and horizon control theory . The key conclusion from the work here is that , in the case under study , the optimal design problem can be partitioned into two sub , namely i the design of an optimal filter by sampling and an optimal , which works directly on the . Simulation are which illustrate the performance of the optimal A converter designed via these . 
 Key : Sampling , quantization signal , data control . 
 In many , one needs to convert , continuous time into discrete time . This to an important set of regarding the best way to represent a signal by a sequence of and , such that the information loss inherent in the sampling and quantization process is in some sense . In the present work , we are interested in how to quantize a possible non band limited signal to obtain the possible reconstruction distortion . 
 We will show that , for a given sampling rate and reconstruction , minimization of reconstruction error , in an L sense , can be converted into a discrete time problem . It turns out that if an appropriate filter is used , then all the information to find the optimal sequence can always be extracted from discrete time of its output , even if the continuous time input signal is not band limited . 
 the optimal quantization problem to finding the solution of a combinatorial optimization , which is in general intractable . Our proposal is to convert the optimal quantization problem into a data moving horizon optimization problem with decision . The method excellent and only limited computational effort . It our previous work in by on data rather than merely on discrete time . 
 Background to the work here from distinct . The first of these is associated with the problem of sampling in the absence of quantization . 
 The second related field of research is concerned with quantization of where the sampling strategy been . The third stream of prior work in the area of data control theory . Here , the emphasis typically been on regulation zero reference with unconstrained decision . In the present work we extend these to account for non zero reference and decision . 
 Our approach from the work above by virtue of the fact that we design the joint optimal sampler and data moving horizon optimization . This to significant performance gains , with alternative which do not take account of the interaction between sampling and quantization . 
 The remainder of this work is organized as : In Section we present the continuous time AD conversion problem and how it can be into discrete time . Section the continuous time horizon . Simulation are included in Section . Section . 
 The sampling interval is constant and equal to . Thus , are at a rate of per second . 
 In Fig , the reconstruction discrete time sequence u into a continuous time signal . For example , in case of zero order hold reconstruction , the impulse response be . In the classical framework of perfectly band limited reconstruction , would be an ideal low pass filter with cutoff frequency t . On the other hand , in most practical , zero order hold or some other form of short impulse response filter sometimes non causal is generally used for reconstruction . 
 The the error frequency weighting filter see , e .., . It one to represent the different impact of the error at different for a particular application . For example , if the system is employed for audio , be designed to model the response of human hearing . We are interested in designing a which the L norm of the frequency weighed error see Fig . , i . e ., the cost function 
 For the analysis below , it is more convenient to rearrange the system of Fig . to the equivalent form shown in Fig . . In this figure , the continuous time filter is by the transfer function 
 In order to represent the continuous time filtering by , we define the continuous time version of u as 
 If we denote the impulse response of the frequency weighting L , then its output 
 Thus , the quest for optimal sampling and quantization can be stated as the optimization problem of finding the sequence u as in that the L norm of the reconstruction error , i . e .: 
 It will now be shown that the L continuous time optimization problem in is equivalent to an discrete time optimization problem , where the weighting depend on the signal inter sample behaviour . This is established in the following lemma , originally without a formal proof in . 
 Lemma frame a frame forLet the sequence of span , i . e . , with Z 
 for all . Let L be defined as in for the sequence of . Let the signal a L , and define , and ,, via 
 We note that the first term in the last line of is well defined since a L as by the Lemma . The second term is finite by virtue of the inequality and the fact that L , 
 which in turn the third integral is also bounded . Since inner are by definition linear and a real , one by substituting into that 
 where u is an un sequence that the global minimum , namely , then 
 Remark From lemma and remark , it is clear that to minimize the only information is an optimal un sequence u or , alternatively , the sequence and the ,. The latter to of the function of , which can be determined off line after choosing reconstruction and error weighting and then incorporated to the quantization algorithm . On the other hand , can be from u by with respect to u and to zero , which to 
 In practice , any quantization algorithm to work with discrete time . and arise the need to determine whether a quantization algorithm can elaborate or obtain from of the input . 
 Consider , first , the determination of the series of . From definition , we have 
 There might exist more than one optimal sequence if the reconstruction stage is redundant . 
 From the last line of , it is clear that the series of through a filter with frequency response can be by passing the input sig given 
 and then taking the every . i . e ., if we denote the impulse response of by , then a , , 
 Let us next consider the determination of u , the sequence of which reconstruction error in the absence of quantization . It is known from sampling theory that , for any input signal , u can be by sampling the output of a filter to the reconstruction filter . From these , for the system in Fig . , the ideal filter for a given reconstruction filter is given by 
 The above suggest that all the necessary information about the input signal for optimal quantization to be feasible can be from of the input signal , and that the filter is not unique . We will provide next necessary and sufficient for a filter to yield that allow for optimal quantization . 
 Consider the discrete transform of u , and let and denote the discrete and continuous of any , respectively . 
 by another filter . The discrete transform of such sequence of would be 
 Recovery of can be in the discrete time domain by a discrete time filter to the sequence , such that 
 From and it can be seen that a sufficient and necessary condition is the existence of a periodic transfer function , such that 
 for the stage to be able to determine u from the and allow for optimal quantization . As a particular case , if A , then , from and , 
 The latter equality can also be from by that A if and only if , ,,,. 
 Of course the stage would need to implement the correction filter based upon knowledge of and , according to . Two important special are to be : 
 If a no frequency beyond rad , then any filter satisfying 
 for some C C would make optimal quantization possible from the . This result is not surprising , since by sampling theorem , the of a band limited signal contain all the information about the complete signal . 
 If is band limited to a frequency a but a energy at greater than a , then any prefilter band limited exactly to a satisfying 
 for some K K would have a feasible correction filter that optimal quantization possible from the . 
 The conclusion from the above is that quantization for optimal reconstruction of an input signal not possible , but either the use of an appropriate filter satisfying to get the from , or , alternatively , the needs to know the signal between sampling . 
 For the general case , minimization of would require the evaluation of for every possible sequence 
 the optimization becomes in u , u U . For sufficiently long , tractable . To overcome this problem , we propose to use from the horizon control framework and optimize over a short horizon of . A based on this idea been recently by the current in for an all discrete time system , near optimal performance with rather short horizon . In what we will extend this idea to the data case , and show , via , that significant distortion reduction is when converting non band limited . 
 Consider the system at . Instead of to optimize the cost over , we will aim to minimize the cost within a finite time interval 
 N , where , are design . We will only concentrate on the optimal sequence of to be for the interval , 
 For the purpose of in the horizon the effect of past , it is convenient to describe the continuous time filter by its state space representation 
 where A , , R and is as defined in . In , for a possible non causal . If is causal , then . 
 Notice that the first term on the right hand side of the effect of the choice of u within the horizon . This term to the forced response of to the input u , which can be conveniently as 
 On the other hand , the second term on the right hand side of the natural response of when the initial state is . We define the difference between this initial state response and the input signal a within the horizon as the target function , for the horizon at : 
 By and , the cost can be expressed as the L norm of the difference between the zero initial state response and the target function : 
 Since L , one can exchange the order of sum and integration in and rewrite it in matrix form as 
 where the vector and the symmetric , positive definite matrix are defined element wise as 
 The derived above for the cost function over a finite horizon allow us to introduce the data horizon . The algorithm , at a given instant , the vector of u that the total reconstruction error from to defined in . Then , the first element of u is sent to the output of the . The horizon is then forward by , and iteration . The algorithm , beginning at instant , can be as : 
 The sequence of step the output of the data horizon sampler . If the input signal is not band limited to , the algorithm and frequency weighted quantization noise . For that purpose , it does simultaneous adaptive filtering on the input signal and adaptive noise shaping of the quantization noise , thus respecting the interaction between both phenomena . 
 It is interesting to note that , as the horizon is made , the output of the sampler defined above the optimal feasible output sequence possible defined in . 
 We will first show an example the performance of the data horizon against the so all discrete time horizon in in the following situation : 
 The input signal , a , is an audio signal that frequency up to . Its frequency energy content is shown in Fig . . 
 Figure : Spectral composition of the input signal a . • The sampling frequency is half the to avoid i . e ., . 
 i . e ., it impulse response . • The frequency error weighting filter to the third order model for the acoustical response of the human ear . Its frequency response is shown in Fig . 
 In the simulation , the full knowledge of , and the matrix defined in . However , it based on direct of the input signal . Thus , since the implicit filter is a unity gain , that the all discrete time will be unable to determine the target function to be by the reconstruction stage . On the other hand , the the same matrix but access to the behaviour of the input signal . 
 Fig . the reconstruction error from the of both , for several horizon . It 
 Figure : Reconstruction error to distortion for from the of all and data , for to , for a non band limited input . 
 can clearly be how , in this case , the converter in the present work the all discrete time converter of . It can also be seen that the distortion a small decrease with the increase of the horizon length . This that , in this example , the main contributor to the distortion is noise , for both . Notice that the without an anti filter . This , together with its much lower distortion in comparison with that of the , that the data horizon optimization algorithm a form of filtering of the input signal that effectively . 
 This paper shown how horizon data control can be to design optimal AD . A key departure from in this area is that we optimize a version of the continuous time reconstruction error for not necessarily band limited input . Inter , we show that the optimal design problem can be decomposed into two , namely the design of an optimal analogue filter together with an optimal . We that the latter is feasible if only of the are available . The efficacy of the method been by an example an audio signal below its rate . 
  
 ﻿ The main purpose of this note is to show that in a realization of the causal information 
 rate distortion function for a th order source , under a single letter sum distortion 
 derived under the assumption that the have a joint probability density function . 
 Consider the causal information rate distortion function for a random source , 
 where the minimization is over all conditional satisfying the distortion constraint 
 If the is by some conditional distribution , the associated pair a realization of . Here we assume that such distribution and that the corresponding realization a joint . This assumption is satisfied if , for example , is and , . 
 The first purpose of this note is to show that in a realization of the causal for a th order source , under the average distortion constraint , and supposing that in such realization the have a joint , it that 
 The given in are a special case of the given by , , , for abstract , where their derivation is not included . The value of our first result in that 
 In this proof , we pose the causal optimization problem with as the decision variable instead of the collection as would be the case in for probability an associated . Accordingly , we impose an explicit causality constraint on , instead of causality structurally by to be the product of , as done 
 The second and main goal of this document is to note that from a it is clear that 
 does not hold , except for . Crucially , does not become true by supposing that the joint stationary , thus , Remark . and what is stated in the discussion paragraph at the end of , Section . 
 The causal under the above is by the solution to the following optimization problem : 
 where the minimization is over the conditional . Notice that d is an explicit causality constraint equivalent to . 
 Before writing the and taking its differential , let us obtain the differential 
 On the other hand , for each i ,...,, the causality constraint d in the as 
 It will be convenient to manipulate this expression so as to give it a structure similar to the other in the . For this purpose , notice that 
 From the theory of optimization on vector is a solution to Optimization Problem only if 
 for every function gyn as defined in , i . e ., for every conditional . This if and only 
 , e , , e , , b 
 In order attain causality in , the must depend only on and . Since for each , the function does not depend on with i , the causality constraint 
 see a . Suppose now that i . e ., causality is satisfied for , for . In such case , 
 which case becomes b . Substituting the latter into and then in a . Finally , from a , it that in a realization of the causal it must hold that 
  
 ﻿The problem of characterizing lower bounds on data-rates needed for closed loop stability has been solved in a variety of settings. However, the available results lead to coding schemes which are very complex and, thus, of limited practical interest. In this paper, we show how simple coding systems comprising only LTI filters and memoryless entropy coded dithered scalar quantizers can be used to stabilize strongly stabilizable SISO LTI plant models over error-free bit-rate limited feedback channels. Despite the simplicity of the building blocks employed, we prove that the data-rates incurred do not exceed absolute lower bounds by more than 1.25 bits per sample. 
 The study of networked control systems (NCSs), i.e., control systems with communication constraints, has emerged as an active research field during the past years (see, e.g., the special issue [1]). Key questions within this framework are related to the way in which network artifacts affect the stability and performance of control loops that employ non-transparent communication channels. Typical channel artifacts include data-rate limits, random delays and data dropouts. A unifying framework for the treatment of the general NCS analysis or design problem does not exist. Nevertheless, there has been significant progress in the study of subproblems. For example, data-rate constraints have been studied in, e.g., [2]–[5]. The issue of data dropouts has been studied in, e.g., [6] and random time delays have been considered in, e.g., [7]. 
 In this paper we focus on feedback loops closed over delay- and error-free bit-rate limited channels. Within this context, a key existing result establishes necessary and sufficient conditions on the channel data-rate that allows one to achieve closed loop stability (in an appropriate sense; see, e.g., [2], [3], [5] and the many references therein). This result is given in terms of a lower bound on the channel datarate (which depends on the unstable plant poles only) over which coding schemes can be constructed so as to achieve stability. These coding schemes are quite complex and are not attractive from a practical point of view. On the other hand, showing that the rate at which a stable control system is transmitting data is always greater than the aforementioned bound (i.e., necessity) is fairly simple (see also [8], [9]), whereas constructing an actual coding scheme that achieves stability at any rate above the absolute limit (i.e., the proof of sufficiency) is much more involved (see [2], [5]). Clearly, the need arises to develop the study of simple stabilizing coding schemes that achieve rates close to the bounds in [2], [3], [5], and of approaches that allow one to exploit the insights obtained when deriving the necessity of the bounds in the construction of coding schemes. 
 Motivated by the discussion above, this paper shows how simple coding systems comprising only LTI filters and entropy coded dithered quantizers (see, e.g., [10]) can be used to stabilize strongly stabilizable SISO LTI plant models incurring in data rates which exceed the minimal ones established in [2], [3], [5] by no more than 1.25 bits per sample. The excess rate of our simple coding scheme is given by the sum of two terms: a first term due to the divergence of the quantization noise distribution from Gaussianity, and a second term that originates in the inefficiency of the loss-less coder that generates the binary words. 
 In our results, the coder and decoder architecture plays an essential role, which, given the results in, e.g., [5], is by no means surprising. Our work also sheds light into the reasons why the results in [11] are not always consistent with the lower bound on data-rates for stability studied in [2]. 
 The remainder of this paper is organized as follows: Section II introduces the notation used in the paper. Sections III and IV present the considered setup and coding scheme, respectively. Section V focuses on a simplified setting where Gaussianity assumptions are imposed. Section VI uses the results in Section V to derive guaranteed upper bounds on the data-rates that allow one to achieve MSS with the proposed coding scheme. Section VII draws conclusions. Due to space constraints, all proofs have been omitted and can be found in [12] or obtained via e-mail from the authors. 
 We define N0 , {0,1,···} and  x < 8}. We use z as both the argument of the z-transform and as the forward shift operator, where the meaning is clear from the context. Given any scalar x, |x| denotes magnitude. 
 The set of all discrete-time strictly proper real rational transfer functions is denoted by Rsp. 
 Unless otherwise stated, random processes are always scalar and defined for k ? N0, we abbreviate {x(k)}k?N0, x(k) ? R, by x, and define xk , {x(0),··· ,x(k)}. 
 E{·} denotes the expectation operator. The variance, at time instant k, of a process x is denoted via sx2(k); similarly, if x is a random variable, then sx2 denotes its variance. We define sx2 , limk?8 sx2(k), provided the limit exists. If x is 
 a wide sense stationary (wss) (asymptotically wss) process, then Sx(ej?) denotes its (stationary) power spectral density (PSD) and ?x(z) denotes any spectral factor of Sx(ej?), a second order one if and only if it has finite mean and finite i.e., . We say that a random variable is 
 If x,y,z are continuous (resp. discrete) random variables, then h(x) (resp. H(x)) denotes the differential (resp. discrete) entropy of x; h(x|y) (resp. H(x|y)) denotes the conditional differential (discrete) entropy of x, given y; I(x;y) denotes the mutual information between x and y; I(x;y|z) denotes the conditional mutual information between x and y, given z. We recall that if x is Gaussian, then h(x) = 
 . In this paper we use logarithms in base e. Thus, information is measured in nats (1 nat = (ln2)-  bits). For details and basic properties of the quantities mentioned above we refer the reader to [13]. 
 In this paper we are concerned about the stability of a closed loop system built around a SISO discrete time LTI plant model, which employs a delay- and error-free data-rate limited channel in the feedback path, as shown in Fig. 1. In that figure, G(z) is the plant model, C(z) is an LTI controller, y is the plant output, u is the plant input, r is a reference signal, and d models disturbance signals. In order to make use of the bit-rate limited channel, the feedback path comprises an encoder E which generates binary words based on information regarding the plant output. These words, which we will denote by s, are then sent delay- and errorfree through the channel. At the receiving end, a decoder D uses the received symbols to generate the signal f that is fed back into the controller. 
 Assumption 1 (Plant model): G(z) ? Rsp is unstable,1 has no poles or zeros on the unit circle, is strongly stabilizable,2 and its initial state is an independent second order random variable.	N 
 Assumption 2 (Reference and disturbance): The signals r and d are second order zero-mean wss processes that admit rational PSDs, and that are mutually uncorrelated. Moreover, 
 Assuming that the plant is strictly proper guarantees that the feedback loop in Fig. 1 is well-posed for all causal controllers, decoders and encoders. The assumption regarding 
 the plant initial state holds for most cases of interest. The remaining assumptions on G(z) are not essential, but we have made them to maintain a straightforward presentation (see [15] for the general case). Assumption 2 is standard except for the fact that we require r or d to be non-zero. We avoid the case r = d = 0 for brevity. 
 For future reference, we define {p1,··· ,pnp} as the set of unstable plant poles, and 
 Throughout this paper we adopt the following notion of stability (see also, e.g., [16]–[18]): 
 Definition 1 (Mean square stability): Consider a system described by x(k + 1) = f(x(k),w(k),k), where k ? N0, f : Rn × Rm × R ? Rn, x(k) ? Rn is the system state at time instant k, x(0) = xo, where xo is a second order random variable, and the input w is a second order wss process independent of xo.  We say that the system is mean square stable (MSS) if and only if there exist finite µ ? Rn and finite M ? Rn×n, M = 0, such that 
 Significant work has been devoted to the study of the MSS of the networked control system in Fig. 1, when arbitrarily complex coding schemes are employed (see, e.g., [2], [5]). However, the study of simple and efficient coding schemes has received much less attention. This is indeed one of the main motivations for the present work. 
 We seek simple, though effective, coding and decoding schemes. We propose to use the coding scheme shown in Fig. 2. In that figure, F1(z) is an LTI filter, and Eˆ, Dˆ are (for the moment) abstract systems that are allowed to exploit the history of their inputs to generate their corresponding outputs. More precisely, for every k ? N0, 
 where Eˆk and Dˆk are (possibly stochastic) mappings that may depend explicitly upon time, and where the range of Eˆk is a collection of prefix-free binary words (see, e.g., [13]). Note that the resulting control loop is well-posed if F1(z) ? Rsp. 
 The coding scheme defined above is a restricted instance of much more general schemes. Indeed, it is a special case of a coding scheme that exploits the history of the plant output and of the received symbols in an arbitrary causal fashion (see, e.g., [2], [5]). 
 Assumption 3 (Coding scheme): The coding scheme in Fig. 2 is such that, ?k ? N0, 
 (a)	knowing sk and vk does not provide more knowledge about w(k) than just knowing sk, 
 (b)	the sequence of mappings {Dˆi}i?{0,···,k} is known at the encoder and is such that sk can be recovered exactly from wk (and vice-versa). Also, 
 (c)	F1(z) ? Rsp and the corresponding initial state is an independent second order random variable. N At each time instant k the encoder transmits the symbol s(k) using a word of length R(k) (measured in nats). We will be interested in the average coding rate and, accordingly, we define the average data-rate as 
 provided the limit exists. R¯ is the average rate at which the symbols are sent trough the channel and, as such, is a measure of the “information flow” at the physical level. 
 A key question that arises when considering any coding scheme is how to characterize lower bounds on the achievable average data-rates. To that end we define the following: Definition 2: Consider two random processes v and w. We define (if the defining limit exists) the average directed mutual information between v and w as4 (see also [19]) 
 We can now characterize lower bounds on the average data-rate for the proposed coding scheme: 
 Theorem 1 (Lower bounds on data-rates): Consider the networked control architecture in Fig. 1 and the specific coding scheme in Fig. 2, where Eˆ and Dˆ are as described above. If Assumptions 3(a-b) hold, then R¯ = I8(v ? w). 
 Theorem 1 relates a physical quantity, namely average data-rate, to an information theoretic quantity, namely average directed mutual information. It is important to note that a different bound on the average data-rate would have arisen if we had considered Dˆ and Eˆ with different information available. 
 Theorem 1 is a key one. However, it is not straightforward to characterize I8(v ? w) unless one makes suitable assumptions on Eˆ and Dˆ or, equivalently, on the signal 
 In the next section we will make some Gaussianity assumptions. Later, in Section VI, we will consider a simple instance of Eˆ and Dˆ and we will exploit the results for the 4I8(v ? w) is sometimes called directed mutual information rate. 
 Gaussian case to provide guaranteed upper bounds on the rates associated to that specific coding scheme, without using the Gaussianity assumptions. 
 As foreshadowed at the end of Section IV, we will assume in this section that the following holds: 
 Assumption 4 (Gaussianity): The noise q (see (4)) is an independent second order zero-mean i.i.d. Gaussian sequence. The initial states of all LTI filters in Fig. 3 (including plant and controller) are independent second order Gaussian random variables, and r and d are jointly Gaussian processes. 
 Consistent with the fact that the noise q depends on the way in which Eˆ and Dˆ are chosen, we will consider the variance of q, say sq2, as design parameter. Assumption 4, when combined with the networked control system in Fig. 1 and the coding scheme in Fig. 2, yields the linear model in Fig. 3. 
 As long as Assumptions 2 and 4 hold and q has finite variance, the networked control system in Fig. 3 is MSS if and only if K(z) is such that the feedback loop in Fig. 3 is well-posed internally stable in the standard deterministic sense (see, e.g., [20], [21]). Thus, if we define 
 then MSS is equivalent to  (provided Assumptions 2 and 4 hold; see [20]).  It is easy to see that K(z) ? S if and only if F1(z) is stable, and C(z) is an admissible controller  for G(z) (see, e.g., [21]). 
 Using Theorem 1 we can now characterize average directed mutual information in the situation under study: 
 Consider the feedback loop in Fig. 3 and assume that Assumptions 1, 2 and 4 hold. If K(z) ? S and  , 
 where Sw(ej?) is the stationary PSD of w, sv2 is the stationary variance of v, and ? is the stationary coding scheme signal-to-noise ratio. Equality in (a) holds if and only if Sw(ej?)sq-2 is constant for every ?. 
 Corollary 1 provides an explicit expression for the average directed mutual information across the coding scheme in the considered situation. Also, it establishes a relationship between the average directed mutual information across the coding scheme and the corresponding signal-to-noise ratio. In particular, it follows from (6) that (provided Assumptions 1, 2 and 4 hold) minimizing the coding scheme signal-tonoise ratio amounts to minimizing an upper bound on the average directed mutual information across it. Moreover, if Sw(ej?)sq-2 is a constant, then we have equality in (6). Hence, under the constraint of having Sw(ej?)sq-2 constant, minimizing the coding scheme signal-to-noise ratio also minimizes the average directed mutual information across it. We will show below that, in our setting, one can restrict Sw(ej?)sq-2 to be constant without loss of generality. Towards that goal, we start with the following theorem: 
 Theorem 2 (Directed mutual information for MSS): Suppose that Assumptions 1, 2 and 4 holds. Then: 
 1)	There exists K(z) ? Rsp × Rp such that the feedback loop in Fig. 3 is MSS if and only if 
 2)	Any I8(v ? w) > I8inf(v ? w) can be achieved in a MSS loop if one chooses a stable and strictly proper F1(z) such that 1 + F1(z) is minimum phase (MP), C(z) is any stable admissible controller for G(z), and sq2 is made sufficiently large (but finite). 
 Theorem 2 establishes a bound on the directed mutual information across the considered coding schemes whose satisfaction is necessary and sufficient to achieve MSS. It also provides a characterization of the controller C(z), the LTI filter F1(z), and the noise variance sq2, that allow one to achieve any average directed mutual information arbitrarily close to the bound established. 
 We can use Theorem 1 to immediately infer that, provided the assumptions in Theorem 2 hold, 
 is a necessary condition that average data-rates need to satisfy in order to guarantee MSS when employing the proposed coding scheme. We note that we have recovered, for the class of considered plant models, the bound on average data-rates for MSS derived in [2]. However, we have not provided a proof of the achievability of this bound. This issue will be explored in Section VI. 
 We next explore the minimal stationary coding scheme signal-to-noise ratio compatible with MSS. This will enable us to state the main result of this section. 
 Theorem 3 (Signal-to-noise ratio for MSS): Suppose that Assumptions 1, 2 and 4 hold. 
 1)	There exists K(z) ? Rsp × Rp such that the feedback loop in Fig. 3 is MSS if and only if the stationary coding scheme signal-to-noise ratio ? satisfies 
 2)	Any ? > ?inf can be achieved in a MSS loop if one chooses C(z) as any stable admissible controller for G(z), F1(z) is chosen as 
 F1(z) = ?p(8)(?p(z)S(z))-1 - 1, (8) where S(z) , (1 + C(z)G(z))-1, and sq2 is made sufficiently large (but finite). 
 Corollary 2: Consider the feedback loop in Fig. 3 and suppose that Assumptions 1, 2 and 4 hold. A solution to the problem of finding the minimal stationary coding scheme signal-to-noise ratio that guarantees MSS provides a solution to the problem of finding the minimal average directed mutual information across the coding scheme that guarantees MSS. 
 Corollary 2 is one of the main results in this paper. Indeed, by virtue of Theorem 1, Corollary 2 allows one to conclude that determining the minimal signal-to-noise ratio for MSS provides an immediate lower bound on the average data-rates that allow one to achieve MSS. The key to establishing these facts lies in that we consider a coding architecture with sufficient degrees of freedom. This allows one to make Sw(ej?)sq-2 constant in the limit as sq2 ? 8, without constraining the minimum achievable directed mutual information across the coding scheme, or the corresponding minimal achievable signal-to-noise ratio (a simple calculation based on the results of Theorem 3 reveals this). We feel that the insights provided by these results are of fundamental importance and, to the best of our knowledge, new. 
 Remark 1: The work [11] also studies minimal signal-tonoise ratio considerations for stability. However, the authors of [11] constrain themselves to the particular case F1(z) = 0, which does not, in general, allow one to achieve the absolute minimal coding scheme signal-to-noise ratio identified in Theorem 3. Hence, it is not surprising that the results in [11] do not always suggest bounds on average data-rates that are consistent with the results in [2], where coding schemes that exploit the history of y and s are used. N 
 Section V has focused on establishing a necessary lower bound on the average data-rate that allows one to guarantee MSS. We have, however, provided no proof of the achievability of this bound. It is well known (see [2], [5]) that there exist coding schemes that allow one to achieve MSS with average data-rates arbitrarily close to the bound identified in Section V. However, since we have chosen a specific coding scheme that is much simpler than those proposed in [2], [5], there exists no guarantee that we will be able to actually achieve the bound. This motivates the study of guaranteed upper bounds on the average data-rate for MSS applicable to the proposed coding scheme. In particular, we will use the results in Section V to study achievable rates, when a memoryless entropy coded dithered quantizer (ECDQ) is used to generate the binary words that are sent over the channel. 
 Fig. 4 shows the architecture of an ECDQ and its relationship to Dˆ and Eˆ in Fig. 2 (see, e.g., [10]). The ECDQ is such that s+(k) = Q(v(k) + dh(k)), 
 where dh(k) is the dither signal which is known at both ends of the channel, and Q corresponds to a uniform quantizer defined via Q(x) , i? for  , i ? Z, where ? is the quantization step. At each time instant, s+(k) is memory- and loss-lessly coded by the entropy coder EC using explicit information about dh(k) and the corresponding binary word is sent over the channel. Upon reception, the decoder ED, which knows dh(k), decodes the received symbol recovering s+(k). Accordingly, the output w(k) becomes equal to s+(k) - dh(k). 
 Lemma 1 (ECDQ (see [10])): Consider an ECDQ as described above. If dh is an independent sequence of i.i.d. random variables, uniformly distributed on  , then the noise q defined in (4) is distributed according to the distribution of -dh, and the scalar mutual information between v(k) and w(k) satisfies I(v(k);w(k)) = H(s+(k)|dh(k)). 
 Remark 2: The proof in [10] assumes no feedback around the ECDQ. If there is strictly causal feedback around it (as in our case), then the same result applies (see detailed proof in [15]).	N 
 The results in Lemma 1 state that, if one employs an appropriately dithered ECDQ, then the difference between the input and output of the ECDQ, namely q, becomes just an i.i.d. source, uniformly distributed on each quantization interval (as in the classical additive white noise model for quantization; see, e.g., [22], [23]). This means that the use of an ECDQ allows one to achieve a noise q that resembles the noise source considered in Section V (except for having a different distribution). Therefore, it is sensible to expect that use of an appropriately designed ECDQ yields average data-rates “close” to the bounds identified in Section V. We will show below that this is indeed true. We note that, given the fact that the dither signal is known at both ends of the channel, choosing dh as in Lemma 1 implies that the ECDQ satisfies Assumptions 3(a-b) (i.e., an ECDQ is a valid instance of Dˆ and Eˆ in Fig. 2). 
 Theorem 4 (Achievable rates): Consider the networked control system in Fig. 1 with the coding scheme in Fig. 2, where Dˆ and Eˆ form an ECDQ as described above. Suppose that the memoryless entropy coder-decoder pair ECED inside the ECDQ uses Huffman coding (see [13]) that Assumptions 1, 2 and 3(c) hold, that the controller initial state is an independent second order random variable, and that the dither signal dh is an independent sequence of i.i.d. random variables, uniformly distributed on  . Then, if one chooses K(z) as in Theorem 3 (Part 2; see (5)) and 
 , with , then the resulting networked control system will be MSS and the corresponding average data-rate will be upper bounded as 
 where  is a continuous and decreasing function of ? such that lim??8 F(?) = 0, lim??0 F(?) = 8. 
 Remark 3 (Bounds are conservative): The upper bound for the average data-rate in Theorem 4 is conservative. Indeed, the rates achieved by Huffman coding are usually closer to the entropy of the source than to the entropy of the source plus ln2 (see, e.g., [24].). As a consequence, the actual data-rates will be usually close to the expression in 
 Theorem 4 is the main result in this paper. It establishes guaranteed upper bounds on the average data-rates that guarantee MSS and that are achievable with the coding scheme in Fig. 2, when Dˆ and Eˆ form a memoryless ECDQ and when the plant model is strongly stabilizable. If one chooses K(z) as suggested in Part 2 of Theorem 3, and ? is chosen sufficiently large (but finite), then one will be able to achieve a rate that is no more than   nats per sample (i.e., 1.25 bits per sample) away from the absolute bound in [2], [5]. This additional rate is composed by two terms: the first one is due to the divergence of the distribution of quantization noise from Gaussianity, and the second one originates in the inefficiency of the loss-less coding scheme employed to generate the binary words. We feel that this extra rate is a fair price to be paid if one constrain oneself to the conceptually simple coding schemes considered in this paper. 
 Remark 4: Note that the results in Theorem 4 rely on the insights developed in Section V. In other words, the insights developed when establishing lower bounds on the average data-rates needed for stabilization were key when constructing an actual coding scheme that achieved a datarate close to that bound. It is also worth noting that the quantization scheme considered in this paper constitutes a well studied building block in the Information Theory literature (see, e.g., [10], [23], [25]). All the above stands in stark contrast to the approach in, e.g., [2]. N 
 In this paper, we have studied the mean square stabilization of strongly stabilizable SISO LTI plant models controlled over bit-rate limited channels. We have proposed a simple coding scheme which uses only LTI filters and an entropy coded dithered quantizer. Within this setup, we have shown that the excess data-rates, which derive from using this reduced-complexity scheme instead of an arbitrarily complex one, are no larger than 1.25 bits per sample. Extensions to more general cases and to performance related questions can be found in [15]. 
 ﻿ The performance of a noisy linear time invariant plant , over a noiseless digital channel with transmission delay , is in this paper . The channel the single measurement output of the plant to its single control input through a causal , but otherwise arbitrary , coder controller pair . An approach is to analyze the minimal average data rate to attain the quadratic performance when the channel a known constant delay on the data . This average data rate is shown to be lower bounded by the directed information rate across a set of and an additive white noise channel . It is that the presence of time delay in the channel the data rate to achieve a certain level of performance . The applicability of the is through a numerical example . 
 INTRODUCTION 
 Rate constrained are generally studied from two of view ; control theory and information theory . In the former case , classical nonlinear control are employed . In the latter case , the key idea is extending information theoretic to the case of closed loop control . In both , stability analysis of linear is well studied see , e .., , as early and , as recent . 
 However , system performance from an information theoretic viewpoint are less abundant in the literature . Fundamental are in . In this work , for a discrete time plant , the well known Bode integral is extended to the case of causal rate limited arbitrary feedback . Along the of , research in , on the minimum data rate which is to attain a quadratic performance level in with delay free . For the lower bound , that the rate constrained optimization to find desired data over causal but otherwise arbitrary coder controller , is reduced to a convex constrained optimization problem over an auxiliary feedback loop closed through an channel . In , , it is furthermore shown that the directed information from the plant output and to the 
 This work received from Young 
 Investigator , under grant agreement No . . 
 control input a lower bound on the rate for any policy , and that it to use linear , when the initial state and external are jointly and the plant is linear . These were used by , to establish a general framework for the problem of control for fully observable multiple input multiple output . 
 In this paper , we address output feedback control of an comprised of a noisy plant and a causal controller set connected through a noiseless digital channel with a constant transmission delay . More specifically , the problem is the on minimal average data rate to guarantee that the steady state variance of an error signal does not become than a certain value . by its such as simplicity and practical appeal , we use the approach in , to gain outer and build upon . However , the main departure of this work from is considering a channel which is not delay free . So , as the first contribution , we fundamental information of the system under the delay assumption . Secondly , we characterize the trade off among performance , delay , and minimal desired average data rate . It is shown through a numerical example that greater transmission delay greater minimal average data rate to guarantee the considered quadratic level of performance . Simulation that by a simple scalar uniform in the architecture that the lower bound , the quadratic performance is by operational average data at most . away from the lower bound . 
 The outline of this work is as . Section the notation and some . Then the problem of interest is in Section . Section is to the lower bound characterization . An illustrative numerical simulation is provided in section . Finally , Section the paper . 
 NOTATION AND 
 The set of real is subset as the set of strictly positive real . the set of natural , based upon which N is defined . Furthermore , is the time index and for random considered in this paper , N . Magnitude and H norm of a signal are by . and . k , respectively . The set U is defined as the set of all proper and real rational stable transfer with that are 
  
 Fig . . Considered 
 stable and proper as well . E the expectation operator and log for the natural logarithm . The entry of the i th row and th column is by i ,. Moreover , min and represent the and magnitude , respectively . 
 All random and in this paper are assumed to be vector valued , unless otherwise stated . A random process is said to be asymptotically wide sense stationary if it and E E E hold , where is a constant . the corresponding steady state covariance matrix upon which the steady state variance of is defined as , trace . The covariance matrix for a scalar random sequence is defined as . Considering 
 , as two square matrices , the and are asymptotically equivalent if and only if the following for finite : 
  
 PROBLEM STATEMENT 
 The structure considered in this work can be found in Fig . an plant with u as control input sensor output . Moreover , there is a disturbance by and the output signal , with , , upon which the desired performance is . The plant the following transfer function matrix description : 
 , 
 in which every is proper and of suitable 
 , , and for G , G , G and G , respectively . The input alphabet of the channel is by A and is defined as a countable set of binary . Due to the delay , the output of the channel to A . The average data rate across the channel is as : 
 , 
 where i the length of the i th binary word i . The channel input is provided by the E based on the following dynamics : 
 , 
 in which e is the side information at the with an arbitrary possibly nonlinear or time deterministic . It should be noted that is a shorthand for . On the side , we have 
  
 is assumed to be an arbitrary deterministic , like , and the side information available at the at time . It should be that E Fig . are possibly time or nonlinear causal . 
 Assumption . : The , proper and free of unstable hidden . Moreover , the open loop transfer function from u single input single output and strictly proper . The disturbance signal ,, is a zero mean white noise with identity covariance matrix I and jointly with x , the initial condition , finite differential entropy . 
 Assumption . : Each of e and is jointly independent of x ,. So regarding the dynamics of the system , I u ; for . Moreover , upon knowledge of and di , the is invertible . It that there a deterministic 
 such that . 
 Now , suppose that Assumption . . Let denote the steady state variance of the all u for and u independent of x and . 
 Then the problem of interest is finding 
 R 
 D 
 for any , , where the search is to be restricted to with and with which satisfy Assumption . and make the of Fig . strongly asymptotically wide sense stationary this notion of stability is defined in . Moreover , the steady state variance of . The optimization problem in is feasible if , see Appendix A for the proof . 
 MAIN 
 Theorem . : For the feedback loop in Fig . and satisfying . and . , the following : 
  
 in which I .;. . conditional mutual information 
 see for the definition . Moreover , as defined in , the directed information rate across the forward channel u with delay . The proof can be found in . 
 Now , a lower bound can be derived on the directed information across the scheme of Fig . . 
 Lemma . : For the of Fig . , assume that 
 ,, u , form a jointly second order set of and that . and . hold . Moreover , take and uG into account as the u where ,, uG , are jointly with the same first and second order cross as ,, u ,. 
 Then . 
 Proof : The following and will justify the claim : 
 Ski I u i ; a I , ; 
 I , ; 
 I , wi ; uG i 
 Ski I , wi , ; uG i 
 e I ; uG i , 
 where a from a slight modification in , Lemma . , from , Lemma . , form the chain based upon b in , Theorem . , is a consequence of Assumption . and being a deterministic function of , and wi , and e from a in , Theorem . and Assumption . . This the proof . 
 In what we will relate the directed information from to uG to their corresponding power spectral : 
 Lemma . : u as jointly . Moreover , suppose that u is with where . Then the following 
 can be : 
  
 in which is a process with independent defined as : 
 , u u , u , E u , . 
 Moreover , the steady state power spectral density of u . 
 Proof : and joint ness of u , in mind and based on , Theorem . with a little modification , we can conclude that is and as well . We start by the following : 
 I u i ; u i u i , 
 u i i u i , 
  
 u i i , 
 i u i i , 
  
 Fig . . Auxiliary 
 where from the definition of mutual information and from the definition of , from , Property and , and i from , Property . So the directed information rate can therefore be as : 
  
 where from the chain rule of the differential entropy and being independent of . Since the process u is with for some , , Lemma . will approve the validity of the leftmost term in . The rightmost term is self explanatory because is and . 
 It from Theorem . , Lemma . and Lemma . that the rate performance pair by any scheme which the , is attainable with a lower rate by a scheme comprised of and an noise source . Such a scheme is in Fig . . 
 The of Fig . is defined under the same Assumption . as the main system in Fig . except for one thing ; the arbitrary are by . Moreover , the communication channel is a channel with noiseless one sample feedback . The dynamics of this auxiliary scheme can be as : 
  
 in which is the with zero mean and variance . This noise is assumed to be independent of x ,. Additionally , we suppose that the initial state of ,, and the delay are deterministic . 
 Theorem . : For the in Fig . and satisfying . and . , is lower bounded as if , : 
  
  
 where the feasible set for the optimization problem u is the noise with rendering the feedback loop of Fig . internally stable and well when . In these , and denote the steady state variance of z and the steady state power spectral density of u in Fig . , respectively . 
 Proof : Since , there is at least one pair , say and , satisfying Assumption . , rendering the of Fig . and , and where and 
  
 can be based on Theorem . , if the in Lemma . and Lemma . are met . It should be noted that the steady state power spectral density of . A scheme comprised of linear with a unit gain noisy channel and generate and which satisfy those and keep within , . Such a scheme is as : 
  
 where a noise with zero mean and independent of . Regarding the causality and linearity of , can be written as : 
 . 
 According to the causality in , joint ness of , and transitivity of asymptotic equivalence for and sum of the matrices noted in , the and are asymptotically equivalent to of lower triangular matrices . Moreover , the internally stable and well . With all of this in mind , by setting a concatenation of linear with the steady state behaviour of in and considering a variance for equal to , the system of Fig . will be well and internally stable where and are . Then according to Lemma . , the following can be : 
  
 which the proof . 
 Lemma . : Consider the loop of Fig . with fixed . Define as : 
 , 
 in which the steady state power spectral density of . Then for any , upon the existence of the pair , B , J making the system of Fig . internally stable and well , there exist another pair , comprised of 
  
 Fig . . Equivalent viewpoint of internal stability 
 the filter J and B , which the feedback loop of Fig . internally stable and well , the the steady state power spectral density of z and the following : 
  
 Proof : It is well known that the system of Fig . is well and internally stable if and only if the transfer ,, , to z , y ,, u in Fig . to . By Ti and , we refer to the set such that , i , . Moreover , and represent of By in the situation where . Now , consider the following set of : 
 J , By 
  
 , 
 in which d the relative degree of J and U is chosen in such a way that . Consequently , J is and T can be written as : 
 T , ,, T 
 d d d . I , I , I , I 
 So regarding the definition of d and of , T if and only if T . Moreover , based on the same argument , B , J would give the same power spectral density for z as for the case where B , J is . Let d ,..., represent the of lying on the unit circle . Now for , we define the following : 
  
  
 . 
 Hence , U and can be for every , . By following the same procedure as for the proof of , Theorem . , the existence of , will be shown in such a way that for any , setting will give a pair B , J that . 
  
 Fig . . Auxiliary system for minimization 
 Corollary . : If . and . hold for the of Fig . and , , then 
 , 
 where the optimization is done over all filter , and the noise variance making the system in Fig . internally stable and well . 
 Proof : According to the feasibility of finding u see for the proof , there exist the triplet , with a proper filter and , that for the system of Fig . . Furthermore , based upon the definition of and in and , the following can be derived for any : 
 . 
 So regarding Lemma . , since there exist a filter and a proper one making the feedback loop of Fig . internally stable and well and keeping intact , the following can be : 
  
 Now the proof is by that for 
 any , 
 To characterize , we will mostly use of linear and some on H optimization with input delay . Consider the structure of Fig . , where except for shifting the delay block to the plant model , which to 
 , 
 the same as Fig . hold . The of Fig . is internally stable and well if and only if the transfer function Ta from ,, , to za , ya , ra , in Fig . is a member of . It can be easily shown that Ta . So the feedback of Fig . and Fig . are equivalent in the sense of internal stability and well ness . Moreover , the and output variance of the in Fig . can be stated in of H as : 
  
 , 
  
 Fig . . Stability analysis of the equivalent system 
 where , and , 
 . Likewise , the following for the structure of Fig . : 
  
 in which Na and Ma . As seen , 
 and the 
 and . So every triplet that can the while making the system output satisfy for the of Fig . , can do the same for the the system of interest , in Fig . , and vice . In other , the in Fig . and Fig . are equivalent regarding the optimization problem in as well . This problem is studied for such feedback as auxiliary system of Fig . in . Consequently , it can be that the problem of finding is equivalent to an constrained optimal control problem which was proved to be convex . As another result , being a monotonically decreasing be . All in all , the interplay between the desired performance , the average data rate and the time delay is through 
 , and . 
 SIMULATION EXAMPLE 
 Consider the the following transfer function representation for the of Fig . : 
  
 where x , Assumption . . the of the previous section , we simulate the lower bound on in regarding five different of delay , , , , and for each , over a range of . Fig . the behaviour of the lower bound with respect . Additionally , it the operational when scalar uniform for and . First , as , in is a monotonically decreasing function of . Secondly and more importantly , when see . So greater delay worse best performance . The most significant 
  
 Fig . . on in for different of time delay h 
 outcome is associated with the behaviour of in with respect to delay . It can be from Fig . that for a fixed , is increasing in . Therefore , a delay in the channel an increase in the data rate to achieve a quadratic level of performance . The greater delay , the higher rate to be spent in order to get a certain level of performance . Indeed , this finding the delay free of . Another observation is the convergence of the data to the minimum rate for as . As in Fig . , high are to attain the ideal non performance . Along the of , , we now simply replace the in the independent scheme in Fig . , by a uniform scalar in order to assess the operational performance by a simple scheme . It is interesting to note that the operational average data rate in Fig . is at most around . away from the derived lower bound at all performance . 
  
 In this paper , rate constrained control noisy , causal but otherwise arbitrary control and digital noiseless communication with time delay , have been studied . For such , a certain level of performance is attainable if and only if the average data rate does not fall below a minimal value . A lower bound on this rate been . Through a numerical example , it been that the channel time delay the average data rate to achieve a level of performance . Moreover , by a simple scalar , operational average data fairly close around . to the lower bound have been . 
 APPENDIX 
 A . Feasibility proof of 
 Suppose that in the standard architecture in Fig . , 
 G , x Assumption . . Regarding the of x the fact , it can be from some in 
 that : 
 , 
 K 
  
 Fig . . Standard feedback loop for proving feasibility of finding 
 in which the variance of is the set of all proper which render the system of Fig . internally stable and well . The considered that finding is feasible . The feasibility of finding u in and in from the feasibility of . 
  
 ﻿ This paper closed form for the space frequency second order statistics for the power gain of indoor . These hold for channel in which path are clustered , both in their arrival and their arrival times . Assuming that the arrival times are independent of the of arrival , we derive a closed form expression for the covariance between the channel power gains at two , at any pair of and . This expression that same covariance asymptotic reduction attainable by increasing , only if , and that the covariance is symmetric in its dependence with and with . Thus , the channel power gain at higher frequency less variation than at lower . Finally , we show that the variable part of the covariance approximately as . 
 The propagation of radio within indoor is a complex phenomenon , depending on the operating frequency , the type and location of the and the presence of a usually large number of . Assuming linearity and that the channel vary slowly , the channel behaviour is fully by its frequency response ,, corresponding to the transform of its time invariant channel impulse response . 
 Due to multiple in the surrounding , is typically composed of a sequence of short with random arrival times and , which tend to decrease with arrival time . It been that , in indoor , both the arrival times and arrival of these path distribute in see and , respectively . As a result , given any two known to belong to the same cluster , one cannot assume their times to be independent . The same is true for their of arrival and for their corresponding . 
 The analysis of the space frequency statistics of the indoor wireless channel commonly been by the second order statistics of the complex frequency response or of its magnitude . The former case the complex correlation 
 This work was by the Research Council under , and Basal project , and by the Research Direction of Mar ´ ´ . 
 where H and H are at receiver or transmitter antenna x and x , respectively , and where complex conjugation . This complex correlation , with , been studied in , and in , analytically . In both , it is found that with the x and x more rapidly when the operating frequency is . The analysis in closed form for c , , for non isotropic three and two dimensional diffuse assuming independent , i . e ., that the complex channel gain associated with from one direction are independent of those from another direction . Under this assumption , it was shown in that for the dimensional case , 
 the speed of light , is the complex series coefficient of the distribution function of scattered power , is the th order function of the first kind , and f is the angle of the vector x x with respect to a reference azimuth direction . 
 The second case envelope correlation is concerned with E H H and the corresponding envelope correlation e , , . The dependence of the latter expectation on the studied in , , from measured data and for . These reveal that , as , e , , as the , although it to approach a minimum , asymptotic value in the range . . , which is significantly than the lower asymptote of c , , . 
 Our focus in this paper is on the channel power gain and its second order statistics , associated with E H H . Considering the latter expectation instead of that of the complex gains or their is relevant for two main . First , the instantaneous channel power gain at a given frequency the associated capacity . Hence , in the second order statistics of would allow for a better estimation of the performance of spatial diversity based upon , for example , maximum ratio combining with associated to different antenna and or carrier . Second , in based upon selection combining , as well as in multiple access subject to the power capture effect , the performance on the joint statistics of two or more power , from different and carrier . 
 In this paper , we find closed form for the second order statistics of the channel power gain , associated with the 
 These depend explicitly upon the , not necessarily equal and the separation distance . Our analysis is valid for an extended model , with with clustered arrival times and , as in the considered in , . The reveal how c , , are , but predict a strictly positive lower asymptote , which is by go to infinity , for any , and by , but only if . 
 In other , there a dependent additive term in c , , and , in the limit , increasing frequency or spatial separation asymptotically this term to zero , yielding the same asymptotic reduction . A second finding is that c , , in exactly the same way with respect to as it does with . The latter , by considering the case , that the power gain of with higher central frequency have smaller variability . On the other hand , the show that , beyond of a few central frequency , the dependent component in c , , as . 
 In the next section , we present the channel model and . The main are derived in Section , by . All are in the Appendix . 
 As in , , the wireless indoor considered here are by their impulse . Each impulse response of a sequence of path with random , at random times and , and where arrival times and are clustered . At a given reference location x in space , such random impulse response h can be written as 
 where ti , is the random arrival time of the th in the i th cluster and ai , is the corresponding real valued amplitude . Our associated with the arrival times , and of are next . 
 where Ti is the random arrival time of the i th cluster , and ti , is the random delay of the th in the i th cluster relative to Ti , where T T and ti , ti , , for all i . By definition , the cluster with its first ray , and thus 
 In the forthcoming analysis , it will be useful to describe the overall distribution of arrival times by their corresponding arrival density . This one to introduce the following preliminary and notation . For an infinite sequence of random arrival times , with x possibly 
 In this expression , is a random variable the amplitude of the i th cluster . The real valued random variable ai , is the amplitude of the th within the i th cluster relative to that cluster amplitude . The polarity pi , take from , with equal probability . 
 The cluster are independent given their arrival times . As in , the dependency of the cluster upon their arrival times can be by the cluster order moment delay profile 
 if T is random . Likewise , the relative are independent given , and we define 
 where , , for every i . As we shall see , here we will only need to consider the ,,..., . 
 Let i , be the angle of arrival , measured with respect to a reference the horizontal plane , of the with arrival time ti ,, As in , , the of arrival of the within the th cluster can be decomposed as 
 In this expression , is the angle of arrival of the th cluster , while , for each are the of arrival of the 
 If the transmitter and receiver and the scattering environment are chosen at random , then the cluster will be uniformly distributed , as in , . Likewise , the absolute will also be uniformly distributed . Moreover , the in , reveal that , in indoor , the distribution of the difference between cluster and the angle of the first cluster in each location is also uniform . This that , conditioned to a given location room and building , cluster arrival can be assumed independent . In contrast , we shall not assume that the are independent , since the within each cluster have been found to arrive from a relatively small range of . Instead , we will assume that , for all , the relative are independent and identically distributed . More precisely , we will work based upon the following assumption : 
 The cluster arrival are independent and uniformly distributed over , p . 
 The relative arrival i , are independent , identically distributed with probability density function : , p , . 
 Cluster arrival and relative arrival are independent , i . e ., , . 
 In any given indoor scenario , the impulse response from transmitter to receiver will depend on their corresponding antenna . Suppose the impulse response is when the antenna is at a given point x on the horizontal plane . If we consider two additional receiver , x x , x x , then their arrival times will increase or decrease , depending on their arrival . More specifically , if the separation is small relative to the distance from the , then the arrival can be considered to be the same at x and x . In such case , an arrival time at position x becomes and , at x and x , 
 is the random delay increment , in , for the th produced by moving from x to x or from x to x and is the speed of light . Thus , the channel frequency for receiver x and x are 
 For the above model and with only the given within it , we will next derive closed form for the second order statistics of , over both space separation and frequency . 
 In this section we will derive a closed form general expression for the space frequency covariance c , , defined in . This result is formally stated as : 
 Theorem : Under the in Section , be two random distributed as 
 , g g , , , b b and where denote , respectively , the transform and the L norm of the function . 
 where a from the fact that ak and that the are zero mean , i . i ., and independent of all the other . On the other hand , 
 Since each amplitude ak and the are i . i .. with zero mean , it that all of in which a given index is different from all the will be zero . This leaves only the following index : 
 Notice that and and to the double index notation for and within , the function can be written as , 
 From Assumption we have that the random variable is identically distributed for for every 
 The corresponding to T , T and the T and T have been in , proof of Theorem . Proceeding as in the latter proof , it is easy to show that these are as in please see for . Substituting , and and into , we obtain , the proof . 
 Theorem some interesting of the space frequency covariance c , , : 
 The difference T T is the variance of , i . e ., the variance of the impulse response energy . This is the only term which does not depend upon , . As already in , Remark , under mild on the impulse response statistics , the T , T on the right hand side of vanish as . The latter that , in our case , the space frequency covariance does not go to zero as the frequency separation unbounded . Interestingly , the expression for c , , here that this covariance asymptote given by the term T T is the same regardless of the separation . 
 The of the suggest that , as , increasing the separation between the two receiver antenna the covariance of the channel power gain between any pair of such that 
 and . Moreover , if the E , is which , then and E should vanish as d 
 the resulting asymptotic covariance reduction would be exactly the same that would be by . 
 We now show that , as , the frequency dependent in do vanish as , if O non zero . However , it turns out that the decay is not faster than . 
 Proposition : Suppose Assumption and let the distribute as in Theorem . Then , 
 are the complex series of . It is well known that the are oscillating and that their decay as . Hence , Proposition that the envelope of E as . The decay rate of E with respect on the transform of . For instance , if a deterministic value , then , 
 , and we obtain E e the last equality from theorem and that are the of the series of cos , see in the proof of Proposition below . On the contrary , if a uniformly bounded probability density function , then , when , E goes to zero as or d , but not faster than that . These are formally stated and proved in the following proposition : 
 Proposition : Suppose the of Proposition hold . Then , for every , there such that 
 As a consequence of Proposition , we have that the frequency dependent part of c , , see with increasing , asymptotically , as . Given the behaviour of the and if the series of decay sufficiently fast with , then c , , will decay approximately as a few of the frequency difference O the latter being given by O , since O for , see . Likewise , for a fixed separation , the two of this term decay as O and , respectively , for relatively large of O and . 
 We have derived closed form for the second order statistics of the power gain of indoor . These were for the extended channel model , wherein path arrival times and arrival are clustered . Our reveal that the c , , , associated with two antenna x , x with channel power gains H and H , respectively , is symmetric with respect to and O . Sine the when O is , it that the variance of also as . We have also shown that the dependent part of c , , goes to zero as O to infinity , which also if O non zero and we let . In addition , we have that the reduction of this dependent term approximately proportional to , for a few of the frequency difference O . 
 Proof : Proof of Proposition From the distribution see the statement of Theorem , we can as 
 where and are uniformly distributed over , p , and and are independent and identically distributed according to the cluster arrival angle distribution with probability density function . From this , 
 which . In order to characterize E , we first recall the identity ,. 
 Proof : Proof of Proposition For real valued , it that see , section . 
 , and because is bounded . in directly to , the proof . 
  
 ﻿ In this work we present an empirical study of the added propagation that may be associated with providing fixed wireless service from near ground base to in a suburban environment . We present for various of , classified according to the existence of in the propagation path and the choice of outdoor outdoor or outdoor indoor service . Our indicate that while on average the additional path associated with lowering the base antenna are relatively small , the variance of these will increase at near ground level , particularly in links . This as a result that the power margin for high availability of a near ground base antenna may be quite significant . 
 Index Channel modeling , fading , fixed wireless service , near ground propagation , path loss . 
 HE explosive growth in wireless demand an intense interest in diverse deployment . New , eager to participate in this very attractive market are considering various , suburban residential where electricity , telephone , cable are being from underground , without the use of . This the need to also evaluate the effectiveness of providing wireless service from outdoor near ground bases . We validate the applicability of well known path loss and small scale fading , which will be useful when designing such . To the best of our knowledge , such have not been in the open literature for this type of environment . Most with near ground bases less than height deal with urban or correspond to sensor in open or forested , as will be in detail below . Suburban have received less attention . Furthermore , as stated in , there is a lack of on the characterization of outdoor indoor wireless links , which are important for voice data transmission and wireless local area . The statistical description of the variation in the channel model , associated with lowering the base station will be useful for a system designer , considering that most currently available data to service provided by bases that are at least at lamp post height , where well established propagation apply . The scenario that is the objective of our study quite significantly from those that have been in the literature before . Therefore the characterization of this new propagation scenario cannot be based on currently known . 
 A thorough bibliographical search of work related to the subject led us to the relevant into . Each category with one specific aspect covered in our study : outdoor indoor propagation , near ground propagation , the effect of vegetation , and path loss . We discuss this classification in what , only those that specifically relate them to our work . 
 Outdoor Indoor Propagation : Various such as and therein have this subject for that in some are similar to ours . However in contrast to our study , in all of them the was at of at least . , while in some , the altitude of the subscriber unit was varied , to not lower than . . We note that due to the considerable difference between the scattering surrounding an outdoor and those in proximity of an indoor , lowering the altitude of the latter cannot be to have the same effect as that of lowering the . However some are similar to those in our work , such as a range of fade for the lower altitude . 
 Near Ground Propagation : This topic been in various , many of which are centered on modeling electromagnetic field propagation , such as . Here the scope is sensor in indoor with both at less than a height and link below . The analytical and empirical address very specific propagation , which can be analytically from electromagnetic and do not lend themselves to the derivation of statistical . Conversely , our study a wide range of of such complexity that they cannot be analytically and instead require statistical such as those in , . Propagation for sensor in an office environment are in , again a setting too far removed from ours to extrapolate valid . for outdoor sensor network propagation with both at low altitude over diverse of open have been in and . In and path for urban low altitude bases are in the context of military . The effect of lowering the subscriber terminal from an altitude of to is in , however this scenario is very different from our application . Nevertheless , among the is the reduction of the factor with antenna altitude , a result to be with that in our work . 
 Effect of Vegetation : Another element that the signal attenuation between base and user is vegetation . Theoretical the effect of these have been in . The effect of vegetation on frequency dispersion was in based on various simulation . Only some of the available theoretical for the attenuation by vegetation have been through empirical data . None of them have been for the residential studied here , but some relate to certain present in our study . In military are considered with bases at exceeding . and links that can involve forested . In this work a decrease in antenna height is also to reduce the factor . Empirical in , , which involve various of vegetation , show excess path between and exceeding . 
 Path Loss : With regard to the statistical description of path loss , single and slope log distance that consider shadow and small scale fading have typically been used . This is also the approach chosen for this work . In the case of outdoor propagation , the two ray model is often considered as a reference , . The presence of a break point leading to the dual slope path loss model been extensively , , . The use of these and the effect of lowering the antenna in a microcellular setting is in , however the considered range between . and . and were carried out on streets rather than in outdoor indoor . 
 The provide a background with regard to methodology and previous theoretical and empirical on modeling propagation links that may involve a terminal at low altitude . As , they do not specifically address the problem that is the objective of our study , namely to draw regarding the effect of providing suburban outdoor outdoor or outdoor indoor residential wireless service from low altitude bases , in contrast to doing so from a conventional height . We here consider both outdoor outdoor and outdoor indoor links , with and without the presence of vegetation . These will be useful when fixed wireless service to suburban , which may also be as a few adjacent in an system . In such of course , the relation between coherence for the specific environment and the transmission will also need to be considered in order to take into account possible fade reduction through frequency diversity . 
 We found that the additional propagation resulting from lowering the base antenna from . to . was not than at the median level , but may grow to . when availability is . We also quantify penetration when serving indoor from the outdoor antenna , and power by vegetation within the link . The latter are relevant when the lowering of the base antenna such an obstruction to the link . The measurement campaign was carried at . a transmitter and a power measuring receiver . The choice of frequency is based on the fact that it been used for fixed wireless service such as . A total of different links were tested in residential urban . For each link , the distance between base and subscriber ranged from to . Two large scale propagation were used as a comparison basis to our empirical model : equation and the two ray model . 
 The remainder of this paper is organized as : Section the measurement hardware and methodology . Section the statistical based on our empirical data . Finally , Section the . 
 The channel sounding system employed of a continuous wave transmitter and a purpose built receiver coupled to a power meter . The receiver is . This is wide enough to capture any frequency dispersion that the transmission , induced for example by movement of vegetation . This receiver was connected to a rotating arm of . length and stepwise under computer control in a displacement of approximately . At each of the angular , consecutive power were made . This consistency and to remove residual temporal . These were found to be very small typically less than . . The resulting was used as the power sample at that range and angular position . At each placement of the rotating arm , the power in a rotation were used to calculate the spatially path loss at that range and to generate the statistics of small scale with respect to that average . The size of the region for the spatial average is well within the shadow fade correlation distance in previous work . In fact the in the literature , corresponding to indoor , are in the range of to , the for outdoor suburban being much , . 
 The operating frequency was . . The and used dipole and patch depending on the type of scenario tested . When the angular spread of in a environment is comparable to the antenna , the received power and thus the calculated will be affected by the antenna gain pattern . For this reason , in the we chose that may be considered representative for the type of service at the specific scenario , as will be detailed later . In the case of the nominally , we took into account in our the minor gain less than over the azimuth range through of the measured antenna pattern . This is in correspondence with the fact that our involve a rotation . In the case where we used patch , in our path loss we considered the measured gain of the antenna in the direction of the direct arrival path . All were measured in an chamber , and measurement were used to ensure repeatability . 
 Before at each environment , two calibration were . First , the transmitter and receiver system were connected back to back with a short , cable and a step attenuator , the , to verify the received power . Next , the transmitter and receiver were connected to the system and a free space calibration was in an open area with the extended . above ground and by . The were to the design of the equipment to confirm its proper operation and to verify the dynamic range for reliable . In all field the received power was at least above the noise floor . were carried out at residential type , representative of a suburban environment where the base station could be at the street curb , at a range of to from a one or two story house . This space was by with and no higher than . We tested suburban in and a Mar , Chile , and on the campus of 
 Mar ´ in ´ iso , which with garden like vegetation outside of ground level . At each of these , multiple were carried out for similar : line of sight : ; outdoor outdoor outdoor indoor . The construction is of the brick and mortar type , with non of size as below . 
 All were made by first the antenna that the wireless service base station at . and then at . at exactly the same spot . This the effect of reducing the antenna altitude . The choice of . as reference height was based on the fact that at such altitude , non links exhibit practically path at the measured , thus providing a convenient reference value . Further in height would have added complexity without providing novel information . 
 outdoor outdoor : In this case , we emulate the terminal by the rotating arm the signal outside the home at from the residence , with link in the range . The in this case is designed to represent a customer terminal mounted on the wall of the residence , being by the . The was in close proximity to an outside wall at an altitude of about . . It was close to a window that would subsequently allow outdoor indoor through that same window without otherwise the setting . In this case , the used a patch antenna with a maximum gain of , typical for a wall mounted element . This was at three different along the wall for each link length tested to measure at nominally similar 
 . The on the rotating arm , whose height was varied from . to . employed a dipole . The setup is in Fig . . 
 outdoor indoor : The setup is shown in Fig . . In this case , the terminal was by the rotating arm inside the construction across a window with respect to the outside the signal . Link ranged from to . The height of the antenna was . , at that would in practice correspond to the possible of indoor or of user . The indoor floor in Fig . was between . and . above ground . The antenna was outside of the residence at adequate for base station , again with of . and . . In this case , both used were . Window sizes varied in width in the range of . to . and in height from . to . . The minimum window area was m . 
 outdoor outdoor and outdoor indoor : These were under the same as in the outdoor outdoor and outdoor indoor respectively , with the only difference that the direct path was by the foliage of one or two of and small . These were usually low , densely foliated of the genus privet , while were typically horse chestnut . The thickness of the foliated obstruction was typically in the range of . to . The link tested ranged from to . 
 This section the of the . We describe for the path loss in each of the previously and we include a thorough statistical description of the effect on the channel model that is a consequence of lowering the base station antenna . 
 outdoor outdoor : We here present the average over one receive antenna rotation . distance for all measured links of this type , at the two base station considered . This is shown in Fig . . In each case the plot as a reference the free space path loss equation and the path loss the two ray model . In the latter case , two for permittivity and conductivity , that cover the range of in the literature have been considered . Although these dielectric are frequency dependent , their change even over of or more will not place them outside the range of considered for our . As seen in Fig . the theoretical two ray path loss model is not very sensitive to the actual dielectric parameter and thus frequency dependent effects will be negligible . The fit of empirical data to the theoretical free space and is remarkably good . The dispersion of the data with respect to the free space model is comparable to the difference between the free space and the two ray model . It should also be that the path plotted here exclude small scale , which as before were out . However as will subsequently be shown , for these the small scale spatial were typically quite shallow high spatial factor . Consequently , the total path loss at any specific antenna position within the rotation is quite close to the average plotted here . It is also interesting to note that the follow the contour of the two ray model particularly at the ground level , even 
 though were made in a variety of , some of which included height of the order of one . In this setting , lowering the base antenna no significant change in path loss , as will be in greater detail in subsection . It is worth that for both , the first Fresnel zone was not blocked . 
 outdoor indoor : As in Section , for in the outdoor indoor scenario , the movable arm antenna was inside the residence at a height of . , user . Several indoor were used , all of them possessing to the outside antenna through a single window . The antenna , outside of the residence , was again at . and at . height . The measured path at both are shown in Fig . , where we also include free space and two ray model path as a reference . As , outdoor indoor path are somewhat than those for the outdoor outdoor case and the dispersion of is also , which may be attributable to the variety of window sizes considered . There is however no evidence that lowering the base antenna will alter the path loss model in a significant way . 
 outdoor outdoor and outdoor indoor : Both considered here correspond to where the direct path was by one or two of typical garden vegetation , basically and small as previously . Fig . the for the outdoor indoor scenario . The scarcity of data between and is produced by the existence of a row of 
 at this distance from the indoor antenna in some of the where were made . Additional vegetation at closer from the window . Therefore , at in excess of were in many by two vegetation . The show added attenuation and dispersion of than in the previous , a logical consequence of the greater variability of the environment . In this case , lowering the base antenna in somewhat greater , as well as greater uncertainty in their . 
 The extensively used log normal model that the spatially path loss in may be as : 
 between the base station and subscriber unit , d is the free space path loss at a distance d in , is the path loss exponent and is a zero mean distributed random variable in with standard deviation . We applied this model to our data after out small scale over one rotation of the arm that the antenna . As conventionally done , we chose d . of the model , multiple or choosing the intercept point d different from the free space path loss did not provide a measurably better fit for the set of link tested , therefore we only used this very simple model . 
 SUMMARY OF PATH LOSS STANDARD VARIOUS OF AND ANTENNA 
 We the empirical Cumulative Distribution Function of the between the best fit linear regression and the measured average path . The of would be that of a zero mean random variable i . e under the assumption that the log normal model . We found that in fact the fit was very good , the corresponding of the not being included here to meet space . The indicate that in general lowering the base antenna will increase the standard deviation of , i . e ., the model path loss prediction becomes more uncertain . established that the log normal model is indeed accurate for our , we summarize in Table I the relevant of the model for all the studied . 
 Our so far have small scale spatial , which as stated were out over the rotation of the arm with the receive antenna . Our analysis of the between the average path loss and the path measured at the various arm revealed that the corresponding are very well by Rice probability . We thus use the factor as a metric to determine the small scale statistics , from the empirical data according to the method in . Fig . the of the for both and for the outdoor outdoor and outdoor indoor , which provided respectively the for the and small scale fade among all tested . As can be seen , no significant in the small scale fade statistics are associated with the in base station altitude . As , small scale spatial are much for the outdoor outdoor case than for the far more complex outdoor indoor environment . 
 We here present the statistics of the difference in path loss associated with lowering the base antenna from the . position to . . We define this difference as 
 where . and . are respectively the path in for base antenna of . and . . These are at exactly the same azimuth position of the rotating arm for both antenna , i . e ., without any small scale path loss . 
 Fig . a the of for the outdoor outdoor and outdoor indoor . As before , these correspond to the two extreme , the falling between the two shown . The ensemble 
 Fig . . of at . and . for outdoor outdoor and outdoor indoor . 
 size for our calculated between and depending on the amount of in each category . As can be seen , for the outdoor outdoor case , lowering the base antenna will result in a slight decrease in path loss when considering the median ensemble value . This is consistent with what can be in Fig . b , which that at some the path loss is with respect to the free space value by the constructive interference of the ground ray . There is a clear increase in the median path loss for the more complex outdoor indoor scenario . For the latter case , as already , there is a variability of path with respect to the mean , particularly at low altitude . The result of this can also be in Fig . a . This figure that a considerable fade margin may need to be added to the link budget if adequate availability is to be when lowering the base antenna . This added margin from about for the outdoor outdoor case to approximately . for the outdoor indoor case at the probability level . 
 In many it may be more informative to have statistical knowledge on the increase in path with respect to a well known and deterministic reference , rather than with respect to at . , which are random . Choosing as reference the free space at the same distance , we thus the statistics of the excess path loss . This is defined as the difference in between the path loss measured at any given azimuth position of the receive antenna and the corresponding from the equation according to 
 Fig . b the of the for the outdoor outdoor and outdoor indoor , for the two antenna considered . Consistent with our previous , for the outdoor outdoor case the of the are quite low for both antenna . At . the is actually somewhat lower than at . . This is consistent with the previously result on the constructive interference from the near ground antenna 
 a of the difference in path loss for antenna of . and . . 
 in Fig . b at some . In contrast , for the outdoor indoor scenario , the is always than at . . As can be seen , to cover of , a fade margin of needs to be added for a base at . , while for a base at . this margin to , i . e . an extra is to achieve the same degree of coverage . This value may appear at first sight to be inconsistent with the . increase in Fig . a . However we note that the statistics of path loss increase in Fig . a are the result of the difference between two random . The increase may occur under where the is high or is low . When percentage of wireless coverage or link , an increase in due to the lowering of the antenna may be of little consequence when the initial value of the is low . In contrast , Fig . b the difference in path at both antenna with respect to an absolute reference . This us to draw on the fade margin with respect to free space propagation at a specific coverage level for both antenna . 
 We have a large amount of statistical data for path loss statistics in suburban , representative of fixed wireless near ground base station . We that for unobstructed outdoor outdoor , with base antenna of . and . , the propagation channel is well by the two ray or even the free space . Our 
 also indicate that the increase in excess path loss over free space propagation , associated with lowering the base antenna is quite modest . Typical are to at the median level and for availability , for all tested , provided all links remain of the same type when lowering the base antenna . However , when we consider all links of all , the will vary over a considerable range . For availability , additional may have to be for an , near ground , outdoor indoor link , while only are if the link is of the outdoor outdoor type . For a system designer this the between providing through a wall mounted outdoor antenna , with additional indoor retransmission hardware considering the need for a significant fade margin to avoid such additional . Both also involve taking into account the associated interference . Extra transmission power should be through link adaptation to limit it to the site specific , while indoor retransmission may need to be to reduce interference between neighboring . 
  
 ﻿In this paper, we study a 3-node asymmetric fullduplex AWGN two-way relay channel with relay private messages where two nodes can communicate with each other only through a third node called the relay, while the relay can itself exchange private messages with two other nodes. Cut-set upper bound and single sided genie upper bound are presented for this channel. To obtain an achievable rate, we use a superposition of nested lattice codes and random Gaussian codes for encoding at the senders, and successive interference cancelation for decoding at the receivers. It is shown that capacities are achieved within constant gaps 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie for restricted and non-restricted models, respectively.
 I. INTRODUCTION
 Relaying has the potential to obtain higher coverage extension and throughput enhancement at lower cost. One of the important research topics in this area is two-way relay channel (TRC) where two nodes can exchange information via a relay [1]-[4]. It is commonly used as a building block in network coding problems. It was shown in [1], that by using joint decode-and-forward and network coding, nearly optimal throughput can be accessible for the broadcast phase but it causes loss in multiplexing as expressed in [2]. In [3], it was shown that compress-and-forward scheme achieves rates within half bit of the capacity region in the Gaussian setting.
 In another powerful method, called compute-and-forward, the relay decodes linear equations of the transmitted messages instead of decoding each of them completely. Users, by knowing their own messages can extract the desired messages by receiving linear equations. This method can exploit interference for cooperative gains as well as noise suppression [5]. Based on this concept, in [4], an achievable scheme based on nested lattice codes was proposed for the asymmetric Gaussian TRC, which was shown to achieve the cut-set bound within 1/2 bit. In [6], by using nested lattice codes, the sum capacity of Y-channel is achieved within additive gap of 2 bits/sec/Hz and multiplicative gap of 4 bits/sec/Hz of single sided genie upper bound. In [7], by using a superposition of lattice codes and random Gaussian codes, the capacity region of two-pair bidirectional Gaussian relay network to within 3 bits/sec/Hz per user is characterized.
 In this paper, we consider an asymmetric full-duplex AWGN TRC with relay private messages where two user nodes can communicate with each other through a third node called the
 978-1-4799-5186-4/14/$31.00 ©2014 IEEE
 	(m mˆ1r, ˆ2r )	(m mr1,	r2)
 (m m12, 1r )(m m21, 2r )
 (m mˆ21, ˆr1)(m mˆ12, ˆr2)
 Fig. 1: The Asymmetric TRPC.
 relay. The relay has its own private messages to be delivered to two other nodes, and also some parts of users messages are to be used only at the relay. For simplicity, we use the term TRPC to refer to this model, in the sequel. There is no direct link between the users. Forward and backward channels between the relay and users are assumed to be asymmetric. We consider both restricted and non-restricted models for this channel. This situation resembles the case where a base station relays messages between users and delivers messages between the backbone system and the users [8]. In [8], the deterministic capacity of TRPC was obtained and was shown to be coincided with single sided genie upper bound. Here we consider Gaussian TRPC and obtain two upper bounds for this channel based on the cut-set upper bound, and the single sided genie upper bound.
 Also we present an achievable rate for this channel based on the schemes proposed in [4] and [7]. In uplink, nested lattice codes are used for messages that must be exchanged between the users while superimposed Gaussian random codes are used for messages that must be decoded at the relay (private parts). In downlink, structured binning and successive interference cancelation are used at the decoders. By examining the proposed achievable rate region, we show that for all channel gains, it achieves within 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie upper bound for restricted and non-restricted TRPC, respectively.
 The paper is organized as follows: In section II, system model and some preliminaries are reviewed. The upper bounds and achievable rates for Gaussian TRPC are proposed in sections III and IV, respectively. In section V, capacity gap calculation is characterized. Some concluding remarks are provided in section VI.
 II. SYSTEM MODEL AND DEFINITIONS
 Consider TRPC as shown in Fig. 1. All nodes are assumed to be full-duplex, and there is a forward and a backward AWGN channel between each node and the relay. Let f+(.) = max(f(.),0), C(x) = 1/2(log2(1 + x)), S = {1,2,r}, and Si = S\{i}, i ? S, (i.e. S excluding i).
 Node i ? S has messages mij ? Mij = [1,2nRij] to be delivered to the nodes j ? Si, where Rij ? R+ (positive real set). The messages are assumed to be independent. The messages of node i ? S are encoded into a codeword xni using an encoding function.
 	 ,	(1)
 where, xik, the kth symbol of xni , is a realization of a real random variable Xik, satisfying the power constraint
  . And,   denotes the previously
 received symbols at node i. This is called non-restricted encoding which can result some dependency between the signals transmitted by different users. In contrast, restricted encoding uses the encoding function
 	 .	(2)
 The received signal at node i ? S at time instant k are given by the following equations.
 yik	=	hrixrk + zik,	i ? {1,2}	(3)
 yrk	=	2
 X
 hirxik + zrk, i=1	(4)
 where hij denotes the channel gain between nodes i,j ? S, (i 6= j). zik is a realization of i.i.d. Gaussian noise {Zi}i?S ~ N(0,1) with zero-mean and unit-variance. Each node i uses a decoding function gi to decode {mji}j?Si as,
  . All rates are shown by
 a vector R = ({Rij}i,j?S,i6=j) and the sum-rate is denoted by Rsum = Pi,j?S,i6=j Rij. The messages Mij, encoding functions fik or fi (restricted), and decoding functions gi define a code (R,n) for TRPC. A decoding error happens if {mˆ ij 6= mij}i,j?S,i6=j. A rate tuple R   is said to be achievable if there exists a sequence of (R,n) codes with an average error probability that approaches zero as n increases. The set of all achievable rate tuples denotes the capacity region C of the TRPC. The sum-capacity is the maximum achievable sum-rate given by Csum = maxR?C Rsum. The degree of freedom (DoF) is defined as
 	Csum	Csum
 	d =	lim	  = lim	 ,	(5)
 SNR?8 0.5log(SNR) P?8 0.5log(P) where SNR denotes the ratio of available transmit power P to noise power which is one.
 Next, we review some preliminaries on lattice coding that will be used in the sequel. For more details see [4] and the references therein.
 A nested lattice code is defined in terms of two ndimensional lattices   and ?n, which form a lattice partition ?nC/?n, i.e., ?n ? ?nC. The nested lattice code is a lattice code which uses ?nC as codewords and the Voronoi region of ?n as a shaping region. For ?nC/?n, the set of coset leaders is defined as C = {?nC mod ?n}.
 In [4, Theorem 1], an achievable rate was proposed for TRC, where two nested lattice codes were designed, one for each source node. The theorem is as follows,
 Theorem 1: For an AWGN TRC, with power constraints P1 = P2 on the transmitted signals of the users, and PR on the transmitted signals of the relay. The following region is achieved,
   	,
   	 	,
 where , and , denote variances of zero-mean AWGN at the relay and users 1 and 2. The achievable rate region in Theorem 1 is within 1/2 bit of the cut-set upper bound, regardless of channel parameters.
 Remark 1: Note that, in the downlink, although the channel setting is broadcast, nodes 1 and 2 achieve their point-to-point channel capacities, (second terms of the two expressions in Theorem 1), without being affected by each other. This is because of the side information on the transmitted message at each node and the binning of message. For the complete proof see [4, Theorem 1].
 III. UPPER BOUNDS FOR GAUSSIAN TRPC
 In this section, we present cut-set upper bound and single sided genie upper bound for both non-restricted and restricted Gaussian TRPC.
 A. Cut-Set Upper Bounds For Non-Restricted Gaussian TRPC
 The cut-set bounds provide upper bounds from a set T ? S to its complement Tc ? S. The cut-set bounds for the TRPC are provided in the following theorem.
 Theorem 2: The capacity region of non-restricted Gaussian TRPC is upper bounded by
 	R12 + R1r = I(X1;Yr|X2) = C(|h1r|2P)	(6)
 	R21 + R2r = I(X2;Yr|X1) = C(|h2r|2P)	(7)
 	R1r + R2r = I(X1,X2;Yr) = C{(|h1r| + |h2r|)2P}	(8)
 	R12 + Rr2 = I(Xr;Y1|X1) = C(|hr2|2P)	(9)
 	R21 + Rr1 = I(Xr;Y2|X2) = C(|hr1|2P)	(10)
 Rr1 + Rr2 = I(Xr;Y1,Y2) = C{(|hr1|2 + |hr2|2)P} (11)
 Proof: It is very similar to what done in documents, e.g. see [6, Appendix A].
 Corollary 1: The sum-capacity of non-restricted Gaussian TRPC, Rsum, is upper bounded by min{C(|hr2|2P) + C(|hr1|2P) + C{(|h1r| + |h2r|)2P},
 C(|h1r|2P) + C(|h2r|2P) + C{(|hr1|2 + |hr2|2)P} (12) and DoF constraint satisfy d = 3.
 Proof: By summing (8)-(10), the first term of (12) is obtained. By summing (6), (7), and (11), the second term of
 (12) is obtained. By using (5), d = 3 is obtained.
 B. Single Sided Genie Upper Bounds For Non-Restricted Gaussian TRPC
 According to the general explanations presented in [8, Sec. V], in the traditional cut set bounds [9, Thm. 15.10.1], the nodes are divided into two sets T and Tc which represent the transmitting and receiving nodes, respectively. Nodes in each of these sets are assumed to fully cooperate by sharing their side information. Applying the traditional cut set bound to relay networks gives loose results. In contrast, in the single sided genie bound, not all the nodes in a set can share their side information. This method was used in the proof of the following theorem.
 Theorem 3: The capacity region of Gaussian TRPC is upper bounded by,
 R12 + R1r = C(|h1r|2P), R21 + R2r = C(|h2r|2P)	(13)
 R12 + Rr2 = C(|hr2|2P), R21 + Rr1 = C(|hr1|2P)	(14)
 R1r + R2r + max(R12,R21) = C{(|h1r| + |h2r|)2P} (15)
 Rr1 + Rr2 + max(R12,R21) = C{(|hr1|2 + |hr2|2)P} (16)
 Proof: Single sided genie approach is used to bound the last two sum-rates as follows: To obtain the bound for (Rr1+ Rr2+R21) in (15), we assume that a genie transfers  
 to node   to node 2. To obtain the bound for (Rr1 +Rr2 +R21) in (15), we assume that a genie transfers   to node 1 and   to node 2. To obtain the bound for (Rr1 + Rr2 + R12) in (16), we assume that a genie transfers   to node 1 and   to node 2. To obtain the bound for
 (Rr1 + Rr2 + R21) in (16), we assume that a genie transfers   to node 1 and   to
 node 2,.
 The Proof of some parts are presented in Appendix A. The whole strategy is similar to what was done in [6] for Y channel. The differences exist in the used side information which is resulted from the differences between the nature of two networks. In Y-channel, the senders are located in the same symmetric situation around the relay and the relay doesn’t have any private message. But in TRPC, the relay also acts as a sender node and is in a different situation in comparison to the other two senders.
 Corollary 2: The sum-capacity of Gaussian TRPC is upper bounded by,
 Rsum = C{(|hr1|2 +|hr2|2)P}+C{(|h1r|+|h2r|)2P} (17) and DoF constraint satisfy d = 2.
 Proof: By summing (15)-(16), the first term of (12) is obtained. Also by applying (5) to (17), d = 2 is obtained.
 C. Upper Bounds For Restricted Gaussian TRPC
 In this section, we consider restricted Gaussian TRPC which uses the encoding function (2) instead of (1). In this model the transmit signals are independent which results to a tighter upper bound. Corollary 1 and 2, and Theorems 2 and 3 are true for this model except that the term (C{(|h1r|+|h2r|)2P}) should be replaced with C{(|h1r|2 + |h2r|2)P} in (8), (12), (15), (17).
 IV. THE ACHIEVABLE RATE FOR GAUSSIAN TRPC
 In this section, we present an achievable rate for Gaussian TRPC. In our scheme, we use (2), so it is an achievable rate for both restricted and non-restricted Gaussian TRPC.
 The scheme uses the superposition of nested lattice codes and Gaussian random codes for encoding at the senders, and successive interference cancelation for decoding at the receivers. It can be considered as a mixture of the proposed method for TRC in [4], and the proposed method for twopair bidirectional Gaussian relay network in [7]. In encoding strategy, each user expresses its message into two parts, one for decoding at the relay and one for decoding at the other user. It uses Gaussian random codes for encoding the first part and nested lattice codes for encoding the second part which is not necessary to be decoded by the relay.
 We should mention that there is a difference between this method and the one used in [7] in which no private messages are assumed by the relay. In fact in [7], if we have e.g., R12 = R21, user 1 splits its message into a Gaussian codeword and a lattice codeword, while user 2 only transmits a lattice codeword. But we believe that by using Theorem 1, it is not necessary to consider Gaussian random codes for encoding the part of the message of user 1 with rate R12 -R21. All can be decoded by nested lattice codes.
 In the next step, the relay decodes Gaussian random code, and the linear equation of two lattice codes, with successive interference cancellation. Then, the relay maps its own private messages intended for each user and the linear equation of two lattice codes, each into a random Gaussian codeword, and sends the weighted superposition of them to two users. The last step is decoding at the users, where each user first decodes the undesired codewords that have larger weights than the desired codewords. Thus, those codewords are decoded and successively canceled from the received signal one by one.
 For the limited space of the paper, in the following, we only consider the case where the rates of relay private messages are greater than the rate of the other messages.
 A. Encoding at the Users
 According to the described strategy, The transmit signals at the nodes are given by,
  
 where  and  are codewords chosen from a random
 Gaussian codebook of size 2nR1r, 2nR2r, and 2nRri, for i ? [1,3], respectively.   and  are nested lattice coded ensembles with sizes 2nR12 and 2nR21 respectively.
 B. Uplink: Decoding at the Relay
 The relay first decodes the Gaussian x1r, x2r, simultaneously, then the lattice point . It can be done successfully so long as,
  
 C. Encoding at the relay
 The relay maps , and t to Gaussian codewords 
 from codebooks of size 2nRri, for i ? [1,3], respectively, with Rr3 = max(R12,R21).
 D. Downlink: Decoding at the Users
 We consider the case |hr2| = |hr1|. Other case can be solved by the same interpretation. The decoding at the users can be done by successive interference cancelation as follows: Decoding at user 2: It can be done successfully so long as,
 (26)
 (27)
 (28)
 Decoding at user 1: First user 1 decodes  , then by using Theorem 1, part t can be decoded. It can be done successfully so long as,
 (29)
 (30)
 In next section, we show that by choosing power coefficient appropriately any rate tuple within a constant of the upper bound is achievable.
 V. CAPACITY GAP CALCULATION FOR GAUSSIAN TRPC
 In this section, we establish the capacity gap calculation for Gaussian TRPC from single sided genie upper bound. Since the gap between the corresponding upper bounds of restricted and non-restricted Gaussian TRPC is within 1/2 bits/sec/Hz, we only characterize the capacity gap calculation for restricted Gaussian TRPC in the following lemma. The proof of this conjecture is shown based on the conclusions in section IV, via the following expressions:
 C{(|h1r| + |h2r|)2P} = C{(2|h1r|2 + 2|h2r|2)P}
 	= C{(|h1r|2 + |h2r|2)P} + 1/2,	(31)
 Lemma 1: By using the described strategy, for any rate tuple (r12,r21,r1r,r2r) satisfying
 r12 + r1r = C(|h1r|2P) - .5,	(32)
 r21 + r2r = C(|h2r|2P) - .5,	(33)
 r1r + r2r + r12 = C{(|h1r|2 + |h2r|2)P} - 1,	(34)
 r21 + rr1 = C(|hr1|2P) - .5,	(35)
 r12 + rr2 = C(|hr2|2P) - .5,	(36)
 rr1 + rr2 + r12 = C{(|hr1|2 + |hr2|2)P} - 1,	(37)
 there exists a choice of power assignments {aij}i,j?S,i6=j such that decoding of codewords with rates {Rij = rij}i,j?S,i6=j can be done with arbitrary small error probability for all channel gain coefficients. (See Appendix B for the proof.)
 Note that if (R > 1/3I), where I donotes the unit vector, satisfies (13)-(16), then the rate-tuple (r = R - 1/3) satisfies the conditions of Lemma 1. Thus, the sum-capacity and the capacity of restricted Gaussian TRPC can be obtained within 2 bits/sec/Hz, and 2/3 bits/sec/Hz per user of single sided genie upper bound, respectively. By respect to (31), these gaps are 2.5 bits/sec/Hz, and 2.5/3 bits/sec/Hz per user of single sided genie upper bound for non-restricted Gaussian TRPC, respectively. (Note that each user i ? S has two corresponding rates, {Rij}j?S\i).
 VI. CONCLUSION
 In this paper, we studied the capacity region of an AWGN two-way relay channel with relay private messages (TRPC) with separated users (no direct link between users). In this model the relay can exchange private messages with the users in addition to establishing the communication between two users. We consider both restricted (independent users signals) and non-restricted (dependent users signals) cases. We obtained a cut-set upper bound and a tighter single sided genie for upper bound for restricted and non-restricted nonasymmetric Gaussian TRPC. Then we extracted an achievable rate for this channel which was shown to be within constant gaps 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie are achieved for restricted and non-restricted TRPC, respectively. The proposed scheme applied the same strategy proposed in [7] which is based on using superposition of lattice codes and random Gaussian codes for encoding, and successive interference cancelation for decoding.
 APPENDIX A PROOF OF THEOREM 2
 Proof: By applying Fano’s inequality, and by considering that side information   is available at user 1 and
   at user 2, we have,
  
 where   as n ? 8. (a) follows from the fact that the messages are independent. Adding (38) and (39) results,
  
 where (b) follows from the fact that conditioning reduces entropy. (c) follows from the fact that given Xrn, ({mij}i,j?S) and   are independent.
 Again, we consider genie transfers side information   and   to users 1 and 2, respectively. By applying Fano’s inequality and assuming independent messages, we have,
  
 where   as n ? 8. Adding the above two terms
 and using chain information results in,
  
 (d) follows from (3) and the fact that Xri is a function of (mr1,mr2,Yri-1). (e) follows from (4) and given   are completely known. (f) follows from (4) and the fact that conditioning reduces entropy.
 APPENDIX B PROOF OF LEMMA 1
 Starting with (21)-(25) for uplink: From (24) and (25), we have, 22r12-2r21 = (|h1r|2a12)/(|h2r|2a21), and,
  ,
 From (24) and (25), we have,
  
 where,
  ,
 Adding a12 and a1r results,
  
 Starting with (26)-(30) for downlink: It is obvious that (26) is redundant since hr2 = hr1 and log(1 + x/(1 + x)) is an increasing function of x. By working on the other equations, we have,
  ,
 Adding the above expressions, we have,
  
 Gaussian autoregressive (AR) source subject to an average mean squared error (MSE) fidelity criterion. Toward this end, we consider the nonanticipative rate distortion function (NRDF) which is a lower bound to the causal and zero-delay rate distortion function (RDF). We use the realization scheme with feedback proposed in [1] to model the corresponding optimal “testchannel” of the NRDF, when considering vector Gaussian AR(1) sources subject to an average MSE distortion. We give conditions on the vector Gaussian AR(1) source to ensure asymptotic stationarity of the realization scheme (bounded performance). Then, we encode the vector innovations due to Kalman filtering via lattice quantization with subtractive dither and memoryless entropy coding. This coding scheme provides a tight upper bound to the zero-delay Gaussian RDF. We extend this result to vector Gaussian AR sources of any finite order. Further, we show that for infinite dimensional vector Gaussian AR sources of any finite order, the NRDF coincides with the zero-delay RDF. Our theoretical framework is corroborated with a simulation example. 
 Zero-delay source coding is desirable in various real-time applications, such as, in signal processing [2] and networked control systems [3]–[5]. Zero-delay codes form a subclass of causal source codes (see [6]), namely, codes where the reproduced source samples depends on the source samples in a causal manner. However, zero-delay source coding compared to causal source coding allow the reproduction of each source sample at the same time instant that the source sample is encoded. Unfortunately, causal source coding does not exclude the possibility of long blocks of quantized samples, which may cost arbitrary end-to-end delays. 
 Zero-delay codes (and causal codes) in constrast to noncausal codes cannot achieve the classical rate distortion function (RDF). Indeed, an open problem in information theory is quantifying the gap between the optimal performance theoretically attainable (OPTA) by non-causal codes, and the OPTA by causal and zero-delay codes, hereinafter denoted by Rcop(D) and RZDop (D), respectively. Notable exceptions where this gap is explicitly found are memoryless sources [6], stationary sources in high rates [7], and zero mean stationary scalar Gaussian sources with average mean squared error (MSE) distortion [8]. 
 Throughout the years, the interest in zero-delay applications is growing, thus, initiating further research on characterizing the fundamental limitations of the OPTA by zero-delay codes. Unfortunately, it turns out that RZDop (D) is very hard to compute and for this reason there has been a turn in studying variants of classical RDF that perform as tight as possible to 
 In this paper, we derive a tight upper bound to zero-delay source coding for vector Gaussian AR sources subject to an average MSE distortion. We consider nonanticipative rate distortion function (NRDF) (see, e.g., [1], [8], [9]), which gives a tighter lower bound to RZDop (D) compared to the classical RDF (see, e.g., [8, eq. (11)]). Then, we employ the feedback realization scheme proposed in [1, Fig. IV.3], that corresponds to the optimal ”test-channel” of NRDF for vector Gaussian AR(1) sources and average MSE distortion. Further, we give conditions to asymptotically stabilize the performance of the specific scheme. By invoking standard techniques using entropy coded dithered quantizer (ECDQ) [10], [11] on the innovations’ encoder of the feedback realization scheme, we derive the tight upper bound. In addition, we show how to generalize our scheme to vector Gaussian AR sources of any finite order. If the vector dimension of the Gauss AR source tends to infinity, we show that the RZDop (D) coincides with the NRDF. We demonstrate our results with a numerical example. Notation: We let R = (-8,8), N0 = {0,1,...}. 
 We denote a sequence of RVs by x  and its realization by xn = xn,xj ? Xj, j = 0,...,n. The distribution of the RV x on X is denoted by Px(dx) = P(dx). The conditional distribution of RV y given x = x is denoted byis denoted byPy|x(dy|xS=T. For a square matrixx) = P(dy|x). The transpose of a matrixS ? Rp×p with entriesS 
 Sij on the ith row and jth column, we denote by diag{S} the matrix having Sii, i = 1,...,p, on its diagonal and zero elsewhere. 
 In this paper we consider the zero-delay source coding setting illustrated in Fig. 1. In this setting, the p-dimensional (vector) Gaussian source is governed by the following discretetime linear time-invariant state-space model 
 N(0;Sx0) is the initial state, and the noise process wt ? Rq is an i.i.d. Gaussian N(0;Iq×q) sequence, independent of x0. We allow A to have eigenvalues outside the unit circle which means that xt can be unstable. 
 The system operates as follows. At every time step t, the encoder observes the source xt and produces a single binary codeword zt from a predefined set of codewords Zt of at most a countable number of codewords. Since the source is random, zt and its length lt are RVs. Upon receiving zt, the decoder produces an estimate yt of the source sample. We assume that both the encoder and decoder process information without delay and they are allowed to have infinite memory of the past. 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable-length binary codes zt. The countable set of all codewords (codebook) Zt is timevarying to allow the binary representation zt to be an arbitrarily long sequence. The encoding and decoding policies are described by sequences of conditional probability distributions as {P(dzt|zt-1,xt) : t ? N0} and {P(dyt|yt-1,zt) : t ? N0}, respectively. At t = 0, we assume P(dz0|z-1,x0) = P(dz0|x0) and P(dy0|y-1,z0) = P(dy0|z0). 
 policies. These design requirements are formally cast by the following optimization problem: 
 In this section, we give the definition of NRDF of vector Gaussian AR sources subject to an average MSE distortion. 
 ,	the	reconstruction	distribution	by P	 ,	and	the	joint distribution by   P(dyn||xn). 
 The marginal on yt ? Yt, P(dyt|yt-1), is induced by the joint distribution P(dxn,dyn). We assume that at t = 0, P(dy0|y-1,x0) = P(dy0|x0). 
 Given the previous distributions, we introduce the mutual information between xn and yn as follows 
 where E{·} is the expectation with respect to the joint distribution P(dxn,dyn). 
 Definition 1. (NRDF with average MSE distortion) (1) The finite-time NRDF is defined by 
 The optimization problem of Definition 1, in contrast to the one given in (2) is convex (see e.g., [12]). In addition, for the source model (1) and the average MSE distortion, then, by [1, Theorems 1], the optimal “test channel” corresponding to (4) is of the form 
 P*(dyt|yt-1,xt) = P*(dyt|yt-1,xt), t ? N0, (5) where at t = 0, P*(dy0|y-1,x0) = P*(dy0|x0), and the corresponding joint process {(xt,yt) : t ? N0} is jointly Gaussian. 
 The authors in [1, Theorem 2] realized the optimal “test channel” of (5) with the feedback realization scheme illustrated in [1, Fig. IV.3] that corresponds to a realization of the form: yt 
 where   is a scaling matrix; vt is an independent Gaussian noise process with N(0;Svt), Svt = diag{Vt} independent of x0; the erroris Gaussian with 
 Gaussian with N(0;?t|t), and  xt|t . Moreover,   are given by the following Kalman filter 
 Fig. 2: Asymptotically stationary feedback realization scheme corresponding to (6). 
 ?˜ t  diag 	= diag  0, and Et ? Rp×p is an orthogonal matrix. It is easy to verify that the following hold: 
 where I ? Rp×p denotes the identity matrix. By substituting (8) in (6), we can also observe that P*(dyt|yt-1,xt) = 
 The realization scheme of (6) becomes asymptotically stationary (stable) if one of the following two conditions hold: (1) A is stable, i.e., its eigenvalues have magnitude less than one; (2) the limit of the covariance matrix ?t|t exists, i.e.,   with	 [13]. This means that   with  , and 
 Next, we briefly discuss the resulting asymptotically stationary realization scheme depicted in Fig. 2. 
 (1) or (2), we ensure that   and it is unique. The error covariance matrix ? is diagonalized by introducing an orthogonal matrix E (invertible matrix) such 
 that E?E = diag. To facilitate the computation, we introduce the scaling process  , where 
 Preprocessing at Decoder: Analogously, we introduce the innovations process {k˜t : t ? N0} defined by (7d) and the scaling process {?˜t : t ? N0} defined by , with 
 asymptotic limits of Ft and Tt, respectively. The fidelity criterion ||kt - k˜t||22 at each t is not affected by the above processing of {(xt,yt) : t ? N0}, in the sense that the preprocessing at both the encoder and decoder do not affect the form of the squared error distortion function, that is, 
 The steady state values of (7b) is . The end-to-end MSE distortion of the scheme in Fig. 2 is 
 = trace(?  Hence, following [1, Theorem 2], the per unit time asymptotic limit of Gaussian NRDF subject to the total MSE distortion can be expressed as follows. 
 Clearly, the optimization problem in (10), is a logdeterminant minimization problem and can be solved using, for instance, Karush-Kuhn-Tucker conditions [14, Chapter 
 5.5.3] or semidefinite programming (SDP). A way of solving (10) is proposed in [15]. However, compared to that work, our realization scheme is implemented with feedback to take into account the effect of unstable sources in the dynamical system. 
 In this section, we derive an upper bound to the zero-delay Gaussian RDF using a subtractively dithered uniform scalar quantizer (SDUSQ) on the feedback realization scheme of Fig. 2. The SDUSQ scheme was introduced in [10] and since then it has been used in several papers (see, e.g., [4], [8], [16]) under various realization setups. However, it has never been documented for the realization scheme proposed in this work. Here, we consider the vector Gaussian AR(1) source of (1), and we quantize each time step t over p independently operating SDUSQ, with their outputs being jointly entropy coded conditioned to the dither. We extend our results when using vector quantization showing that at infinite dimensional vectors, the space-filling loss due to compression and the entropy coding extinguishes, i.e., Rna(D) and RZDop (D) coincide. 
 Next, we use the asymptotically stationary feedback realization scheme illustrated in Fig. 2 to design an efficient {encoder/quantizer,decoder} pair. 
 We select the quantizer step size ? so that the covariance of the resulting quantization error meets Sv. The encoder does not quantize the observed state xt directly. Instead, it quantizes the deviation of xt from the linear estimate   of xt. This method is known in least squares estimation theory as 
 Fig. 3: Scalar quantization by replacing a p-dimensional AWGN channel with p independently operating SDUSQ. 
 innovations approach and, therefore, the encoder is named as an innovations’ encoder. We consider the zero-delay source coding setup illustrated in Fig. 2 with the additional change of the p-parallel AWGN channels with p independently operating SDUSQ. This is illustrated in Fig. 3. Note that, all matrices and scalings adopted in Fig. 2 still hold when the aforementioned replacement is applied. 
 For each time step t, the input to the quantizer, is a scaled estimation error defined as follows 
 Moreover, at is an Rp-valued random process. The parallel p-dimensional AWGN channel is replaced by p independently operating SDUSQ, hence we can design the covariance matrix Sv of the AWGN corresponding to the p-parallel AWGN channels in such a way, that for each t, each diagonal entry of Vii,i = 1,...,p, i.e., Sv  diag{V }, to correspond to a quantization step size ?i,i = 1,...,p, such that 
 This results creates a multi-input multi-output (MIMO) transmission of parallel and independent SDUSQ. We apply SDUSQ to each component of at, i.e., 
 and we let rt be the Rp-valued random process of dither signals whose individual components {rt,1,...,rt,p} are mutually independent and uniformly distributed RVs rt,i ~ Unif  independent of the corresponding source input components at,i, ?t,i. The output of the quantizer is given by 
 Note that ß˜t = (ß˜t,1,...,ß˜t,p) can take a countable number of possible values. In addition, by construction (see Fig. 2), the sequences {at : t = 0,1,...} and {ß˜t : t = 0,1,...} are not 
 Gaussian any more since by applying the change illustrated in Fig. 3, {at : t = 0,1,...} and {ß˜t : t = 0,1,...} contain samples of the uniformly distributed process {rt : t = 0,1,...}. As a result, the Kalman filter in Fig. 2 is no longer the least mean square estimator. 
 Entropy coding: In what follows, we apply joint entropy coding across the vector dimension p and memoryless coding across the time, that is, at each time step t the output of the quantizer ß˜t is conditioned to the dither to generate a codeword zt. The decoder reproduces ßt by subtracting the dithered signal rt from ß˜t. Specifically, at every time step t, we require that a message ß˜t is mapped into a codeword zt ? {0,1}lt designed using Shannon codes [17, Chapter 5.4]. For a RV x, the codes constructed based on Shannon coding scheme give an instantaneous (prefix-free) code with expected code length that satisfies the bounds H(x) = E(l) = H(x)+1. 
 Since the SDUSQ operates using memoryless entropy coding over time, the following theorem holds. 
 Consider the realization of the zero-delay source-coding scheme illustrated in Fig. 2 with the change of AWGN channel with p-parallel independently operating SDUSQ illustrated in Fig. 3. If the vector process {ß˜t : t = 0,1,...} of the quantized output is jointly entropy coded conditioned to the dither signal values in a memoryless fashion for each t, then the operational Gaussian zero-delay rate, , satisfies 
 where p is the dimension of the state-space representation given in (1), while the average MSE distortion achieves the end-to-end average distortion D of the system. 
 The previous main result combined with the lower bound on Gaussian zero-delay RDF, leads to the following corollary. 
 Consider the realization of the zero-delay source-coding scheme illustrated in Fig. 2 with the change of AWGN channel with p-parallel independently operating SDUSQ as illustrated in Fig. 3. Then, for vector (stable or unstable) Gaussian AR(1) sources the following bounds hold 
 Proof. This is obtained using the fact that Rna(D) = RZDop (D), (10) and Theorem 1. 
 The bounds derived in Corollary 1 based on the realization scheme of Fig. 2 hold for vector Gaussian sources of any order. 
 Proof. This is shown by augmenting the state of the statespace model of (1). For details see [18, Appendix B]. 
 (1)	For stationary stable scalar-valued Gaussian AR sources, our upper bound in Theorem 1 coincides with the bound obtained in [8, Theorem 7]. However, the upper bound in [8] is obtained using a realization scheme with four filters instead of only one that we use in our scheme. In addition, our result takes into account unstable Gaussian sources too. 
 (2)	Compare to [16], we use ECDQ based on a different realization setup that results into obtaining different lower and upper bounds. 
 Example 1. We consider a two-dimensional unstable Gaussian AR(1) source as follows: 
 where xt ? R2, the parameter matrix A is unstable because one of its eigenvalues, denoted by ?i(A), has magnitude greater than one, the pair (A,B) is stabilizable and wt ~ N(0;I2×2). By invoking SDPT3 [19] we plot the theoretical attainable lower and upper bounds to the zerodelay RDF. This is illustrated in Fig. 4. As expected from theory,bits/source sample. 
 It is interesting to observe that if instead of uniform scalar quantization we quantize over a lattice (vector) quantizer followed by memoryless entropy coded conditioned to the dither, then the upper bound in (15) becomes 
 where Gp is the normalized second moment of the lattice [11]. If we take the average rate per dimension and assume an infinite dimensional vector Gaussian source, then by [11, Lemma 1], Gp ? 2pe1 , and the terms due to space-filling loss and the loss due to entropy coding in (18) asymptotically goes to zero. Utilizing the latter, and the fact that Rna(D) = RZDop (D), we obtain 
 We considered zero-delay source coding of a vector Gaussian AR source under MSE distortion. Based on a feedback realization scheme that quantizes the innovations of a Kalman filter with a SDUSQ, we derived an upper bound to the zerodelay RDF. We discussed the performance of this scheme when using lattice quantization. For infinite dimensions we observed that the NRDF coincide with the zero-delay RDF. An illustrative example is presented to support our findings. 
 As an ongoing research, we will apply the proposed coding scheme based on SDUSQ to find the actual operational rates corresponding to the zero-delay RDF. Moreover, we will examine similar coding schemes for fixed-length coding rate. 
 ﻿ This paper novel on the joint problem of sampling , reconstruction and quantization of . literature on this topic exclusively with band limited in form . Our key departure from is that we deal with continuous time reconstruction of not necessarily band limited . Our approach and from optimal data and horizon control theory . The key conclusion from the work here is that , in the case under study , the optimal design problem can be partitioned into two sub , namely i the design of an optimal filter by sampling and an optimal , which works directly on the . Simulation are which illustrate the performance of the optimal A converter designed via these . 
 Key : Sampling , quantization signal , data control . 
  
 INTRODUCTION 
 In many , one needs to convert , continuous time into discrete time . This to an important set of regarding the best way to represent a signal by a sequence of and , such that the information loss inherent in the sampling and quantization process is in some sense . In the present work , we are interested in how to quantize a possible non band limited signal to obtain the possible reconstruction distortion . 
 We will show that , for a given sampling rate and reconstruction , minimization of reconstruction error , in an L sense , can be converted into a discrete time problem . It turns out that if an appropriate filter is used , then all the information to find the optimal sequence can always be extracted from discrete time of its output , even if the continuous time input signal is not band limited . 
 the optimal quantization problem to finding the solution of a combinatorial optimization , which is in general intractable . Our proposal is to convert the optimal quantization problem into a data moving horizon optimization problem with decision . The method excellent and only limited computational effort . It our previous work in by on data rather than merely on discrete time . 
 Background to the work here from distinct . The first of these is associated with the problem of sampling in the absence of quantization . 
  
 Number : EX 
 The second related field of research is concerned with quantization of where the sampling strategy been . The third stream of prior work in the area of data control theory . Here , the emphasis typically been on regulation zero reference with unconstrained decision . In the present work we extend these to account for non zero reference and decision . 
 Our approach from the work above by virtue of the fact that we design the joint optimal sampler and data moving horizon optimization . This to significant performance gains , with alternative which do not take account of the interaction between sampling and quantization . 
 The remainder of this work is organized as : In Section we present the continuous time AD conversion problem and how it can be into discrete time . Section the continuous time horizon . Simulation are included in Section . Section . 
 PROBLEM FORMULATION 
 The general form of the under study is in Fig . . 
  
 Figure : Block diagram of the general sampler system . 
 The sampler the continuous time signal a into a sequence 
 u u , u U , Z 
 where U is the finite and given set of 
 U s ,..., 
 corresponding to the available quantization . 
 The sampling interval is constant and equal to . Thus , are at a rate of per second . 
 In Fig , the reconstruction discrete time sequence u into a continuous time signal . For example , in case of zero order hold reconstruction , the impulse response be . In the classical framework of perfectly band limited reconstruction , would be an ideal low pass filter with cutoff frequency t . On the other hand , in most practical , zero order hold or some other form of short impulse response filter sometimes non causal is generally used for reconstruction . 
 The the error frequency weighting filter see , e .., . It one to represent the different impact of the error at different for a particular application . For example , if the system is employed for audio , be designed to model the response of human hearing . We are interested in designing a which the L norm of the frequency weighed error see Fig . , i . e ., the cost function 
  
 where the standard L norm over the real line , i . e . 
  
 For the analysis below , it is more convenient to rearrange the system of Fig . to the equivalent form shown in Fig . . In this figure , the continuous time filter is by the transfer function 
  
 and its associated impulse response . 
  
 Figure : Equivalent block diagram 
 In order to represent the continuous time filtering by , we define the continuous time version of u as 
  
 The output of , namely , is given by 
 Substituting into one 
  
 If we denote the impulse response of the frequency weighting L , then its output 
  
 As a consequence , the frequency weighted error can be written as 
  
 Thus , the quest for optimal sampling and quantization can be stated as the optimization problem of finding the sequence u as in that the L norm of the reconstruction error , i . e .: 
 u min 
 L . Reformulation in Discrete Time 
 It will now be shown that the L continuous time optimization problem in is equivalent to an discrete time optimization problem , where the weighting depend on the signal inter sample behaviour . This is established in the following lemma , originally without a formal proof in . 
 Lemma frame a frame forLet the sequence of span , i . e . , with Z 
  
 for all . Let L be defined as in for the sequence of . Let the signal a L , and define , and ,, via 
 Y 
 F , 
  
 where the standard inner product in L . 
 Then : 
  
  
 Proof Substituting in we can write 
  
  
 We note that the first term in the last line of is well defined since a L as by the Lemma . The second term is finite by virtue of the inequality and the fact that L , 
 which in turn the third integral is also bounded . Since inner are by definition linear and a real , one by substituting into that 
  
 which is equivalent to . 
 Remark If we make the change of 
  
 where u is an un sequence that the global minimum , namely , then 
  
 Remark From lemma and remark , it is clear that to minimize the only information is an optimal un sequence u or , alternatively , the sequence and the ,. The latter to of the function of , which can be determined off line after choosing reconstruction and error weighting and then incorporated to the quantization algorithm . On the other hand , can be from u by with respect to u and to zero , which to 
  
 . filter for Optimal Quantization 
 In practice , any quantization algorithm to work with discrete time . and arise the need to determine whether a quantization algorithm can elaborate or obtain from of the input . 
 Consider , first , the determination of the series of . From definition , we have 
  
  
 There might exist more than one optimal sequence if the reconstruction stage is redundant . 
 From the last line of , it is clear that the series of through a filter with frequency response can be by passing the input sig given 
 by 
  
 and then taking the every . i . e ., if we denote the impulse response of by , then a , , 
 Let us next consider the determination of u , the sequence of which reconstruction error in the absence of quantization . It is known from sampling theory that , for any input signal , u can be by sampling the output of a filter to the reconstruction filter . From these , for the system in Fig . , the ideal filter for a given reconstruction filter is given by 
  
 where 
  
 is the discrete time transform of the 
 . Notice that e 
 The above suggest that all the necessary information about the input signal for optimal quantization to be feasible can be from of the input signal , and that the filter is not unique . We will provide next necessary and sufficient for a filter to yield that allow for optimal quantization . 
 Consider the discrete transform of u , and let and denote the discrete and continuous of any , respectively . 
 Since u is a sequence of of a by , we have 
 of the input signal 
 by another filter . The discrete transform of such sequence of would be 
  
 Recovery of can be in the discrete time domain by a discrete time filter to the sequence , such that 
  
 From and it can be seen that a sufficient and necessary condition is the existence of a periodic transfer function , such that 
  
 for the stage to be able to determine u from the and allow for optimal quantization . As a particular case , if A , then , from and , 
 G . 
 The latter equality can also be from by that A if and only if , ,,,. 
 Of course the stage would need to implement the correction filter based upon knowledge of and , according to . Two important special are to be : 
 If a no frequency beyond rad , then any filter satisfying 
  
 for some C C would make optimal quantization possible from the . This result is not surprising , since by sampling theorem , the of a band limited signal contain all the information about the complete signal . 
 If is band limited to a frequency a but a energy at greater than a , then any prefilter band limited exactly to a satisfying 
  
 for some K K would have a feasible correction filter that optimal quantization possible from the . 
 The conclusion from the above is that quantization for optimal reconstruction of an input signal not possible , but either the use of an appropriate filter satisfying to get the from , or , alternatively , the needs to know the signal between sampling . 
 THE DATA HORIZON 
 For the general case , minimization of would require the evaluation of for every possible sequence 
 the optimization becomes in u , u U . For sufficiently long , tractable . To overcome this problem , we propose to use from the horizon control framework and optimize over a short horizon of . A based on this idea been recently by the current in for an all discrete time system , near optimal performance with rather short horizon . In what we will extend this idea to the data case , and show , via , that significant distortion reduction is when converting non band limited . 
 . Optimal Quantization Over a Finite Horizon 
 Consider the system at . Instead of to optimize the cost over , we will aim to minimize the cost within a finite time interval 
 N , where , are design . We will only concentrate on the optimal sequence of to be for the interval , 
 defined as 
  
 The number , therefore , for the non causality of . 
 For the purpose of in the horizon the effect of past , it is convenient to describe the continuous time filter by its state space representation 
 x Ax 
 where A , , R and is as defined in . In , for a possible non causal . If is causal , then . 
 The cost function to minimize is 
  
 known , then , from and , can be determined by 
  
 for . 
 Notice that the first term on the right hand side of the effect of the choice of u within the horizon . This term to the forced response of to the input u , which can be conveniently as 
  
 On the other hand , the second term on the right hand side of the natural response of when the initial state is . We define the difference between this initial state response and the input signal a within the horizon as the target function , for the horizon at : 
 By and , the cost can be expressed as the L norm of the difference between the zero initial state response and the target function : 
 Substituting into one 
  
 Since L , one can exchange the order of sum and integration in and rewrite it in matrix form as 
  
 where the vector and the symmetric , positive definite matrix are defined element wise as 
 , 
  
  
 j , , ,... 
 . The Data Horizon 
 The derived above for the cost function over a finite horizon allow us to introduce the data horizon . The algorithm , at a given instant , the vector of u that the total reconstruction error from to defined in . Then , the first element of u is sent to the output of the . The horizon is then forward by , and iteration . The algorithm , beginning at instant , can be as : 
 Step . Calculate the matrix in 
 Step . Calculate Y 
 Step . Find the by 
 Step . Output , the first element of Step . Increment and go to Step . 
 The sequence of step the output of the data horizon sampler . If the input signal is not band limited to , the algorithm and frequency weighted quantization noise . For that purpose , it does simultaneous adaptive filtering on the input signal and adaptive noise shaping of the quantization noise , thus respecting the interaction between both phenomena . 
 It is interesting to note that , as the horizon is made , the output of the sampler defined above the optimal feasible output sequence possible defined in . 
 SIMULATION STUDY 
 We will first show an example the performance of the data horizon against the so all discrete time horizon in in the following situation : 
 The input signal , a , is an audio signal that frequency up to . Its frequency energy content is shown in Fig . . 
  
 Figure : Spectral composition of the input signal a . • The sampling frequency is half the to avoid i . e ., . 
 The order hold reconstruction , 
 i . e ., it impulse response . • The frequency error weighting filter to the third order model for the acoustical response of the human ear . Its frequency response is shown in Fig . 
  
 Figure : Frequency response of filter . 
 • No filtering is used , i . e ., . 
 In the simulation , the full knowledge of , and the matrix defined in . However , it based on direct of the input signal . Thus , since the implicit filter is a unity gain , that the all discrete time will be unable to determine the target function to be by the reconstruction stage . On the other hand , the the same matrix but access to the behaviour of the input signal . 
 Fig . the reconstruction error from the of both , for several horizon . It 
  
 Figure : Reconstruction error to distortion for from the of all and data , for to , for a non band limited input . 
 can clearly be how , in this case , the converter in the present work the all discrete time converter of . It can also be seen that the distortion a small decrease with the increase of the horizon length . This that , in this example , the main contributor to the distortion is noise , for both . Notice that the without an anti filter . This , together with its much lower distortion in comparison with that of the , that the data horizon optimization algorithm a form of filtering of the input signal that effectively . 
  
 This paper shown how horizon data control can be to design optimal AD . A key departure from in this area is that we optimize a version of the continuous time reconstruction error for not necessarily band limited input . Inter , we show that the optimal design problem can be decomposed into two , namely the design of an optimal analogue filter together with an optimal . We that the latter is feasible if only of the are available . The efficacy of the method been by an example an audio signal below its rate . 
  
 ﻿ This paper presents novel results on the joint problem of sampling, reconstruction and quantization of analog signals. Existing literature on this topic deals exclusively with band-limited signals in sampled form. Our key departure from earlier results is that we deal with continuous time reconstruction of not necessarily band-limited signals. Our approach utilizes concepts and tools from optimal sampled-data and receding horizon control theory. The key conclusion from the work presented here is that, in the case under study, the optimal quantizer design problem can be partitioned into two sub-problems, namely (i) the design of an optimal analog pre-filter followed by sampling and (ii) an optimal quantizer, which works directly on the pre-sampled signals. Simulation results are presented which illustrate the performance of the optimal A-D converter designed via these principles. 
 Key Words: Sampling, quantization, frames, signal processing, sampled-data control. 
 In many applications, one needs to convert analog, continuous time signals into quantized discrete time signals. This leads to an important set of questions regarding the best way to represent a signal by a sequence of sampled and quantized values, such that the information loss inherent in the sampling and quantization process is minimized in some sense. In the present work, we are interested in how to quantize a possible non band-limited signal to obtain the lowest possible reconstruction distortion. 
 We will show that, for a given sampling rate and reconstruction filters, minimization of reconstruction error, in an L2 sense, can be converted into a discrete time problem. It turns out that if an appropriate pre-filter is used, then all the information required to find the optimal quantized sequence can always be extracted from discrete time samples of its output, even if the continuous time input signal is not band-limited. 
 Solving the optimal quantization problem amounts to finding the solution of a combinatorial optimization programme, which is in general computationally intractable. Our proposal is to convert the optimal quantization problem into a sampled-data moving horizon optimization problem with quantized decision variables. The proposed method gives excellent results and incurs only limited computational effort. It generalizes our previous work reported in [1][2][3][4] by concentrating on sampled-data signals rather than merely on discrete-time sequences. 
 Background to the work described here arises from distinct streams. The first of these is associated with the problem of sampling in the absence of quantization [5][6]. 
 The second related field of research is concerned with quantization of signals where the sampling strategy has been pre ordained [7][1][8][9]. The third stream of prior work arises in the area of sampled data control theory. Here, the emphasis has typically been on regulation (zero reference) problems with unconstrained decision variables [10] [11]. In the present work we extend these concepts to account for non zero reference signals and quantized decision variables. 
 Our approach differs from the work described above by virtue of the fact that we design the joint optimal sampler and quantizer using sampled data quantized moving horizon optimization. This leads to significant performance gains, compared with alternative approaches which do not take account of the interaction between sampling and quantization. 
 The remainder of this work is organized as follows: In Section II we present the continuous time AD-conversion problem and how it can be translated into discrete-time. Section III introduces the continuous time receding horizon quantizer. Simulation studies are included in Section IV. Section V draws conclusions. 
 The sampling interval is constant and equal to t seconds. Thus, quantized samples are generated at a rate of 1/t samples per second. 
 In Fig 1, the reconstruction filter F converts the discrete time sequence u into a continuous time signal. For example, in case of zero-order hold reconstruction, the impulse response of F would be f(t) = µ(t)+µ(t-t). In the classical framework of perfectly band-limited reconstruction, F would be an ideal low-pass filter with cutoff frequency 1/2t [12]. On the other hand, in most practical applications, zero-order hold or some other form of short impulse response filter (sometimes non-causal) is generally used for reconstruction. 
 The filter H is the error frequency weighting filter (see, e.g., [4]). It allows one to represent the different impact of the error at different frequencies for a particular application. For example, if the system is employed for audio signals, then H could be designed to model the psychoacoustical response of human hearing [13]. We are interested in designing a quantizer which minimizes the L2 norm of the frequency weighed error  (see Fig. 1), i.e., minimizes the cost function 
 For the analysis below, it is more convenient to rearrange the system of Fig. 1 to the equivalent form shown in Fig. 2. In this figure, the continuous time filter ? is characterized by the transfer function 
 In order to represent the continuous time filtering performed by ?, we define the continuous time version of u as 
 If we denote the impulse response of the frequency weighting filter H by h(·) ? L2, then its output satisfies 
 (10) Thus, the quest for optimal sampling and quantization can be stated as the optimization problem of finding the sequence u as in (1) that minimizes the L2 norm of the reconstruction error, i.e.: 
 It will now be shown that the L2 (continuous time) optimization problem in (11) is equivalent to an	2 (discrete time) optimization problem, where the weighting values depend on the signal inter-sample behaviour. This is established in the following lemma, originally introduced without a formal proof in [2]. 
 Lemma 1frame boundsbe a frame forLet the sequence of functions0 span= R{=?P <(· - kt8), i.e.}k?Z = {W ??(· -Lkt2, with)}k?Z 
 for all w ? W. Let w(·) ? L2 be defined as in (8) for the sequence of scalars . Let the signal a˜ ? L2, and define F[j,k] and Y [k], k,n ? Z via 
 We note that the first term in the last line of (17) is well defined since a˜ ? L2 as required by the Lemma. The second term is finite by virtue of the Cauchy-Schwartz inequality   and the fact that w ? W ? L2, 
 which in turn implies the third integral is also bounded. Since inner products are by definition linear and a˜ and w are real signals, one obtains by substituting (8) into (17) that 
 where u is an un-quantized sequence that yields the global minimum of V in (16), namely V , then 
 Remark 2 From lemma 1 and remark 1, it is clear that to minimize (3) the only information needed is an optimal un-quantized sequence u (or, alternatively, the sequence {Y [k]}k?Z) and the coefficients F[j,k]. The latter corresponds to samples of the autocorrelation function of ?(·), which can be determined off-line after choosing reconstruction and error weighting filters and then incorporated to the quantization algorithm. On the other hand, {Y [k]}k?Z can be obtained from u by differentiating (16) with respect to u and equating to zero, which leads to 
 In practice, any quantization algorithm has to work with discrete-time values. Remarks 1 and 2 arise the need to determine whether a quantization algorithm can elaborate or obtain  from samples of the input signals. 
 Consider, first, the determination of the series of coefficients {Y [k]}k?Z. From definition (13), we have 
 There might exist more than one optimal sequence if the reconstruction stage is redundant. 
 From the last line of (22), it is clear that the series of coefficientsnal through a filter with frequency response{Y [k]}k?Z can be obtained by passing the input sig-GY (j?) given 
 and then taking the samples every t seconds. i.e., if we denote the impulse response of GY by gY (·), then Y [j] = (a * gY )(jt), ?j ? Z, 
 Let us next consider the determination of u, the sequence of samples which minimizes reconstruction error in the absence of quantization. It is known from sampling theory [14][15] that, for any input signal, u can be obtained by sampling the output of a pre-filter GS(j?) matched to the reconstruction filter. From these results, for the system depicted in Fig. 2, the ideal matched pre-filter for a given reconstruction filter ?(j?) is given by 
 The above results suggest that all the necessary information about the input signal for optimal quantization to be feasible can be obtained from samples of the filtered input signal, and that the required pre-filter is not unique. We will provide next necessary and sufficient conditions for a pre-filter to yield samples that allow for optimal quantization. 
 Consider the discrete Fourier transform of u, and let fˆ(ej?t) and gˆ(j?) denote the discrete and continuous Fourier transforms of any , respectively. 
 pre-filtered by another filter GX(j?). The discrete Fourier transform of such sequence of samples would be 
 Recovery of  can be achieved in the discrete-time domain by applying a discrete-time filter G(ej?t) to the sequence v, such that 
 From (26) and (28) it can be seen that a sufficient and necessary condition is the existence of a periodic transfer function G(ej?t), such that 
 for the quantizer stage to be able to determine u from the samples v (and allow for optimal quantization). As a particular case, if A? = 1, then, from (23) and (24), 
 The latter equality can also be obtained from (21) by noting that A? = 1 if and only if F[j,k] = dj,k,?j,k ? Z. 
 Of course the quantizer stage would need to implement the correction filter G(ej?t) based upon knowledge of GS(j?) and GX(j?), according to (29). Two important special cases are to be highlighted: 
 If a has no frequency components beyond p/t [rad/s], then any pre-filter GX satisfying 
 for some constants 0 < C1 = C2 < 8 would make optimal quantization possible from the samples v. This result is not surprising, since by Shannon’s sampling theorem, the samples of a band-limited signal contain all the information about the complete signal [12]. 
 If ? is band-limited to a frequency a = p/t but a has energy at frequencies greater than a, then any prefilter GX band-limited exactly to a satisfying 
 for some constants 0 < K1 = K2 < 8 would have a feasible correction filter that makes optimal quantization possible from the samples v. 
 The conclusion from the above cases is that quantization for optimal reconstruction of an input signal not bandlimited to p/t is possible, but demands either the use of an appropriate pre-filter satisfying (29) to get the samples from, or, alternatively, the quantizer needs to “know” the signal between sampling instants. 
 For the general case, minimization of (16) would require the evaluation of (16) for every possible sequence 
 the optimization programme becomes computationally in-{u[k]}k?Z , u[k] ? U. For sufficiently long sequences, tractable. To overcome this problem, we propose to use concepts from the receding horizon control framework [16] and optimize over a short receding horizon of samples. A quantizer based on this idea has been recently proposed by the current authors in [3][4] for an all discrete-time system, achieving near optimal performance with rather short horizon lengths [17]. In what follows we will extend this idea to the sampled data case, and show, via simulations, that significant distortion reduction is obtained when converting non band-limited signals. 
 Consider the system at . Instead of attempting to optimize the cost over t ? R, we will aim to minimize the cost within a finite time interval 
 N)t), where M,N ? Z+ are design parameters. We will only concentrate on the optimal sequence of quantized coefficients to be generated for the interval  , 
 For the purpose of including in the horizon the effect of past errors, it is convenient to describe the continuous time filter ? by its state space representation 
 where A ? Rn×n, B ? Rn×1, C ? R1×n and uc(·) is as defined in (6). In (35), ? = 0 accounts for a possible non-causal ?. If ? is causal, then ? = 0. 
 Notice that the first term on the right hand side of (37) captures the effect of the choice of u within the horizon. This term corresponds to the forced response of ? to the input u, which can be conveniently represented as 
 On the other hand, the second term on the right hand side of (37) represents the natural response of ? when the initial state is x . We define the difference between this initial state response and the filtered input signal a˜(t) within the horizon as the target function , for the horizon at t = 	t: 
 CeA  By using (38) and (39) , the cost (36) can be expressed as the L2 norm of the difference between the zero-initial-state response  and the target function : 
 Since ?(·) ? L2, one can exchange the order of sum and integration in (41) and rewrite it in matrix form as 
 where the vector   and the symmetric, positive definite matrix FN ? RN×N are defined element-wise as 
 The expressions derived above for the cost function over a finite horizon allow us to introduce the sampled-data receding horizon quantizer. The algorithm finds, at a given instant	t, the vector of quantized coefficients u that minimizes the total filtered reconstruction error from (	-M)t to  defined in (36). Then, the first element of u is sent to the output of the quantizer. The horizon is then shifted forward by t, and iteration  begins. The proposed algorithm, beginning at instant	t, can be formalized as follows: 
 The sequence   of step 4 forms the output of the sampled-data receding horizon sampler quantizer. If the input signal is not band-limited to p/t, the algorithm reduces filtered aliasing and frequency weighted quantization noise. For that purpose, it does simultaneous adaptive filtering on the input signal and adaptive noise shaping of the quantization noise, thus respecting the interaction between both phenomena. 
 It is interesting to note that, as the horizon is made larger, the output of the sampler-quantizer defined above approaches the optimal feasible output sequence possible defined in (11). 
 We will first show an example comparing the performance of the proposed sampled-data receding horizon quantizer (SDRHQ) against the so called all-discrete-time (DTRHQ) receding horizon quantizer introduced in [3] in the following situation: 
 The input signal, a, is an audio signal that has frequency components up to 22 [kHz]. Its frequency energy content is shown in Fig. 3. 
 Figure 3: Spectral composition of the input signal a(·). • The sampling frequency is half the required to avoid aliasing i.e., 1/t = 11 [kHz]. 
 i.e., it has impulse response f(t) = µ(t) + µ(t - t). • The frequency error weighting filter corresponds to the third order model for the psycho-acoustical response of the human ear [13]. Its frequency response is shown in Fig.4 
 In the simulation, the DTRHQ has full knowledge of the filters H and F, and utilizes the matrix FN defined in (43). However, it operates based on direct samples of the input signal. Thus, since the implicit pre-filter is a unity gain, (29) predicts that the all-discrete-time quantizer will be unable to determine the target function to be approximated by the reconstruction stage. On the other hand, the SDRHQ utilizes the same matrix FN but has access to the intersample behaviour of the input signal. 
 Fig. 5 shows the normalized reconstruction error from the outputs of both quantizers, for several horizon lengths. It 
 Figure 5: Reconstruction error (normalized to DTRHQ distortion for N = 1 ) from the outputs of all-discretetime quantizer (DTRHQ) and sampled-data quantizer (SDRHQ), for N = 1 to 3, for a non band-limited input. 
 can clearly be appreciated how, in this case, the sampleddata converter proposed in the present work outperforms the all-discrete-time converter of [3]. It can also be seen that the distortion exhibits a small decrease with the increase of the horizon length N. This suggests that, in this example, the main contributor to the distortion is aliasing noise, for both converters. Notice that the SDRHQ operated without an anti-aliasing filter. This, together with its much lower distortion in comparison with that of the DTRHQ, suggests that the sampled-data receding horizon optimization algorithm accomplishes a form of pre-filtering of the input signal that effectively reduces aliasing. 
 This paper has shown how receding horizon sampled-data control methods can be utilized to design optimal AD Converters. A key departure from earlier results in this area is that we optimize a filtered version of the continuous time reconstruction error for not necessarily band limited input signals. Inter alia, we show that the optimal design problem can be decomposed into two subproblems, namely the design of an optimal analogue pre-filter together with an optimal quantizer . We showed that the latter is feasible if only samples of the signals are available. The efficacy of the proposed method has been illustrated by an example using an audio signal sampled below its Nyquist rate. 
 ﻿ This work the problem of universal density estimation under an operational data rate constraint . We present a theorem that necessary and sufficient to learn and transmit a memoryless source distribution with arbitrary precision in total , under an asymptotic zero rate regime , in per sample . In the process , we propose a concrete scheme to achieve this learning objective , the Skeleton estimate by . , . 
 This work the problem of universal density estimation under an operational data rate constraint . The basic setting of an agent the sensor observing i . i .. from an unknown distribution with the objective of jointly learning and a finite description of to a second agent the receiver , which that information to construct an estimate . This density estimation and problem taken the attention of the community because of its role in sensor , and because of its strong connection with 
 Making echo of the seminal work of , it is well understood that the problem of universal lossless source is connected with the problem of distribution estimation , as there a one to one correspondence between and finite entropy discrete , in the finite alphabet case . This interplay , however , is less obvious when we move to the source scenario . this issue , recently that connect the problem of fixed rate universal source , with the problem of the source distribution with arbitrary precision , from one point to another , under an asymptotically zero rate operational constraint . This connection was made under the two stage joint modeling and framework . Taking from statistical learning , the data was split in training and testing , where first , the training data is used to construct a finite description of the source distribution first stage , and the second stage the first to pick a with respect to the distribution source code to encode the test data . Remarkably , in this joint modeling framework , the existence of a zero rate consistent estimate of the distribution in total , is sufficient to show the existence of a universal fixed rate source scheme , the 
 distortion rate function , for any given rate , and for any distribution within a bounded parametric family with some regularity , Th . . . This the question of whether there are of non parametric for which this result is also valid . 
 In this work we study in the problem of universal density estimation under an asymptotically constraint . Our main result is a theorem that necessary and sufficient to guarantee that zero rate is achievable for this learning problem . Interestingly , there is a tight connection with the rich collection of L totally bounded . Furthermore , we propose a concrete scheme , the Skeleton estimate by , , , to achieve our objective , which is a concrete demonstration of its information theoretic , something that was by and . and which , to the best of our knowledge , not been before . In the parametric scenario considered in , the Skeleton scheme 
 an optimal learning rate of O p under the regime , where , furthermore , this rate is extended for general non parametric . 
 Let be a separable and complete subset of i . e ., is a Polish subspace of . Let be the collection of probability in , and let denote the set of probability absolutely continuous with respect to the measure . For any , the Radon derivative of with respect to . 
 For the estimation problem the fidelity criterion adopted is the total variational distance . be two probability in . The total variation is given by 
 A absolutely continuous with respect to a measure , by , if for any event A such that A , then A . 
 Consequently is well defined , which is the Radon derivative or density , and furthermore , A , A RA . 
 which is a bounded metric in and been widely adopted in density estimation , . For the case when , to , the ‘ identity ´ a connection between total variation and the L norm of the involved , , more precisely , 
 , with ,, respectively by , : , then , , , . 
 Let : be an indexed collection of of interest . the index set of , which can live , in general , in an infinite dimensional space a scenario . 
 Definition : A , learning rule of a pair of ,, with : and :, a finite set and 
 The composition of these two : the explicit learning rule taking in the set : , which is the reproduction of ,. For an arbitrary learning rule , the operator log , which is the description complexity of the range of , in per sample . 
 Definition : A finite description learning scheme with rate sequence is a collection of learning for all possible finite , i . e ., , : such that , for all . 
 Let X , X ... be independent and identically distributed i . i .. of a measure . In this context , the product measure of the block in , and the entire process distribution of X , X .... Hence , the problem is to study the existence of a universal learning scheme , : , such that its finite description density estimate induced by the data to , in the following sense , 
 In other , we are interested in a zero distortion estimate for the family , a scheme , and consequently , in the necessary and sufficient , if any , that this consistency assumption on the intrinsic complexity of . 
 Definition : Let : be an indexed collection of in ,. We say that the rate is asymptotically achievable for , if there a learning scheme , : , with such that 
 In this case we say that is an rate uniformly consistent estimate for the class . 
 Definition the density inwith zero distortion . We are interested in the zero distortion i . e ., lossless operational point , as it is the standard objective in density estimation , and because it is fundamentally related with the problem of universal source . The focus of the next section , the main of this paper , is to characterize the class of that a scheme for lossless density estimation . 
 Definition : Let be a class of . We say totally bounded if for every , there a finite covering of 
 is the L ball of radius centered at . the positive integer that . is the covering number is the 
 ‘ entropy of the class . Finally , an covering such that , is an Skeleton of F 
 THEOREM : Let : be an indexed collection of in with index set that could be an infinite dimensional set in general . Then , there a zero rate uniformly consistent scheme for the class , if and only if , is totally bounded . 
 Hence , the rate zero is achievable with zero distortion for the class , if and only if , is totally bounded . This a concrete correspondence between zero rate density estimation and L totally bounded of . The proof two which are in detail next . 
 For the argument , we present a concrete learning scheme that consistency in the sense of , under the desired zero asymptotic rate . The scheme is based on the Skeleton estimate , Chapter by . 
 . In what , we use as a short hand for the in , and we define to represent the index set of . The idea of was this collection of probability with a set of measurable in . More precisely , let us define the class of by 
 set of ´ with respect to , . Then the idea is to a the Skeleton index to adopt a minimum distance principle , . More precisely , given i . i .. X ,.., with Xi , the Skeleton estimator is given by , 
 where is the standard empirical distribution . Note that is the minimum distance approximation of with of , the similarity measure in , that is reminiscent of the total variational distance in . The following key theorem a performance bound with respect to the minimum distance decision on the knowledge of the true distribution . THEOREM : For any , 
 This bound of an approximation error and an estimation error , the first and second term on the right hand side of , respectively The approximation error is bounded by the definition of , totally bounded . On the other hand for the estimation error , the use of the ‘ inequality . 
 Note that for any fixed , infinity one can make the estimation error component in arbitrarily small . The beauty of Theorem is that is in particular valid for any and , furthermore , given its finite length nature non asymptotic , it is valid even if the approximation fidelity to is chosen as a function of the amount of data . Consequently , for any sequence of non increasing positive , 
 for all . Here we can consider the sequence that a balance between the estimation and the approximation error , and by doing so , get a consistent estimate of in the sense of . A simpler idea was stated by and . If the classis totally bounded , we can consider the sequence , which is well defined and clearly to zero infinity . Consequently , it 
 To conclude , for any skeleton learning rule in , let us characterize its pair by , 
 for any sequence and for any , where is the empirical distribution induced from the argument . Then the learning scheme the uniform consistency re 
 , which by construction is O . Consequently the skeleton estimate a zero rate lossless learning scheme for the family , which the part . 
 Associated with the learning rule of length , we have its reproduction that we denote by 
 This that for all there , such that for any arbitrary , where by construction . totally bounded , the proof of Theorem . 
 In this section we explore density with extra on top of the totally bounded assumption 
 to achieve the critical O p asymptotic rate of convergence in total variation . On this , we follow the by who , in the context of the minimum distance estimate , of with finite and dimension , also classes , . 
 The following focus exclusively on the skeleton based learning scheme in Section A for the part of Theorem , i . e ., indexed by a sequence of precision . 
 Definition : Let : be an indexed collection of . The class for such collection is given by , 
 Definition in Appendix A and the complexity with 
 Proof of Theorem : We consider the and in Section A . Let us first focus on the convergence rate of the skeleton estimate . First , from Theorem 
 with the class of the Skeleton . It is clear that . Then by 
 for all and for any distribution . Here is where we use the assumption that AT finite dimension , which see the in . and . in that 
 we can achieve the same rate O for the approximation error in if condition is satisfied . 
 Remark : From Definition , is inversely proportional to . In fact , depending of how rich is can go from being , passing from being polynomial in , to being see a number of in , 
 . and therein . In fact a bound on how tend to infinity as goes to zero , to guarantee zero rate in the learning scheme . It is simple to show with , is sufficient to get log N being o . This is a condition satisfied by a large collection of totally bounded classes in see . 
 The so far are of theoretical interest , because they relay on the Skeleton partition of , which is typically unknown . Moving on the direction to make the Skeleton learning scheme of practical interest , we revisit the scenario studied in , in which : is indexed by , a compact set leaving in a finite dimension space , or what people in learning theory call parametric . Interestingly , in this context we can achieve optimal learning a practical of , induced by a uniform partition of . Let us first start with some . 
 Definition : Let : with . Let IF : be the index function of , that to . IF is said to be locally uniformly , if there and , such , , 
 , where the ball of radius with respect to the norm in centered at . 
 for some constant . Substituting this result in , Under the of Proposition , let 
 The argument by , a a sub optimal covering of , solution which the intended rate of convergence by see of this construction in Appendix . in . Finally by construction , the rate of the learning rule Then by definition , 
 N e of length is , which to zero by . this last part from Corollary . With this , let 
 this result , by imposing extra our practical learning scheme regularity on and , we 
 and the IF : is locally uniformly , totally bounded . 
 can achieve the optimal rate of convergence to estimate uniformly in . More precisely , by imposing that the 
 class is a class , we can achieve O rate of convergence for the estimation error in and , on the other hand , in a position to integrate the in Section 
 This parametric case is the setting considered by for the problem of fixed rate source and modeling . 
 Theorem , Proposition , Corollary and Remark to state the following implication . 
 THEOREM : Under the of Proposition , let us in addition assume that the collection AT 
 Proof : Let be the covering induced from the uniform partition Appendix . From this we can construct the minimum distance estimate in the class of with index set , i . e ., from : 
 the same in the proof of Theorem , we can obtain an equivalent version of , i . e . 
 from Corollary . To conclude note that the same result to the Skeleton estimate of Section A as . 
 The behavior of the distortion overhead of in , a faster asymptotic rate than its counterpart by O , under the same parametric setting and with the same per sample of O . In addition , an scheme , as its minimum distance decision is carried out on a finite number of . 
 Let be a collection of measurable , and be a sequence . 
 Then we define by the number of different in x , x ,.., n :, and the shatter coefficient of 
 C by is an indicator of the richness dichotomize a finite sequence of in the space , where by definition n . 
 Definition : The is strictly less than n is the and dimension of . If n for all , then the class is said to have infinite dimension . 
 First note in a compact set , , then there a finite 
 ing such that . On the other hand , from the locally uniformly assumption on IF :, there and such that , , , . Then let us consider , then by construction 
 where :, is the ball induced from the total variational , and the last inequality is from the condition . Hence from , there 
 Let , be the uniform that characterize the condition of IF Definition . Without loss of generality , let us assume the critical regime where , hence from is upper bounded by , which is the covering number of see Appendix . As 
 , we will work with a uniform partition of to find a bound for . Let . Then a product type partition , where in each we have uniform length , we have the covering . The number of is , which as a function of . 
 The work of . Silva is by from Grant , Chile .. The work of .. is by from post doc project . , and the project ACT . 
  
 ﻿ We present empirical on the achievable cellular system throughput gains stemming from the use of remote radio in an urban environment . Our work is based on simultaneous path loss of the base station and links to outdoor street level . We calculated the increase in received power , when a is added to improve the coverage by a . We consider diverse for the and diverse coverage for the mobile station , the effect of height and position with respect to the intended . We also compare the power gains that would be in practice from combining such as selection combining and maximum ratio combining . 
 We conclude that under practical , the of will depend very strongly on the existence of line links between the and the intended . For at low , below the clutter , only in a position with respect to the will obtain a significant benefit . Our data also that the gains in signal to noise ratio when are only marginally better than those of the much simpler . 
 Index Channel modeling , maximum ratio combining , remote radio , selection combining , small , wireless . 
 I . INTRODUCTION 
 O 
 NE of the great for today wireless communication is to provide adequate spatial coverage in a cost effective way , while efficiency and interference adequate for high frequency re use . To meet this challenge there been a growing interest in the study of , relay and low complexity , . These may constitute relatively simple , and easy to install when to the deployment of an additional base station to serve mobile within the cell . 
 While are as a small coverage with low transmit power connected to a wired network , a is 
 connected to a through a wireless link , being able to repeat or re code the data by amplify and forward or forward , among . A repeater may be thought of as an amplify and forward relay with no or ability . In this context , there are two basic of : wireless , and remote radio , also known as fiber , connected to a by an optical fiber . will suffer large and small scale fading at both and links . In contrast , for wireless , achievable gains will only depend on the quality of the connection , which it an attractive solution to provide connectivity to wireless in dense urban , where the links to the may experience significant shadowing . 
  
 Proper placement of a is obviously a fundamental factor in the compromise between the desired gains and the cost associated with its installation . In this regard , in an urban environment it is reasonable to expect that while coverage will improve with antenna height , this will at the same time have a negative impact on deployment and on channel interference , thus affecting frequency reuse in a large system , . If the is to cover an area of a size comparable to a small cell , then it is reasonable to assume that this is best by its antenna in normally used by . In such well established propagation such as those of and al . will be adequate to predict the coverage by the . Alternatively , they may be positioned at lower with the aim of in more limited , i . e ., generating within a large cell . Coverage in growing interest as it can provide local in signal to noise ratio and thus high data , . This may be without generating excessive interference in neighboring and at lower implementation when to a , , , particularly when below clutter height . Numerous empirical have been for the statistical characterization of path loss in relatively low height radio links . Analytical based on optical geometry have also been for in urban , . Goldsmith al . , based on a collection of , a mathematical description of the radius of coverage of a for an urban environment with and without line of sight . of local mean attenuation are for two , and it is found that these have the shape of convex . al . in an indoor outdoor environment operating under Amplify and Forward and Decode and Forward . In , the studied different of and in cellular , concluding that careful placement of can improve capacity substantially , transferring traffic from heavily loaded to lightly loaded , thereby improving network capacity . They also conclude , based on their , that it is best to place on a horseshoe layout with a sum transmission power of all comparable to the transmission power of a single macro base transceiver station . 
 The . Relay Task Group and the WINNER Consortium have the use of both empirical and theoretical to predict path for wireless links with transmission below the clutter of , also applicable to links . While the accuracy of for diverse been the subject of extensive , their use in wireless more than treating each link individually , since the joint statistics of and links may not correspond to those of independent random . 
 In this work , we report an empirical study that the joint statistics for the path of the links involved in a cellular system by . Specifically , our are based on simultaneous path loss of the and links in an outdoor urban environment . Based on our we evaluate diverse related to the performance improvement that can be when a in an area by a . We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user . Our will be useful to calculate the achievable gains in transmission experienced by mobile . 
 Our empirical study firstly considered the influence of the height . Toward that end , we by the in a tall building position , typical for an urban radio cell , and then lowering it stepwise to a height of . In this study , we considered street level user spread over a disk with a radius of centered at the . Path loss data for all links involved was collected at each height . The us to quantify the between height and the achievable gains in coverage and received power . For our test , it was found that on average an increase in height of the an increase in received power of about for every . Overall , the suggest that the effectiveness of a system is quite limited if a large area is to be covered , unless the is positioned at such a height and at such that in practice it becomes another urban . This conclusion led to the second part of our study , where we restricted to a disk with a radius of only centered at the . In addition , we the at lamp post , considering that this type of setting will be typical for a practical deployment of low cost . In this part of the study we also considered two of . One was chosen so as to maximize the likelihood of to the and the other chosen in close proximity to the first , but from direct street view by construction . In this way we were able to evaluate the effect of blocking a , as may occur when a surrounding construction is after the placement of the fiber repeater . Our are only directly applicable to narrow band transmission , for example a single carrier of an transmission , as our were done with unmodulated . We also report that involve spatial , i . e ., the elimination of small scale . This is equivalent to spatial diversity , which often similar to frequency diversity . 
 From the joint data of path loss for the and links we were able to compare the effectiveness of Selection Combining and Maximal Ratio Combining at the mobile terminal . To this effect we considered typical transmit and antenna gains at the and and calculated received signal power at the under the condition of equal noise power for both . It was found that the statistical gain of over is less than under all tested , a consequence of the random power imbalance among the links . 
 Our also us to calculate between for the and links for all . The low validate the approach based on considering them as independent random . 
 The rest of this paper is organized as : In Section we present a detailed description of the measurement scenario and . The related to the effect of height on the achievable gains are in Section . In Section , we present the empirical when a at lamp post height . are in Section . 
 . MEASUREMENT SCENARIO AND 
 A . Description of the Urban Environment Tested 
 The measurement campaign extended over a period of during summertime . The urban area used as in Mar , Chile , a mix of high rise and two story with ranging from to , built on a plane region at sea level . of received power at were carried out at . The area is by nearby that a transmitter at a location typical for a covering a relatively large urban area . The height was above the measurement region at a distance of to the position . The was mounted at various on the external wall of a high building as will be later . The streets in the measurement area are lined with with ranging from to . All links were of sight , with due to the surrounding construction . A schematic description of the scenario and terrain profile is in . and . 
 The second scenario was based on the same urban for both base and but with all within a disk with a radius of centered at the position of a . The was this time at above street level . The disposition of the user is shown in Fig . . 
  
  
 Fig . . Placement of , and in urban environment for the first part of this study . a Tested , D view . 
  
 B . Measurement System 
 A block diagram of the measurement system is shown in Fig . . At the position , a continuous wave source based on a oscillator a . unmodulated carrier with output power through a sector type antenna , corresponding to an of . The antenna used had azimuth and elevation 
  
 Fig . . Placement of and in urban environment for the second part of this study . 
  
 Fig . . Measurement system block diagram . 
 at . All were within this . At the , the antenna used was a vertically dipole with gain , a signal at . with output power , corresponding to an of . The choice of was based on the fact that they fall inside a band used for Metropolitan Area such as and that considerable previous work in this band been , providing a reference framework on propagation in urban . The was to contain a transmitter and a receiver tuned to the frequency . This us to remotely detect any possible anomaly at the , which was at a considerable distance from our measurement scenario . The separation between and provided enough isolation to allow simultaneous transmission and reception by the . 
 Power at the were carried out an model NA spectrum analyzer that simultaneously tracked the at . and . . The antenna used was a gain vertically dipole at a . height . The dynamic range of the measurement setup is limited from below by the system noise floor . only the spectrum analyzer as receiver , this proved to be inadequate for several of the user and would have our if we had only selected the where were feasible . We thus included a gain amplifier at the mobile antenna terminal to achieve a system noise figure of and thus a noise floor of in a . The resulting link budget us to measure path of up to in the link and in the link . At all selected the average received power the noise floor by at least . The measurement system included a computer that acquired the spectrum analyzer at a rate of per second . The received power from the and us to calculate the corresponding path for both links . 
 C . Measurement Procedure 
 The two measurement involved moving the along a straight path of about . at each of the diverse in the region tested . This was done a very slow moving vehicle on which our measurement equipment was . We traversed each . sector at a constant speed in such a way that approximately equally spaced power were collected . At the frequency , this that more than of the are at spaced at least half a apart . From these we the average path loss at the location and the small scale fade statistics with respect to this average . 
 The first measurement campaign was at the effect of height . This involved the at different , ranging from to above street level . measurement for the were selected at various from the : within , between and and between and . These are marked with in Fig . . for in the range from to did in some result in path exceeding the reliable measurement range for received power . Thus , to avoid our , the statistics for this part of our study exclude data for below . 
 The second set of , at the practical performance gains for low , was based on , but all within of the , as in Fig . . Of these , were within , between and , and between and of the . The at a height of above street level , was at two different : one at an intersection so as to simultaneously illuminate two streets and the other one within of the first placement , but with no direct path to the adjacent streets . In the first case there are and user , while in the second all are . 
 . EMPIRICAL : EFFECT OF HEIGHT 
 In this section we quantify the effect of lowering the antenna from a position where in practice it can be considered another , to close to street level . To characterize our scenario , we first contrast the measured path with the corresponding by propagation . In various for path loss prediction are , some of which correspond to similar to the one used here as a test bed . These do not explicitly consider the height variation of the or relay , but are based on the relative position of these with respect to the user , i . e ., , above or below clutter . Among these , we chose the one that best our . Specifically we considered the alternative WINNER model for urban Type E links , with a above height and mobile below . The by this model were to the path loss link , considering the various . The for these links range to . The comparison is shown in Fig . where at all and for all are included . For this comparison we out small scale , thus generating one path loss value for each combination of height and position , which is to be to the model prediction . The separation of the empirical data into and was based on whether or not the line of sight path was blocked by a large structure such as a building . We considered that a tree in the direct path does not alter a condition . To avoid the figure we combined the for the various into three : to , to and to . The corresponding by the model is plotted considering a at a height in the center of the range of for each group . We note that the model is little affected by this height . We also show for reference the free space path loss . As can be seen , the path loss are within range of the model , although we some path loss reduction with height , not for by the model . The same figure also the of the path loss considering all , which as fall well below the model prediction . 
 We further the statistics of our empirical path for each height to those of the classical log distance model with log normal shadow fading and Rice small scale . As is common practice , we chose the model to adjust to the free space path loss at the reference distance of . We found that this model provided a very good fit , with a path loss exponent ranging from . to . when considering . When considering only the , the path loss exponent varied between . and . . The were at the highest position , with a clearly observable increase as the was towards street level . The standard deviation with respect to the regression line was in the range of to , with little difference between and links . For all of the , the statistics of the small scale fit the distribution , within the accuracy that result from a finite data set . For the links the Rice distribution 
  
  
 Fig . . Empirical path loss to WINNER model according to . j recommendation . 
 provided a similarly good fit , with that were quite small , no than . 
 The above indicate that our choice of measurement scenario the propagation behavior of an urban environment , as in the literature , by shadowing and by strong propagation . 
 To evaluate the achievable received power gain as a function of the height , we calculated the received power at the , assuming realistic for transmit power and antenna gains . We note that once the path for all links have been , we are free in subsequent to assume any transmit power for the and the , not necessarily those used before in the channel sounding process . In the calculation of received power at the , we thus used the actual measured path for the links , and transmit power that we reasonable for an actual deployment . From the collection of calculated power at the , we the cumulative distribution function of the received power . For this we considered transmission by alone , by alone , and transmission by both and with or at the . We considered a with of transmission power , typical for a . The and are assumed to be the same as the used in our and , respectively . For the we assumed that in most practical the would be attached to a wall and consequently only illuminate a sector of in azimuth . All our are in fact within a sector of such angular width . We thus considered that a typical , sector antenna would be used , consistent with in . For the transmit power at the we used two that cover what could be considered the high and low of a practical deployment : and . The lower value is representative for , while the high value is in the power range for and . 
 Figure the of the received power assuming that the at . The consider all power at all . We 
  
 Fig . . of received power at for , coverage radius and power . 
 for all between and but we here only show the corresponding to . The for essentially match the of the power received from the alone , with a shift to the right of less than at all probability . We do not include them in the figure to avoid the graph . All other fall between these two and are also for the sake of clarity . The improvement in coverage over that by the alone was only found to be significant when the a high position . Not shown here is the case where the at , which in virtually no gain at any position . 
 Fig . that the received power gain of over is almost insignificant . This is due to the fact that the likelihood of similar received power from and is quite small , and as well known , even for equal power the gain is only . We repeated these for diverse of to power with the result that the improvement in received power of over under all was no greater than . As before , this equal receiver noise in both , thus the power gain is equivalent to gain . Based on the above , we describe below the that were for the link in this setting . All our subsequent are based on the assumption of a at the user terminal , but for all practical the would be no different a . 
 A . Coverage Extension with Respect to that Achievable by 
 From Fig . it is seen that , under the stated , of will receive at least when only the is operating . a at the maximum height , at this coverage to . At the other extreme , a at will provide virtually no statistically significant improvement in coverage , as the corresponding of received power was found to exhibit a right shift of less than . Obviously lower will further reduce any . We note that these rather limited gains were considering a relatively 
  
 height 
 Fig . . Power gain of over alone , for diverse as function of height with transmission power of . 
 high transmit power , only less than that of the base , which does however have a advantage over the , due to the difference in the corresponding antenna gains . 
 B . Received Power Gain 
 To quantify the gains we define as the difference in received power that can be to a proportion of . We refer to this proportion as the availability level associated with that power gain . We denote the of received power from the alone and from of and as and respectively . The gain in received power is then : 
  
 The same definition of course to the gain through . We now use to calculate , at various availability , the power gain resulting from the use of a . This is shown in Fig . for the full range of at of , and , considering of transmit power at the . To construct this figure we considered the empirical as in Fig . , but calculated for each height . In Fig . we also indicate as a reference the that would have been if instead of the measured for path , these had been calculated at each location a fixed path loss equal to the average value by the before . Specifically , for the link we used the alternative WINNER model for type E in while at the links we used the model , to a frequency of . . Since height dependence is not part of the model in , the resulting gain , which we found to be . , is only a function of the in the region . In contrast , the empirical gains show considerable height dependence . 
 From the data that Fig . is based on , we calculated by linear regression the increase in received power . height of the . The result was that under the stated , 
  
 Fig . . and power gains of over alone , for diverse as function of height with transmission power of . 
 the power gain is approximately for every in height increase , at of and . If the power is reduced to this gain becomes only . for every . We note that these represent the range of height related gains for our particular choice of and thus cannot be easily generalized . From a qualitative perspective however , the may suggest that the gain with height is mostly due to the fact that at the higher , the number of that exhibit a link to the is . To investigate this aspect further , we segmented the , separating the into those with , and those without , for each height . The resulting power gain . height is shown in Fig . . In this figure we also show the percentage of . As seen , for the there is little gain improvement as the height . On the other hand , for the , there is more evidence of gain increase with height . The main conclusion that may be derived from this figure is that the benefit significantly more from increasing the height of the than those that are . We also note some gain with height , even when the percentage of location a saturation level . Thus , the growth in the percentage of links with does not by itself account for the gains with height . 
 Finally , to evaluate the degree of dependence of the in the and links we calculate the coefficient between these for each height as 
  
 E E E 
 where and represent respectively the path , the small scale , from the and the to a specific position . E and E are the statistical of these path at the distance under consideration , from a linear regression of path loss . distance . and are the standard of the with respect to these . 
 This calculation was repeated for each height . The were very small . Only at the height did approach . , while at all it was less than . in modulus . We also calculated the of the shadow , i . e . the after out the small scale . This would be more representative for links with space or frequency diversity , which to suppress small scale . In this case was somewhat , but still not significant , reaching . at the height and between positive and negative less than . in modulus at above . 
 . LOW HEIGHT 
 The of the previous section suggest that even with a position at a height that is comparable to that of an urban cell , only modest gains can be at a large coverage radius . This even at a relatively high transmit power . In this section we therefore explore more practical , limited to much smaller coverage and a lower height above street level , as we have in Section . In this setup we also the effect of choosing a position with an uncluttered path to two streets , in contrast with it in a nearby position by construction . The first placement is here as Non and the second as O . The position links to some of the chosen . For a setting where both are below clutter , links are as being in a street canyon . Correspondingly , we refer to the links as non street canyon . In the radius , half of the links are of the type while all O links are non street canyon . Within the radius all links to the become of street canyon type . This classification according to the link is , as before , at which can be in practice to benefit most from a given placement . We again first that the chosen are such that the path loss statistics for the environment correspond to well established propagation . 
 For the links , we found a good match to the model and to the alternative WINNER path loss model for the type case in , an operating frequency of . . With respect to these , our measured were slightly higher , and on average , with root mean square ... of and respectively . The measured for the street canyon links were far lower than those for links at the same distance , from to , with on average . They were however higher by about on average than those by the advanced model for Type with both and below . The path loss these are shown in Fig . . As before , we also that our data fit the statistics of the log distance model , with log normal shadow fading and Rice small scale . The resulting were . and . for and links respectively . We note that , as before , most of our links were at least partially by relatively large . 
  
 Fig . . Empirical path loss to reference for low height . 
  
 Fig . . of received power at for coverage radius and power . height was . 
 Fig . the of the received power at the as previously for Fig . but within the radius . We illustrate as an example , only the case where the at for both . All other are as before . As seen , the achievable gains very much depend on the specific placement of the . For example , at coverage , the will guarantee a power of at least . an O will guarantee the same power to of , while an will increase this coverage to . When this calculation under the assumption of a transmit power of , the coverage to and respectively . We summarize the gains for various in what . 
 A . Gains as Function of Coverage Radius 
 To evaluate the effect of coverage range we divided the where the power were carried out into three with disk radii of , and , to the . Table I the power gain as 
  
 defined in , with a at the availability of , and , for two transmit and for the two . The are almost identical . We recall that for a radius of or less the placement street canyon type links to all , while only of the are in that condition for the radius . The tabulated provide some interesting insight into the that can be from the use of a . We first note that for and availability , there are consistent in power gain as the coverage radius , particularly when all user become of street canyon type with respect to the . This increase is especially sharp at availability . The at availability , while not as relevant in practice , serve to illustrate the gains available to a minority of that have a particularly low path loss to the , for example due to an unobstructed path at a short range . We in fact see that the gains available to of at a radius are comparable to those for within the shorter . The two smaller radii include only street canyon type links for the case , however the variation in distance from the various user to the is obviously for the than for the case . At high availability this significantly power gains for the smaller radius , as it will exclude most user with higher path to the . Not as obvious is the fact that when considering availability , the power gains can actually decrease with the radius , as seen in the table . This is due to the fact that the smaller radius may also exclude some user that exhibit particularly low path to the . It is precisely these few well with respect to the that dominate the achievable gains for low availability . In the same context , we observe that for the , a increase in transmit power in almost exactly the same increase in received power to the best . This again that for such , the received power is completely dominated by that of the . 
 Both Fig . and Table I illustrate the importance of proper of the . This can also be associated with the fact that the position street canyon type links , particularly at the radius of and . We further explore this below . 
 B . Gains for Street Canyon and Non Street Canyon Links 
 Based on our above we divided the for all links with up to length into street canyon and non street canyon , rather than by distance as before . Table the for the power gain under this classification , again the previously detailed in Section . 
 The illustrate that very considerable power gains are available for street canyon links , even at low . In contrast , the row corresponding to the non links very modest gains regardless of the position . The latter were considering only that are of non street canyon type under both , so that are based on a common set of data . 
 TABLE I 
 SELECTION COMBINING GAIN WITH LOW HEIGHT , WITH . 
  
  
 power 
  
 Coverage 
 radius 
 O O 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 y . . . . 
 TABLE 
 SELECTION COMBINING GAIN FOR STREET CANYON AND 
 NON STREET CANYON , WITH 
  
 To illustrate the gains achievable under the most favorable and , we present in Fig . the for received power considering all street canyon within the disk of radius , at a power of . When this figure with Fig . , it becomes clear that the street canyon within the radius are those that contribute most significantly to the in received power . For example the will provide at least to of . This same minimum power will be available to of if a is included . this for an assumed transmit power coverage to virtually . 
 Finally we again calculated correlation of for the and links , for O and . In both these were found to be below . . When are out , this to . . 
 V . 
 Our work that in an urban environment a cannot be to yield significant for an ensemble of that is distributed over a disk region with a radius 
  
 Fig . . of received power at street canyon for coverage radius and power . 
 of centered at the . Under our , based on typical of power and antenna gains , only limited in received power or coverage are available even when the is at considerable height and at only less power than the . Limiting the coverage radius to , significantly the from a . It important gains even when at as low as and operating at the relatively low power of . However , these gains are basically associated with the existence of or , equivalently , links to , and thus heavily depend on site specific and user . 
 In all , the gains achievable are only marginally than those by a , to the fact that , in practice , the likelihood of the and the base providing comparable is very small . 
 Our show that the of the user link and the base user link are essentially independent . Moreover , measured path for such links were found to be quite consistent with average from accepted for the corresponding urban setting . Thus such can lead to a basic prediction of the gains achievable from the use of a . 
  
 ﻿ This work a necessary and sufficient condition for a stationary and ergodic process to be compressible in the sense by Amini , and Compressibility of deterministic and random infinity Signal Process ., vol . , no . , . The condition to check that the moment of the invariant distribution of the process is well defined , which and the result by , and in Compressible for high dimensional statistics , Theory , vol . , no . , , Prop . . Furthermore , for the scenario of non compressible ergodic , we provide a closed form expression for the best term relative approximation error in the norm sense when only a fraction rate of the most significant sequence are kept as the to infinity . We analyze basic of this rate approximation error curve , which is again a function of the invariant measure of the process . the case of i . i . we completely identify the family of compressible , which to look at a polynomial order decay heavy tail property of the distribution . 
 Index Asymptotic analysis , best term approximation error analysis , compressed , compressibility of infinite , compressible , ergodic , heavy tail . 
 I . INTRODUCTION 
 of compressibility for a stochastic process , meaning that with high probability of the process can be well in some sense by its best term sparse version , been a recent topic of active research , , . compressibility for random and the identification of compressible and sparse are relevant considering the recent development of the compressed theory and its . These can play an important role in regression , signal reconstruction for instance in the classical compressed setting , Th . , inference , and decision making , . One important case is such a compressibility notion for i . i .. where the probability measure is with a density function , . In this context , of the process are non sparse almost surely , and conventional ways of compressibility for finite dimensional , based on the power law decay of the best term approximation error or that belong to the weak ball , are not applicable either , as shown in , . 
 by this problem , Amini al . and al . have new for compressible random . These are not based on the typical absolute approximation error decay pattern of the , but on a relative best term approximation error behavior . In particular , Amini al . formally define the concept of compressible process in Section below . This new definition a meaningful way of i . i .. random and their , in of the probability that almost all the relative energy of the process is concentrated in an arbitrarily small sub dimension of the domain , as the block length to infinity . Under this context , they provide two important the theory of order statistics . First of all , , Theorem that a concrete family of i . i .. heavy tail is compressible the generalized , and log logistic , while on the other side , , Theorem that with exponentially such as generalized are not compressible . Therefore , it is interesting to ask about the compressibility of i . i . not considered in that analysis . In this direction , we highlight the work of al . , which under an alternative notion of relative 
 compressibility almost sure instead of convergence in measure , which was the criterion adopted in and a different analysis setting fixed rate instead of the variable rate used in , an exact dichotomy between compressible and non compressible i . i . This the question of whether it is possible to connect Amini al . compressibility with the more refined almost sure a .. convergence analysis of the best term relative approximation error in , Prop . , with the idea of the analysis of and . 
 X . Personal use is permitted , but republication redistribution permission . See : index . for more information . 
 To address this question , we extend the analysis from i . i .. to stationary and ergodic . In this setting , the main result Theorem a necessary and sufficient condition for a stationary and ergodic process to be compressible in the sense of Amini al . , for any arbitrary . Furthermore , for the case of non compressible ergodic , we provide a closed form expression for an achievable rate approximation error function . The key element in the proof is the application of the ergodic theorem and the derivation of intermediate almost sure convergence Lemma and in Section that match and extend the approximation result by al . , Prop . for the i . i .. case . A corollary of Theorem a necessary and sufficient condition to categorize i . i .. random in of compressibility , which the analysis in and . In addition , for the class of non compressible ergodic , we provide an analysis of its rate approximation error curve that is continuous , differentiable Theorem and is convex under some Theorem 
 . Finally as an application , we revisit the interplay between compressible ergodic and the performance of the classical compressed setting , in the asymptotic regime when the block length to infinity . the well known instance performance guarantee of the scheme , , , we show Theorem that an arbitrarily small number of linear is to achieve zero distortion , in an noise to signal ratio sense . A preliminary version of this work was in . The current version the presentation and analysis of the main result , further analysis of noncompressible ergodic and with compressed . 
 The rest of the paper is organized as . Section some preliminary and . and are devoted to the presentation of the main result on the characterization of compressible ergodic and its proof , respectively . of the the rate approximation error curve for non compressible . Finally , Section an interplay between 
 compressibility and compressed . Some of the and are in the Appendix . 
 . AND BASIC 
 For a finite dimensional vector in , let denote the ordered vector such that 
 . For some and 
 , let 
  
 denote the norm of the best term approximation of , where by definition . In addition , 
  
 the best term approximation error of , in the sense that if is the collection of sparse , then . For the analysis of infinite , Amini al . and 
 al . have the following relative best term distortion indicator : 
  
 with the objective of extending of compressibility to that have infinite norm . 
 A . Rate of Innovation . Distortion for Infinite 
 Definition : For a sequence , the rate distortion pair is if , there sequence of positive such that and 
  
 where is the finite block version of length of . 
 Note that the use of the relative best term distortion in the analysis of with infinite norm . 
 Definition : For a sequence and , we define its rate distortion approximation function by 
  
 for all . 
 A simple consequence of these is the following result : 
 Proposition : For all such that then . The proof is in Appendix A 
 Hence , can be seen as the critical asymptotic rate of innovation of when a relative best term approximation error of magnitude is . 
 Alternatively , Amini , and have a notion of critical dimension for finite length , and from this , a notion of compressibility for infinite . We revisit those here : 
 Definition : For and , let us 
 define 
  
 Then , a sequence is compressible if , 
  
 where is the truncated finite block vector of 
 . 
 This notion of compressibility that when the block length to infinity , a negligible fraction of the is to represent with an arbitrary small distortion in the sense of . Note that is signal dependent and a variable rate sequence . In addition , it the critical number of to achieve a best term approximation error smaller or equal to in the sense of . From this , it should be related with the critical rate from a fixed rate analysis in Definition . That relationship is in the following result result : 
  
  
 The proof is in Appendix 
 A corollary of this result that if the limit 
 of , then 
 . In particular from Lemma , if is compressible , then for all . We refer the interested reader to Amini al . for further discussion and of compressible . 
 B . Rate of Innovation . Distortion for Random Analogous of rate of innovation . best term 
 distortion and compressibility can be stated for the case of random or . Let be a random sequence with in and by its consistent family of finite dimensional 
 , where and the space of probability for the measurable space . As a short hand , we denote by the process distribution of 
 . 
 Let us define the following measurable set : 
  
 where the equality is by . Then in analogy with Definition , Amini al . the following : 
 Definition : and Let be a random sequence with . Then for any and 
 , 
  
 is the critical number of that the set typical with respect to . The process and , respectively is said to be compressible , if 
  
 Alternatively , we can consider the following fixed rate : 
 Definition : Let be a process by , and let us consider , and . We say that the rate distortion pair , is achievable for with 
 probability , if there a sequence of positive 
 such that and 
  
 Definition : The rate . best term approximation error function of in short the rate approximation error function of with probability is given by : 
  
 A simple relationship between and the critical number of in can be established in the asymptotic regime when goes to infinity , showing that our fixed rate concept is a one . 
 Proposition : For any 
  
 The proof is in Appendix 
 In the next section , we will study the class of stationary and ergodic , where the best term approximation measured in of will be in closed form . Furthermore , it will be shown for this class of random that 
 for all and , refining the basic relationship in Proposition . 
 . ANALYSIS OF ERGODIC 
 Let be a stationary and ergodic process with distribution , where its marginal shift invariant distribution . For simplicity , we assume that where the measure . 
 Then is with a probability density function and 
 . 
 For a measure on , a measurable function 
 is said to be integrable with respect 
 to if 
  
 where the collection of integrable . 
 We are in the position to state the main result : 
 Theorem : Let be a stationary and ergodic process with shift invariant distribution such that . Then for any , we have the following dichotomy : 
 i If : then is compressible , 
 i . e . 
  
 If : then is not compressible . Furthermore , if we introduce the induced probability measure in by : 
  
 then and 
  
 where 
  
 and is a solution of the identity : 
  
 The proof is in Section . 
 A . Discussion and Interpretation of Theorem 
 : Theorem a necessary and sufficient condition for a stationary and ergodic process to be compressible in the sense in Definition . 
 : In the case of non compressible , i . e ., when 
 , Theorem what we call the 
 achievable rate distortion region for the process , given by the set of critical rate distortion : 
  
 This region solely on the shift invariant measure and its induced measure in . More on the characterization of this region will be in Section . 
 : Under the assumption that and 
 , we have that any rate and distortion 
 are achievable see proof in Section and more in Section . This fact is used to derive a concrete analytical expression for in . Furthermore , from the characterization in and , it can be shown that is a continuous and differentiable function with respect to see Theorem in Section . 
 : In both i and , the critical a stationary and ergodic process is independent of . The reason is that asymptotically as goes to infinity , the characterization of to compute on that belong to the tail field of the process , which is known to be trivial i . e ., their have zero or one probability for the case of ergodic , , . Therefore , we obtain almost sure convergence that make independent of the value of our object of interest see Section for . 
 : A natural order among stationary and ergodic process can be established from Theorem . Proposition : If is compressible for some , then is compressible for all . 
 Proof : If , then for all 
 . 
 Proposition : If is not compressible for then is not compressible for all . 
 Proof : If , then for all 
 . 
 : the i . i .. scenario , we want to highlight the by Amini al . related to compressibility in the sense of . In particular , , Theorem that if is such that for some , 
 then the i . i .. process is not compressible . In contrast , , Theorem that if to the domain of attraction of an stable distribution , Chap . . 
 with , then the process is compressible . First for , Theorem a refined result , revealing a indeed , the complete family of i . i .. that are not compressible . In fact , in addition to that go to zero exponentially , and consequently , Gamma , heavy tail whose density function are tail lower and upper dominated by a power law decay of the form with are not compressible either . On the other hand , concerning , Th . , it is simple to verify that any that is in the domain of attraction of an stable law with that 
 see Appendix E for , and consequently , part i of Theorem this family of compressible i . i . 
 : the previous point , from Theorem we can state the following : 
 Corollary : Let be a stationary and ergodic 
 process , if where , then 
 Therefore , stationary and ergodic with a shift invariant distribution that a , generalized , and Gamma are not compressible in the sense of Definition , for any . In addition , if is finitely , i . e ., where , then its process is not compressible for any . 
 Corollary : Let be a ergodic process with invariant distribution and density If as for some , then 
 Therefore , for shift invariant by a power tail behavior , which belong to the category of heavy tail , a complete picture of the range in which its ergodic process is compressible is . 
 : For the proof of Theorem , we derive almost sure convergence see Lemma and in Section . 
 In the case when : if is such that for some , then 
 Furthermore , for for some , it that In the case when : if , then 
 These are con 
 and extend the result by al . , Prop . , which for the i . i .. case the same almost sure convergence limit for the object . Their proof was based on the lemma of order statistics see in , Th . . In contrast , our proof is based on the use of the tail in , some induced empirical on those , and the convergence of those empirical through the application of the ergodic theory see Section for . The idea adopted in our proof was to look at the empirical of and as the of interest , instead of the partial 
 A measure is tail lower and upper dominated by a no negative function , if there and such that for any such that then . Here the of . 
 We say that as if there and 
 and , where : if , 
 then , ; if , then 
 , ; and otherwise , then . 
 Fig . . Graphical representation of the relationship between , and in Theorem . Notice that is the total area under the bottom curve . 
 of the ordered statistics considered in , . That difference was essential to extend the almost sure convergence in Lemma and to the family of stationary and ergodic . 
 : Under the assumption that , Theorem another interesting dichotomy : 
 Corollary : If for some and for some 
 it that 
 then the latter also for all and for all 
 . Likewise , if for some and some then for all and all 
 . 
 B . Graphical Interpretation of 
 Note from and that and 
 , respectively . The numerator in the last expression to the value of , for a random variable where . Similarly , from , the optimal rate 
 the value of . Thus , to the area under the tail of , which the of , in Fig . top , while with the area under the curve to the left of , in Fig . below . This graphical representation for an intuitive interpretation of the relationship between and the compressibility of a given stationary ergodic process . In order for this process to be compressible , must be zero for every and for every . Equivalently , and from that is a limit , it must be possible to achieve any while keeping an arbitrarily small fraction of the of the process , as . Such requirement is satisfied if and only if i . e ., if , which that no matter how large is chosen , the shaded area in Fig . bottom , being infinite , will yield a zero . 
 . PROOF OF THE MAIN RESULT 
 Proof : Let us first consider the case when 
 . For the rest , it is important to note that given that 
 , then for all there such that 
 , and for all there such that 
 . 
 For , we can define : 
 a 
 where from the ergodic theorem , Th . . 
 b 
 c 
 The second almost sure convergence is from the assumption that . Then , we can state the following : 
 Lemma : Let be a stationary and ergodic process with distribution and . Then for any and sequence such that , 
 we have that 
  
 In addition 
  
 where is a solution of . The proof is in Section A 
 In order to prove , let us fix . Then there 
 , such that and from Lemma if is such that , then 
 Let us consider an arbitrary such that 
 , then again from Lemma , if a sequence is such that , then 
  
 , almost surely to a distortion strictly less than , and then for all : 
  
 Hence from the definition of in , we have that eventually in , which that 
  
 This upper bound is valid for any such that 
 , then 
  
 The first inequality comes from Proposition and the last equality from the fact that the function is continuous with respect to as . 
 To derive a lower bound , let us consider an arbitrary 
 . We know that there such that 
 . Again from Lemma , for all such that 
 , then , 
 .. Therefore , 
  
 This result that eventually in , and , consequently , 
  
 On the other hand from , we have that it is necessary that 
 . This last inequality and are valid for any 
 , then and 
 , which from . 
 Moving to the case where , we have that 
 , then from the ergodic theorem , 
  
  
 Equation comes from 
 a .., and 
 . In other , that 
 , Furthermore , 
 we have the following : 
 Lemma : Let be a stationary and ergodic process with distribution and . Let us consider an arbitrary and such that , 
 then 
  
 The proof is in Section 
 Let fix an arbitrary and where , then from Lemma we have that : 
  
 Then for all and , 
 and therefore eventually in . From this , for all , which the result from Proposition . 
 A . Proof of Lemma 
 Proof : To begin let us prove the fixed rate result in . It is first important to concentrate in the case when and to show that 
  
 For any , let us define the 
  
 where from b and c , because 
  
 arbitrary and let and be such that , which that . 
 Let us take an arbitrary . From the assumption on , the definition of and the fact that 
 , such that which that . Therefore considering that , 
  
 the same for the sequence and accordingly , we have from that 
 , 
  
 Then from the sigma additivity of which the result in . 
 with this result , for an arbitrary let us consider such that . We know that there such that , and for this we consider 
 the and as defined in . Then for any 
 it that 
  
 Furthermore , from definition of ordered , it is simple to verify that see : 
  
 This is where the zero rate result in is used . In particular , if we consider , from , and the fact that is in 
 , we have that 
  
 the last equality in from definition of in . Finally from b and c , 
 which . 
 Concerning the fixed distortion result in , for 
 let be such that . Let us consider an arbitrary where and consequently note that and are mutually absolutely continuous . Again for this , we use the in , 
 where for all , it that , such that 
 Considering , it by definition in that : 
 then . 
 We can do the same for the countable collection : 
  
 where for any , 
  
 The first equality from the continuity of the function with respect to as . Then from the fact that , we have that , 
 , a .. Finally , proving that a .. an equivalent symmetric argument and we omit it . 
 B . Proof of Lemma 
 Proof : For let us consider and , such that and such that . Considering the 
 and , we have that from and . Let us fix an arbitrary . Considering that 
 , then eventually in , and therefore eventually , which from definition of that . Finally , the fact the event almost surely the result . 
 V . OF THE RATE APPROXIMATION ERROR CURVE FOR NON COMPRESSIBLE 
 For the family of non compressible ergodic , in this section we study two tail that characterize the achievable rate distortion region in . Let be stationary and ergodic with its invariant probability measure . Here we focus on the case where , then the measure in is well defined and by construction , where the Radon derivative or density of with respect to is given by for all . Furthermore , from the strict positivity of 
 on , it is clear that , where 
 and then these two a mutually absolutely continuous 
 , i . e ., 
  
 This a close interplay between the tail probability 
  
 that characterize in 
 . The following basic can be stated : 
 Proposition : 
 a and are left continuous . 
 b . Then , is continuous at , if and only if , 
 . 
 c For any , is continuous at , if and only if , is continuous at . 
 d For any pair if and only if , 
 . 
 e and 
 . 
 From these we can state the following : 
 Theorem : Assuming that , then for any : 
 is a continuous function with respect to 
 . 
 there is such that and there such that . Then , 
 the collection of critical 
 all the in , and any distortion is in with a given rate . 
 For , then . 
 is a differentiable function in , and 
  
 where and the inverse of the auxiliary function for . 
 The proof is in Appendix I . 
 In summary , the rate distortion approximation function is continuous , and all the , in the sense that for any there is only one such that . In addition , it is strictly decreasing and the following boundary : and . Furthermore , it is simple to verify that and 
 . Then from , it that 
 should be a non increasing function as progress to considering that , and consequently , 
 should present a convex behavior eventually as to . The next section the convexity of more formally . 
 A . Convexity of 
 First in this section we show that is a convex function of . Then , we provide a necessary and sufficient condition for to be a convex function of . 
 Proposition : For every , is a convex function of over . 
 The proof is in Appendix 
 Theorem : Let by a non compressible stationary and ergodic sequence with . Then 
 if , is a convex function for all . 
 Otherwise , is convex , if and only if , 
  
 where is the of , with , and 
 . 
 The proof is in Appendix . 
 Remark : The condition in is implicit and may be difficult to verify . A simple to check sufficient condition for the convexity of is the following : 
  
 where . In particular , a non increasing i . e ., almost everywhere in a convex rate approximation error function . 
 B . of Heavy Tail and Exponentially Tail 
  
 We present few of rate approximation error of non compressible i . i . In particular following , we consider the exponentially distribution , which is non compressible for any , from Corollary , and the family of Student distribution with parameter 
 , whose goes to cero as . From Corollary , the i . i .. process with a Student distribution is compressible for any and non compressible for 
 . 
 To compute the rate distortion function , 
 we use the fact that 
 Then the problem to compute and 
 , for all . For this we consider an estimation approach . Considering a sufficiently large set of i . i .. of let say , the law of large us that for any and , 
  
  
 The proof is for the sake of space . 
 The of a Student distribution with of freedom is given by , where is the gamma function . 
 Fig . . Numerically rate approximation error for several non compressible i . i .. and one compressible i . i .. process . 
 with probability one , assuming that and 
 the use of . Then , by sampling the space of and considering a sufficiently large , we can estimate with an arbitrary good precision the rate distortion region 
 . Following this path , Fig . the rate approximation error for the , and several Student distribution for . We verify that some Student distribution are compressible and and are non compressible , and 
 as the Theorem . More interesting is to validate in all the of non compressible , that the have a convex behavior , which is from . Furthermore , the density with the exponentially tail is less compressible than any prior with a power law decay , in the sense that for a distortion the i . i .. process needs a higher rate . From these , as goes to infinity the i . i .. process with a heavy tail distribution the approximation error behavior of the law . 
 Fig . the rate approximation error for the prior for different of . It is interesting to observe the increasing monotonic behavior of as , for any fixed value of . Again all have a convex behavior . To contrast , Fig . a set of for the distribution i . e ., Student distribution with 
 , where no clear monotonic pattern is as a function of . 
 . COMPRESSIBILITY AND COMPRESSED 
 We conclude this work compressible stationary and ergodic , as in Theorem , in of their ability to be with an arbitrary small proportion of linear for that the classical compressed measurement and reconstruction setting . In particular , the focus is on compressible , as the standard i . i .. linear acquisition and 
 minimization sparsity of , offer a well known instance guarantee stated below in Lemma that the modeling assumption of compressible . 
  
 A . Compressed in a Nutshell 
 In the finite dimensional setting , the analysis phase of the 
 is a linear operator , that given a signal a measurement vector . The case of interest is on the under regime , i . e . where under sparse or compressible on , can offer perfect or near optimal reconstruction by the solution of the following linear problem : 
  
 Notably , the theory , based on the restricted isometry property RIP , sufficient over and implicitly over the number of in order that 
 , when for some . The next result , in its original form stated in , that random offer a solution to that problem with a near optimal relationship between and . 
 Lemma : , Th . . and , Th . . Let be a random matrix whose are driven by i . i . of a distribution or a binary variable with uniform distribution over . For any arbitrary and , we have that : 
  
 if 
  
 with a probability , over the sampling space , at least equal to . Here is the solution of 
 and and are positive universal independent of and . 
 The proof of this result directly from , Th . . and , Th . . 
 B . Zero Rate Reconstruction for Compressible 
 Here we formalize the reconstruction of infinite . For this , we consider a finite length or fixed rate approach , where the idea is to analyze consecutive finite block of the sequence , i . e ., to sense and reconstruct for any , and study reconstruction in the limit when the block length to infinity . 
 More precisely , let be a sequence of positive such that . From this sequence , we consider the family of where for any , is the random matrix of by i . i .. as in Lemma , and is the function from to that the minimization problem in . Given a sequence and any finite block length , we can apply the approach over to recover , which is a random reconstruction function of the matrix . In relation with the relative approximation error in , we consider as a fidelity indicator the noise to signal ratio given by : 
  
 More generally , if we have a process with distribution and a sequence of with its associated finite block scheme , we can also analyze the finite block performance of the scheme by the object 
 , which is a random variable 
 of two independent random : the vector and the random matrix . Therefore , it is important to consider the average with respect to the statistics of the source i . e ., 
  
 Then the question we focus here is : for an compressible process that , what is the minimum rate of i . e . or more generally 
 of the classical scheme that 
 that : 
  
 with probability one with respect to the statistics of the sequence of random matrices 
 From Theorem and the RIP based instance result of the setting in Lemma , we can state the following result : 
 Theorem : Let be a stationary ergodic process . If is compressible , then for any sequence such 
 that , it that 
  
 almost surely with respect to the statistics of and the joint statistics of and 
 , respectively . 
 The proof is in Appendix . 
 C . Discussion and Interpretation of Theorem 
 : This result that in order to achieve zero distortion in the reconstruction for an compressible process , almost surely in the sense of , the scheme needs an arbitrary small number of per sample . In other , under the compressibility model assumption for the process , the minimum rate to achieve zero distortion is zero for the scheme . Then , it is remarkable to validate that is able to achieve the same zero critical rate that it is by the analysis of the pure oracle best term approximation error of compressible process see the result in Lemma . 
 : This result that the lucid notion of compressibility by Amini al . really in a meaningful performance result for the classical setting in the asymptotic regime when the goes to infinity . In other , we can say that 
 compressibility , meaning a sort of zero rate of innovation in the process , zero rate of per signal dimension for perfect recovery in the sense of distortion for the scheme . This result a gap not in between their notion of compressibility and performance guarantee in the asymptotic regime . 
 : Concerning compressibility of random and performance guarantee , we want to highlight the work of al . for the case of i . i . They show in , Theorem that if then 
 for 
 with a and they enunciate a version 
 of Theorem for the i . i .. case , Remark . Then , we want to give credit to this contribution to be the first result that a connection between of compressibility for i . i .. based on relative approximation and the performance in the asymptotic regime of the classical scheme . In this context , Theorem can be seen as an extension of these to the case of stationary and ergodic and , in the technical side , an extension on the use of the instance property of the 
 minimization . On the other hand , on the i . i .. context , and offer a way to verify that Amini al . compressible notion variable rate in nature a connection with the in al . , Th . and . in of what can for the case of compressible . 
 . DISCUSSION AND FINAL 
 The main result of this work Theorem a connection between al . , Prop . almost sure convergence result of relative approximation , and Amini al . notion of compressibility for random . More importantly , Theorem new to extend that connection and , consequently , a dichotomy between being and non being compressible random to the family of stationary and ergodic . This extension is over the almost sure convergence of the empirical of and , respectively see in the statement of Theorem to the true on the family of tail in Section . 
 The idea of looking at specific empirical as the basic object of interest , in b and c , instead of the statistics of the sum of the ordered sequence as considered in , Prop . and , was essential to extend the analysis from the i . i .. case to the case of stationary ergodic . 
 Finally , one can notice from the proof of Theorem that this result does not rely on a stationary property , as it is essentially over the family of indicator of the tail in . Then , we conjecture that the analysis of compressible can be extended over a family of random with a specific ergodic property over the tail , which is an interesting direction for future work . This observation us to put the attention on the general theory of non stationary with ergodic , , . 
 APPENDIX I 
 PROOF OF THEOREM 
 Proof : We first verify the second point to then move to the rest of the . 
 Point : of all and in : Proposition and and the hypothesis that 
 , it that and are continuous 
 in . Then , Proposition e , we have that all the and in the range . 
 Point : Continuity of : 
 First , it is important to verify that the implicit characterization of in and a well defined function . By contradiction , let assume that such characterization is not a function in the sense that for a given distortion 
 there are two solution of associated to two different . This last condition that from , which the fact that and are of . 
 Moving to the continuity , let us fix an arbitrary and . Then by the of the , 
 such that . On the other hand , by the of the , there , where 
 and without loss of generality we assume that . Then from of we have that for any , 
 . At this point , we can obtain the 
 , where the strict in 
 that relate them follow from the fact that by 
 and . Then , we can define 
 , where for any 
 we have by the of the function that there such that , and consequently , 
 the last set of from and . As 
 and are arbitrary , this the continuity of 
 . 
 Point : Strict of : 
 Let fix . By definition , we have that 
 . Furthermore , from the 
 given in and , we have that there 
 such that and . This that , and consequently from we have that . 
 Point : of : 
 First , it is clear that both and are differentiable by construction . In fact from 
  
 where and denote the of and , respectively . 
 Furthermore , we can introduce the auxiliary function that is differentiable , and 
  
 Finally , for a fix there such that 
 and , consequently , 
  
 the first equality by the characterization of 
 and , and the third that and . 
 APPENDIX 
 PROOF OF THEOREM 
 Proof : Considering that , let be its . In view of Theorem , for a fixed , there such that can be expressed as : 
  
 where , for which we introduce the short and 
 . Then , 
  
 By construction is non decreasing with . Hence , when 
 , the middle term in is negative and with , proving the convexity for that case . For the case , from the right hand side of convexity will hold , if and only if , 
 for all . To check that , it is 
 useful first to note that 
  
 Then 
  
 where from . Hence , convexity will hold , if and only if , for all . 
 APPENDIX 
 PROOF OF THEOREM 
 Proof : Let fix and . Assuming that is compressible , from Theorem we have that and , consequently , it that : 
  
 in . Again 
 this inequality is valid with probability over sampling space of the random object . There 
 fore taking the limit when goes to infinity , we have that 
  
 with probability one with respect to distribution of 
 , which is valid for any arbitrary small and . In other , if we define the set 
 , 
 then , and . Finally , 
 from sigma additivity of , which 
 that 
  
 with probability one with respect to distribution of 
 . 
 For the almost sure convergence result on the sequence 
 , we need a slightly different argument . Under the assumption that , it is simple to verify that there a sequence of positive such that and , more importantly , it that 
  
 The almost sure convergence with respect to the statistics of 
 from the Lemma and the fact that as by hypothesis . 
 Following similar than before , there such that we have that , and therefore , from Lemma , 
  
 with probability in . It is important to define the set where Lemma us that . Furthermore , from 
 Lemma , we have that 
  
 with probability one with respect to the process distribution of . In other , if we define the set 
 , we have that 
 from . Finally , we are interested in the set 
 where it is simple to show that , by and the of and . Hence , the problem to evaluate , 
  
 the last equality from the fact that . Then from the additivity and monotony of the measure , which the result by the definition of . 
 APPENDIX 
 COMPLEMENTARY A . Proposition 
 Proof : From the definition of and the , it where 
 and . Consequently eventually in which that eventually . This the result . 
 B . Lemma 
 Proof : As a short hand , let for all . 
 By definition for all , and consequently , 
 . Then from and . , it that . 
 For the other inequality , we consider the case when . We prove it by contradiction assuming that . Then from , there and a sequence such that 
 . Under the fact that , there such that this and the definition of in 
  
 where , consequently , 
 . Therefore , 
  
  
  
 This approach can be a finite number of times independent of the length 
  
 which . 
 C . Proposition 
 Proof : Fixing and , let us define 
 Then by , 
 , where from it that 
 . 
 D . Proof of Proposition 
 Proof : the to prove Theorem Point , if we consider , then for any there such that . This last auxiliary function is with respect to and we have that : 
  
 the last equality by considering that 
 . Since is non decreasing 
 with , it that is negative and increasing with , i . e ., is a convex function of . 
 E . Analysis of the Domain of Attraction of 
 Stable 
 The family of stable is the class of non degenerate that are limit in distribution of of random of the form : 
  
 where are i . i . of a random variable , and and are a of real . For the scenario when , the Central Limit Theorem us that the limit is a normal law . For the case , we have the less known family of stable , whose characteristic function is given by , Th . . : 
  
 being the exponent of the law , and , and . 
 Definition : . The distribution is said to be in the domain of attraction of an stable law with 
 , which we denote by , if there and such that : in distribution and the stable distribution in . 
 The collection is non empty and is by the following result : 
 Theorem : , Th . . and Prop . . Let and let us define : 
  
 on . Then to , if there and with such that : 
 i , 
 that is slowly , and 
 that is slowly . 
 Then we can state the following : 
 Proposition : If for some , then 
 . 
 Proof : Let and . Without loss of generality let us assume that is slowly . Then it is simple to verify that , and such 
 that . Therefore 
 . 
  
 ﻿ Based on for a street canyon type cell we compare the spectral of various cell interference mitigation . This Zero Forcing , Zero Forcing and . As a reference for comparison we consider and . Out of cell interference , as additive noise , is included in two : a all transmit simultaneously and neighboring transmit in time . We find that , and can offer in spectral efficiency over and by of around under interference limited . This with gains of only around when a standard mode instead of our measured data . This difference is due to the greater inter cell isolation that our street canyon type test environment . We find that over of the spectral efficiency under virtually all operating . 
 Index outdoor to indoor , cell , inter cell interference , spectral efficiency 
 are being to address the explosive demand for higher capacity in wireless , an increase in user density at where cannot cope with the traffic load . Spectral can be further some of the we discuss below . 
 Multiple Input Multiple Output transmission have received great attention . However , inter user interference in a low Signal to Interference plus and corresponding performance loss . This led to the proposal of MU , where transmission or cell interference . In cell , inter cell interference additional on performance . Many to reduce or avoid this interference have been . For example , almost blank can avoid inter cell interference when a small cell pico cell or cell is within a macro cell to address a local traffic . 
 Massive , a large number of at the Base Station to mitigate cell and interference , with a considerable increase in complexity and higher backbone overhead to carry inter cell Channel State Information . A far more elaborate scheme is the transmission , alternatively known as Network , where neighboring encode and decode for multiple simultaneous . Although , in theory , such a scheme can produce very large in throughput , only limited gains have been so far under . In it is shown , based on , that Network no gains with respect to an based scheme if the of the former are considered . 
 In this work we present an based evaluation of for single antenna , i . e ., Multiple Input Single Output , on the case where only cell is available . 
 We discuss below relevant . can be classified into nonlinear or linear . Among the nonlinear , Dirty Paper is a valuable capacity bound , , even though no practical have been . Simpler , suboptimal linear include Zero Forcing and Zero Forcing . In , cell interference is by choosing a matrix that the channel gain matrix and then the user via the water filling algorithm . It been shown that can asymptotically achieve the theoretical limit when considering the case of a large number of . poorly at low Signal to , when spectral are noise rather than interference limited . This effect becomes particularly significant for ill conditioned channel matrices . been to resolve this problem . In this scheme the matrix to be inverted noise dependent that , at low , compensate for the eigenvalue disparity of the channel matrix . 
 Simpler transmission that need little or no at the transmitter include and Time Division Multiple Access . In , cell interference is reduced by separating the coverage sector into smaller through the use of directive . The only feedback is that resulting from the association of a user terminal to a . Such a system , with no inter 
 Fig . . Example of measured scenario for a the cell campaign and the inter cell campaign . 
 user capability , will be limited by cell interference at high transmit power . 
 In , the transmission frame is divided into time , and each user is at full power during one of those time . The capacity gain of over for was in , but without inter cell interference . The in that work are used as reference for our . 
 The above are especially important when outdoor are intended to serve indoor , yet these have only been studied relatively recently . on based outdoor to indoor channel are in , but these were not in the context of MU . Only a limited number of include empirical data in the evaluation of MU . In spectral efficiency is for a distributed transmission system in an indoor environment , while a similar setup Network to compare the spectral efficiency of several , finding that and achieve about a three fold increase in per user data over non transmission and frequency division . In the rate distribution among is for an indoor MU scenario when time , frequency and spatial . In the aggregate capacity in a MU system , and is and with the sum of individual of various single user in outdoor and indoor . Our work is related to the above but inter cell interference and actual cell channel . 
 We evaluate the spectral efficiency of several MU in outdoor to indoor with along the streets , below the serving the of one city block . This from the often used assumption of hexagonal which is unrealistic in our deployment scenario . matrices are measured cell while out of cell interference is included as additive noise , as in , with power calculated a measurement based model . We treat out of cell interference mitigation through two simple . The basic time across . We also include a simplified version of the method in where inter cell interference is by . In particular , we the achievable spectral of and with . The is assumed to serve single antenna simultaneously . power is varied to cover noise and interference limited . Our measured outdoor to indoor scenario , in the next section , from those previously for MU : 
 In the following we describe the measurement setup and the statistical of our test environment . A summary of the various transmission is included in Section . The empirical and their comparison with those based on various channel are in Section , leading to the of our work . 
 Channel were done at ´ Mar ´ campus in ´ iso , Chile , and campus in , Chile . Two measurement were carried out : one at the channel matrices for the cell being by the and another at the interference by neighboring . 
 To obtain the channel matrices within the cell , we used a linear array to emulate an outdoor . It of vertically coaxial spaced half a apart , with a reflective . With the , each antenna a maximum gain of and a half power . This array was along the street at a perpendicular distance from the building wall ranging from to . The antenna corresponding to the possible Subscriber Unit was a single gain vertically coaxial dipole antenna mounted on a long swivel arm . This antenna was rotated step wise in to obtain local statistics . A . , . Continuous Wave signal is through this antenna . For convenience , we measured the propagation transmission . 
 were chosen inside in with outside facing sized . by . , from the outdoor array by between and . The array were at . above ground level , i . e . a height that would allow easy installation by a service provider aiming to illuminate or that are across the street . The indoor antenna was at height on a first floor , i . e , at roughly the same height as the array . The perpendicular distance from the exterior wall to the from to . The placement may be consistent with an access point in a street side room , which can also act as an indoor wireless router . 
 A schematic representation of one measured scenario is shown in Fig . a , where the inside are the of the and the bar the position of the array . the array at various along the direction of the street measuring to at angular of up to off array . The are made of concrete and the glass have steel and aluminum . The were carried out during the day in the absence of pedestrian traffic , to ensure static channel . A shadow antenna not used in is included at each end of the array to preserve the impedance due to mutual coupling in all the . A reference antenna in line with the , at a distance of about from its shadow antenna , measuring the relative phases of the array . A system of any one of three of four to a channel receiver , while the reference signal is permanently connected to the channel . The receiver the carrier frequency to and these are fed to a channel data acquisition system . A computer is to automatically control the of the swivel arm , the switching and the data acquisition . synchronize the transmitter and the channel receiver for phase coherence . Fig . a schematic of the measurement system . The are for at a rate of 
 , per channel , total . This interval is partitioned into contiguous sub of per position of the swivel arm . a Fast Transform algorithm , a value of magnitude and phase is per sub interval . Due to exact synchronization of sampling , the data correspond to an integer number of of the , the without . The existence of consecutive for any transmit receive combination of verification of proper equipment operation and the reduction of residual measurement noise through . We measured a total of of of the outdoor and indoor , i . e . by matrices . 
 As already , in our study we have not considered joint among . Instead we treat the out of cell interference as noise . To estimate this interference power we measured path over a wide range of since it is not a obvious which will cause significant interference . The sounding system was basically the same of the first campaign , except that at the end we used a single antenna , since only the scalar path loss is . We as . Over extending from to we measured the outdoor to outdoor path loss . This included same street as well as around the corner in the grid type street . For a subset of these we also determined the path loss from the to the indoor location that the outdoor position as shown in Fig . . This us to model the outdoor penetration loss . We describe this in further detail in the next section . We found that the outdoor to indoor statistics can be quite accurately by a penetration loss to the corresponding outdoor to outdoor path loss , as in . 
 The based channel data us to obtain the channel gain statistics and in the following section . From these we also assessed and the achievable spectral for the various . These are subsequently to those that would be with various . This is in detail in Section . 
 In this section we statistically characterize the propagation channel for the chosen environment . As will be seen , our are consistent with those in the literature for similar , which that we have chosen that are representative for urban . 
 To model this loss we used both cell and inter cell to compare received power in for by the exterior wall . The average power was from the of the swivel arm , and thus the small scale . The are shown in 
 Fig . . a Measured penetration loss and fit . model . Outdoor to indoor path loss at long . 
 Fig . . Fig . a the difference in measured outdoor and indoor as a function of the incidence angle defined in Fig . . For comparison we show the mean penetration loss of the model . We a version of the model given by to our data , a Minimum Mean Square Error criterion to obtain the best fit for the average penetration loss . These are listed in Table I 
 where is a zero mean random variable , which we found to match a distribution . Its standard deviation was found to be angle dependent , with of . for links growing to . as . The latter correspond to those of inter cell interference links in our street canyon geometry . For cell links the average penetration loss is essentially constant at . , much less than by the model . In Fig . we show measured for outdoor to outdoor and outdoor to indoor links at relevant for inter cell interference . As will be in subsection , for up to the average outdoor path loss for a linear fit model a slope only slightly than that of free space links . The outdoor average path loss was found to have the same slope , but as shown , its are offset by an average penetration loss of . , consistent with . 
 cell links were in general Line Of Sight through a window , with minor in the path . This is often to as Line Of Sight . The large scale path loss is calculated at each transmit receive placement , by again the received power of the of the swivel arm . these we fit a classical log distance model to the path loss in as : 
 where is the path loss at the reference distance d , is the path loss exponent and is the shadow fading term , a zero mean random variable with standard deviation . It is assumed that . The value of is found through a regression fit of the empirical of path loss . The of , are shown in Table . We tested various other . This included the free space path loss model i . e . plus the average cell penetration loss from , a two slope model and an exponential model based on . j Type . The improvement of the fit in of is negligible . Fig . illustrate the comparison . In the same figure we also included for reference the line that to the free space path loss and that by the model for the outdoor to indoor case in urban micro with a grid layout . As will be later , we found that the smaller cell penetration loss with respect to the model had a significant impact on the achievable spectral . We a small proportion less than of path that are lower than those in . This is likely due to effects in our street canyon environment , particularly at shallow incidence . 
 Based on our inter cell measurement campaign , we an outdoor to indoor street canyon path loss model in order to evaluate the inter cell interference from neighboring . We firstly found that a two slope model accurately 
 Fig . . Single slope fit and exponential fit for the measured cell in function of the distance , in logarithmic scale . For comparison , free space and are shown . 
 where the PLout is the outdoor to outdoor path loss , is the break point distance the change in slope and is the corresponding path loss . The outdoor to indoor inter cell path were found to be well by the sum of the average penetration and path from and , respectively , plus a shadow fade term with . standard deviation . We note that for the of incidence that will characterize most inter cell interference , the average penetration loss is . . This is consistent with what been in the literature . Table the of the model . 
 Through the same outdoor to outdoor street canyon we found that Non Line of Sight corner links show a path loss that is at least above that of links for the same distance , which is consistent with . Thus , the interference by around is negligible to that from neighboring same street bases . For this reason , the former are not considered in the performance analysis of Section . Our path loss model does not differ very significantly from previously and we therefore omit further . 
 The small scale fading statistics for the cell links , from any array element to the , were from the complex channel gain corresponding to a rotation of the swivel arm , to their local spatial average . As for links rich in path , we found that a distribution a good fit for the channel gain envelope statistics . The corresponding factor was the two moment method . The average percentage of absolute value error in the fit of the empirical and model Cumulative Distribution 
 was . . Higher were when fitting other commonly used small scale fade To account for possible due to the separation of the array , we the fade statistics per element and subsequently the individual . These were found to be similar , the being by the limited data set size . Following , , we a log normal distribution to the factor statistics , i . e . ,, with its shown in Table . 
 The correlation between the received by the different array of the serving was calculated all . To this effect we each by channel matrix by its norm , in order to combine the data corresponding to with different large scale path . We initially a single exponential model to the as done in , but found that a double exponential model provided a much better fit , which significantly the accuracy in the prediction of the spectral . For example , for the case , the estimation error of the capacity with the single exponential model is , which to when a . The general expression for both is given by 
 where is the correlation between by a spacing of , i . e . to adjoining and , the number of in the array . The corresponding model A ,, r and r were by a least fit , and are listed in Table . 
 with being the component of the matrix , , where is the distance between adjoining expressed in , , , ,..., is a vector and is the steering angle of the signal with respect to the of the array . In our simulation based this angle was as a random variable with a uniform distribution over to cover the range of possible user . is a vector of circularly symmetric zero mean complex random with unit variance , correlated following the procedure in . To this effect we used a 
 For the inter cell propagation model all links are , due to the shallow angle of incidence between the neighboring and the indoor . We found that the small scale in a neighborhood around the antenna are very close to distributed . This the in without a dominant component . The factor the two moment method was essentially zero . Empirical were found on average to differ from a distribution by no more than absolute value . However as will be in greater detail in Section , small scale in the interference calculation was found to have little effect on the resulting spectral . We confirmed this for several and subsequently small scale , which very considerably the . 
 When our measurement based with the above model we proceed as : 
 Step : Choose a a angle theta from the to the . The distance must be chosen in the range from to , and the in the range within with respect to the of the array . Also choose the number the . 
 Step : Calculate the shadow fading and path loss and the cell of Table . 
 Step : Generate a log normal distributed random variable with as in Table . 
 Step : Generate the correlation matrix and the of Table . 
 Step : Calculate by generating a by matrix of i . i .. circularly symmetric zero mean complex random with unit variance and correlate them the correlation matrix and the procedure detailed in . 
 Step : Generate the matrix the chosen angle and the appropriate value of . 
 Step : Calculate . Add the previously path loss according to generate the channel matrix to 
 After generating a channel matrix , the interference by the neighboring to that is by the following : 
 Step : Determine the distance between the neighboring and the street position directly across from the indoor . 
 Step : Choose the Sum Power Constraint and an azimuthal gain the neighboring in the direction of the street position of the previous step . 
 Step : Calculate the shadow fading and path loss to the indoor and with the inter cell from Table , the penetration loss from Table I and the distance calculated in Step . 
 where neigh sin neigh neigh the gain of the antenna array , is a factor to adjust the width of the lobe and neigh is the direct path angle to the street position across from the indoor In order to the same as in our , the lobe width is set at , with . . is the total power and is the path loss between the cell and the user , which is dependent on the by . 
 We compute the per user spectral efficiency aggregate spectral efficiency number of for several a broadcast channel in the . We assume a transmission by the array with . There signal each with a single antenna . Thus , there an by channel , by the column vector , between the transmitter and the th user , with ,...,. The is formed by row into a channel dimension 
 where the random vector is the signal , is the random noise at the receiver and is the total inter cell interference reaching the receiver . The signal is expressed as 
 where u is an independent zero mean complex random vector , is a linear matrix and : is a non linear transformation in the case of , and the identity matrix in the case of and . 
 In all the tested , a is on the power as done in . This is written as : 
 where is the trace of the matrix , wi is the i th column of , is the maximum power and pi is 
 with being the i th element of u and , the complex conjugate operator . The power to each user then is , for ,...,. 
 For our comparison we chose a receiver noise variance set at , which to a of and a receiver noise figure of . As already , the interference from neighboring is as additive 
 is the sum of the interference from all neighboring . The procedure for the calculation of will be in the next Section . We note that as the transmit power , will do so in the same proportion , which will lead to the saturation of the achievable spectral . For our analysis we assume a antenna base and up to . The channel matrices will thus be by , i . e . . When , the knowledge of the equivalent each . If more were available at the , treating interference as noise would be pessimistic in that it the possibility of the interference signal structure in a detection MUD type receiver . We now briefly describe the various transmit considered . 
 This scheme is included as against which to assess the of the below . Each user is during one of with power . We note that in this scheme we are only separating into time within a cell , cell being separately . The achievable spectral efficiency is therefore 
 In a lower bound for spectral efficiency is , which is tight for . This bound is when the best user is all the time with power P 
 This is the capacity technique and it is to be considered as an upper bound of actual performance , given that its implementation is complex and to date there is no cost effective algorithm to implement it . 
 In , there is no a choice for the matrix , and full is at the transmitter . The algorithm this knowledge to avoid , for the th user , the known interference produced by the intended to the , ,..., namely , the previous . In doing so , the block by a transmit the sum random , as K 
 where i , is the information bearing vector intended to the i th user . the covariance matrices Si , 
 , the maximization problem for the spectral efficiency is as stated in : 
 An efficient numerical method to find the solution to this problem is in , the duality between the and the multiple access channel . 
 This scheme the weight cancel the interference in the . Thus , is , in general , the pseudo inverse of the channel matrix which is assumed full rank : 
 In particular , non singular , is . The spectral efficiency is therefore calculated as 
 are to each user weighting the water filling solution . 
 Although the simplicity in the selection of the scheme attractive , poor performance if the channel matrix is ill conditioned . In these , usually one or a few of the matrix are very large to the rest . At low transmit , where spectral are limited by noise rather than by interference , the solution which sum rate may result in some being a zero data rate . 
 was to correct the problem by in presence of ill . A regularization factor is in the pseudo inverse of , which to normalize the of the matrix , allocate power to all and avoid noise enhancement . In this case , is chosen as 
 with a being the regularization factor . In it is shown that the optimum a for a large number 
 with . Despite not being the optimal value for a finite , for simplicity we will use this value of a . not exactly invert , cell interference will now affect . Thus , the spectral efficiency is calculated as 
 In , the term i the total interference reaching the i th user . are by water filling , an iterative algorithm that the in each iteration considering the cell interference . 
 In this scheme the sector is divided angular of the same width . In practice , this can be done a directive antenna per . To simulate the effect of various of directive in the we took advantage of our channel with the antenna array . This us to generate antenna based on between and dipole . We note that this steering with appropriate , which , however , remain fixed , i . e . which cannot be adaptively according to user . We define the antenna weight matrix as . The available power is distributed equally between and it is assumed that one user is associated with each of them . Then pi , i ,...,. The spectral efficiency of is thus calculated as 
 In this Section we present the main of our work . The general under which we the various are in sub section A . In present the achievable spectral based on the measured data . This , in . and the various cell interference avoidance in . . Finally , in evaluate the spectral channel matrices by various simulation . This , a i . i .. model for cell small scale and our own model in Section . These are with those in Subsection . . 
 our empirical data and we now evaluate the per user spectral efficiency aggregate spectral efficiency number of of , and for a cell system disposed along a street , as shown in Fig . . As a basis for comparison we considered with . In all we have considered perfect knowledge of cell at the serving and a sum power constraint with a range of realistic for the transmit power . In order to establish a for our comparison , we also calculate , as in , the upper bound for capacity , the spectral efficiency for with equal time allocation to and the achievable spectral efficiency when the sum rate is . To avoid , we will henceforth use the term cell when to the system where the time division multiple access only within the cell , in contrast to the case where the among each other , which we denominate time reuse . 
 Although our treatment on cell interference cancellation , for we have additionally included , based on , the case where extra allow mitigation of inter cell interference . We chose a very simple scheme in which the have and the extra of freedom are used to steer towards the being in the adjacent . Given our street canyon geometry this is likely to eliminate the of interference . We treat . Firstly we obtain the achievable performance when full is available across contiguous . We call this system . Alternatively , we consider the case where the information between is limited to channel correlation matrices . Although this is less effective , it will not require such frequent , less burden on the network . We denominate this as . 
 The cell system along a street , as shown in Fig . by . Each cell is assumed to be close to the building facade in the middle of the block and service to the across the street . This closely our measurement scenario . The spectral are for the in the central cell . We found that more interfering in the system did not impact our . 
 Fig . . Topology scheme of the system . Each red towards the city block across the street . 
 The evaluation of the achievable will consider two : no between reuse , and a scheme that time reuse , where adjacent transmit in alternate time to eliminate the source of inter cell interference . For the case of reuse , the achievable in Section are appropriately scaled . 
 At each scenario such as in Fig . we measured at least placement of the array and of the rotating dipole that to possible . Each such combination a by channel matrix , where each row to a possible user position . Since our work a antenna array , multiple of adjoining in the antenna array can be selected . Out of this ensemble we used a subset of for each placement to obtain the statistical description of the channel spectral . We found that further in the number of in negligible of the . Consistent with Release , Rev , we grouped , dividing the coverage region into . We further assume that each is illuminated by a pilot beam of a different frequency the . The corresponding phase are chosen so that in free space the would be in the direction of the center of the corresponding . A power distribution with level suppression was assumed . This choice was based on the fact that this level of suppression good for the case , as will be below . these and the measured channel gains corresponding to each user position , we associated the to the that them with the highest power , thus disjoint . One user is randomly chosen from each group , leading to one possible realization of the by channel matrix . There are however where no user with a , because their are such that all receive more power from one of the other . When this , the number limited to the number of . Given the extensive set of available , this was found to occur only rarely . In such we considered a by channel matrix . 
 The per user spectral efficiency of the is for each by matrix , for a given dividing the by the number of . The from all were then to generate of spectral efficiency for all . The total number of channel matrices used in our statistical was , per placement . 
 In the case of cell , the power is the same as the of the and the aggregate spectral efficiency from is divided by the number of , to obtain the per user spectral efficiency . 
 We found that the interference from neighboring , as additive noise , could be included only the scalar path loss model in Section , i . e . the effect of the small scale . This very considerably simplified and the . To validate this approach , we firstly calculated for a subset of , the spectral for , and with the small scale included . For each placement we considered randomly chosen channel . the for and is straightforward as it only and to take into account that the interference is dependent on the small scale fade affecting each user . For small scale is considerably more complex and to much longer . Following the reasoning in Chapter . , page , it is necessary to obtain the covariance matrices of the neighboring in order to calculate the inter cell interference they generate as in Section .. These covariance matrices are a method in . Then , the received power from one neighboring cell is calculated as 
 where is the channel vector between the neighboring cell and the user in the central cell and is the covariance matrix for the i th user in the neighboring cell . We found that for and , small scale on average in the spectral efficiency by less than . For the case of his difference was less than . Based on this finding we used the simplified approach that small scale and calculated the equivalent noise power due to interference as : 
 where is the interference power due to the i th neighboring cell , calculated as in , i . e . small scale . 
 Optimization of for the Single Cell Case : We evaluate as a reference the spectral of when considering only cell interference . As will be seen later , for this type of interference completely that from neighboring . Since this dominant cell interference on the shape of the radiation that cover the cell , our initial objective is to evaluate this aspect . We found that generating with only is far from optimum and thus we other that more flexibility in and 
 Fig . . Per user spectral efficiency of . , for different number of in the array , at availability . 
 suppression . Both and suppression will affect the interference that the is exposed to . the beam pattern will eventually lead to the sector not being fully covered by the and at the same time the appearance of poorly suppressed , which will contribute to the interference in the neighboring . At the other extreme , very large suppression of in an which may invade the neighboring . For our measurement , we found that a suppression in excess of produced virtually no further with regard to the reduction of interference . The reason for this is that at a certain point the energy scattered from the of a into the adjoining region over that due to the . It from the above that for any antenna array there will be an optimum . To observe this effect , we between and antenna to synthesize radiation with suppression ranging from to . The power was set at , a power level that assured interference limited operation . 
 Fig . the . We have plotted the median per user spectral efficiency for various of directive . In this and in subsequent we will refer to the spectral efficiency available to of as the spectral efficiency at availability . We note that the independent variable chosen for Fig . is the antenna array , which is actually a result of the choice of suppression and number of antenna used in the directive array . As can be seen , more to synthesize the the achievement of higher spectral as it of freedom in the reduction of interference . In show for the median spectral efficiency that can be in our , the optimum choice of for various of directive . We include the relevant that characterize the antenna in free space and the attenuation of the at the sector border . As seen , under interference limited the width must be made much narrower than the sector width to achieve the best possible spectral efficiency . 
 Spectral Efficiency of Cell Interference Reduction : We now evaluate the per user spectral efficiency of the interference reduction in the non 
 cell system , the power limit from to in of . From the of the spectral we then obtain the spectral available to and of . Fig . the . For reference we include to as and cell , both considering inter cell interference . For we used the best possible radiation for both and antenna as previously in Table . When the is limited to , the theoretical capacity upper all other at any power level and any availability . However , and can achieve an important proportion of spectral efficiency . In fact , and at for and availability respectively , while and . The improvement of over , particularly at availability , is attributable to dealing effectively with poorly conditioned channel matrices and high interference . As we will see later , these differ significantly from those through simulation a model with default . Our predict spectral much higher than those from that model . 
 The comparison with the case where the are assumed to have and is shown in Fig . . We here repeated the of and to contrast them with . For the latter we considered with and without inter cell interference mitigation . performance at the expense of added transmit is of course present in the region . However the more significant gains occur as the transmit power where the spectral efficiency of the without inter cell interference cancellation approach saturation around . Full exchange across will of course be best and in our case a relatively modest amount of information exchange . only the channel correlation matrices , which may require less frequent , more limited . 
 a very clear saturation for much lower than the interference . The reason for this is that with growing transmit power , for cell interference spectral efficiency before the effect of neighboring in . This is by the fact that the saturation for these match the best in Fig . , where no cell interference was included . As a result of this interference , will only achieve spectral comparable to the interference in the noise limited case , i . e ., for low transmit . In 
 Fig . . Per user spectral efficiency . power constraint of various for a availability and availability . 
 Fig . . Per user spectral efficiency . power constraint of inter cell interference mitigation for availability . with and and with are shown for comparison . 
 the saturation region it no better than cell , which is particularly simple to implement . In this region , around of spectral efficiency and of spectral efficiency for and availability . 
 As seen in Fig . , the bound in considerably its actual spectral efficiency , which is and of the bound at and availability , respectively . The spectral efficiency gain of over is in without taking into account inter cell interference . It is shown that this gain to one for low and is bounded by min , for high . This gain is shown for our data in Fig . for , and availability as a function of the . We have plotted the spectral efficiency gains for our cell system and , for comparison with the in , for the case when the neighboring are turned off , to as single cell . As seen , the spectral efficiency is always above that of cell , even for low . For high power , the gain asymptotically the bound in the single cell case , but well below this value for the cell case . 
 Since inter cell interference the asymptotic spectral efficiency , we the performance of the above reuse system with the reuse system , considering only the case where the have . Fig . this comparison 
 Fig . . efficiency gain of over cell for , and availability in the single cell and cell , in function of the . 
 for , and antenna . Under our , reuse worse than reuse for and availability . In the case , which is cell interference limited , reuse in a net loss of about half the spectral efficiency at all . It was found that for our data reuse only an advantage over reuse for availability and above in the case of . The reason for the advantage at high availability is that in such the spectral efficiency is limited by a small number of poorly , subject to particularly high inter cell interference . In contrast , for the rest of , the of reuse will not offset the disadvantage of only half the time . Similar have been found for hexagonal macro cellular with such as frequency reuse , fractional frequency reuse ., where it is also found that essentially those few at the between the interference . The proportion of such is even smaller in our linear layout . 
 We also the effect of the inter cell interference on the spectral efficiency of the most exposed to it . For this purpose we grouped the into edge in the external and central in the central . We compare their per user spectral by plotting the Complementary for at of . We this for reuse and reuse . The are shown in Fig . . As seen , in both 
 Fig . . Comparison between reuse and reuse for the per user spectral efficiency . power constraint of , and antenna 
 Fig . . Comparison of of the per user spectral efficiency of for , for edge red and center black when reuse and reuse . 
 reuse is on average a better option than reuse . We illustrate this effect for , as due to the form in which spectral are for , it is not possible to obtain the individual per user separately . 
 In order to test the sensibility of the to the level of inter cell interference , we repeated the above assuming penetration of and . at of exceeding . . As , this in lower spectral and the interference limited zone starting at a slightly lower transmit power . We show this for and in Fig . . In particular , for a maximum penetration loss at availability , and come within of the saturation spectral efficiency at . and . lower than in the . penetration loss . The corresponding for . penetration loss are . and . . Not surprisingly , the benefit of reuse as the penetration loss . In fact we found that at availability , reuse will perform on par with reuse in the saturation region when the penetration loss is . At the same availability , when the penetration loss is . , the spectral of reuse at saturation are around higher than those of reuse for both and . However , reuse is still worse than reuse at availability . 
 Finally we our based with those that are different simulation . This : the model based on our empirical data as in Section , a model based on our see . and combined with small scale i . i .. and two based on the . We compare the per user spectral efficiency at availability . power constraint for and under the same used to obtain the shown in Fig . . The are the empirical setup but the various channel to generate the gain matrices . for the array were , each with a distance between the and the center of the room determined by a uniform random distribution in the range from to . For each placement a set of one per is , all within a range of with respect to the previously chosen distance . are per array placement . The are shown in Fig . . 
 The model considered is in detail in . We by the model with the indoor urban default . We used these for both the cell being and for the interfering neighboring . This is as the default model in Fig . . As seen , this did not yield a good prediction , as it spectral efficiency by a large margin around of empirical spectral . We thus introduce a version that we refer to as , which better our measured . This from the default model in that for the central cell i . e . the cell being we use the outdoor to outdoor model to calculate the path loss , which we combine with outdoor small scale default . For the neighboring we use as before the outdoor to indoor urban scenario as in the model . This choice was by our empirical data . As in Section , the cell path loss for the outdoor to indoor scenario was close to that of a link , but at the same time the were quite small , typically below one . Thus 
 Fig . . Effect of maximum penetration loss on the per user spectral efficiency of and for availability . 
 despite an excess path loss that is much smaller than that of the model , the scenario is still by very rich propagation . The resulting model a better prediction with an error of for . 
 The two based on our empirical data result as would be in the best match and provide added insight into the effect of the channel . The model that only based path loss combined with i . i .. small scale spectral efficiency , but the error is in general not very large . For , it is in the saturation region . Our model the small scale slightly the spectral . We found that the difference between both is essentially due to the inclusion of channel correlation . In fact , when this correlation in our model , the resulting spectral become very close to those of the i . i .. model . This is consistent with what was in this regard in . 
 We have the advantage of cell interference by their spectral with that of , which only interference fixed antenna . We found that for a typical scenario , by a street canyon type setting , the spectral efficiency gains over can reach between and , the exact value depending on the specific system and the percentage of being considered . is limited by cell interference and in fact does not perform better than cell . Among the cell interference , can achieve more than spectral efficiency . Interference from neighboring the performance , but we found that to improve this by alternating the transmission of contiguous in general in a net loss of throughput . Considerable gains are achievable if the interference of the neighboring is , but at the expense of added transmission and extra overhead . Our assumed service environment significantly higher spectral by a factor of or than by the standard model . This is because we that same cell links had significantly less path loss than 
 Fig . . Per user efficiency versus power constraint comparison empirical and data for availability . 
 by the model when the is at indoor free of significant blockage to the for example close to an outdoor facing window . The resulting advantage in spectral efficiency that choosing an adequate position for the in combination with indoor , will provide considerable gains over to directly serve indoor at all . 
  
 ﻿ We obtain the maximum average data achievable over block fading when the receiver perfect channel state information , and only an entropy constrained approximation of this is available at the transmitter . We assume channel gains in consecutive are independent and identically distributed and consider a short term power constraint . Our analysis is valid for a wide variety of channel fading statistics , and fading . For this situation , the problem into designing an optimal entropy constrained to convey to the transmitter and to define a rate adaptation policy for the latter so as to maximize average data rate . A numerical procedure is which the and reconstruction of the optimal , together with the associated maximum average , by finding the of a small set of scalar of two scalar . this procedure , it is found that the maximum average capacity , in some , time between two . In addition , it is found that , for an entropy constraint log , a with more a small capacity increase , especially at high . 
 Index Channel state information feedback , Information , fading , quantization , radio communication . 
 INTRODUCTION 
 I 
 T is well known that the achievable data for reliable communication over a fading wireless channel depend on the availability of channel state information at the transmitter and end , . For single input flat fading , the of channel gain and phase . If perfect is available at the transmitter perfect and at the receiver perfect , the channel is slowly fading and the transmission is subject to a long term average power constraint , then the average capacity is by rate and power to the channel gain in a time fashion , . By contrast , if an instantaneous per block maximum power constraint is , the are ergodic and the transmission are long enough so that the fade statistics over each block converge to their ensemble statistics , then the ergodic channel capacity is achievable without , . Else , if the fading is so slow that channel gain can be as constant within each block which to a block fading scenario then is beneficial . In this case , with perfect and per block power constraint , the capacity is by at maximum power , with only the data rate being to the channel gain in each transmission block . 
 If perfect is available and the receiver back this via an with limited information throughput , then only imperfect will be available at the transmitter . In a block fading situation , the uncertainty at the transmitter about the true channel gain in each block a trade off between throughput and reliability : the the data rate chosen by the transmitter , the higher the probability of exceeding the channel capacity during the transmission block . This the problem of the at the receiver and it at the transmitter i . e ., choosing rate and power in a rate distortion optimal fashion , where the distortion is some measure of the decrease in throughput , as in , or the increase in error probability , as in . 
  
 The capacity of memory less block fading with long term power constrained transmission and fixed rate constrained feedback was studied in . A similar situation was considered in , assuming a scheme in which data are perfectly or totally lost if transmission data rate is , respectively , below or above the channel capacity during the block . The idea in was to design a with a fixed number of quantization so as to maximize the rate , i . e ., the number or long term average of successfully . Also for a constraint in the number of quantization , studied the maximization of throughput considering a noisy feedback channel . There exist also numerous related to throughput maximization for multiple output wireless see , e .. , and the therein . Although not directly related to the problem which is the focus of this work , it is worth that , in all the in , and the therein , the only constraint on the where there is a is its . In , the maximum average throughput under a long term power constraint and for a fixed number of quantization is . The performance of to as MASA was against that of average reliable throughput to as ART , which allow for to occur . It is shown in that , in some , when the additional feedback load of the ART associated with the transmitter of a previous outage is in , MASA outperform ART . In that context , the feedback load to the entropy of the plus and that are sent to the transmitter . However , in this entropy is a , i . e ., after the have been without considering entropy as a constraint . 
 Thus , in all these , the design of optimal been only considering a constraint on the number of quantization or . However , if one the question what is the maximum throughput that can be if there is a constraint on the amount of information that can be sent to the transmitter for the , then it is more appropriate to consider an entropy constraint instead of a constraint for the . On the other hand , the entropy of the output , say , is a lower bound to the average number of to represent this output . At the same time , by , it is possible to find prefix free for each outcome with an average length not greater than per realization . Moreover , in a situation in . i .. are at a time which would happen , for example , in an system fading , joint entropy would yield with an average length , at most , bit . Since , in general , information to feed back for each realization less average power , or time , the latter can be directly associated with a low entropy . This a practical motivation for considering entropy , instead of , as a constraint for the . However , to the best of the knowledge , there are no available on average throughput maximization in which the to encode for the transmitter is to be designed subject to a constraint on the entropy of its output . 
 With the stated in the previous paragraph , in this paper we study the problem of finding entropy constrained with any given number of quantization , for block channel gains for the transmitter , that yield the average data rate . In our setup , the channel is assumed to experience i . i .. block fading , with associated gains and phases perfectly known to the receiver . We consider a wide family of fading statistics , general enough to include and fading with one or more of freedom . As in , the over which is fed back is an , zero delay channel . To solve this problem , we propose a numerical method which the optimal quantization and reconstruction for any given number of quantization and average channel signal to noise ratio . The optimization problem is partly similar to the design in because of the common entropy constraint . However , as we shall see , since the distortion measure in this case is the decrease in average rate not mean squared error , the resulting situation is vastly different from the one in standard entropy constrained quantization . The problem turns out to be non convex , and our analysis that it , in general , several local . Its formulation , for quantization , to a system of N non linear in N , each of which taking over the non negative real . Since each of these must be numerically , and due to the high dimensionality of the search space , direct solution of this system of a high numerical complexity task . The numerical procedure in this paper greatly this complexity by turning the problem into finding the of a small set of scalar of two scalar , only one of which unbounded support . The evaluation of each of search with respect to monotonic . By this procedure , it is found that , in general , the maximum average a given entropy is a non concave function . Since in our formulation time between two an average capacity and entropy equal to the weighted of the capacity entropy of each regime , the region of all achievable , is given by the convex hull of curve . On the other hand , it is found that if is log of the number of available quantization , then the so as to obtain is nearly optimal . Our also allow one to find the gain in average throughput of an optimal entropy constrained instead of a constrained optimal . For instance , when the average is , then an entropy with and an average rate of bit per realization an increase in average throughput over an optimal fixed rate with two i . e ., the same average rate . The performance of the latter fixed rate with the one found in . It is also found that for any given maximum entropy constraint log , the increase in maximum capacity by a with more relatively small . Moreover , our analysis also that , for any given , the maximum average capacity is a with a finite number of . This with what is also for an exponentially distributed source but with as the distortion measure , wherein the optimal turns out to be uniform with infinitely many . 
 In the following section we present a precise model description , introduce some notation , and formally state the problem of interest . To illustrate some of the of this problem and its , we first analyze the case two quantization , which can be explicitly , in Section . Then we extend the analysis to the case in Section , where we introduce the numerical procedure to solve the problem in its generality . the with this procedure for the case and under fading . Finally , Section . 
 Fig . . Transmitter and receiver connected by and . 
 PROBLEM FORMULATION 
 We consider a block fading additive white noise channel , a transmitter , a receiver and an error free , zero delay channel , as in Fig . . In the transmitter , the binary message into consecutive of . During each block , a real valued sequence is over the channel . The random block channel gain magnitude for the th block is assumed constant within each block . Channel gains in consecutive are i . i .. according to a probability density function satisfying the following : 
 Assumption : The of the the channel gain squared magnitude the form 
 u K e u , 
 for appropriate K , K , where the differentiable function is such that the ratio u u is non increasing with respect to u over , . 
 The structure of the is fairly general . For example , if channel gain magnitude is distributed , then the form 
 , 
 where I is the function of the first kind of order zero . From direct comparison with , we obtain , for this case ,, K s and u . It can be numerically that the latter form of the by Assumption . Likewise , if channel gain are by a distribution , then the form 
 m 
 u u e u 
 m 
 and we have K , K and u um . If , it is easy to verify that u also the 
 by Assumption . 
 to Fig . , the real valued random process , ,...,, is with sample variance N . Thus , if were the , taken at frequency , of continuous time band limited to , then the 
 The necessity of the condition upon in Assumption will become evident in Lemma Section , in which it us to prove the convexity of a function a key role in the problem under study . 
 two sided of the latter would be N . On the other hand , the information bearing signal is subject to a per block power constraint of the form 
 , 
 where . With this constraint , if the block large , then the maximum achievable data rate during any given be well by capacity formula as , where 
  
 is the mean at the receiver for a channel power unit mean value . 
 At the other end of the channel , the receiver is assumed to acquire a perfect estimation of prior to or at the beginning of the th transmission block . This channel power gain is instantaneously and entropy , with the resulting being sent over a zero delay , error free channel . These about the feedback channel have been considered before in , . The zero delay condition can be to be a good approximation when the time spent to feed the back to the transmitter is much shorter than the duration of a frame . In turn , it is possible to have an almost error free feedback channel if the feedback is sufficiently large and or strong forward error correction is employed for the . And naturally , if in a given situation these latter are not present , then our would provide upper to achievable performance . 
 As in the Introduction , it is possible to translate a small entropy of the into less average power , or time to convey this to the transmitter . At this point , it is perhaps worth that if only a single realization is and fed back at the beginning of each block , then these may require one to match the channel and modulation scheme in the feedback link to the variable coming out of the entropy coder . For instance , an off the shelf channel coder and modulator in the feedback channel would yield an that only of fixed length data . Such choice which would entail significant when variable length , in comparison to sending . However , in this scenario wherein a single realization is and fed back at a time , the 
  
  
 Fig . . Illustration of the i th quantization cell . 
 feedback channel and modulation can be chosen so as to handle variable length bit or the associated unequal probability of the as efficiently as it is possible for fixed rate . This can be done , e .., by variable length error correcting or joint source channel see , e .., and the therein . Although the design of such and is beyond the scope of this work , we illustrate this fact with an example a simple scheme similar in spirit to , which can be found in Section A in the Appendix . 
 Upon the , the transmitter a transmission data rate from a discrete set of data . To define the and its reconstruction , let 
 be the number of quantization or , and let denote the set , where 
 . 
 Define also the quantization , , i ,...,. As in , whenever the channel power gain within cell , the transmitter a satisfying , belonging to the i th amongst , one for each cell . This is capacity for some nominal channel power gain associated with the cell , i . e ., 
 g , 
  
 i ,...,. 
 Thus , the power gain can be seen as the set of reconstruction or of the , as in Fig . . 
 Since in each block the transmitter information over the a capacity code for a nominal channel gain , all the are correctly if . Else , if , then is not by the channel , and the receiver an outage , all the information received during the th block . From this , it is clear that a necessary condition for a set of reconstruction to be optimal is 
 , i ,...,. 
 Let the random variable , taking in with , denote the output of the for the th block . As already in Section I , we focus on that satisfy an entropy constraint , which we now formally state as 
 H 
 block , , 
 with the maximum entropy for the output . 
 We are interested on finding the i . e ., the and reconstruction satisfying the entropy constraint and the average in the channel , defined as the average number of correctly . The average number of correctly sometimes to as average or average reliable throughput is also the the objective function in , , . It is a reasonable figure of merit if one forward error correction by been applied to the data being sent over the , so that , with high probability , lost due to outage do not cause irrecoverable . Otherwise , and if all data is to be correctly , and would have to be sent back over the to request retransmission of lost data . As we shall see from the in Section , at least for fading and for equal and , the optimal entropy constrained are such that only the first cell V for outage . The latter that if sending and is necessary , then it would mean at most V per realization to the . The extra bit rate is upper bounded by V block because only when V it becomes necessary to send an or during block , which at most extra bit every time V . 
 Since in consecutive are i . i .. and ergodic , over a large number of converge to ensemble . Thus , for notation simplicity , in the following we drop the frame and channel . With this , the design problem can be stated as finding the and satisfying and , that maximize the average data rate in the channel without exceeding the entropy constraint in the . More precisely , combining , , and , we state the optimization problem in canonical form as minimize : 
 a 
 subject to : b 
 c 
 with and , and where is the cumulative distribution function of . 
 This optimization problem is difficult to solve primarily because the entropy constraint c is non convex . As we shall see , this to the existence of several local , which , in principle , one to run an optimization program several times with a potentially large number of different starting . In the following we solve this optimization problem , first explicitly for the case , and then numerically by of optimization and a novel procedure which greatly the overall complexity of the task . 
 PROBLEM SOLUTION FOR TWO 
 We now address the optimization problem stated in for the case , corresponding to two quantization . In this case , to : minimize : , i 
 subject to : 
 c d 
 e 
 This problem can be explicitly without by that the entropy associated with the two only on the threshold . Supposing is given , the optimal value of u is found by the objective respect to it and to zero : 
 J 
  
 u 
 u 
 u u u 
  
 We see from this equation that for every u there a unique u for which u . On the other hand , in order to determine the optimal value of u given , we notice that this value to minimize the term u u in a . Although , in general , such value cannot be found explicitly , the following lemma that it is unique and that it can easily be found numerically : Lemma : a satisfying Assumption . Define u 
 Then , for any , the function 
  
 is convex for all u 
 The proof of this lemma , which will play a key role later in Section , can be found in the Appendix , at the end of this document . 
 It turns out that the value of u which u , u u with . More precisely , define the function U 
 . Then , under Assumption , it 
 that 
 . 
 To prove this claim , suppose that . 
 This that 
  
 for any , with the inequality being a consequence of the fact that is non decreasing . from Lemma that u , is convex in u , we conclude that becomes zero at a single value of u greater than or equal to . This that for all , . 
 The latter result that the optimal value for u must belong to the interval ,, where . 
 Also , by Lemma with , it is readily found that the unique value of u a also . The convexity by Lemma that the latter function can be easily numerically by line search . Moreover , Lemma that u u , which to the conclusion that the optimal value of u given is 
 u , . 
 It then that , if u is part of an optimal for some entropy constraint , then the optimal can be explicitly from u by , and then the optimal u can be directly derived from . In this manner , by increasing u from to and and u with the latter , one a family of the optimal for every value of . 
 Fig . top the curve of capacity versus entropy by the method in the 
 paragraph , for two quantization , under an average , left and , right . More precisely , for every , the corresponding was calculated . Then , for this value of , the optimal u was determined . In this way , for each value of u , a different was . The pair capacity , output entropy associated with each a single point in the top two in Fig . . In this example , channel gain distribute , so exponential , chosen to yield , i . e ., u e u , u , . Notice that , for every value of , there exist two to and , corresponding to different local constrained . There is one associated with each of these two . One of these the upper section of the curve in each of the shown in top , and the other one the lower section . Of course , the optimal are those responsible for the upper part of their respective , i . e ., those which yield the maximum capacity for a given entropy constraint . As , in general , capacity when the entropy of the available to the transmitter entropy is . 
  
  
 Fig . . to and two quantization for fading with unit mean channel power gain . Top : under an average , left and , right . Bottom : and code for the optimal solution as a function of entropy under an average , left and , right . 
 Note that the maximum capacity for , . , at an entropy of . block , not with maximum entropy , which , for a two cell is bit block . This is expectable for a with a fixed number of , since there is no reason why all being equally likely the only situation in which the entropy is should yield the capacity . Also , the maximum for a two cell at and coincide with what was in , where average capacity was without an entropy constraint on the output . At an average of , Fig . top right that the maximum capacity closer to maximum entropy , and that , corresponding to this maximum entropy , are near optimal . Interestingly , the maximum capacity curve at this a at an entropy of about . block where the curve itself in Fig . top right , which it non concave . For this case , this that better performance for below block can be by doing time between two : one with a single cell and zero entropy , and another with two and an entropy of about . block . By choosing one regime more frequently than the other , it is possible to achieve a capacity and an entropy equal , respectively , to the weighted of the and of both . In this manner , all capacity entropy within the convex hull of the can be . 
 Fig . bottom the evolution of and for the optimal solution , as the entropy of the output . For , on the left , u at all . We see also that higher are by and code closer together . For the case , shown in Fig . bottom right , the in with a change in the arrangement of and code in the . Except in a neighborhood to the right of this , we find that the u with its left boundary threshold . 
 PROBLEM SOLUTION FOR MORE THAN TWO QUANTIZATION 
 A . 
 The straightforward approach in the previous section cannot be extended directly to the case in which there are more than two quantization . optimization can be instead , which , as we shall see , to a system of non linear that must be numerically . The non convexity of the optimization problem and the existence of several local satisfying the the need to solve this system of non linear possibly many times with different initial . However , we will introduce an algorithm , in the same spirit as the strategy in the previous section , that one to simplify the optimization problem to a sequence of simple line search , each with a single solution . 
 Before the associated with , we note from a that , at the optimum , the inequality stated in e are not active . Similarly , constraint d for the case i is not active since increasing u above would raise the average data rate without increasing the entropy of the output . 
 Taking the above into account , the associated with the following form 
  
 with , , and where are 
 . with respect to u and to zero 
  
 a 
 Notice that a is identical to , which that u is a solution to a only if 
 u b 
 L with respect to the other and to zero 
  
 c , N 
 Hence , we obtain non linear , which must hold simultaneously . 
 On the other hand , the with respect to the we obtain 
  
 to be satisfied simultaneously . 
 Finally , the Tucker , provide another set of , 
  
 plus the requirement that 
 h 
 , ,...,, i 
 all to be satisfied simultaneously . 
 Although it is possible to solve this system of N non linear by standard numerical , the existence of numerous local minima one to apply these repeatedly , each time with different initial . This shortcoming is by the fact that the vector of initial plus threshold and code point in a N dimensional space , which a large number of initial is to obtain a reasonably good coverage of the search space . In the following section we will show how these can be by an approach similar to the one in Section . More precisely , we derive a method which , for this problem , one to find all local minima in a systematic , sequential manner , greatly reducing the number of . 
 B . An Efficient Algorithm for More than Two 
 In this section we exploit the recursive nature of to reduce its induced system of non linear into a sequence of line search over bounded , in a spirit similar to the one behind the approach in Section . 
 To begin with , recall that the imply that , for every constrained local minimizer of a , the multiplier only if i . e ., only if the associated constraint is active . The next corollary of Lemma an verify condition for to hold in such a minimizer : 
 Corollary : Let be a satisfying Assumption . 
 Suppose are a solution to optimization problem . Then for some ,..., if and only if 
 Proof : The result directly from Lemma , since it the convexity of the function u u , and from a , upon that the entropy of the output does not depend on the choice of . 
 Corollary will allow us to find and from , , and in a simple manner . For this purpose , define , for ,..., where 
 c 
 is the complementary . With these , the combined c , d , g and i can be written in the following equivalent form 
 , , ,..., a , ,...,. b 
 j 
 Fig . . of , and defined in , as of . Left : a case in which . Right : a case in which 
 . 
 Figure a qualitative description of , and as of . It can be seen that both are monotonically increasing , the first one being affine , the second one convex . 
 A look at Fig . immediately that , for any given , and depending on the of the , and , there will be , in general , more than one pair of of and satisfying . Let us find out which actually by first considering the under which the constraint is active or inactive : 
 • Inactive Constraint : In this case which see b . In view of , this is equivalent to 
 , . 
 The first equality of is satisfied by a unique value of the argument of , say , shown in Fig . . 
 From b this value is given explicitly by 
 . 
 From the definition of , this quantity must be . On the other hand , Corollary that if and only if 
 , 
 situation and in Fig . right . In addition , Lemma that , if this inequality is satisfied , then there a unique satisfying the second equality of . Thus , there a solution to for which the constraint is inactive if and only if and . In this case , we say solution for . 
 • Active Constraint : In this case , , and can be positive . By looking at , a solution satisfying this condition if and only if there is for which 
 a 
 , , 
 which correspond to of the of , and on the first quadrant in Fig . . Since , with respect to , the function , is affine and increasing , and the function is convex and monotonically increasing , it that is satisfied for either none , one or two of . Indeed , these imply that two to equality a in will exist if and only if 
 , 
 where 
  
 is the unique value which , , see Fig . . It is also easy to show that if , these two , say and , will lie at opposite sides of . Also , it is straightforward to verify that both are not than . Therefore , 
 . to our original 
 problem , each solution to equality a in will also be a solution to if and only if it is non negative and it a non negative value for . This one to discard solution or if 
 . On the converse , if these two do not hold , solution will be non negative and yield 
 , i . e ., it will be a valid solution to . In this case , it is easy to verify that 
  
 a situation which is in Fig . right . 
 Making use of all these all these , and 
 , with i , , , be the threshold value associated with , we can devise the following procedure to find the to for a single , given : 
 Procedure : Suppose , and that , and are given , with . Then , in order to find the to for , 
 Calculate and explicitly , , and , respectively . 
 If 
 a Find by equality a in with respect to by line search over , 
 set , in which case solution 
 . 
 b If and and 
 , find equality a in by line search over unless , in which case . Set , and 
 . In this case , solution . 
 If , find 
 .. by line search over and set 
 . In this case , solution . 
 in which . 
 The last step , where yielding are whenever , to the fact that is a complementary value . This requirement is only if one is calculating the last threshold i . e ., when , to allow a higher level routine to iteratively adjust so that . Such a routine is by Procedure below . Of course , unless , solution can be before doing the corresponding line search if . The same for solution if . 
 In each of the line in Procedure , there a single solution over the corresponding search interval , since , in all , the involved function is monotonic within it . This each step straightforward to execute . Notice also that for every , and depending on the of , and , there are between zero and three of for , , namely , and , which satisfy for that . 
 The above procedure can be applied sequentially to find all , , for , ,...,, that satisfy . More precisely , one can first choose for u , and , with which one can calculate explicitly from a . Then , setting , Procedure can be carried out to find at most three of for u , satisfying for . Each solution can be considered a branch in a tree structure . one and the procedure for each branch , until , the complete tree of valid , each of which is associated with a path in the tree . Thus , for each choice of u and , there can be at most N different , each associated with a sequence of and satisfying for all , ,...,. However , we shall see in the following that the number of valid in practice is much smaller than N . 
 Following our notation for adopted in Procedure , we label each solution path in the tree the of the solution associated with its , thus to a sequence . For a given choice 
 of u and , we define the set of valid solution as 
 O u , 
 is a valid solution path for 
 It is important to note that there is a one to one relationship between every solution satisfying for a given choice of u , and every path in O u ,. Indeed , the path associated with any such solution can be easily determined by each of its code point threshold the the reasoning that led to Procedure . 
 Now , suppose one to know whether a given path is associated with a valid solution to and then find this solution . Instead of Procedure to find the entire tree of valid and then if is one of them , one can apply the following algorithm , derived directly from Procedure , for this purpose : 
 Procedure : Let u ,, and the corresponding be given by a . Let be a path . 
 Then the following can be taken to determine whether and find its associated and code 
 : 
 Set 
 Calculate explicitly , , and , respectively . 
 If : 
 a If and : Set 
 and calculate 
 by line search over . 
 b Else , declare and quit . 
 Else if : 
 If 
 and : Calculate equality a in by line search over . 
 If and 
 solution . Else , declare 
 O , u and quit . 
 Else , declare and quit . 
 Else if 
 If : Calculate equality a in by line search over . 
 If and , output solution . Else , declare and quit . 
 Else , declare and quit . 
 If , declare and quit . Else , set and go to step . 
 With the above procedure , the resulting of all and are a function of . For convenience , we denote this function by , defined 
 as 
  
 † from Procedure , if 
 , otherwise 
 where we have chosen † as a special symbol to indicate that the path does not yield a valid solution . For future use , we also define 
 from 
 Although being a solution to is a necessary condition for code and to be a solution to , two additional must be satisfied for sufficiency . The first one is the entropy constraint , expressed in the equation f . The second is the construction condition or , equivalently , . For a given path , u and , this condition can be expressed as . 
 Therefore , 
  
  
 is the set of all , for the first u and multiplier associated with to for some entropy constraint , the set of all 
 such . Finally , by and 
 , respectively , as the entropy and capacity associated with , the original optimization problem can be stated as 
  
 Thus , we have reduced the problem from a set of N non linear , over a N dimensional space , into a moderate number of one per valid path , over two , the evaluation of a scalar function to compute . 
 The following is a procedure that can be to solve 
 Procedure : 
 Define a grid of of u over ,, say U , and a grid of for say , for some . 
 For each u in U , 
 For each in , run Procedure to find all valid . 
 Detect of consecutive of between which a sign change of for some valid path . In each of the formed by such , find the value of for which by line search , running Procedure for the corresponding path . 
 Calculate and for each of the found in the previous step . Select the combination that the capacity with an entropy not greater than . 
 In the following section we present an example in which Procedure was to find the solution to for . 
 EXAMPLE 
 In this section we apply Procedure to find the set see , for the case , i . e ., for four , assuming is fading . The latter set the , for u and that characterize a solution to , i . e ., and code that yield a local maximum or minimum capacity for some fixed maximum entropy constraint . The are in Fig . , for , on the left , and for , on the right . 
 The two top graphics in this figure correspond to for all to . It can be seen that , although for each there exist several such , for each entropy constraint the number of these is significantly smaller than N . For the case , the absolute maximum capacity is . , with an entropy of . block , by a solution with associated path . The curve that this maximum is ended at that point and , to the eye , it as if there was a missing segment which would connect it to the curve that the right boundary of the plot , at an entropy of block . The absence of this segment can be to the fact that in the formulation of the problem , the entropy is an inequality and not an equality constraint . 
 The solution yielding the maximum capacity for a given entropy is given by the highest curve in each of the . Interestingly , for somewhat below log and log block , the curve with the optimal solution with and quantization , respectively . This can be seen by from the bottom in Fig . that a few block below these entropy , the optimal solution is such that one , two or three , respectively , tend to infinity , effectively leaving three , two and then one cell , as the entropy is . Such behaviour that for a finite entropy constraint , the maximum capacity over all is with a a finite number of . 
 As already for the same and two , the region of achievable average is given by the convex hull formed by all the in the . Unlike what is for , since the composite maximum capacity curve for Fig . top right is not concave , the maximum capacity for some entropy constraint between two . 
 A . Comparison with Fixed Rate , Constrained Quantization 
 The point of maximum capacity for each number of quantization to the solution when a with that number of is for maximum average throughput without an entropy constraint . Therefore , those peak are the found in for single layer , where the was under a constraint only . From this fact , we can conclude from Fig . top left that at , and for the same average rate of an optimal fixed rate from with two that is , bit per realization , which . , an optimal entropy with quantization approximately . . This an increase of roughly in average throughput for the same average rate . The corresponding increase with respect to a fixed rate with goes from . at a fixed 
  
  
 Fig . . entropy for the to , for fading with unit mean channel power gain , up to quantization . Left : Average . Right : Average . 
 rate of log . realization to approximately 
 . an optimal entropy with . For an of , Fig . top right that an optimal entropy constrained smaller gains over fixed rate optimal . Notice also that can only operate at a limited set of given by log realization , Thus , entropy quantization is the only scheme which one to send other average for example , below bit realization . 
  
 We have a numerical procedure to find the maximum average capacity over block fading , under a fixed per block power constraint , when the receiver perfect and an error less , delay free , channel is available to convey to the transmitter . This procedure , which a smaller numerical complexity than trying to directly solve the associated with the problem , also the and that achieve the optimal solution . Our are valid for a broad class of channel fade , and fading . We have applied the procedure here to find optimal and quantization . The revealed that , for a given number of quantization , say , maximum capacity is at an entropy slightly below log block . Furthermore , our show that for any entropy below log block , there is little to be in average capacity by more or . This that for any finite entropy , the optimal a finite number of quantization . Our analysis also revealed that for high average , the maximum average capacity time between two with different and associated . 
 As a final remark , we would like to mention that , after several , the have found that the and here for a short term power constraint do not seem to be applicable for the long term power constrained version of the problem . Indeed , the latter problem to be significantly harder to solve than both the one in this paper and the long term problem without the entropy constraint . 
 APPENDIX 
 Efficient Channel for the Output of an 
 Here we provide an example to illustrate how a discrete random source with small entropy which can be associated with the variable length coming out of an entropy coder can be efficiently a channel coder . The latter coder is able to transmit the low entropy source less power and with a smaller message error probability than what is when modulation and maximum likelihood . For this purpose , suppose we have two , each with four quantization . The first is not entropy , and each of its , two , equal probability . For simplicity , suppose that this is by a rate error correcting channel coder and that is . Assume each symbol in the constellation unit energy and that there is a memoryless channel in the , with complex circularly symmetric white noise with variance . Therefore , each outcome or message is into of length two that is , two consecutive are sent for each message , which an average energy of . At the other end , the is done by by the most likely symbol sequence given the received signal . For this scheme , it can be found either analytically or via that at an of , the message error probability is approximately . . 
 For the entropy , suppose that its four possible , say m , m , m , m , have , , and , respectively . An entropy coder for this for example , a coder would output , , and , respectively , for each of its . In order to send these four or over the feedback channel , consider a time digital modulator generating from the shown in Fig top . Each of these except symbol o unit energy . 
 An outcome from the entropy is fed back to the transmitter by sending a sequence of three channel , each coming from the first , second and third constellation , as in the table of Fig . Notice that with higher probability are to symbol with smaller total energy , the latter being proportional to the length of the by a entropy coder i . e ., proportional to log , where is the probability of the i th message or outcome . This to the sequence length or energy distribution that average energy , the latter energy being in this case equal to . Therefore , the average energy by the channel scheme for the entropy is just of the mean energy associated with the fixed rate . On top of this power reduction gain , if this variable rate scheme to the entropy is combined with a maximum likelihood sequence at an of , then each message is with a message error probability not greater than . . This that if the of the have uneven which to a smaller entropy than with probable , then it is possible to transmit the less power and with a smaller probability of error than when the of a fixed rate . 
 Proof of Lemma 
 Proof : We have that 
  
 again , 
  
  
 where we a short hand notation for . For every u such that u , it readily from the structure of see that u and therefore immediately da u . The same for u . Thus , it is only left to consider of u and such that u . If da u , then , from , 
 . 
 Since we are only considering the in which u , u . This one to 
 substitute into , 
 Thus , for every u at which , the following 
 implication 
  
 On the other hand , if da u , then 
 . 
 Since we are considering of u such that u , we can substitute into , 
  
 It then that also if da u . We therefore conclude that , irrespective of the sign of da u , 
  
 Now , from the definition of , we have that 
  
  
 Fig . . Top : Three consecutive of a digital modulator for the of an entropy with quantization . a ,,, have unit energy . Bottom : A between quantization m to m and channel symbol . The 
 error refer to message , and the associated is . 
 On the other hand , 
  
  
 Therefore , 
  
  
 Since is a non increasing function , it that the of is positive . Therefore , we obtain from that a u is also convex for all u , such that u . This the proof . 
  
  
 ﻿ In , Theorem . it is that , for a one sided random source , the 
 search for the non anticipative i . e ., causal rate distortion function can be restricted to y which are jointly stationary with x . In this technical report we show that the proof of , Theorem . is invalid because it on , Theorem . , the proof of which , as we also show , 
 The manuscript , Theorem to prove the claim that , for one sided x , the non anticipative i . e ., causal rate distortion function can be by a reconstruction process y which is jointly stationary with x . To do so , it on , Theorem . . 
 In this note we argue that the proof of , Theorem . , and hence that of , Theorem . , are flawed . For that purpose , we will first recall the and in . After that , we will present the in and show , under the stated there , the by , Theorem the basis of , Theorem . of are not met . 
 Throughout , the search in the associated with various of i . e ., causal rate distortion is stated over of joint probability between source and reconstruction as opposed to the usual , in which the search is over conditional , see , Chapter , . Since the distribution of the source is given , it is that for every k k , all the joint to be considered yield the 
 same given distribution of the source for the corresponding block , say . This requirement can be as that , for a set of admissible joint , k 
 where and are , respectively , the to which and belong , and is a 
 s algebra over . In , this admissibility requirement is in the definition of the of which meet the distortion constraint , next . 
 The fidelity criterion for every pair of k k is expressed in as to belong to a non empty set of hereafter to as distortion feasible set , k , a condition written as . In this definition , the number an 
 admissible distortion level . Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation . 
 As above , the admissibility requirement is expressed in the in . . The latter equation can be written as 
 In . and . , the distortion feasible are assumed to satisfy the concatenation condition 
 With this , . defined the epsilon entropy of the set of 
 are satisfied . Then . the message generation rate as 
 The analysis in considered both and continuous time , but here we only refer to the discrete time 
 The actual term employed in is epsilon entropy of the message where the term message 
 An alternative message generation rate is also considered in by the set of distortion admissible process as : 
 Definition . The set of all two sided random process for which there exist k k such that and 
 when the limit , where the is taken over all of satisfying the causality 
 The proof of , Theorem . on the claim stated in , Theorem . , namely , that an equality similar to 
 We demonstrate that the proof of , Theorem . is not valid and hence that of , Theorem . 
 is flawed . We do this by showing next that , Theorem . two , namely : a one of 
 the causal considered in it does not coincide with , and the proof of , Theorem . 
 The already first problem of , Theorem . as a basis for , Theorem . 
 na as from the fact that its alternative causal function R 
 where as defined in the text just below equation . in Q , is the set of conditional of y given x such that the causality 
 e although this equality is not explicitly Thanks to , it readily that R 
 Since the only causal defined in as an lim is , one must conclude that Re as equivalent to D . However , in view of Definition and , such equivalence is not valid 
 since the distortion feasible of Definition are not compatible with the distortion constraint . 
 it does not mean that . As a consequence , one of the necessary for , Theorem is not shown to hold . 
 The second issue with , Theorem . is the validity of its proof . To begin with , the only 
 argument used in it is that the source is stationary and , Theorem . However , the latter theorem 
 only that , and thus the proof of , Theorem . there is flawed . 
 Although not to in that proof , the reverse inequality in , Lemma . would be all that is to show that . However , the proof of , Lemma . , 
 and then that the claim by taking the over Q , . The problem with this reasoning is that does not follow from . A rigorous reasoning that when taking the limit as 
 Thus , one cannot choose to the of this inequality over Q , and expect the in 
 equality to hold , since one can easily find a pair of whose conditional distribution 
 Unfortunately , the latter is not true since , as already , Q , of random 
 such that E , for all thus reaching the limit distortion from above , and thus such that , for all . Therefore , does not hold . 
 leading to an inequality in the same direction as the one provided by , Theorem , i . e ., that 
  
 ﻿In order that signals can be stored, transmitted or processed it is necessary that they first be converted into digital form. This, in turn, raises the problem of how to digitize data so as to achieve the best trade-off between data load and performance, i.e., “how to make the most out of a little”. Two issues are involved in this problem, namely temporal quantization (i.e., sampling) and spatial quantization. These two problems have traditionally been addressed separately. Indeed, there exists substantial literature dealing with the temporal quantization problem, covering both band-limited and non-band-limited signals. The usual underlying paradigm is that of an analysis filter, followed by a sampler, followed by a reconstruction filter. Various parts of this architecture can be optimized once other parts have been specified. On the other hand, spatial quantization has been studied extensively for a given sampling strategy, particularly in the framework of sigma delta conversion. Finally, it is also possible to formulate the joint design problem for sampling and spatial quantization. This typically leads to enhanced performance compared to that achievable by considering the two aspects separately. 
 This paper will survey the general area of sampling and quantization and analyze methods for achieving efficient data representations for signal processing and control applications. We will show how, on the one hand, contemporary control theory can contribute to the design of sampling and quantization systems and, on the other hand, how these systems impact on the performance of modern feedback control systems. 
 Key Words: Sampling, quantization, frames, model predictive control, constrained control, networked control systems 
 We live in a data rich world. Most technological systems operate by first convertingcontinuoustime, continuousamplitude signals from the analog world into digital representations. This is a necessary precursor to allow signals to be stored, transmitted and processed without degradation other than that introduced by the analog-to-digital conversion itself. 
 The above was indeed the motivation that led Alec Reeves to invent pulse-code modulation (PCM) seven decades ago [1]. In his 1938 patent [2], Reeves highlighted the main benefits of PCM, namely: 
 These are remarkable statements for the time they were formulated. Indeed, most of these benefits have only become reality in recent times. Furthermore, the validity of the first two claims began to be formally determined years after they were formulated, and is still subject of ongoing research. In the pursuit of better quality at lower bit-rates (and lower costs), increasingly parsimonious methods are continually developed so as to acquire, process and represent signals digitally. 
 This topic has also motivated important theoretical results, from areas such as information theory, functional analysis, optimization, communication theory, frames, wavelet theory, etc.. As we will discuss in this paper, also control theory has much to contribute to this circle of ideas. Conversely, much of the theory and techniques from digital signal processing are highly relevant to several aspects of control, e.g., networked control, where parsimonious signal representation is a key element, see, e.g., [3][4][5]. 
 In the present work we present some of the main strategies of sampling, quantization and reconstruction of analog, continuous-time signals. We will describe reconstruction quality and relate it to design constraints such as filter complexity, data-rate and sampling frequency. We also present some ideas concerning the joint problem of sampling-quantization, on one side, and reconstruction on the other. We limit our analysis to uniform sampling of scalar signals, sampling and reconstruction by single filters (as opposed to filter-banks), quantizers with scalar output and we will not discuss any issues related to further symbol encoding. 
 The layout of the remainder of the paper is as follows: Section 2 presents the basics of PCM quantization and discusses some of the shortcomings that justify the introduction of a more general model for a samplingquantization-reconstruction system. Section 3 poses the sampling and reconstruction processes in a frame theoretic perspective. Section 4 is a review of some recent generalized results on the sampling and reconstruction problem. In Section 5 we present some basic aspects of scalar memory-less quantization and oversampling. Section 6 describes feedback quantizers. In particular, some of the basic principles of predictive and noise shaping (S?) analog-to-digital converters are presented. In Section 7 we present noise shaping quantizers that generalize S? converters based on model predictive control. Section 8 gives elements to analyze the joint problem of the quantization and sampling-reconstruction design, including some recent results and insights. In Section 9 we show how concepts related to sampling and quantization can be utilized in control problems. Section 10 draws conclusions. Finally, an Appendix is included with some of the basic concepts of frame theory necessary to understand several of the results presented in the main body of the paper. 
 In this section we will first describe PCM as a basic architecture used in AD–conversion applications. Various shortcomings of PCM will then motivate us to introduce later a more general framework. 
 We consider the (simple) and idealized PCM system represented by the block diagram in Fig. 1. 
 The usual paradigm associated with this setup is that the input signal a(t), t ? R, is taken to be band-limited to some frequency, say, fmax [Hz]. Then, in accordance with the Shannon-Whittaker sampling theorem [6], the sampling step is chosen as t = 1/(2fmax). Since the input signal is directly sampled, we have c[k] = a(tk), ?k ? Z. The nearest neighbour scalar quantizer in Fig. 1 corresponds to the non-linear transfer function Q?(·), defined by 
 where ?> 0 is the quantization step (see Fig. 2) and  denotes rounding to the closest integer value greater than a/?. 
 Thus, the output of Q? in Fig. 2 is the sequence of quantized values {u[k]}k?Z, where 
 The synthesis filter R in Fig. 1 is, in the simplest case, an ideal continuous time low-pass filter with cut-off frequency fmax = 1/(2t) [Hz] and impulse response sinc(2fmaxt), t ? 
 R, where sinc . The output of R is the analog, continuous time signal ˜a, given by the mixed convolution 
 If there were no quantization (i.e., if ?=0), then u[k] would equal c[k] for all k. In this situation, ˜a(t) in (3) would equal exactly the input a(t) for all t ? R, since, by virtue of the Shannon-Whittaker sampling theorem [6], if a(t) is bandlimited to fmax, it can be reconstructed from samples by the interpolation formula 
 In the presence of quantization, it is generally no longer true that ˜a = a. Nevertheless, it is reasonable to expect that, if the quantization step is small, then the quantized samples {u[k]}k?Z will be close to the analog samples {a(kt)}k?Z for all k, and the output of the simple PCM system of Fig. 1 will be close (in some sense) to the analog input a. Unfortunately, this and other assumptions in the above model are often far from realistic, as discussed next. 
 Whilst the PCM method described above is certainly attractive, it suffers from several shortcomings that hinder its usefulness in many practical situations. In what follows, we will describe some of the main deficiencies of this architecture. 
 Synthesis Filter The ideal low-pass filter used in Fig. 1 for synthesis cannot be implemented in practice. Firstly, it is non-causal. A very close approximation of the ideal low-pass filter would still be non-causal, which rules it out from any delay sensitive application. 
 Secondly, an ideal low-pass filter has an infinite impulse response length. For practical low pass filters, the closer they mimic the ideal filter, the longer the impulse response will be. One problem with a long, slow decaying impulse response is that it affects the stability of the reconstruction, in the sense that bounded errors in the samples are able to produce unbounded point-wise error in the reconstructed output. As an example, consider the ideal low-pass reconstruction in (4). It is easy to show that any bounded periodical error in the samples a(kt) of the form , with |?| > 0, will yield an unbounded reconstruction error in the L8 norm. The second difficulty with a synthesis filter with long (but finite) impulse response is cost and complexity: In applications where synthesis is accomplished via discrete-time FIR filters, longer impulse responses require higher computational complexity. 
 Another problem with the ideal-low pass synthesis filter model is that, in many practical applications, the synthesis filter is not a design choice, but is prescribed by other considerations. In such cases, the synthesis filter can have almost any frequency response. An important example of this situation is that of sampled-datacontrolsystems, where the plant itself can be thought of as comprising part of the synthesis filter R in Fig. 1. We will return to this situation later in Section 9. 
 Not Necessarily Band-Limited Input Signals The assumption of band-limitedness of the input signal a is also very restrictive. Most real applications have to deal with signals over a finite time interval (strictly speaking, any non-zero finite duration signal is not band-limited [7]). Even when processing a virtually infinite duration, perfectly band-limited signal, only a finite number of samples can be used for the reconstruction. This introduces truncation errors [8], i.e., part of the inter-sample behaviourof the input signal is not captured by the samples. On the other hand, it is often the case that the sampling rate cannot be made high enough to completely avoid aliasing. Whilst this is commonly dealt with by using a low-pass anti-aliasing filter before sampling, this paradigm may have significant shortcomings whenever the signal carries relevant information in the high frequency part of its spectrum, or when the reconstructionfilter is not perfectly band-limiting (see, e.g., [9, 10]). In this case other types of analysis filters should be considered. 
 Availability of the Input Signal Before being able to sample the value of any physical variable, it is necessary to convert it to an electrical signal by means of a transducer, which in itself is a dynamical system. It is often the case that sampling is performed in the transducer itself. In this case, one does not have access to the underlying continuous time signal, but only to the samples taken. Depending on the situation, this can deprive further stages of knowledge of important inter-sample behaviour of the physical variable. It is then necessary to make a wise design of the synthesis stage, so that the input signal can be well approximated at the output (see, e.g., [11, 12, 13]). 
 Quantization, Sampling Frequency and Data-Rate In the simple PCM system of Fig. 1, quantization is done element-wise by a nearest neighbour quantizer, see (2). Thus, if one wishes to obtain a small reconstruction error, one would naturally aim at reducing the quantization step. In practice, however, the reduction of ? is limited by cost and structural constraints. Alternatively, if the statistics of the input signal are known, then the mean square reconstruction error can often be reduced by using a quantizer in which the quantization step is not uniform along its dynamic range. 
 Moreover, even though the Shannon-Whittaker sampling theorem shows that when the samples are un-quantized an increase of the sampling frequency cannot improve reconstruction (since it is already perfect), the situation with quantized coefficients is different. More precisely, when quantization is introduced, sampling abovethe Nyquist rate (oversampling) can be utilized to reduce quantization error (see Sec. 5.2) . Thus, one often has the chance to compensate the effects of coarse magnitude quantization by means of a finer time quantization, i.e., faster sampling rate. (The reader may be well aware of this in 1 bit DAC’s used in some CD players.) 
 In practice, the productof the sampling rate and the number of quantization levels is often constrained by data-rate limitations. This is so because, although not explicitly shown in Fig. 1, the sequence of quantized values {u[k]}k?Z, in binary form, has to be stored or transmitted before reconstruction takes place at another location in time and space. This means that the total number of bits, or similarly, the data-rate, is limited. In principle, if the quantizer has nU ? N levels, then the data-rate will be approximately given by 
 It is possible, however, to reduce the data-rate by an efficient encoding of the sequence of quantized values (compression). When such encoding is applied, the data-rate limitation translates into an information-rate limitation, precisely given by the entropy of the sequence of symbols at the output of the quantizer [14]. Systems with entropy coding are also called variable-rate encoders. In this paper, however, we will not consider such coding methods. Thus, we will only consider fixed-rate encoding, and the data-rate will be given by (5). 
 In view of the limitations of PCM conversion discussed above, a more general model for the analysis of sampling, quantization and reconstruction systems is presented in Fig. 3. 
 Weighting Filter Figure 3: A more general sampling, quantization and reconstruction system. 
 For the remainder of this work, we will restrict our analysis to input signals a which are modeled as finite energy scalar functions of a single parameter t (i.e., a ? L2(R)). For example, we could think of t as denoting time, for the case of time varying scalar signals. Thus, the analysis filter S in Fig. 3 accounts for all the continuous-time linear processing of the input that occurs before the sampling takes place. The sampling process is assumed uniform (i.e., regular sampling), with fixed sampling interval t. 
 The synthesis filter R in Fig. 3 represents the linear processing in continuous-time (possibly with some discrete-time pre-filtering) applied to the quantized samples {u[k]}k?Z. The output of R is denoted as ˜a. It approximates a in some well defined sense. 
 The quantizer QU in Fig. 3 is labeled generalized because it is allowed to have access to previous and future input samples during operation, and scalar, because it generates a sequence of scalars, one at a time. We will only discuss quantizers of this type in the remainder of this paper, which justifies a moreprecise definition of the class of generalized scalar quantizers: 
 Definition 1 (Generalized Scalar Quantizers). Any quantization strategy that can be devised within the following conditions 
 •	The quantizer has no access to the continuous time signal a, but only to the samples {c[k]}. 
 •	The quantizer outputs a sequence of scalars {u[k]} at a constant rate, one element every t units of time. The total elements in the output sequence equals the number of input analog samples. 
 •	Each of the elements in the output sequence of the quantizer can take values only from a finite, given and fixed set of scalars U, i.e, the output of the quantizer satisfies 
 Note that this definition allows for the uniform, nearest neighbour scalar quantizer in (2) as a special case. The last condition in Definition 1 means that the generalized scalar quantizer in Fig. 3 is allowed, in principle, to determine the output u , for any , based upon knowledge of the entire sequence {c[k]}k?Z, i.e., it is a dynamic system. Therefore scalar quantizers with memory (such as the predictive and noise shaping quantizers to be discussed in Section 6) are special realizations of the generalized scalar quantizer  . 
 In Fig. 3 an error frequency weighting filter H has been added. Inclusion of this filter reflects the fact that, depending on the application, the practical impact (or cost) of the reconstruction error is frequency dependent. Accordingly, H filters the instantaneous error a(t)- a˜(t) to produce a frequency weighted error signal eH(t). Based on the general setup illustrated in Fig. 3, throughout the remainder of this work the performance of the system will be assessed in terms of the squared L2 norm of the generated signal eH: 
 As mentioned above, a paradigm which underlies many signal processing schemes consists of a pre-filtering (or analysis) stage, a sampling stage, a digital, discrete-time processing stage and a post-filtering (also referred to as synthesis or reconstruction) stage. It has been shown that these processes are equivalent to a sequence of mappings between Hilbert spaces (see, for example, [18, 19, 20] and [9]). This viewpoint allows one to use the powerful tools of Hilbert spaces, frames and algebraof operatorsto study and design sampling and reconstruction systems. It allows for elegant solutions to otherwise complex design optimization problems, by using innerproductsand projection operators. 
 To the best of our knowledge, the first author to apply Hilbert spaces theory to the sampling problem was F. Beutler in 1961. In [21] he derived sampling theorems for random stationary processes using complex exponential Fourier expansions. Further insight and results for bandlimited signals were provided by K. Yao in 1967 for other expansions, see [22]. Several publications with the Hilbert space approach to the sampling problem followed in subsequent years. Among others, a 1986 paper by Hidemitsu Ogawa [23] presented a unified approach to generalized sampling theorems. It introduced the idea of regarding the approximation of signals in a, more general, finite dimensional reconstruction space, instead of restricting to perfect reconstruction by Fourier expansions. Interestingly, in [23], a finite number of samples of a filtered signal was used, as opposed to an infinite number of “raw” samples. By the early nineties, the recently arrived wavelet theory [24, 25] began to stimulate a strong revival of sampling theory (see, for example, [26, 27, 28]), by using the mathematics of basis and frames in Hilbert spaces. This framework allowed for the re-formulation of the sampling and reconstruction problem in more general and practical situations, including, inter alia, sampling and reconstruction from finite samples [23, 29], study of arbitrary input and reconstruction spaces [11, 30, 31], sampling of non-band-limited signals [27, 10], oversampling [32, 33], non-uniform sampling [34, 18], filter-banks [35, 36], and splines and interpolation [37, 38]. 
 tation of the sampling and reconstruction processes in a Hilbert space frame theoretic context. For a more complete formal analysis, see, for example, [18, 9, 34, 28, 20, 39] . 
 It will be shown next that the analysis and sampling stages, which map continuous time signals into discrete time sequences, can be seen as the analysis operator of the sampling frame. This frame is made of translates of the time reversed impulse response of the analysis filter S. Similarly, the reconstruction process, which maps discrete time sequences into continuous time signals, can be seen as the synthesis operator of a reconstruction frame. It is made of translates of the impulse response of the reconstruction filter R. 
 Filtering and Sampling Consider the input signal a in the block diagram of Fig. 3. Assume that a is known to belong to some space of signals, say A ? L2. Let y be the output of the analysis filter S, which has impulse response ?(t) ? L2. Then, y(t) is given by the convolution: 
 If one now creates a sequence c  by taking the values of y(t) at time instants t = kt, k ? Z (sampling process in fig. 3), one obtains c[k] = y(kt) = (a*?)(kt) 
 where  . One can see that the last integral in (8) corresponds to the conventional inner product in L2, defined in (54), between a(t) and f(t -kt). If we now define the shift operator Tkt by 
 Therefore, the sampled filtered input signal can be seen as the result of a sequence of inner products. From (10) and Definition 6 (see Appendix), this is indeed the process described by the analysis operator F* associated with the frame {Tktf}k?Z. As a consequence: 
 Synthesis (or Reconstruction) Consider now the conversion from the discrete-time sequence {u[k]}k?Z to the continuous-time signal ˜a(t), see Fig. 3. If we denote the impulse response of R as ?(t), then the band-limited Shannon-Whittaker reconstruction scheme in (3) can be generalized to: 
 It is clear from Definition 5 (Appendix) that the reconstruction process (11) can be represented by the synthesis operator ? associated with the frame {Tkt?}k?Z: 
 In this new setting, ?(t) becomes the generating function for the principal shift invariant reconstruction space W  span{Tkt?}k?Z, which is, in general, different from the space of band-limited signals . 
 which it takes a discrete time sequence u and a continuous time function ?, yielding a continuous time function ˜a. If the impulse response ?(t) is chosen such that {Tkt?}k?Z is a Bessel sequence (and therefore a frame for span{Tkt?}k?Z, see (56) ), then ? is a bounded operator, and the output ˜a(t) = ?u ? L2 for all sequences 
 The Combined Sampling and Reconstruction Process It follows from the above that the sampling (analysis) and reconstruction (synthesis) process can be stated as a sequence of operators between Hilbert spaces: 
 Therefore, in the absence of quantization (i.e., if u[k] = c[k], ?k ? Z), the complete process can be expressed as 
 It is interesting to note that the above results allow one to determine the ultimate limitations and capabilities of a sampling and reconstruction system in terms of the Hilbert spaces related to sampling rate and filters. More precisely, the analysis and synthesis filters alone determine, respectively, the largest class of signals that can be sensed (i.e., the sampling space) and the largest class of signals that can be generated (i.e., the reconstruction space). A rather remarkable implication is that in the intermediate (discretetime) stages one can only design the mapping between these spaces, but not expand the sampling and reconstruction spaces themselves. 
 As a consequence, the design of an AD conversion scheme can be thought of as involving two aspects, namely: 
 2.	Design of the mapping between signals in the sampling space and signals in the reconstruction space (i.e., design of discrete-time processing, including quantization). 
 In what follows, we will describe aspects of the separate design of the sampling/reconstruction strategy and of the quantization method. Some aspects of the joint design problem will be discussed later in Section 8. 
 In this section we discuss the effect that analysis and synthesis filters have on the reconstruction quality. We will assume that the input and output spaces are given and will neglect quantization effects. The implicit trade-off here is between the quality of the reconstruction and the computational complexity (and delay) incurred in the sampling and reconstruction processes. 
 As concluded in Section 3, the ultimate sampling and reconstruction capabilities of a system are limited by the sampling and reconstruction spaces. These, in turn, are entirely determined by the choice of analog filters S and R, as well as the sampling interval t. This suggests that, whenever possible, the design of S and R should focus mostly on the sampling and reconstruction spaces that one wishes to obtain. Further refinement can be achieved by careful design of discrete-time filters which can be located right after the analysis filter S and before the synthesis filter R, see Fig. 3. Interestingly enough, it has been shown that, in general, the optimal mapping is obtained by making the sampling and reconstruction frames duals of one another [9, 40]. To achieve this for a given analysis frame , one can insert a discrete-time correction filter before the synthesis filter to make the synthesis frame the dual of the analysis frame. Although, in general, the dual frame of some given frame is not unique, there exists only one shift-invariant dual frame (i.e., a unique correction filter) for each given shift-invariant frame [40]. In what follows, we will consider the following situation: 
 Depending on the relation between the input space A, sampling space S and reconstruction space W , we will consider three types of reconstruction notions, namely: consistent, orthogonal and perfect reconstruction. 
 Consistent Reconstruction The first and most generally attainable reconstruction goal is that of consistent reconstruction, first introduced in 1994 by Unser and Aldroubi, see [11] . A signal approximation is said to be consistent if it yields the same samples (observations) as the original signal when re-injected into the system, i.e. a˜ ? W is a consistent approximation of a ? A if and only if 
 The idea of consistent reconstruction is depicted in Fig. 4.a); in this figure, ˜a is projected onto W along S?, the null space of S, see (49). 
 Figure 4: a) Consistent reconstruction (oblique projection); b) MSE reconstruction (orthogonal projection). 
 The notion of consistent reconstruction was first introduced for Riesz bases in [11], and then extended for frames in [31, 19, 13, 41, 40] and [42]. 
 Orthogonal Reconstruction The second type of reconstruction is orthogonal reconstruction, also called MSE reconstruction. It requires additional conditions (see next section). In this type of reconstruction, the system generates, for any input a ? A, the output ˜a ? W that minimizes 
 It is well known that this notion is equivalent to an orthogonal projection of the signals of A onto the output space W (see Appendix A.1.1). The intuitive notion of orthogonal projection is illustrated in Fig. 4.b). Note that the ˜a shown in this figure is, indeed, the closest point to a in the output space W . 
 Perfect Reconstruction The third, and most demanding notion is that of perfect reconstruction, i.e., 
 As will be shown below, depending on the spaces A, S and W , perfect reconstruction can still be possible, even, for example, for non band-limited signals [43, 10, 27]. In the remainder of this section we will describe conditions on the sampling and reconstruction method which ensure that each of these notions can be achieved. 
 Under the assumption that the sampling and reconstruction spaces satisfy the direct sum condition 
 necessary and sufficient conditions have been found in order to make the sampling and reconstruction system achieve consistent reconstruction and, as particular cases, optimal and perfect reconstruction as well [19, 40, 42]. For shift invariant frames and spaces, the direct sum condition can be conveniently expressed in the frequency (Fourier) domain  based on the functions 
 and the null sets of A? and Af, denoted, respectively, as N (A?) and N (Af), where 
 Proposition 1 ([40, Proposition 4.8]). Let ?,f ? L2(R), and assume that {Tkt?}k?Z and {Tktf}k?Z are frame sequences. Then the following are equivalent: 
 It is shown in [42] that, if the direct sum condition is satisfied, then ˜a ? W is a consistent reconstruction of an input a ?H if and only if ˜a is the oblique projection of a onto W along S?, the null space of S (see Appendix A.1.1). Such a projector, denoted by EWS?, is defined as 
 The following defines the concept of oblique dual frame and establishes its relation with the oblique projector: 
 Lemma 1 (from [40, Lemma 3.1]). Assume that {fk}k?Z and {gk}k?Z are Bessel sequences in H and let S = span{gk}k?Z, W = span{fk}k?Z. Assume that H = W ? S?. Then the following are equivalent: 
 Furthermore, if the above three equivalence conditions are satisfied, then {gk}k?Z is an oblique dual frame of {fk}k?Z on S and {fk}k?Z is an oblique dual frame of {gk}k?Z on 
 From Lemma 1 one can see that ?F* becomes an oblique projector if and only if {fk}k?Z is an oblique dual frame of {?k}k?Z in S. 
 Although, as with the conventional case considered in Definition 7 (see Appendix A.4), the oblique dual frame within a given space is not unique, the shift-invariant oblique dual frame of a shift invariant frame is unique [40]. This means that, once reconstruction and sampling spaces are defined, there exists a unique analysis filter that makes the analysis frame the oblique dual of the reconstruction frame. Conversely, there exists a unique reconstruction filter that turns the reconstruction frame into the dual of the analysis frame. An expression in the Fourier domain for the oblique dual frame condition in terms of the frequency responses of the analysis and reconstruction filters is given in [40], which, by virtue of Proposition 1, can be rewritten as follows: 
 Theorem 1 (from [40, Theorem 4.3]). Let ?,f ? L2(R) and assume that {Tkt?}k?Z and {Tktf}k?Z are frame sequences, spanning the closed spaces W and S, respectively. If L2(R) = W ?S?, then the following holds: 
 •	The result in (18) allows one to obtain a shift invariant oblique dual frame for {Tkt?}k?Z on S for a given f by inserting a continuous or discrete-time correction filter Qf? with transfer function 
 just before or just after the analysis filter. With such an arrangement, and provided Conditions (i) and (ii) in Theorem 1 are satisfied, the system will yield perfect reconstructionfor all inputs a?W and consistent reconstruction for all inputs a ? L2, as required. 
 •	Conversely, from the reciprocity of oblique dual frames, (18) also allows one to obtain the obliquedual frame for {Tktf}k?Z on W for a given ?. This can be achieved by inserting a continuous (or discrete) time correction filter with transfer function Qf?(?) defined in (19). Notice that this correction filter does not alter the space associated to the stage in which it is inserted, i.e., if the impulse response of Qf? is qf?(t), then  . 
 From the previous results it follows that, if the direct sum (17) and duality (18) conditions are met, then necessary and sufficient condition for each type of reconstruction can be stated as follows: 
 Conditions for Perfect Reconstruction Perfect reconstruction only for all inputs a ? W is possible, without any further requirement 
 Conditions for (MSE) Reconstruction (Orthogonal Projection) If, additionally, S = W , then EWS? becomes an orthogonal projector onto W , i.e., EWS? = PW, see Appendix A.1.1. This guarantees that the output signal ˜a will be the best approximation in W for the input signal a ? H , i.e., it will minimize . 
 Conditions for Consistent Reconstruction (Oblique Projection) Consistent reconstruction will be achieved for all a ? L2 without further requirements. 
 Quantization is the process of translating analog values into values which belong to a finite set. The representation of analog samples with infinite accuracy would require an infinite number of bits. Quantization allows one to achieve a controlled approximate representation of infinite analog values, which in turn can be represented with a finite number of bits. Hence, the main purpose of analog to digital conversion is to compress data, whilst aiming to obtain the best possible approximation of the analog signal. This is to be achieved within data-rate constraints and according to some fidelity criterion, i.e., “making most out of a little”. As already mentioned in Section 2.3, the quantizers to be discussed in this paper belong to the family of generalized scalar quantizers, see Definition 1. As such, quantizers generate an output sequence {u[k]}k?Z whose values are constrained to belong to a set of nU elements (see (6)) , the quantization alphabet U, now formally defined as: 
 Traditionally, quantization has been analyzed only in terms of discrete-time performance, usually looking at the MSE between input samples and quantized samples. Denoting the input and outputsequencesof the quantizeras {c[k]}k?Z and {u[k]}k?Z, the MSE is given by : 
 We will next briefly discuss the simplest realization of the generalized scalar quantizer in Fig. 3: the zero-memory scalar quantizer. Its performance will be analyzed in terms of the MSE as defined in (21). Other realizations of the generalized scalar quantizer, such as quantization with memory (by means of feedback) and quantization with memory and “preview”, will be analyzed in Sections 6 and 7, respectively. For a more comprehensive analysis of quantization see, e.g., [45, 16, 17]. 
 Scalar quantization is also referred to as zero-memory quantization, since each analog sample is quantized ignoring previous or future samples. Scalar quantizers partition the real line into a set of nU disjoint and consecutive intervals I = {I1,...,InU}, Ii ? R. A unique scalar from U is associated to each interval in I, usually satisfying µi ? Ii,i = 1,...,nU. Depending on the choice of the partition intervals, either a uniform or a non-uniformscalar quantizer is obtained. 
 Uniform Quantizer The simplest scalar quantizer is the nearest neighbour uniform quantizer introduced in Section 2.1, where the partition of the input space (the real line) is given by (1) and the elements of U satisfy µi+1-µi = ?, i = 1,...,nU-1 
 Defining the positive constants extreme output value M and extreme input value C as 
 the quantizer is said to be overloaded if the input |x| >C. If the probability density function of the analog samples is smooth and the quantization step is small enough, then the quantization error can be approximately modeled as a random variable with uniform distribution over [-?/2,?/2] (see [46] for precise conditions), and the mean squared error between the input x and the output u = QU(x) of the quantizer is given by the distortion measure: 
 In terms of the number of bits utilized to represent each sample, we first note that 
 Non-Uniform Quantizer For a given number of bits per sample, the distortion D can be further reduced if the probability density function (PDF) of the analog samples is known. This can be achieved by utilizing a non uniform quantization step. Any form of non-uniform quantization can be accomplished by placing complementary non linear elements before and after a nearest neighbour quantizer. The first block is a compressor, and its transfer function C(x) is a monotonically increasing function satisfying 
 The complementary block placed after the quantizer is called expander, and has a transfer function C -1. Adapting an expression first derived in [47], one has that, for a non uniform quantizer with a large number of quantization levels, compressor characteristic C(x) and without overload, the MSE due to quantization is given by 
 where fx(x) is the PDF of the analog samples and C  dC/dx. The no overload assumption implies -C = Xmin and Xmax =C, and that fx(x) = 0, ?x ?/ [Xmin,Xmax]. Notice that for C  1 (i.e., with a uniform quantization step), (25) becomes (24). 
 Clearly, minimization of DC in (25) requires a compressor curve C matched to the PDF of the input signal. The optimal compressor characteristic C * is given by the solution to 
 where a is a constant such that C(C) =C. When the solution of (26) is inserted into (25), the MSE without overload and for large B is found to be 
 where s2 and fxN(x) are the variance and the normalized PDF of an individual input analog sample, respectively. In relation to (27), it must also be pointed out that C (see (23) ) must be made several times larger than s for the nooverload assumption and (27) to be valid. For more details about the derivation and applications of this and other results related to scalar quantization, see, e.g., [48] and the references therein. 
 It is possible to further reduce the reconstruction MSE, while keeping the quantization step constant, by increasing the sampling frequency above the Nyquist frequency fN. This technique is called oversampling. For oversampling ratio r  1/(tfN) not too large, the mean square error is reduced as r-1, i.e., 
 where D1 is the MSE when r = 1 [47]. Notice that this can also be seen as a particular case of the resilience properties of redundant frame expansions discussed in Appendix A.5 (see also, e.g., [32, 49]). However, as the sampling frequency is increased, quantization noise becomes more and more correlated and the decrease rate of Dr diminishes. Furthermore, Dr asymptotically approaches a lower, strictly positive limit. The bigger ? is, the higher this limit becomes. A larger quantization step also causes the decrease rate of Dr to depart from (28) “sooner” as r is increased [47]. 
 The reconstruction error can be further reduced, for a given oversampling ratio, by the use of feedback . Furthermore, feedback A/D converters yield a MSE that decreases steadily as r is increased. Thus, one can obtain an arbitrarily low MSE, for a given ?, by sampling fast enough. These converters are briefly described in the next section. 
 Quantization schemes that use feedback can be grouped into two main families: predictive quantizers and noise shaping quantizers. Examples of the first type are the delta modulator and differential pulse code modulator (DPCM) (see, e.g., [50]). The popular S? (sigma-delta) converter, see, e.g., [51], belongs to the latter type. 
 The following is a basic description of the main characteristics of both converter families, based mainly on the approach proposed in [52]. In the sequel, the quantization process is modeled as additive noise, corresponding to the quantization error of a scalar quantizer. 
 In this diagram, U(Z) and C(z) correspond, respectively, to the Z-transforms of the analog samples sequence {c[k]} and the quantized output sequence {u[k]} depicted in Fig. 3. Thus, the quantizer contained in the dashed line rectangle in Fig. 5 is a particular realization of the generalized scalar quantizer in Fig. 3. The filter Hp-1(z) included at the end of the chain in Fig. 5 can be considered as part of the reconstruction stage in Fig. 3. The terms E(Z) and D(Z) in Fig. 5 correspond to the Z-transforms of the discrete-time signals in each of the respective nodes. N(z) is the Z-transform of the error introduced by the scalar quantizer, i.e., N(z) = U(z)-E(z). From Fig. 5, the expression for the output U(z) is found to be 
 The key to the noise reducing capabilities of the predictive quantizer rests on the prediction filter Hp(z). This filter is designed to minimize the variance of the prediction error 
 see Fig. 5. It is common to assume that the quantization noise is uncorrelatedto any of the signals in the loop [51] . Thus, Hp(z) is chosen so as to reduce the contribution of 
 C(z) to E(z) in (31). By doing so, the variance (energy per sample) of the analog sequence that enters the quantizer is reduced. This in turn allows one to reduce the quantization step ? in the embedded scalar quantizer, without increasing the number of quantization levels needed to avoid overload. Thus, by reducing a measure of the term Hp(z)C(z) in (31), one is also reducing the quantization noise contribution, and the MSE is reduced accordingly. Of course, how much distortion reduction is achieved will ultimately depend on how predictable the sequence {c[k]} is, i.e., on the autocorrelation of {c[k]}. It will also depend on how well the prediction filter Hp(z) is able to capture this predictability 
 It has been shown [52] that the MSE of the scheme in Fig. 5 decreases with the oversampling ratio not “faster” than r-(2np), where np is the order of the filter Hp(z). If an additional ideal low pass filter with cut-off frequency fN/2 is placed after Hp-1(z) (see Fig. 5), then the MSE is reduced at most as r-(2np+1) A common choice of Hp(z) is of the form (1-z-1)np. 
 Note that the predictive quantizer in Fig. 5 can reduce distortion even if signals are sampled at Nyquist frequency, as long as the input analog samples are correlated. If the input samples are uncorrelated (white noise), then the predictive quantizer is unable to yield any MSE reduction at all. It is the increase in the autocorrelation of the input samples produced by oversampling which allows for the r-2np behaviour in the MSE reduction rate. 
 The second main category of feedback quantizers corresponds to the noise-shaping quantizers such as S? A/D converters, first proposed by Inose and Yasuda in [59]. One possible form to represent a noise shaping quantizer is depicted in Fig. 6. Again, C(z) and U(z) correspond, respectively, to the Z-transforms of {c[k]} and {u[k]} in Fig. 3. The noise shaping quantizer within the dashed line rectangle in Fig. 6 is a particular realization of the generalized scalar quantizer in Fig. 3. 
 where the noise shaping filter Hn(z) constitutes a degree of freedom in the design process. Since C(z) is band-limited, and because of oversampling, it is generally convenient to choose Hn(z) to be a high-pass filter, see, e.g., [51]. With this choice, the quantization noise is attenuated within the signal band whilst increased outside of it (see Fig. 7). This compensatory increase in the off-band quantization noise is unavoidable, as determined by the Bode integral theorem [60]   Because of the frequency shaping of the quantization noise, most of its energy can be suppressed by low pass filtering U(z), leaving only the in-band portion of the quantization noise. By doing so, it is verified in [52] that the MSE decays by increasing oversampling ratio at most as r-(2nn+1), where nn is the order of the noise shaping filter Hn(z). Most common choices for Hn(z) have the form (1-z-1)nn/P(z), where P(z) is an FIR filter. 
 As in control systems, one of the beneficial aspects of using feedback in analog-to-digitalconvertersis the increased robustness of the resultant system. Indeed, if properly designed, feedback converters allow one to achieve high accuracy quantization despite the use of inaccurate building blocks (such as the scalar quantizer itself, which can be allowed to have a very coarse and uncertain quantization step). This makes feedback quantizers the preferred choice for many practical applications. 
 It should also be noted that the above mentioned decay rate of the MSE with increasing oversampling ratio is not fast enough to be rate-distortion efficient. Indeed, oversampling AD converters require, in general, a higher data-rate than a system with finer quantization and no oversampling to achieve the same distortion. This can be seen by noting that, for feedback converters, the MSE decays only polynomially with increasing the oversampling ratio, as  O(r-(2n+1)), while the MSE decreases with increasing the bits per sample (i.e., reducing ?) as O(2-2B), i.e., exponentially. Nevertheless, recent results show that the L8 norm of the reconstruction error in S? converters can be reduced as O(?-r), ? > 0, by selecting for each oversampling ratio an appropriate noise shaping filter from an infinite set of filters [64]. Following a different approach, quantization schemes based on threshold crossings exhibit a reconstruction MSE that decays exponentially with increasing oversampling ratio [65, 66], and are thus rate-distortion efficient. 
 Interestingly, control theory can be used to design the generalized scalar quantizer in Fig. 3. More precisely, since the output of the quantizer is constrained to belong to a finite alphabet of values, the situation can be regarded as a control problem with input constraints. This point of view motivated has motivated us to apply Moving Horizon Optimization (MHO) tools to achieve a more effective noise shaping quantizer. This paradigm uses Model Predictive Control, and has proved to be a powerful tool for dealing with constrained systems [67, 61, 68, 69, 70, 63]. The quantization scheme so obtained, named Multi Step Optimal Converter (MSOC) [71], typically outperforms S? quantizers, while embedding the latter as a particular case. We will present next some of the fundamental principles behind the MSOC. The remainder of this section has been basically adapted from [71], 
 A more general formulation to analyze the discrete-time performance of noise shaping quantization can be derived from the block diagram depicted in Fig. 8. 
 In Fig. 8, {c[k]} and {u[k]} represent, respectively, the input analog samples and the quantized output sequence. The motivation for quantization noise-shaping has been incorc[k]	Hd(?)  eHd [k] 
 Figure 8: Scheme to generate the frequency weighted quantization error sequence eHd [·]. 
 porated by introducing a frequency weighted reconstruction error sequence, denoted by 
 In (33), Hd is a stable, causal, linear, time-invariant filter, which can be characterized via : 
 where A ? Rn×n, B ?Rn×1, C ? R1×n and n ?N is the state dimension, i.e. the order of the filter Hd. This filter can, e.g., represent the typical low-pass filter utilized in oversampled conversion, see e.g. [72], in order to decimate the converter output. In audio applications it makes sense to choose Hd as a psycho-acoustic model of the human hearing, compare also with work in [73, 74]. The performance of the quantization process in Fig. 8 will be evaluated by the measure 
 The cost V penalizes the distortion introduced in the conversion process in a frequency-selectivemanner. If the generalized scalar quantizer in Fig. 8 is designed to minimize the performancemeasureV, then its quantizedoutput u will approximate the input c, while the un-filtered quantization error, a-u, will tend to have a spectrum similar to that of the inverse of the filter Hd. Thus, the method will shape the quantization noise spectrum, just as the S? converter discussed in Section 6 does. 
 Unfortunately, minimization of V by using expression (35) is not possible in practical applications, due to the complexity of solving the associated combinatorial optimization problem. Furthermore, in the general case, an optimal quantizer would need to pre-view the entire signal c. This is clearly unsuitable for on-line applications. 
 In order to obtain a more practical method to minimize the cost in (35), it is convenient develop a recursive conversion method, which can be implemented on-line. For that purpose, we will first introduce a cost measure over a finite horizon, to deploy later the concept of moving horizon approximation, see [63]. 
 Finite Horizon Formulation A practical conversion scheme, suitable for online applications, must operate sequentially, evaluating a restricted number of decision variables and considering a moderate number of future values of c. For this purpose, it is convenient to characterize eHd as the output in a state space representation of Hd 
 This relation follows directly from (34). In (36), x ? Rn is the state vector. Note that, due to the Markovian structure of (36), at time k =  the impact of the past trajectories of c and u on future values of eHd is exactly summarized by means of the present state, x[k]. 
 Given the above, we next replace the infinite horizon cost function (35) by the finite horizon cost: 
 In (37), N ? N determines the prediction horizon and P is a given positive semidefinite matrix. 
 VN is a measure of the filtered distortion eHd over the prediction horizon plus a measure of the final state, x  N]. These predicted quantities are formed based upon the model (36). 
 The finite horizon cost VN  proposed in (37) takes into account only a finite number N of constrained values. The value of N determines the computational complexity required for the minimization of VN . This should be compared with the infinite number of decision variables in the original cost V. Using a finite horizon N also reduces the required pre-viewing of c to N -1 samples. Since N is a design parameter, it can be chosen so that the minimization can be carried out on-line. 
 Moving Horizon Approach As noted above, the optimizer to VN , contains a feasible output sequence for time instants  1. Thus, in principle, one could think of an implementation in blocks, where the minimization is carried out every N sampling instants. Unfortunately, the last few elements of  depend only on a small window of the filtered distortion, eHd . To improve performance, the multi-step optimal converter utilizes only the first element of , say u . It becomes the -th element of the converter output sequence, by setting: 
 At the next sampling instant, this new state value is used to minimize the cost VN , yielding u . This procedure is repeated ad-infinitum. As illustrated in Fig. 9 for the case N = 3, the prediction horizon of the criterion VN(k) moves (slides) forward as k increases. The past is propagated forward in time via the state sequence x, thus, yielding a recursive scheme. 
 The resultant architecture defines the MSOC. It constitutes an analog-to-digital converter architecture which optimizes the frequency weighted conversion distortion, based upon Model Predictive Control principles. 
 Interestingly, it has been shown that the MSOC with N = 1 and P = 0 reduces to the S? converter, see [71]. However, it is easy to see that, in general, larger values for N provide better performance, since more data is taken into account in the decision process of allocating scalars from U to the elements in the sequence u. In fact, one can expect that, if N is chosen large enough relative to the time scale of 
 Hd, then the effect of u  for j N will be negligible and the performanceof the MSOC will approach that obtained, if the infinite horizon measure of (35) were to be minimized directly (which, for the reasons explained above, is impractical). This asymptotic behaviour has been experimentally confirmed, see [71]. 
 In summary, the prediction horizon N allows the designer to trade-off performance versus on-line computational effort. Interestingly enough, excellent performance can often be achieved with relatively small horizons (see, e.g., [71]), thus rendering the scheme quite easy to implement in practical cases. 
 Another advantage of the MSOC when compared to the S? converter resides in that the matrix P in (37) can be designed to ensure stability like properties of the MSOC, see [71]. 
 Given that digital signal processing systems have to interact with the real, physical world, the design of a quantization scheme should take into account the sampling (continuous to discrete-time) and reconstruction (discrete to continuous-time) stages between which it is to be inserted. Unfortunaly, there exists only partial understanding of how sampling-reconstruction strategies interact with a given quantization method in terms of the resulting, overall reconstruction error. Furthermore, most literature analyzes the performance of quantizers in terms of how close the input analog samples are approximated by the output, quantized samples, and not by comparing the analog, continuous-time underlying signal entering the system against the analog, continuous-time reconstructed signal that comes out of the reconstruction stage of the system. Accordingly, performance is most often measured by the 2 norm of the sample approximation error, see 21. Similarly, traditional works on sampling and reconstruction theory build their analysis based first upon ideal, nonquantized samples, incorporating later the effect of quantization viewed as the corruption of ideal samples by white additive noise. Although it has been shown that this whitenoise model of quantization is indeed accurate for small quantization steps and input samples whose PDF satisfies certain rather weak requirements, it is certainly not accurate, for example, when quantization steps are large, or when feedback structures are deployed. As presented in Sections 6 and 7, it is often the case that quantization noise is deliberately made non-white by the quantizer so as to minimize a frequency weighted measure of the reconstruction error. 
 Within the setup depicted in Fig. 3, we aim to present in this section some results and additional insight related to the joint problem of designing systems that make use of the pre-filtering, sampling, quantization and reconstruction paradigm . 
 The Hilbert spaces model of the sampling and reconstruction process described in Section 3 leads to a somewhat trivial but nevertheless important result: it allows for a decomposition of the final reconstruction MSE between the analog input a and the analog output ˜a (see Fig. 3) of a sampling-quantization-reconstruction system into two terms. The first term corresponds to the error due to the “spaces mismatch”, i.e., the non coincidence of input and outputsignal spaces. The second errorterm comes fromthe deviation of the discrete-time processing (both linear and non-linear) from the optimal mapping between input and output vectors in the sampling and reconstruction spaces, respectively. The following proposition formalizes this idea: 
 Proposition 2. Let ?(·) ? L2 be the impulse response of the reconstruction filter R, such that {Tkt?}k?Z is frame for W  span{Tkt?}k?Z, and let t be the sampling interval. Then, the mean square reconstruction error between any input signal a? L2 and an approximationa˜ ?W generated by the reconstruction stage can always be decomposed as follows 
 Since (a-PW a)? W ?, and because w ? W , we have that  PW a,  0 (see (50)), and (40) follows. 
 Corollary 1. From Proposition 2, it follows that for any a ? L2, choice of quantization scheme and/or discrete time processing, the continuous time reconstruction error is lower bounded by 
 We emphasize that the lower bound in (41) corresponds to the minimum continuous-time error attainable by any discrete-time scheme, once the output space is given, even if and no quantization is applied to the samples. From Proposition 2, it is clear that the performance of discrete-time processing (e.g., discrete-time filtering and quantization) should be evaluated in terms of the second term of the right hand side of (40), that is, the L2 norm of PW a -a˜. In relation to the design of quantizers, this gives rise to the question of what information is needed by a generalized scalar quantizer to minimize PW a . We have addressed this question in [79]. A summary of the analysis and results therein is presented below. 
 As noted above, the reduction of the continuous time MSE by discrete-time processing takes place by minimizing the second term on the right hand side of (40). For the general system under study (see Fig. 3), the signal to be approximated is actually a convolved with h ? L2, the impulse response of H: 
 Defining ? as the impulse response of R, the approximation of a generated by the system becomes 
 see, Fig. 10. The impulse response of W determines the reconstruction frame {?k}k?Z, which spans the reconstruction Hilbert space 
 As described in Section 4.2, the generation of the optimal output PW a can be accomplished by applying the pre-frame operator ? associated with {Tkt?}k?Z to the sequence of scalars , i.e. 
 where ?° * is the analysis operator associated to {Tkt?°}k?Z, the canonical dual frame of {Tkt?}k?Z (see Definition 7 in the Appendix). We will denote this optimal, un-quantized sequence of samples by 
 It is clear from the above that any quantization algorithm that attempts to minimize the continuous time error   needs to be able, in the first place, to obtain the target sequence u? in (43). From the results presented in Section 4.2, this implies that the first necessary condition for the feasibility of optimal quantization is that sampling and reconstruction stages be matched for orthogonal (MSE) reconstruction. 
 If we now suppose that the quantizer has access to u?, then the problem of optimal quantization is that of choosing the optimal quantized sequence u, defined as 
 The solution to (44) requires one to solve a continuous-time optimization problem with discrete-time, quantized decision variables. It is shown in [79] that this can be converted into an equivalent discrete time optimization problem. More precisely, 
 The operator   is characterized by the Gram matrix (see [80, sec. 3.5]) of the reconstruction frame, which is defined element-wise as 
 where  u and u are the vector representations of the sequences u and u?, respectively. 
 Figure 10: The sampling, quantization and reconstruction system from Fig. 3 revisited. Impulse responses and frame 
 The direct consequenceof (46) is that a quantizer can determine the optimal output sequence without full knowledge of the inter-sample behaviour of the impulse responses of the reconstruction filter. Indeed, quantization performance can be measured by the weighted 2 norm implicitly defined in (46). Note that the design of an optimal quantizer is not possible without knowledge of the matrix G?. operators are shown for each filter. 
 In general, minimization of (46) would require one to evaluate it for every sequence {u[k]}k?Z, u[k] ? U?k ? Z, that can be generated by the quantizer. This optimization programme, however,becomes intractable for sufficiently long sequences. Given the similarity of (46) and (35), one can use the ideas introduced in Section 7 and optimize over a short moving horizon of samples. Details can be found in [81, 79], where a sampled-datamulti step optimal converter is proposed. Preliminary results show that, interestingly, significant distortion reduction is obtained even when converting non band-limited signals. Indeed, since the focus is on the reduction of the total continuous-time reconstruction error, if the sampling rate is lower than the Nyquist rate, the resultant converter will attempt to reduce not only quantization noise, but also aliasing noise. Furthermore, as the horizon is made larger, the output of the converter approaches the optimal feasible output sequence, defined in (44). 
 In previous sections of this work we have illustrated that the power of feedback can be used in the design of ADconversion schemes. In particular, we have shown in Sections 7 and 8 that careful deployment of elements of Model Predictive Control may lead to high-performance conversion techniques. The purpose of the present section is to highlight the role played by sampling and especially quantization in feedback control applications. Efficiency in data representations plays a central role in any control system where parsimony aspects need to be taken into account. Thus, quantization and sampling are worth investigating, for example, in the following situations: 
 •	when signals need to be transmitted over a digital network, i.e., in Networked Control Systems (NCS’s) [3, 82, 4]; 
 •	when plant inputs need to be quantized (e.g., relay feedback, on-off control, digital control, or also due to the presence of a human operator)[70]; 
 •	in large scale systems, such as those related to mining operations and supply chain management. 
 In the following, we will briefly describe how concepts surrounding sampling and quantization translate into the design of these types of control systems. 
 Sampling and Reconstruction In the design of a sampling/reconstruction scheme for a control system, traditional reconstruction criteria should be complemented with more appropriate performancenotions. Indeed, reconstruction quality is only of secondary importance. The main objective is measured at the plant output. In particular, as shown in [83, 5] for NCS’s, open loop performance measures should be replaced by closed loop ones. This can be achieved through consideration of frequency weighted measures such as (7). 
 Quantization Interestingly, the noise shaping ideas described in Sections 6-8 can also be applied to control systems where signals are quantized; see, e.g., [70]. For example, when focusing on the design of controllers for plants with quantized inputs, a key point resides in realizing that the AD-conversion scheme of Fig. 3 is related to a quantized control system with plant H: The plant input u[k] is to be chosen such that the plant output Ha˜(t) tracks the reference signal Ha(t). Thus, performance can be measured via the frequency weighted error signal eH(t), see (7) and also (35). 
 Details on how to apply principles of Moving Horizon to NCS’s can be found, for example, in [84]. It is interesting to note that the framework can also be enriched to incorporate dynamic scheduling into NCS’s. The resultant methodology can be regarded as incorporating sampling and quantization on demand and is, thus, highly efficient from a data representation perspective, see [84]. 
 This paper has reviewed basic results and methods related to the process of sampling, quantization and reconstruction of scalar signals. With the introduction of a frame theoretic viewpoint, three notions of sampling and reconstruction have been discussed. We have described several generalized scalar quantization schemes, and have showed how control theory has contributed to signal processing theory. Furthermore, we have given insights into the joint problem of sampling, quantization and reconstruction, and have outlined how these stages interact. Finally, we have examined the role played by sampling and quantization in control systems. 
 Definition 2 (Hilbert Space). Let W be a vector space with an inner product  W and the induced norm ·W  . If such a space is complete under its norm then it is a Hilbert Space. 
 Definition 3 (Riesz Basis). A sequence of vectors (functions) {?k}k?K, K ? Z, in a Hilbert space W is a a Riesz basis for W if and only if W = span{?k}k?K   and there exist two constants 0 < m = M < 8 such that 
 •	The elements ?k in (47) are orthogonal if and only if m = M and orthonormal if and only if m = M = 1. 
 •	The lower bound in (47) is equivalent to saying that {?k}k?Z is a set of linearly independent vectors. 
 •	The higher bound in (47) guarantees that ?k?Kc[k]?k will be bounded for any choice of c . 
 If W ? H is a Hilbert space, then the best approximation in W (in the sense of the norm ) of any h ? H is given by the orthogonal projection of h onto W , denoted by PW h, and defined as the operator 
 The orthogonal projection from a Hilbert space H onto W ? H implicitly defines the null space of W : 
 If {?k}k?K, is an orthonormal basis of W , then the orthogonal projection operator can be explicitly written as 
 Orthogonal projection permits elegant solutions to some otherwise complex optimization problems in functional analysis. This makes Hilbert spaces and operators a natural framework for studying the problem of efficient sampling and quantization. 
 A fundamental property of Hilbert spaces and operators is that they are able to define a precise form of equivalence between two different Hilbert spaces. It is called isomorphism: two different Hilbert spaces are isomorphic if they have the same dimension . An isomorphism is indeed any linear invertible  operator from one space onto the other. Of particular interest for our analysis are the isomorphisms between any separable Hilbert space W ? L2 (function space) of dimension |K|, where K ? Z, and R|K| (Euclidian space). Such an isomorphism can be stated by considering any orthonormal basis of W , namely {?k}k?K, and constructing the associated analysis operator 
 The analysis operator ?* defined in (52) is an unitary isomorphism. This means that the respective images in R|K| through ?* of any group of vectors in W preserve their respective norms and relative orientations, i.e. 
 This remarkable property of isomorphic spaces allows one to study the relation between elements of a Hilbert space by looking at their images through ?* in another, more convenient Hilbert space. Actually, one can argue that all digital signal processing (including digital control) is made possible becauseof the existence of isomorphismbetween signal spaces and subspaces of 2. 
 Some of the basic concepts of Hilbert spaces of signals and bases presented so far will be illustrated by the following simple example. 
 Let W be the space of all real valued functions w(t) satisfying the following conditions: 
 •	The derivatives of w(t) are constant over any of the open intervals ik = (kt,kt+t),k = 0,1,2. 
 Figure 11: Example of a functional space, an orthonormal base and a unitary analysis operator. a) Functions w1, w2 and w3 belong to the Hilbert space W ; b) The functions ?1,?2 ? S constitute an orthonormalbasis for W ; c) Image of the functions w1, w2 and w3 in R2 through the analysis operator ?*. 
 W becomes a Hilbert space. The inner product (54) also defines a norm in W , given by 
 It is easy to show that W is a two-dimensional space. This can be intuitively verified by noting that any function w ? W is completely determined by exactly two parameters, such as, for example, the values of the functions evaluated at t and 2t. A basis for a Hilbert space of dimension two contains two elements. Figure 11.b) shows a pair of orthonormal functions ?1,?2 in W which form an orthonormal basis for W . 
 Figure 11.c) shows the images of w1, w2, w3, ?1 and ?2 through the analysis operator ?* (see (52) ) in R2. As expected, the images of the orthonormal functions ?1 and ?2 are orthonormal vectors in R2. How “close” is w1 to w2 in their spaces norm?. Since the analysis operator ?* is a unitary isomorphism between W and R2, we have, from 
 Consider now the case of a function h(t), t ? R, that belongs to a space H ? L2, such that W H and h ?/ W . 
 Figure 12: a) Orthogonalprojection onto W ?H . a) Function h(t) belongs to H . b) Relative positions between h, ?1, ?2 and PW h represented in an isomorphic Euclidian space. 
 The magnitudes and relative directions of h with respect to an orthogonal basis for W such as  are shown in the 3 dimensional representation of Fig. 12.b). Here it can be seen that h is outside W but has a non zero orthogonal projection onto W . This orthogonal projection is the closest vector to h in W , in accordance with (48), and is given by (51). Consequently, the best approximation (in an L2 sense) of h in W is, expressed as a function of time 
 Despite the computational convenience of bases, one often needs to study spaces generated by a set of linearly dependent vectors (over-complete basis). The concept of frames, introduced by Duffin and Schaffer [85], allows one to analyze such cases. Situations with over-complete bases arise in practice not only by chance. It has been shown that the redundancy of frames is beneficial, for it can reduce the effect of errors in the expansion coefficients, see [39]. The formal definition of a frame is given next. 
 Definition 4 (Frame). A sequence {?k}k?K of elements in a Hilbert space W is a frame for W if there exist constants A,B > 0 such that 
 The largest number A and smallest number B that satisfy (55) are called frame bounds. Some important remarks about frames are as follows: is a frame for a Hilbert space W , then ?k k?K = W . 
 •	A frame is said to be tight if one can choose A = B as frame bounds. If A=B=1, it is called a Parseval frame. 
 •	If a frame ceases to be a frame when an arbitrary element is removed, it is called an exact frame. An exact frame is equivalent to a Riesz basis. 
 •	If the elements of a normalized frame are linearly independent then A = 1 = B (see [39]). 
 The redundancy of a frame with |K| vectors for a space W is defined as the ratio 
 It is easy to show that, for a normalized tight frame, r = A, where A is the lower frame bound in (55). Another important property of the elements of a frame {?k}k?K is that they are also a Bessel sequence, i.e., they 
 From remark 2 and the above properties, orthogonal bases are a special type of Riesz basis, whilst Riesz bases are exact frames. Thus, by basing our analysis on frames, one is also including orthogonal and Riesz bases as special cases. 
 Definition 5 (Synthesis Operator). The synthesis (or preframe) operator for a frame {?k}k?K is defined as 
 (56)), the synthesis operator for a frame with frame bounds A,B is bounded, with operator norm  B, i.e., B is the minimum constant such that  . 
 Definition 6 (Analysis Operator). The analysis operator for a frame {?k}k?K is defined as 
 Definition 7 (Dual Frame). Let {?k}k?K be a frame for a Hilbert space W . Another frame for W , namely, {gk}k?K that satisfies 
 As can be seen in (57), a dual frame provides an explicit method for representing any signal w ? W in terms of coefficients (samples), from which w can be exactly recovered through the synthesis frame {?k}k?K. 
 Definition 8 (Frame Operator). The frame operator of a frame {?k}k?K is defined as 
 Lemma 2 (from [80, Lemma 5.1.5]). Let {?k}k?K be a frame with frame operator S and frame bounds A,B. Then the following holds: 
 The frame operator defined in (58) is of particular importance for the problem of sampling and reconstruction,since it provides an explicit way to obtain a dual frame (see (57)). More precisely, with S as defined in (58), if {?k}k?K is a frame for W , then the frame   is a dual frame for {?k}k?K in W , i.e. 
 The frame  is called the canonical dual frame of {?k}k?K in W . This is a reciprocal relation, i.e., 
 If the frame coefficients in (62) were contaminated by additive noise e[k], k ? K, then the reconstruction formula (62) would yield a reconstruction error 
 Early referencesto the fact that the redundancyof the frame reduces the reconstruction error were provided in [24], whilst proofs can be found in [32] and [39]. Due to the importance of this property of redundant frames, we present next an adaptation of the result in [32], which is also illustrative of the importance of the frame bounds. 
 Proposition 3. Let {?k}k?K be a frame of unit-norm vectors with frame bounds B = A > 0 and let e[k], k ? K be a sequence of independent random variables with mean zero and variance s2. Then the mean square value of we in (63) 
 Proof. If e[k], k ? K is a sequence of independent random variables with mean zero and variance, we have 
 because {?k}k?K is a normalized frame. Combining (64) with (65) gives the result. 
 ﻿The main purpose of this note is to show that in a realization   of the causal information 
 rate-distortion function (IRDF) for a ?-th order Markovian source xn1, under a single letter sum distortion 
 derived under the assumption that the sequences   have a joint probability density function. 
 Consider the causal information rate-distortion function (IRDF) for a random source x , 
 where the minimization is over all conditional PDFs  satisfying the distortion constraint 
 If the infimum is achieved by some conditional distribution, the associated pair of sequences x  is called a realization of . Here we assume that such distribution exists and that the corresponding realization has a joint PDF. This assumption is satisfied if, for example, xn1 is Gaussian and ?(x,y) = (x - y)2. 
 The first purpose of this note is to show that in a realization of the causal IRDF for a ?-th order Markovian source xn1, under the average distortion constraint (2), and supposing that in such realization the sequences have a joint PDF, it holds that 
 The expressions given in (4) are a special case of the ones given by [1, equations (16),(17),(18)] for abstract spaces, where their derivation is not included. The value of our first result resides in that 
 In this proof, we pose the causal IRDF optimization problem with   as the decision variable (instead of the collection  as would be the case in [1] for probability measures having an associated PDF). Accordingly, we impose an explicit causality constraint on , instead of enforcing causality structurally by restricting   to be the product of  , as done 
 The second (and main) goal of this document is to note that from (4a) it is clear that 
 does not hold, except for ? = 1. Crucially, (6) does not become true by supposing that the joint PDF of x  is stationary, thus contradicting [2, Remark IV.5] and what is stated in the discussion paragraph at the end of [1, Section V]. 
 The causal IRDF under the above conditions is yielded by the solution to the following optimization problem: 
 where the minimization is over the conditional PDF  . Notice that (7d) is an explicit causality constraint equivalent to (3). 
 Before writing the Lagrangian and taking its Gateaux differential, let us obtain the Gateaux differential 
 On the other hand, for each i = 1,...,n, the causality constraint (7d) appears in the Lagrangian as 
 It will be convenient to manipulate this expression so as to give it a structure similar to the other terms in the Lagrangian. For this purpose, notice that 
 From the theory of Lagrangian optimization on vector spaces   is a solution to Optimization Problem (7) only if 
 for every function gyn1 |xn1 as defined in (8), i.e., for every conditional PDF . This holds if and only 
 Fk(xn1,y1k) = e-(?¯k(xk1,y1k)-?k(x1n,y1k)) Z e-s?(xk+1,yk+1) Fk+1(x1n,y1k+1)dyk+1	(43b) 
 In order attain causality in (41), the functions  must depend only on and . Since for each k, the function Fk+1 does not depend on terms   with i = k, the causality constraint 
 (see (43a)).  Suppose now that (44) (i.e., causality) is satisfied for k + 1, for some k > n. In such case, 
 which case (50) becomes (4b). Substituting the latter into (44) and then in (41) yields (4a). Finally, from (4a), it follows that in a realization of the causal IRDF it must hold that 
 ﻿ This paper novel on the characterization of the the causal information function for arbitrarily distributed one sided stationary th order source . It is first shown that on the of the to the causal stated for two sided stationary do not apply to the commonly used family of asymptotic average single letter distortion criteria . Moreover , we show that , in general , a reconstruction sequence cannot be both jointly stationary with a one sided stationary source sequence and causally related to it . This that , in general , the causal for one sided stationary cannot be by a stationary distribution . However , we prove that for an arbitrarily distributed one sided stationary source and a large class of distortion criteria , the search for can be restricted to which yield the output sequence y jointly stationary with the source after . Finally , we improve the definition of the stationary causal previously by and for two sided stationary and show that for a two sided source ..., , , ,... for the associated one sided source , ,.... This that , for the quadratic case , the practical zero delay by for approaching achieve an operational data rate which by less than . log p e . per sample . 
 INTRODUCTION 
 The information Rit for a given one sided random source be defined as the of the mutual information rate , Section . 
  
 between source and reconstruction y such that a given fidelity criterion does not exceed a distortion value . If one to this definition the restriction that the output can only depend causally upon the source , one what is known as the causal , , non anticipative or sequential . All these are equivalent and will be as , defined in 
 of the mutual information as 
 where the is taken over all joint of y given x such that the causality which will be to as the short causality constraint 
  
 hold and which yield distortion not greater than , for some fidelity criterion . Notice that , if one is given a two sided random source process x ..., , , ,... instead , and one is interested only in and the x , then the causality may be stated as 
  
 as done in . This notion of causality will be to as the long causality constraint . 
 The motivation for considering in this work one sided instead of two sided and thus instead of from the aim of building which operate with zero delay the same motivation behind the causality constraint . To see this , notice that the causality constraint for two sided to the situation in which source in the infinite past exist and are available to the . This may require an infinite delay before actually beginning to encode and decode . By contrast , the causality constraint the case when the source is a one sided process and only upon x 
 Remark . It is important to highlight at this point that even though the causality condition can also be applied to a two sided source process , it would not ensure causality in that case . To see why , consider the situation in a binary i . i .. source where each the or with equal probability . Suppose y is built as , where the exclusive OR operator . It is easy to see that , even though y non causally on x . 
 The above observation that if the source is two sided but only the x are and the process is one sided y , then one needs to impose instead the more general 
 causality constraint 
  
 which . Besides causality , these guarantee that even if the source is a two sided process , its and reconstruction proceeds as if it were a one sided process . 
 Notice that and . For this reason , will be to as the strong causality 
 . 
 As we shall see in and , this situation , where at can take only as input , significant due to the unavoidable need to deal with transient phenomena . 
 The operational significance of from its relation to the causal operational , as . The latter is defined as the of the average data which are achievable by a sequence of causal , yielding a distortion not greater than . is important because every zero delay source code suitable for such as low delay streaming or control , must be causal . 
 An is said to be achievable if it the under the same , . As far as the are aware , the of not been yet , for any source and distortion measure , and thus the gap between and is unknown in general . However , it is known that , Section 
  
 and for it is possible to construct causal with an operational data rate exceeding by less than approximately . sample . sample for zero delay , once the statistics which realize the latter are known . This the importance of the 
 causal . 
 To the best of the knowledge , no closed form are known for , except when considering mean squared error distortion and for i . i .. or AR , either scalar , Section or vector valued . However , there exist various structural of the causal that have been found in literature when or is assumed to admit a stationary realization . 
 Indeed , the of the of the causal a crucial role in the computation of for st order and distortion in . It also been a key implicit assumption in , and an explicit assumption in works such as and . In particular , for a stationary two sided random source , Definition the stationary causal 
  
 where the is taken over all of y given x which yield a one sided reconstruction y jointly stationary with x , satisfying and an asymptotic average distortion constraint on . For the case of a source , it was shown in that an operational data rate exceeding by less than . log p e . sample was 
 achievable a entropy uniform surrounded by linear time invariant operating in steady state . These illustrate the relevance of whether or in which the causal a stationary realization . 
 To the best of our knowledge , the only work which given an answer to this question in a general framework is . Under a set of in Section below , it is shown in , Theorem that the search for the causal for a large class of two sided and distortion criteria can be restricted to which are jointly stationary with the source . Unfortunately , as we show in Section , the on the fidelity criteria in leave out some common such as the family of asymptotic average single letter fidelity criteria , and the statement of , Theorem an assumption whose validity to be proved . More importantly , the entire analysis of is built for two sided the causality constraint , which the question of whether its could apply to one sided as well , with the causality constraint . 
 In this paper we give an answer to these and use the to prove some novel of the causal associated with the of its . Specifically , our main are the following : 
 We show in Theorem that if a pair of one sided jointly stationary , with the latter depending causally on the former according to but otherwise arbitrarily distributed , then it must also satisfy the 
  
 which is a fairly restrictive condition . In particular , as we show in Theorem , jointly and y causally upon x , then joint x is an i . i .. or st order process . This in stark contrast with what was shown in for two sided stationary and a of what is stated in , Theorem . . 
 Despite the above , we show in Theorem that for any th order one sided stationary source x and a large class of distortion , the search for the causal as defined in can be restricted to output causally related to the source and jointly stationary with it after , and such that . We refer 
 to such of as being quasi jointly stationary this notion is formally in Definition below . A consequence of this result is that for any th order stationary the corresponding one sided 
 stationary source x . The relevance of this finding is that for stationary and asymptotic distortion , an operational data rate exceeding and thus by less than approximately . sample , when operating causally , and . bit sample , in zero delay operation , is achievable by a scalar as in . 
 The remainder of this paper with Section , in which the leading to , Theorem are and the of that theorem are . In Section we prove that , in general , it is not possible to have two one sided which are jointly stationary and , at the same time , satisfy the causality constraint . Section our main theorem Theorem , which that the search for the causal for one sided th order stationary can be restricted to . Finally , main of this work . All are in section the Appendix , which also some technical by these . 
 Notation : the real , the , is the set of natural positive , and N , , ,.... For every , the ceiling operator the integer not less than . We use non for scalar random 
 , such as . Random are as . 
 For a random one sided process x we will sometimes use the short hand this meaning is clear from the context . When convenient , we write a random sequence , as the column vector , the so that the index goes above the one , thus the usual index order in a column vector . The entry on the th row and th column of a as ,, with being the sub matrix ,. 
 For a random a given alphabet set , we write to denote a sigma algebra associated : , to denote its probability distribution or probability measure . We describe the fact same probability distribution as , state independent . We write the condition in which two random a , are independent given a third random chain notation a . a set of probability , then the set of all random whose probability distribution to . The expectation operator is as E . We write as a shorthand for . The mutual information between two is defined as , Lemma . 
 , 
 where the is over , and ,, and , are the joint and marginal of and , respectively . If , have joint and marginal probability density ,, and , respectively , then 
 I ; , E . 
 The conditional mutual information I a ; is defined via the chain rule of mutual information I a ; , I a ;, I a ;. The mutual information rate between two 
 x and y is defined as in . The variance of a real valued random as 
 . The auto correlation function of a random process x is , , E , ,. 
 The following of the mutual information any random a ,, will be and to throughout this work : 
 P . I a ;, I a ;, with equality if and only if I a ; . 
 P . I a ;, I a ;, with equality if and only if I a ; . 
 We will also make use of the following fact : 
 Fact . Let a ,, be three random with an arbitrary joint distribution . Then , there a random element a equivalently , a joint distribution a ,, such that 
 a , a , 
 a 
 AND ITS INAPPLICABILITY TO ONE SIDED 
 In order to assess whether or to what extent , Theorem could provide support to the made in , e .. , , , , it is necessary to take a closer look at the made in and the statement of its Theorem . For that purpose , the first part of this section is an exposition of the and leading to , Theorem . The second part is an analysis which the of , Theorem and its inapplicability to the case in which the source and reconstruction are one sided . At the same time , this section also and part of the notation to be in the remainder of this paper for convenience , a summary of these is in Table I below . 
 A . A Brief Review of 
 Throughout , the search in the associated with various of i . e ., causal rate distortion is stated over of joint probability between source and reconstruction as opposed to the usual , in which the search is over conditional , see and , Chapter , . Since the distribution of the source is given , it is that for every k k , all the joint , to be considered same given distribution of the source for the corresponding block , say . This requirement can be as that , for a set of admissible joint 
 , k defined as 
 , 
 where and are , respectively , the to which and belong . In , this admissibility requirement is in the definition of the of which meet the distortion constraint , next . 
 The fidelity criterion for every pair of k k is expressed in as to belong to a non empty set of hereafter to as distortion feasible set , a condition written as . In this definition , the number an 
 admissible distortion level . Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation . 
 As above , the admissibility requirement is expressed in the in . . The latter equation can be written as 
 . 
 We believe this re exposition of to be valuable in itself since on the one hand , it the minimal set of to formulate and understand its Theorem , and on the other hand , it an clearer presentation than the one found in an translation from , which is not easy to read due to its notation , some mathematical and the low resolution of its available form . 
 The analysis in considered both discrete and continuous time , but here we only refer to the discrete time scenario . 
 In . and . , the distortion feasible are assumed to satisfy the concatenation condition 
 . 
 With this , . defined the epsilon entropy of the set of as 
 , 
 where the is taken over all of random such that the causality 
 x 
 are satisfied . Then . the message generation rate as 
  
 when the limit . An alternative message generation rate is also considered in by the set of distortion admissible process as : 
 Definition . The set of all two sided random process x , y for which there exist k k such that and 
 . 
 N 
 With this , . 
  
 when the limit , where the is taken over all of satisfying the causality 
 x . 
 Notice that these imply and differ from the latter in that here the reconstruction y is a two sided random process . 
 Now assume that Xi and , for all i , for . Define , for any given non negative sequence such that , the distribution 
 s 
 , E a , E , k k , E k k k k . 
 s s 
 The actual term employed in is epsilon entropy of the message where the term message to the random in . 
 We can now re state Theorem in as : 
 Theorem Theorem in . Suppose that 
 x is stationary . 
 Stationary distortion feasible : For every and are 
 identical . 
 The concatenation condition . 
  
 . 
 For every set of non negative such that , 
  
 where the are distributed according to . 
 Then , the analysis of the lower bound in can be confined to jointly stationary of random 
 satisfying the causality constraint . N 
 For convenience , Table I a summary of the and notation so far , together with some which will be defined in the following . 
 B . Analysis of Theorem and its Inapplicability to One Sided 
 We now discuss three of Theorem which are relevant when trying to establish whether the causal of a one sided stationary source a stationary realization . 
 Limitation : The first obvious limitation is that even if source and reconstruction are , every distortion criterion which only their positive time part cannot be expressed by a distortion feasible set given by Definition if the , satisfy condition in Theorem . To see this , notice that if , then such distortion criterion which non positive times would require , to admit all joint probability satisfying . Combining this with condition in Theorem that every set , , with , which to imposing no restriction on the distortion at all . 
 It is natural to think that such elemental shortcoming could be by simply condition in Theorem by a one sided version of the form : 
 For every t ,, t such that t t : , t and , t are identical . 
 Leaving aside the fact that this alternative condition is not sufficient for Theorem to hold , it is worth pointing out that , the commonly family of asymptotic single letter fidelity 
 Table I 
 SUMMARY OF THE MAIN IN THIS PAPER . 
  
 such that the associated marginal distribution 
 the given distribution of the source sequence , i . e ., 
 Distortion feasible set . The set of all joint which satisfy a given constraint given by see before 
 The set of all of such that . See also the 
 Notation subsection at the end of Section I . 
 Generic distortion feasible set of probability for of one sided 
 . In this paper , we state some minimal on in Assumption and 
 some additional structural in Assumption . 
 Q , , ,... The set of all joint of of one sided random 
 such that , are jointly stationary and 
 see Definition . 
 , , ,... and The of causally related one sided of see Definition . 
 C The set of one sided of causally related according constraint see Definition . to the short causality 
 The set of causal for of the form the long causality constraint see Definition Such satisfy 
 criteria can not be expressed by a distortion feasible set given by Definition , as the following lemma its proof can be found in Appendix A . 
 Lemma . Let be any given distortion functional which as argument a joint distribution , and a non negative real value . Let AD be the set of all of stationary , with pair wise , which satisfy the asymptotic 
 single letter fidelity criterion 
 Then , there exist an infinite collection of distortion feasible , k k k satisfy 
 ing such that the associated given by Definition AD . N 
 Limitation : The second limitation associated with Theorem is that its application 
  
 one to prove its condition , i . e ., the unproven supposition that The only work we are aware of which upon Theorem is , and , accordingly , , Theorem . , which that a similar equality . Unfortunately , as shown in , the proof of , Theorem . is flawed . 
 We note that Lemma in Section A below two alternative sufficient for an equality similar to but for one sided to hold . 
 Limitation : The third limitation of Theorem for its applicability to one sided is the fact that the entire framework built in is stated for two sided and , crucially , for the corresponding causality restriction given by chain . This difference cannot be simply while Theorem to remain valid . Indeed , as we show in the next section Theorem , a pair of random can be jointly stationary and at the same time satisfy the causality chain only if is independent is given . Moreover , we prove that joint and causality are incompatible when the source is a th order one sided process with . 
 FOR JOINT AND CAUSALITY TO HOLD TOGETHER 
 In this section we address the question of whether there a one sided reconstruction process y jointly stationary with a source x and which also the causality constraint . 
 Each source random sample i to some given set source alphabet and is to have an arbitrary distribution . Recall that a random process , the reconstruction alphabet and Y ,, is said to be jointly stationary with x if and only if , for every , the distribution of does not depend on , for , ,.... 
 The next theorem that , for such one sided , joint and causality may hold together only if is independent of when is given . 
 Theorem . If x and y are jointly stationary and y is causally related to x according to , then 
 x . 
 Proof . If does not hold for if y and x are jointly stationary , then N 
 x 
 does not hold , which to not satisfying for , the proof . 
 To illustrate how restrictive condition is , the next theorem that , for a th order stationary source x , causality and joint is possible only if x is i . i .. or . Recall that a random vector or scalar valued process th order if is the non negative integer such that 
 x 
 Theorem . Suppose x is a zero mean stationary process , and assume that , for some 
 are jointly and jointly stationary , with yN being causally related 
 to according to . Then th order with . N 
 Proof . Since and yN are jointly and the latter causally upon the former , it that 
 , E 
 for some lower triangular matrix A ai ,, A i ,, i , ,...,. On the other hand , the fact that and yN are jointly stationary that and are matrices . From , considering the on the first and second of and 
 , E , , .. , 
 this condition that 
 a , a , a , a , a , . a , , 
 Therefore , , ,..., , which for a stationary sequence that 
 E . For random the latter is equivalent to the , which a st order process if or an i . i .. process if . This the proof . 
 In the next section we will see that if x is th order , then it is possible to build a pair causally related according to such that x , y is stationary . Moreover , we will show in Theorem below that the minimization associated with the causal can be restricted to such . 
 THE SET OF QUASI JOINTLY STATIONARY IS SUFFICIENT 
 In this section we show that for any th order one sided stationary source x the search for the causal as defined in and for a large class of distortion criteria can be restricted to output y causally related to the source , jointly stationary with it after , and such 
 that . We refer to such of as being quasi jointly stationary 
 , and define the set which them as : 
 Definition Set of quasi jointly stationary process . The set of is composed of all joint , y of of one sided random which satisfy 
 are jointly stationary 
 . 
 N 
 Notice that Q to the set of joint associated with all jointly stationary one sided process . 
 As in , we write when the distribution of to the distortion 
 feasible set ,, defined as in . 
 One can define a distortion feasible set for of one sided , say , from the finite length distortion feasible ,, in more than one manner . A minimal condition we shall require for such definition is the following . 
 Assumption . The distortion feasible set of for of one sided the following : 
 If , then the given probability distribution of the source process , say 
 . That is , P , see . 
 If is any given pair of one sided , and there an infinite collection of increasing k k such that , for all , 
 then . 
 For any pair of , and if , then the ¨ , , , , 
 satisfy . 
 N 
 Notice that if this assumption and if the in Definition were restricted to be positive , then we would have see Definition . However , the one way in Assumption allow to be than . 
 We now define the of causally related of and . 
 Definition Set of Causal . Define as the set of all one sided random which satisfy the causality constraint 
 x 
 The set of causally related one sided process C is defined likewise but for one sided 
 which satisfy the causality constraint . 
 With the above minimal , one can define two causal , namely 
 , 
 provided the exist . The causal with if in one k and k . By contrast , from in that the latter is associated with the less general distortion feasible set see Definition . 
 Since our main result will be stated with the assumption that , we develop next two sufficient for such equality to hold . 
 A . Sufficient for 
 We begin by a useful construction of a pair of from a finite length sequence and some if the of the former . 
 Proposition . Let , be given , with stationary and such that the causality condition for , ,... Build the as : 
 Choose with the same distribution of , i . e ., 
 x . a 
 For every N , choose the conditional distribution of given as 
 . b 
 Then 
 . 
 . 
 Also : 
 If , Assumption and 
 ,, N : , ,, 
 hold , then . 
 If th order , then 
 x . 
 N 
 Proof . The first equality in b is equivalent to the 
 y 
 The second equality in b together with the fact that is stationary imply that , and thus . On the other hand , we have that 
 l 
 I ; XI n ; n nI n ; n , 
 i 
 where the first inequality due to Proposition , in the Appendix it successively to the 
 , which can be done because they satisfy . This . 
 The fact that , the definition of and the condition imply that , for all N . The latter together with Assumption 
 that . 
 On the other hand , together with the fact that for , ,.., 
 to 
 y . 
 The latter the 
 . 
 b , a 
 Supposing th order , it that 
 . 
 Proposition with a ,,, according to the in and in , we readily obtain that , a , 
 y . 
 Thus the causality condition . This the proof . 
 We now state a technical lemma which is akin to , Theorem but for one sided , the proof of which can be found in Appendix A . 
 Lemma . Let be a stationary one sided source and suppose the distortion feasible , and satisfy Assumption and the condition 
 ,, N : , ,. 
 Then 
 . 
 N 
 Next , we propose a possible definition of general enough to encompass the asymptotic fidelity criteria by . For that purpose , we need to define 
  
 and require the distortion feasible to satisfy the following assumption : 
 Assumption . The distortion feasible , can be expressed as 
 , , : , , Do 
 k 
 for some non negative distortion . Moreover , the distortion feasible 
 set for one sided , , the form 
  
 N 
 Notice that with such construction , does not necessarily satisfy Assumption . Also , the distortion feasible , with the specific form given by do not necessarily satisfy the condition . 
 This definition , based on the limit of a sequence of distortion , is clearly capable of the general asymptotic single letter criteria of while satisfying Assumption . Recall that , as shown in Lemma , it is not possible to do this with the distortion feasible set from , given by Definition . In addition , the construction of provided by for several specific criteria commonly found in the literature , such as the one in and in the definition of a rate distortion achievable pair in ,. . 
 We are now in the position to provide two independent that are sufficient to ensure the proof is given in Appendix A . 
 Lemma . Consider the same given in the statement of Lemma . If , in addition , any of the two following 
 For every , there such that , 
  
 is continuous and Assumption , 
 then 
 . 
 N 
 B . Main Result 
 With the above , we can state the main result of this section , akin to Theorem but for one sided and for the corresponding causality condition given by the proof is in Appendix A : 
 Theorem . Let the source x be a one sided stationary th order process and suppose that , where and are as defined in and , respectively . Furthermore , suppose that the distortion feasible , , satisfy Assumption and The shift invariance condition : 
 . 
 The condition given by . 
 The first condition : There a pair of random 
 and such that . 
 Then the minimization in the definition of in can be restricted to of 
 x , y with which , in addition , satisfy , . N 
 aside the obvious difference between Theorem and Theorem from the fact that the former as one sided and the latter two sided , it is worth drawing a parallel between these two . The requirement of Assumption in Theorem is than the requirement of to conform to Definition in Theorem . Thus , Theorem for a class of fidelity criteria . The assumption that can be seen as the 
 equivalent of condition in Theorem to the setting of one sided . The same is true with condition in Theorem with respect to condition in Theorem . However , no are stated in which suffice for condition in Theorem to hold . In contrast , we have provided Lemma , which two independent under which is satisfied . The other in Theorem differ from those in Theorem . Condition in Theorem is than condition in Theorem . Condition in Theorem is absent in Theorem , and is in our proof as a consequence of the transient behavior from treating one sided see Theorem in Section . 
 Remark . Among the distortion criteria which satisfy the of Theorem , we find the family of asymptotic single letter of by , , : , 
 . Recall that this class of distortion criteria cannot be expressed 
 a distortion feasible set to Definition , and hence it is not covered by Theorem . 
 N 
 As pointed out by Remark in the Introduction , if now one that x is the positive time part of a two sided stationary process , then the fact that does not guarantee 
 that y causally on x . For the latter to hold in this situation , it is that . This that for this case , the definition of the causal as stated in needs to be extended to 
 strong 
 x , y 
 x , y 
 Notice that when the source a negative time part , to , and thus strong becomes equal to . 
 The above raise the question of whether Theorem can be extended for the case in which the one sided source is the positive time part of a two sided stationary process . This considering strong instead of , or , equivalently , the strong causality constraint instead of the short causality constraint . 
 It turns out that Theorem can indeed be extended for this situation , thanks to the following proposition , the proof of which can be found in Section A . 
 Proposition . Suppose that x is the positive time part of a two sided that 
 the one sided causality condition , i . e ., 
 x . 
 Then there or , equivalently , one can construct a one sided random process such that 
  
  
 N 
 Thanks to Proposition , we have the following extension of Theorem . 
 Theorem Extension of Theorem . Let the source x be the positive time part of a two sided stationary th order process . Under the same made in the statement of Theorem , the in the definition of can be restricted to of 
 which satisfy and such that and . Moreover , 
 . N 
 Proof . The only difference between the of and is that they consider the causality and , respectively . First , notice that 
  
 and thus strong . On the other hand , from Theorem , the yielding can be carried out considering only of satisfying and 
 n , x , y . But as a consequence of Proposition , for every such x , y , there a process such that and 
 , 
 , 
 . 
 This that the minimization associated with strong can be restricted to satisfying , and proving the first claim of the theorem and that strong . 
 The latter and the reverse inequality that strong , concluding the proof . 
 C . Correspondence Between and 
 The purpose of this section is to establish a correspondence between in . As we discuss next , drawing an appropriate comparison between these two causal two to the definition of already on page . 
 The first modification of extending to account for arbitrary fidelity criteria in an arbitrary distortion feasible set P , . 
 The second modification is necessary in order to make a lower bound to the corresponding operational data rate . To see why , it is necessary to recall how lower the operational data rate of x and it as y . For this purpose , let be the random binary sequence produced by the from time let be the length of in . Since the code must be uniquely , the bit string the Kraft inequality , § . . In general , can be by not only but also x , and thus 
 a b 
 E , 
 where a from , Theorem . . and is a consequence of the data inequality , Theorem . . . Thus , lower the operational data rate E as tightly 
 as if and only if 
 y . 
 This chain , combined with the causality constraint 
 y , 
 from Proposition in the Appendix that 
 y , 
 which is precisely the causality constraint for one sided . But , as we have shown in and , such causality constraint is , in general , incompatible with the joint of 
 . As a consequence , since such joint is by , chain cannot hold . This that when the causality constraint for a two sided source established by is 
 , is a lower bound to the operational data rate than . 
 Following these , we propose here the following definition of . 
 Definition An and More General Definition of . For any sided 
 stationary source , redefine the causal stationary in , Definition as 
 , lim 
 ; yn 
 , y : x , y Q n 
 x , y 
 where Q is the set of all one sided jointly stationary random , and is the set of all of random which satisfy the causality constraint . 
 We can now state the following corollary of Theorem , the proof of which can be found in Appendix A : 
 Corollary . Under the same of Theorem , it that 
 . 
 N 
 One important consequence of this result from the fact that , for a th order Gauss stationary source and quadratic distortion , can be found by a convex optimization problem over frequency response of linear time invariant around an additive noise channel , and . 
 The operational relevance of Corollary is that when the latter channel is by an entropy scalar , one a source scheme whose operational rate by at most . sample when operating causally and by at most . sample when operating with zero delay , section . Thanks to Corollary , it turns out that the operational data rate of such scheme within the same with respect to 
 itself . 
  
 We have shown that , in general , the causal information rate distortion function for one sided stationary cannot be by a reconstruction which is jointly stationary with the source . Nevertheless , if the source is th order , then the search for the causal can be restricted to which are jointly stationary with the source from the th sample . This led us to prove that actually with for a large class of distortion criteria . This that for Gauss and quadratic distortion , can be found by the convex optimization problem derived in . It also that for the same source and distortion , a zero delay average data rate exceeding by not more than approximately . sample is achievable with the scheme in . 
 APPENDIX 
 A . 
 Proof of Lemma . We will resort to a contradiction argument , and thus start by supposing that there AD . 
 Since is non negative , there must exist a pair of random and a value 
 for which with equality . Hence and thus . 
 From the definition of AD , any other pair of which exactly as 
 everywhere except on a single positive index , say , in which , P , and 
 , , 
 will also belong to AD and therefore . The latter that , according to 
 Definition , there a pair of l , l with l such that l l and 
 . 
 This , together with the fact that the , satisfy , that 
 , 
 where , l l . Hence , any pair of random with pair wise given by 
 ,, : 
 ¨,¨ 
 ,, : 
 together with the collection of satisfy the of Definition , and thus . However , 
  
 meaning that . This the initial supposition that AD , the proof . 
 Proof of Lemma . From the definition of we have that 
 . 
 By the definition of , we have that o , No , there such that 
 . 
 Now consider the pair such that 
 x 
  
 and build the as in Proposition . The mutual information rate between and 
 y can be upper bounded as 
 . 
 From we also obtain that , for all i , 
  
 that the latter for all i and that , we obtain 
 . 
 Since this inequality is satisfied for all o , o , it that , the 
 proof . 
 Proof of Lemma . Since the of Lemma are satisfied , we have that . Therefore , it to show that . 
 We will first show that . By the definition of we have that 
 . 
 The latter that for all o , there a finite No such that 
 . 
 Also , since , it from that there a that 
 , for all . Since all the latter for all No ,, we obtain 
 . 
 The latter is equivalent to 
 . 
 Since this inequality for all o , o , it that , the first part of the proof . 
 We shall now prove that Assumption and the continuity of . The continuity assumption on that 
  
 od , for all . By the definition of , we have that , for every , o , there a pair of such that 
 . 
 The latter that , for every o , there a finite No such that , 
 . 
 Also , since and from the definition of in , it that there a 
 finite such that 
  
 Thus , , o : 
  
 Since this inequality for all , o , o , and that od when , it that Rit , the proof . 
 Proof of Theorem . Since , it that for all o , there a finite No such that 
 . 
 Thus , for all o and for all No there a pair of such that 
 . 
 The fact that n as one to define the stationary extension of n such that 
 x 
 . 
 The latter , together with the fact that that for 
 , ,... 
 Starting from we build the as in Proposition . From this construction 
 and the assumption on the distortion feasible given by , we have that 
 , 
 and , from , that 
 . 
 Then , for any , ,.. , if we define , we have 
  
 . 
 Dividing both sides obtain 
 . 
 Now suppose a random variable uniformly distributed over , ,.. and independent of . Let and define the pair of 
 x ,, , ,... a 
 y ,, , ,... b 
 A similar construction , for two sided , was in the proof of , Theorem seemingly for the first time , building upon . The same idea was in the proof of , Theorem . . Here we adapt it for the case of one sided . 
 It is easy to verify that from the of , and that 
 are jointly stationary . Thus th order stationary . These imply , in view of and , that the pair may not be causally related according to . However , since see Proposition , we have that does satisfy the 
 causality 
 x . 
 On the other hand , in view of and thanks to Assumption , we have that . 
 Thus , from shift invariance condition , it readily that 
 . 
 Now let 
 , 
 and build such that and the chain 
 , 
 . According to the first condition in the statement of Theorem 
 , and . Now , concatenate with so as to 
 obtain the pair of one sided 
 x ¨ , ,.. , , ,... a ¨ , ,.. , , ,... b 
 Since and , it from Assumption that 
 and . 
 On the other hand , and imply that ¨ , i . e , 
 . 
 The pair further two important . First , Lemma that the of are i . e Second , as we show in Lemma , the mutual information rate of the pair lower , for all . Thus , for every No ,, 
 . 
 Since the existence of a pair of such as which also satisfy 
 is for every o and o , it readily that the search for 
 the on the of can be confined to such , the proof . 
 Lemma . Let be the defined in , which are built from 
 see and see and and the text between these and satisfy and . Then . N 
 Proof . By construction , are jointly stationary . Thus , all that remains to prove is that see Definition . For this purpose , notice that for all i , 
  
  
  
  
  
  
 where all the stem from the chain rule of mutual information , a because I a ; , and is a consequence of the fact that ¨ , and 
 chain . The mutual information in the middle of can be upper bounded as 
  
  
 , 
 , 
 where the are due to the chain rule of mutual information , a because mutual information is non negative , and ¨ . 
 the definition of ¨ ,¨ see we have that , for the case i , 
 , i , 
 , 
 I b , b ; a , a a , a ,, 
 where the random 
 a , n a , t a 
 a , a , i b 
 b , b , i c 
 are so as to streamline the presentation of the following . The between all these is in Fig . . Notice that for these the translate into 
 b a , a a , a , b 
 b a a , a , a , b 
 With this , we can continue from and deduce that 
 , 
  
 I b ; a , a a , a , I b ; a , a b , a , a , 
  
 I b ; a , a a , a , 
  
 I b ; a , a , a , a I b ; a , a 
 t 
  
 I b ; a , a I b ; a , a a , a , 
  
 I b ; a , a 
 t 
  
 where a because mutual information is non negative and is due to and the fact that 
 . 
 Substituting into and then the latter into , we arrive at 
 . 
 Dividing by i and taking the limit as i , we conclude that , for all , 
 , 
 proving the claim that . 
  
 Figure . Schematic representation of the change of in . Each dot one element in the and , with time increasing from left to right . 
 Lemma . Let the defined in , built from and and and the text between these . Then 
 . 
 N 
 Proof . First , notice that , and imply 
 . 
 On the other hand 
  
 , 
 ; 
 , 
 where a is due to the non negativity of mutual information and because . But 
 t 
 ; m 
 , 
 where a from the fact that , for all 
 see . Substituting this into we obtain that for , 
 . 
 The proof is by taking the limit as and substituting in it . 
 Proof of Proposition . From Fact , there such that and which 
 x . 
 Combining with one 
 . 
 d c 
 On the other hand , readily that a 
 . 
 , a 
 Proposition with a ,,, corresponding to the under the in and , we obtain directly , the proof . 
 Proof of Corollary . We will first show that and then that the reverse inequality is true as well . 
 From Theorem . Also , Theorem that the search for 
 can be confined to of which satisfy and such that and . For each such pair , one can construct the pair of 
 as 
 x ¨ a 
 . b 
 This construction that 
  
 since 
 , 
 where the last equality from the fact that and that and the definition of . In addition , the 
 fact that that 
  
 which in turn directly to 
 x ¨ 
 and to 
 . 
 This to 
 . 
 Therefore , for every pair of which the associated with , there another pair which the in the definition of and the same information rate . This that . 
 In order to show that , consider any pair satisfying and such that . Construct the pair of as 
 a 
 x ,, b 
 and let the joint distribution of the pair be such that and 
 y 
 the existence of such pair is by the first condition in the statement of Theorem . From Assumption , this construction 
 . 
 The fact that into 
  
 The th order of that 
  
 da 
 Proposition to and with in the proposition assigned according to the under and , we obtain that 
 y 
 which combined with . In addition , 
  
  
  
 . 
 On the other hand , 
  
 for all . Thus 
  
  
 Hence , for every pair such that there a pair 
 satisfying . This readily that , 
 the proof . 
 B . Other Technical 
 Proposition . For any random a , a , b , b satisfying the 
 a , b a b 
 a , b a b 
 it that 
 I a , a ; b , b I a ; b I a ; b I b ; b 
 Proof of Proposition . The mutual information between a , a and b , b is given by 
 N 
  
 I a , a ; b , b I a , a ; b I a , a ; b b 
  
 I a ; b I a ; b a I a , a ; b b 
  
 I a ; b I a , a ; b b 
  
 I a ; b I a , a , b ; b I b ; b 
  
 I a ; b I a ; b I a , b ; b a I b ; b 
  
 I a ; b I a ; b I b ; b , 
 where all stem from the chain rule of mutual information . 
 Proposition . Let a ,,,. Then 
 a a , a ,. 
 Proof . The chain rule of mutual information 
 N 
 I a ;, I a ; I a ;,. 
 Since mutual information is non negative , it that I a ;, if and only if both I a ; and I a ;, are zero . The proof is by that the statement I u ; is equivalent to the chain u . 
  
 ﻿This paper studies networked control systems closed over noiseless digital channels. We focus on noisy linear timeinvariant (LTI) plants with stationary Gaussian disturbances, Gaussian initial state, scalar-valued control inputs and sensor outputs. For this set-up, we show that the absolute minimal directed information rate that allows one to achieve a prescribed level of performance (not necessarily stationary), over all combinations of encoder-controller-decoder is achieved when the decoder output is jointly Gaussian with the other signals in the system. This directed information rate lower bounds the achievable operational data rates. When restricting our attention to encoder-controller-decoders which make the random processes in the loop (strongly) asymptotically wide-sense stationary, this bound can be expressed in terms of their asymptotic power spectral densities. Then we show that the directed information rate and stationary performance of any such scheme can be achieved when the concatenated encoder, channel, controller and decoder behave as an AWGN channel with LTI filters. We also present a simple coding scheme that allows one to achieve (operational) average data rates that are at most (approximately) 1.254 bits away from the derived lower bound, while satisfying the performance constraint. A numerical example is presented to illustrate our findings.
 Index Terms—Networked control systems; optimal control; average data rate; information theory; signal-to-noise ratio.
 I.	INTRODUCTION
 This paper studies networked control problems for linear time-invariant (LTI) plants where communication takes place over a digital communication channel. Such problems have received much attention in the recent literature [1], [2]. This interest is motivated by the theoretical challenges inherent to control problems subject to data-rate constraints, and by the many practical implications that the understanding of fundamental limitations in such a set-up may have (some applications are illustrated in, e.g. [3]–[6]).
 The literature on networked control systems subject to datarate constraints can be broadly classified into two groups. A first group, which includes [7]–[13], uses approaches that are rooted in non-linear control theory. An alternative approach that uses information-theoretic arguments has been adopted in, e.g., [14]–[20]. A key question addressed by the works in the latter group is how to extend, or adapt if necessary, standard information-theoretic notions to reveal fundamental limitations in data-rate-limited feedback loops. Related results have been published in [21]–[23], where the interplay between information constraints and disturbance attenuation is explored.
 The most basic question in a data-rate limited feedback control framework is whether closed-loop stabilization is possible or not. Indeed, stabilization is possible only if the channel data rate is sufficiently large [11], [24]. These early observations spawned several works that study minimal data rate requirements for stabilization and observability (see, e.g., [16], [25]–[27]). A fundamental result was presented in [15]. For noisy LTI plants controlled over a noiseless digital channels, it is shown in [15] that it is possible to find causal coders, decoders and controllers such that the resulting closed-loop system is mean-square stable, if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. A thorough discussion of this and related work can be found in the survey paper [1]. Recent extensions, including stabilization over time-varying channels, are presented in [28]–[32].
 It is fair to state that, for LTI plants, stabilization problems subject to data rate constraints are well-understood. However, the question of what is the best closed-loop performance that is achievable with a given data rate is largely open. Such problems are related to causal (and zero-delay) rate-distortion problems (see, e.g., [33]–[37]). In the latter context, the best results are, to our knowledge, algorithmic in nature, derived for open-loop systems and, at times, rely on arbitrarily long delays [33], [34]. It thus follows that the results in the above references are not immediately applicable to feedback control systems.
 In the rate-constrained control literature, lower bounds on the mean-square norm of the plant state have been derived which show that, when disturbances are present, closed-loop performance becomes arbitrarily poor when the feedback data rate approaches the minimum for stability [1], [15]. This result holds no matter how the coder, decoder and controller are designed. Unfortunately, the bounds in [1], [15] do not seem to be tight in general. In contrast, for fully observable noiseless LTI plants with bounded initial state, [18] shows that one can (essentially) recover the best non-networked LQR performance with data rates arbitrarily close to the minimum average data rate for stabilization. Other results valid in the noiseless or bounded-support noise cases can be found in, e.g., [38]–[40].
 Relevant work on optimal control subject to rate-constraints, and dealing with unbounded support noise sources, include [1], [17]. Those works establish conditions for separation and certainty equivalence in the context of quadratic stochastic problems for fully observed plants, when data rate constraints are present in the feedback path. It is shown in [1] that, provided the encoder has a recursive structure, certainty equivalence and a partial separation principle hold. The latter result is relevant. However, [1] does not give a practical characterization of optimal encoding policies. The results reported in [17] share a similar drawback. Indeed, performance-related results in [17] are described in terms of the sequential ratedistortion function, which is difficult to compute in general. Moreover, even for the cases where an expression for such function is available, it is not clear whether the sequential rate-distortion function is operationally tight [17, Section IVC]. Partial separation in optimal quantized control problems has been recently revisited in [41].
 Additional results related to the performance of control systems subject to data-rate constraints are reported in [38], [42] and [43]. In [38], noiseless state estimation problems subject to data rate constraints are studied. The case most relevant to this work uses an asymptotic (in time) quadratic criterion to measure the state reconstruction error. For such a measure, it is shown in [38] that the bound established in [15] is sufficient to achieve any prescribed asymptotic distortion level. This is achieved, however, at the expense of arbitrarily large estimation errors for any given finite time. On the other hand, [43] considers non-linear stochastic control problems over noisy channels, and a functional (i.e., not explicit) characterization of the optimal control policies is presented. In turn, [42] presents a computationally-intensive iterative method for encoder and controller design for LTI plants controlled over noisy discrete memoryless channels. Conditions for separation and certainty equivalence are also discussed in [42] for some specific set-ups.
 In this paper, we focus on the feedback control of noisy LTI plants, with one-dimensional control inputs and sensor outputs, that are controlled over an error-free (and delay-free) digital channel. The plant initial state xo and the disturbance process d are assumed to be jointly Gaussian. By considering causal but otherwise unconstrained coding schemes, we study the minimal (operational) average data rate, say R(D), that guarantees that the steady-state variance of an error signal is below a pre-specified level D > 0. Our approach is mainly information-theoretic and thus some of our findings complement previous results in [21], [44].
 We have previously addressed this problem in [45] and [46]. In [45] we have shown that R(D) is lower bounded by the directed information rate I8(y ? u), where y and u are the input to the encoder and output from the decoder, respectively. Then, for a restricted class of encoder-decoder pairs, called “independent coding schemes” (characterized by behaving as a unit transfer function form y to u plus coloured additive wide-sense-stationary noise independent of (xo,d)) we showed that I8(y ? u) is minimized only if y and u are jointly Gaussian. For the latter case, it was shown that this information rate can be written as  where Su is the asymptotic power spectral density of u and sn2 is the asymptotic variance of the error in estimating u(i) from u(1),··· ,u(i - 1) and y(1),··· ,y(i - 1). It was also shown that an operational average data rate exceeding I8(y ? u) by less than (approximately) 1.25 bits/sample was achievable by using an entropy-coded dithered uniform quantizer (ECUDQ). In [46], and also for the class of independent coding schemes, we then derived the minimum feedback directed information rate I8(y ? u) for an asymptotic quadratic performance measure.
 An important departure of this paper from our previous work is that here the encoder-decoder is not constrained to exhibit a unit transfer function from y to u (i.e., it is not restricted to be an independent coding scheme). As a result, the encoderdecoder pair also performs as a controller for the plant, and thus reaching R(D) means designing the jointly optimal encoder-controller-decoder system. For this broader context, the main contributions of this paper can be summarized as follows:
 1)	We show that, for any given performance D, I8(u ? y) is minimized if and only if u and y are jointly Gaussian (in Section III).
 2)	For the jointly Gaussian case, we also show that I8(y ? for the class of coding
 schemes yielding strongly asymptotic wide-sense stationary (SAWSS)  processes u and y. This class (namely, SAWSS schemes) is much larger than (and includes) the family of independent coding schemes. For instance, it allows for encoder-controller-decoders built using any combination of linear and non-linear causal mappings, provided the SAWSS behaviour is reached. This result is presented in Section III.
 3)	In Section IV we show that the asymptotic rateperformance of any Gaussian SAWSS scheme can be achieved also by a set of LTI filters and an exogenous additive white Gaussian noise.
 4)	In Section V we use the latter result to show how to minimize I8(y ? u) within the class of SAWSS schemes by solving a related stochastic control problem subject to an SNR constraint in the feedback channel (Section V). This same SNR-constrained problem has been addressed in [47] and in [48] (for different but related networked control problems subject to SNR constraints, see, e.g., [49]–[51]).
 5)	Exploiting the fact that the latter problem was shown to be convex in [35], [47] and the convenient procedure to solve it presented in [48], we explain how to design a simple encoder-controller-decoding scheme which performs within (approximately) 1.25 bits/sample of R(D) within the class of SAWSS schemes, for any prescribed performance D > 0. This result is presented in Section VI.
 To illustrate the above results, section VII presents a numerical example. Concluding remarks are presented in Section VIII.
 Notation: R denotes the set of real numbers, R+ denotes the set of strictly positive real numbers,  ,
 N0 ? {0,1,···}. In this paper, log stands for natural logarithm, and |x| for the magnitude (absolute value) of x. We work in discrete time and use k for the time index. An LTI filter X is said to be proper (i.e., causal) if its transfer function X(z) remains finite when z ? 8, and it is said biproper if it is proper and limz?8 X(z) = 0? . We define the set U8 as the set of all proper and stable filters with inverses that are also stable and proper.
 In this paper, all random processes are defined for k ? N0. All random variables and processes are assumed to be vectorvalued, unless stated otherwise. Given a process x, we denote its kth sample by x(k) and use xk as shorthand for x(0),...,x(k). We say that a random process is a secondorder one if it has first- and second-order moments that are bounded for every k and that also remain bounded as k ? 8.
 Gaussian processes are, by definition, second-order ones [52]. We use E[·] to denote the expectation operator. A process x is said to be asymptotically wide-sense stationary (AWSS) if and only if there exist µx and a function Rx(t), both independent of the statistics of x(0), such that limk?8 E[x(k)] = µTx and limk?8 E[(x(k + t?) - E[x(k + t)])(x(k) - E[x(k)]) ] =
 Rx(t) for every t N0. The steady-state spectral density of an AWSS process is denoted by Sx (and defined as the Fourier transform of Rx(t) extended for t < 0 according to Rx(t) = Rx(-t)T). The corresponding steady-state covariance matrix is denoted by Px, and sx  ? trace{Px}. Jointly second-order and jointly AWSS processes are defined in the obvious way. The covariance matrix of a random sequence   is denoted by  ···	E[u1n])T], where u1n ? [u(1) u(n)]T. For a matrix A, the notation [A]k,l refers to the element of A on the k-th row, l-th column, while ?m(A) and ?M(A) denote the eigenvalues of A with the smallest and largest magnitude, respectively. Two sequences of matrices , with An,Bn ? Rn×n, are said to be asymptotically equivalent (see [53]) if the following two conditions are met:
 for some M < 8. Appendix B recalls some useful notation and results from Information Theory [54].
 II.	PROBLEM SETUP
 This paper focuses on the networked control system (NCS) of Fig. 1. In that figure, P is an LTI plant,e u ? R is the control input, y ? R is a sensor output, e ? Rnd is a signal related to closed-loop performance, and d ? Rn is a disturbance. The feedback path in Fig. 1 comprises a digital channel and thus quantization becomes mandatory. This task is carried out by an encoder whose output corresponds to a sequence of binary words. These words are then transmitted over the channel, and mapped back into real numbers by a decoder. The encoder and decoder also embody a controller for the plant.
  
 	decoder	encoder
 Fig. 1. Control system networked over a digital channel.
 We partition P in a way such that
 	  ,	(3)
 where Pij are proper transfer functions of suitable dimensions. We will make use of the following assumptions.
 Assumption 2.1: P is a proper LTI plant, free of unstable hidden modes, such that the open-loop transfer function from u to y (i.e., P22 in (3)) is single-input single-output and strictly proper. The initial state of the plant, say xo, and the disturbance d are jointly Gaussian, d is zero-mean white noise with unit variance Pd = I, and xo has finite differential entropy (i.e., the variance of xo is positive definite). ?
 We focus on error-free zero-delay digital channels and denote the channel input alphabet by C, a countable set of prefix-free binary words [54]. Whenever the channel input symbol yc(k) belongs to C, the corresponding channel output is given by uc(k) = yc(k). The expected length of yc(k) is denoted by R(k), and the average data rate across the channel is thus defined as2
 	R .	(4)
 We assume the encoder to be an arbitrary (hence possibly non-linear and time-varying) causal system such that the channel input yc satisfies
 	 ,	(5)
 where ak is shorthand for a(0),...,a(k), SE(k) denotes side information that becomes available at the encoder at time instant k, and Ek is a (possibly non-linear and time-varying) deterministic mapping whose range is a subset of C. Similarly, we assume that the decoder is such that the plant input u is given by
 	 ,	(6)
 where SD(k) denotes side information that becomes available at the decoder at time instant k, and Dk is a (possibly nonlinear and time-varying) deterministic mapping.
 Assumption 2.2: The systems E and D in Fig. 1 are causal, possibly time-varying or non-linear, described by (5)–(6). The side information sequences SE and SD are jointly independent of (xo,d), and the decoder is invertible upon knowledge of ui and , i.e., ?i ? N0, there exists a deterministic mapping g?i such that .
 The assumption on the side information sequences is motivated by the requirement that (causal) encoders and decoders use only past and present input values, and additional information not related to the message being sent, to construct their current outputs (see also page 5 in [55]). On the other hand, if, for some encoder E and decoder D, the decoder is not invertible, then one can always define an alternative encoder and decoder pair, where the decoder is invertible, yielding the same input-output relationship as E and D, but incurring a lower average data rate [45, Lemma 4.1]. Accordingly, one can focus, without loss of generality, on encoder-decoder pairs where the decoder is invertible.
 In this paper, we adopt the following notion of stability (see also [56]):
 Definition 2.1: We say that the NCS of Fig. 1 is asymptotically wide-sense stationary (AWSS) if and only if the state of the plant x, the output y, the control input u, and the disturbance d, are jointly second-order AWSS processes. ?
 Remark 2.1: The notion of stability introduced above is stronger than the usual notion of mean-square stability (MSS) where only supk?N0 E{x(k)x(k)T} < 8 is required to hold
 (see, e.g., [15]).	?
 In what follows we will be interested on expressing information rates in terms of the asymptotic spectral densities of the processes in the system. To do so, it will be necessary to use a stronger notion of asymptotic wide-sense stationarity:
 Definition 2.2 (Strongly AWSS Process): An AWSS scalar random process u1 which converges to a WSS process u¯ is said to be strongly asymptotically wide-sense stationary (SAWSS) if the sequences  are asymptotically equivalent. Likewise, a NCS is said to be SAWSS if the covariance and cross-covariance matrices of all the processes in it are asymptotically equivalent to their stationary counterparts. ? Clearly an SAWSS NCS is also AWSS, but the converse may not be true, in general. The same holds for processes.
 The goal of this paper is to characterize, for the NCS of Fig. 1, the minimal average data rate R that guarantees a given performance level as measured by the steady-state variance of the output e. We denote by Dinf the infimal steady-state variance of e that can be achieved by setting u(k) = Kk(yk), with Kk being a (possibly non-linear and time-varying) deterministic mapping, under the constraint that the resulting feedback loop is SAWSS. With this definition, we formally state the problem of interest in this paper as follows: Find, for any D ? (Dinf,8) and whenever Assumption 2.1 holds, 
 R(D) ? inf2 R,	(7) se=D
 where se2 ? trace{Pe}, Pe is the steady-state covariance matrix of e, and the optimization is carried out with respect to all encoders E and decoders D that satisfy Assumption 2.2 and render the resulting NCS SAWSS.
 It can be shown that the problem in (7) is feasible for every D ? (Dinf,8) (see Appendix A). If D < Dinf, then the problem is clearly unfeasible. On the other hand, achieving D = Dinf incurs an infinite average data rate, except for very special cases. We will thus focus on D ? (Dinf,8) without loss of generality.
 The remainder of this paper characterizes R(D) within a gap smaller than (approximately) 1.254 bits per sample. Such characterization is given in terms of the solution to a constrained quadratic optimal control problem. We also propose encoders and decoders which achieve an average data rate within the above gap, while satisfying the performance constraint on the steady-state variance of e.
 III.	AN INFORMATION-THEORETIC LOWER BOUND ON R(D)
 This section shows that a lower bound on R(D) can be obtained by minimizing the directed information rate across an auxiliary coding scheme comprised of LTI systems and an additive white Gaussian noise channel with feedback. The starting point of our presentation is a result in [45], which does not require any stationarity assumptions.
 Theorem 3.1 (Theorem 4.1 in [45]): Consider the NCS of
 Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. Then,
 R 
 where I(·; ·|·¦) denotes conditional mutual information (see Appendix B).
 The quantity I8(y ? u) corresponds to the directed information rate [55] across the source coding scheme of Fig. 1 (i.e., between the input y and the output u of the source coding scheme). Note that I8(y ? u) is a function of the joint statistics of y and u only.
 We will now derive a lower bound on the directed information rate across the considered coding scheme, in terms of the directed information rate that would appear if all the involved signals were Gaussian.
 Lemma 3.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If, in addition, (xo,d,y,u) are jointly second-order, then I8(y ? u) = I8(yG ? uG), where yG and uG are such that (xo,d,yG,uG) are jointly Gaussian with the same first- and second-order (cross-) moments as (xo,d,y,u).
 Proof: Our claim is due to the following chain of equalities and inequalities: k-1 I(u(i);yi|ui-1) (=a) I(xo,dk-1;uk-1) i=0
 (9) (=b) I(xo,dk-1;uGk-1) (=c) ?ki=0-1 I(uG(i);yGi |uGi-1),
 where (a) follows from Assumption II.2 and Lemma B.4 with
 (x1,o,d1) = (xo,d), y¯ = y, u¯ = u and (x2,o,d2) = (SD,SE), and (b) follows from Lemma B.1 in Appendix B. In turn, equality (c) in (9) holds since
  
 (=b) ?k-1 I(xo,di;uG(i)|ui-1)
 i=0
 (=c)k-1 [I(xo,di-1,yGi ;uG-(i)|ui-1)	|	]
 i=0
 I(yGi ;uG(i) ui-1,xo,di)
  = ?	[I(yG;uG(i)|u -1) + I(xo,d -1;uG(i)|u -1,yG)]
 i=0
 where (a), (c) and (e) hold from the chain-rule of mutual information, (b) stems from (52b) in Theorem B.3, (d) is a consequence of   being a deterministic function of  , and (f) follows from (52a) in Theorem B.3. This completes the proof.	 
 It follows from Theorem 3.1 and Lemma 3.1 that, in order to bound R(D) from below, it suffices to minimize the directed information rate that would appear if the source coding scheme of Figure 1 (including the channel) were replaced by a block which yields signals y and u which are jointly Gaussian. Again, this holds irrespective of the stationarity of the signals involved.
 Now we can relate the directed information rate from yG to uG to their associated spectral densities. For that purpose (and from here on), we shall focus on encoder-decoder pairs which render the NCS SAWSS.
 Lemma 3.2: Assume that u and y are jointly Gaussian AWSS processes and that u is SAWSS and such that  , for some d > 0. Then,
 	 	(10)
 where Su¯ is the steady-state power spectral density of u, and sn2 is the steady-state variance of the Gaussian AWSS sequence of independent random variables n, defined via n(k) ? u(k) - uˆ(k), uˆ(k) ? E[u(k)|yk,uk-1]. (11)
 Proof: We start by noting that, since (u,y) are jointly Gaussian AWSS processes, a simple modification of the proof of Theorem 2.4 in [58, p. 20] yields the conclusion that n is also Gaussian and AWSS. To proceed, we note that
 I(u(i);yi|ui-1) (=a) h(u(i)|ui-1) - h(u(i)|yi,ui-1)
 (=b) h(u(i)|ui-1) - h(n(i) + uˆ(i)|yi,ui-1)
 (=c) h(u(i)|ui-1) - h(n(i)|yi,ui-1)
 	(=d) h(u(i)|ui-1) - h(n(i)),	(12)
 where (a) follows from Property 1 in Appendix(b) follows from the definition of uˆ, (c) follows from Property 2 in and the fact that, by construction, uˆ(i) is a deterministic function of (yi,ui-1), and (d) follows from Property 3 in Appendix n(i) is independent of (yi,ui-1). Now, (12) and the definition of directed information rate yields
  
 where (a) follows from Properties 3 and 4 in Appendix and the fact that, by construction, n(k) is independent of nk-1, and (b) follows from Lemma B.5 in the Appendix and the fact that n is Gaussian AWSS and that u is SAWSS with
  , for some d > 0. The result is now
 immediate from (13).	 
 Lemma 3.2 characterizes the directed information rate between Gaussian SAWSS processes in terms of the spectrum of the process towards which the mutual information is directed. Lemma 3.2 generalizes Theorem 4.6 in [59], where the author calculates directed information rates between Gaussian processes that are linked by an additive white Gaussian noise channel with unit transfer function. Thus, Lemma 3.2 generalizes the latter theorem by considering a channel with coloured Gaussian noise and with an input-output transfer function different from unity.
 Remark 3.1: It is worth mentioning that simply requiring u to be AWSS (instead of SAWSS with  for all n) may yield a left-hand side of (56) smaller than the term on its right. Indeed, to the best of our knowledge, the only result related to (56) available for AWSS processes is [60, Lemma 4.3], which yields an inequality (=) in (56) instead of equality. To see why AWSS is not enough for equality, consider the following example:
 Let G be an unstable LTI system with zero initial state that can be stabilized by unit feedback. Let q be a white Gaussian process with zero mean and variance sq2. Construct u as
  . This corresponds to putting G in a unit-
 gain closed loop with feedback AWGN noise q. In this case, u can be written as will be AWSS with asymptotic power spectral density (PSD) Su¯(ej?) = sq2/|1 + G|2. Then, the entropy rate of the asymptotic process u¯ is
  
 where the last equality follows from Jensen’s formula (Bode Integral Theorem) and pi are the poles of G.
 However, it turns out that, for all , for some lower triangular matrix Fn with ones along its main diagonal. Therefore,
  
 and hence   which is strictly smaller than   (compare with (14)).
 Thus, the notion of AWSS is too weak to provide the equality we need. 	?
 x
 e'
 d	o
 	P	
  		
 		 v	y
 	z	
 -1
  		
 				
 u'
 q'
 Fig. 2. Auxiliary LTI system that arises when the encoder and decoder of Fig. 1 are replaced by proper LTI filters F and L and an additive white noise channel with (one-step delayed) feedback.
 IV.	OPTIMALITY OF LTI CODING SCHEMES
 In this section we show that, for any given performance D, the directed information rate achieved by any SAWSS scheme can also be achieved by an encoder-decoder pair built with only LTI filters and an exogenous source of white Gaussian noise. To that end, we begin by noting that Theorem 3.1 and Lemma 3.1 readily imply that for any encoder and decoder satisfying Assumption 2.2, and rendering the resulting NCS SAWSS, R = I8(yG ? uG), where (yG,uG) are such that (xo,d,uG,yG) are jointly Gaussian with the same first- and second-order (cross-) moments as (xo,d,u,y). We also note that the adopted performance measure is quadratic. The above observations imply that one can always match (or improve) the rate-performance trade-off of a given encoder-decoder pair by choosing, instead, an encoder and a decoder which, besides rendering the NCS SAWSS, yields (u,y) jointly Gaussian with (xo,d). Since the plant initial state and the disturbance are Gaussian, and the plant is LTI, one possible way of achieving such pair of signals (u,y) is by using the LTI feedback architecture of Fig. 2. We formalize these observations below.
 Define the auxiliary LTI feedback scheme of Fig. 2, where everything is as in Fig. 1 except for the fact that we have replaced the link between the plant output y and the plant input u by a set of proper LTI filters, F and L, and an additive noise channel with (one-step delayed) feedback and noise q' such that
 	u' = Fw',	w' = v' + q',	v' = Ldiagz-1,1[w'],
 y'
 (15)
 where z-1 stands for the unit delay. In Fig. 2, we assume that the plant P, the disturbance d and the plant initial state xo satisfy Assumption 2.1, that the initial states of F,L and of the delay are deterministic, and that q' is zero mean Gaussian white noise, independent of (xo,d), and having constant variance sq2'.
 In Fig. 2, we have added apostrophes (as in e') to all symbols that refer to signals that have a counterpart in the scheme of Fig. 1 with possibly different statistics. To streamline our presentation, we adopt the convention that, whenever we refer to the auxiliary feedback system of Figure 2, it is to be understood that we are implicitly working under the assumptions stated in the above paragraph.
 Theorem 4.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If D ? (Dinf,8), then
 R 
 (16)
 where the optimization defining ?'u(D) is performed with respect to all proper LTI filters L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of Fig. 2 with
 F = 1 internally stable and well-posed, and Su' and se2' denote the steady-state power spectral density of u' and the steady state variance e' in Fig. 2, respectively.
 Proof: Denote by CD the set of all encoders E and decoders D such that, when placed at each end of the discrete channel of Fig. 1 satisfy Assumption 2.2, render the NCS of Figure 1 SAWSS and guarantee that se2 = D. Also, define CD,G to be the set of encoder-channel-decoder triplets which consider a unit-gain (i.e., continuous alphabet) noisy channel with (possibly time-varying) linear filters as encoders/decoders which render u and y jointly Gaussian and yield se2 = D. (Since D > Dinf, both CD and CD,G are non empty; see Appendix A.) The fact that D > Dinf guarantees the existence of at least one encoder-decoder pair in CD, say (E˜,D˜), producing processes y˜ and u˜, such that its average data rate
 R˜ is bounded. From Theorem 3.1, it follows that
 R˜
 (17)
 where the last equality is a consequence of Lemma 3.2. Since y˜G and u˜G are jointly SAWSS, their statistics (and corresponding variance of e˜G) are yielded by an encoderchannel-decoder triplet in CD,G, consisting of a sequence of linear mappings Lk, k ? N0, such that
 	 ,	(18)
 where n˜G(k) is zero-mean Gaussian and independent of
  . Since Lk is linear and causal, it induces a relationship between  and  which can be expressed
 as
 	u˜kG = Bkn˜kG + Gky˜Gk ,	k ? N,	(19)
 where {Bk} and {Gk} are sequences of lower triangular matrices such that, ?k ? N, Bk-1 and Gk-1 are the top left corners of Bk and Gk, respectively. Now, since (y,˜ u˜) are jointly SAWSS, the covariance matrices  , and associated cross-covariance matrices are asymptotically equivalent to Toeplitz matrices as well. Using the transitivity of asymptotic equivalence for products and sums of matrices in [53], it is possible to show that {Bk} and {Gk} must be asymptotically equivalent to a sequence of lower triangular Toeplitz matrices as well (see, e.g. [53, Section 6.3]). Also, the associated limiting Lk renders the resulting NCS internally stable and well-posed (otherwise the underlying encoder and decoder would not be in CD,G), and defines the steady-state spectrum Su˜ of u˜ and the steady-state variances  of both n˜ and e˜. (Here, we use the fact that n˜ is also SAWSS; see the proof of Lemma 3.2.)
 Now, consider the auxiliary LTI feedback system of Figure 2 described before. Assume that F = 1, that L reproduces the steady-state behaviour of Lk in (18) (i.e., L is comprised of two LTI filters, having as impulse responses the last rows of
 variance equal tolimk?8 Bk and lims2 k(?8see previous paragraph). With the aboveGk, respectively), and that q' has a
 n choices for F, L and q', and since the matrix representations of L for each k are asymptotically equivalent to the sequence
   , it follows that the feedback system of Figure 2 is internally stable and well-posed. In particular, it yields the an SAWSS set of processes in the closed-loop such that the plant input u' admits a steady-state power spectral density Su' that, by construction, equals Su˜ in the previous paragraph. Similarly, the error signal e' in Figure 2 admits a steady-state variance se2' that equals . It thus follows from Lemma 3.2
 that
 	 	(20)
 We thus conclude that, for any encoder and decoder (E˜,D˜) in CD producing processes y˜ and u˜, there exist a proper LTI filter L and a Gaussian white noise source q' such that, when F = 1, the mutual information rate I8(y' ? u') in Fig. 2 equals I8(y˜ ? u˜) while achieving . Recalling that R(D) is the infimum of R over CD, it follows from (17) and (20) that R(D) is lower bounded by ?'u(D) as in (16), completing the proof.	 
 Theorem 4.1 states that a lower bound on the minimal average data rate that guarantees a given performance level, can be obtained by solving an optimization problem which is stated for the auxiliary LTI feedback system of Fig. 2, where communication takes place over an additive white Gaussian noise channel with feedback.
 We finish this section by deriving a simpler lower bound on R(D). To that end, we will first state an auxiliary result.
 Lemma 4.1: Consider the LTI feedback system of Fig. 2. Fix sq2' ? R+ and define (whenever the involved quantities exist)
  
 where Sw' is the steady-state power spectral density of w'. If the pair (F,L) = (F(0),L(0)) renders the feedback system of Fig. 2 internally stable and well-posed, then there exist a second pair of filters, namely (F,L) = (F(1),L(1)), with F(1) biproper, that also defines an internally stable and well-posed feedback loop, leaves the steady-state power spectral density of e' unaltered, and is such that for any (arbitrarily small)  .
  
 Fig. 3. Auxiliary feedback system for stability analysis.
 Proof: Consider Fig. 2 and the partition for P in (3). Introduce proper transfer functions Ly and Lw such that L = [Lw Ly ] (see (15)). A standard argument [61] shows that the feedback system of Fig. 2 is internally stable and well-posed if and only if the transfer function T between [q' d n1 n2]T and [e' y' w' u']T in Fig. 3 is stable and proper. It is straightforward to see that T has the structure shown in the unnumbered equation at the top of the next page, where
 	S ? (1 - Lwz-1 - P22FLy)-1 .	(23)
 We will write T(i) to refer to the matrix T that arises when (F,L) = (F(i),L(i)), i ? {0,1}. Similarly, Ly(i) and L(wi) refer to the components of L, when L = L(i). Set
 	F(1) = zn0F(0)X-1,	L(1)y	= z-n0L(0)y ,
 (24)
 L(1)w = z(1 - (1 - L(0)w /z)X-1),
 where1. Given (24) and the fact thatn0 is the relative degree ofXF? U(0),8X, it follows that? U8 and X(8F) =(1)
 is biproper and that
 T(1) = {zn0I,zn0I,X,zn0I}T(0)×
 diag{I,z-n0I,z-n0I,z-n0I}.
 The	definition	of n0	and X	guarantees	that	the	pair
 (F(1),L(1)) renders the feedback system of Figure 2 internally stable and well-posed if and only if (F(0),L(0)) does so. It also immediately follows that (F(1),L(1)) defines the same stationary spectral density for e' than (F(0),L(0)).
 To complete the proof, we now propose specific choice for X. Denote by w'(i) the signal w' that arises when (F,L) = (F(i),L(i)). Write Sw'(0) = |?w'(0)|2, where ?w'(0) is stable, biproper, and has all its zeros in {z ? C : |z| = 1}. Denote by c1,...,cnc the zeros of ?w'(0) that lie on the unit circle. Define, for ? ? (0,1),
 nc
 	?˜w'(0) ? ?w'(0) ?z(z - ci)-1,	c
 i=1
 X? ??˜w'(0))-1 ?˜w'(0)(8)?n z(z - ?ci)-1.
 i=1
 By construction, X?(8) = 1 and X? ? U8 for every ? ? (0,1). It now follows, by proceeding as in the proof of Theorem 5.2 in [45], that there exists ? ? (0,1) such that setting X = X? in (24) guarantees that (F(1),L(1)) is such that (22) holds for any ? > 0.  
   ,
 Corollary 4.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If D ? (Dinf,8), then
 R
 '
 (25)
 where the optimization defining ?'(D) is performed with respect to all proper LTI filters F and L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of
 Fig. 2 internally stable and well-posed, and sv2' and se2' denote the steady-state variances of v' and e' in Fig. 2, respectively.
 Proof: Consider the LTI feedback system of Figure 2 and recall the definition of both ?'u(D) and ?'w in (16) and (21). Since D > Dinf, the problem of finding ?'u(D) is feasible (see Appendix A). Thus, for any ? > 0, there exist a proper LTI filter L?, and s?2' ? R+, such that se2' = D and ?'u(D) + ? = ?'w(1,L?,s?2'), (26)
 where we have used the fact that u' = w' whenever F = 1. On the other hand, Lemma 4.1 guarantees that there exists a pair of proper filters (F¯?,L¯?), with F¯? biproper, such that the auxiliary feedback system of Figure 2 is internally stable and well-posed,
  se2' 
 where the inequality follows from the definition of ?'(D). Since (28) holds for any ?,? > 0, our claim is now immediate from Theorem 4.1.	 
 Corollary 4.1 shows that a lower bound on R(D) can be obtained by first characterizing ?'(D), i.e., by first characterizing, for the auxiliary LTI feedback system of Fig. 2, the minimal steady-state SNR ?' = sv2'/sq2' that guarantees that the steady-state variance of the error signal e' is upper bounded by D. Section VI-A discusses how to obtain a numerical approximation to ?'(D).
 V.	AN UPPER BOUND ON R(D)
 This section shows that it is indeed possible to achieve any distortion level D ? (Dinf,8) while incurring an average data rate that exceeds the lower bound on R(D) in Corollary 4.1 by less than (approximately) 1.254 bits per sample.
 Definition 5.1: The source coding scheme described by (5) and (6) is said to be linear if and only if, when used around an error-free zero-delay digital channel, is such that its input y and output u are related via
 diag 
 where v and w are scalar-valued auxiliary signals, q is an independent second-order zero-mean i.i.d. sequence, and both F and L are the transfer functions of proper LTI systems that, together with the unit delay z-1, have deterministic initial states. ¦
 Remark 5.1: In Definition 5.1, the requirement of q being independent (without reference to other random variables or processes) is to be understood as requiring q to be independent of all exogenous processes and initial states in the (feedback) system in which the source coding scheme is embedded. In particular, when an independent source coding scheme is used in the NCS of Fig. 1, q is to be assumed independent of (xo,d). ¦
 The class of linear source coding schemes is motivated by the results of Section III and generalizes the class of independent source coding schemes introduced in [45].  We note that independent source coding schemes do not necessarily satisfy Assumption 2.2.
 Linear source coding schemes are defined in terms of their input-output relationship with no regard as to how the channel input yc is related to the source coding scheme input y. A simple way of making that relationship explicit is by using an entropy-coded dithered quantizer (ECDQ; [45], [62]). When using such a device, v and w in (29), and the channel input yc and output uc, are related via
  
 where dh is a dither signal available at both the encoder and decoder sides, Q : R ? {i?;i ? Z} denotes a uniform quantizer with step size ? ? R+, Hk is a mapping describing an entropy-coder (i.e., a loss-less encoder [54, Ch.5]) whose output symbol is chosen according to the conditional distribution of s(k), given dh(k), and Hk-1 is a mapping describing the entropy-decoder that is complementary to the entropy-coder at the encoder side.
 Lemma 5.1 (Theorem 5.3 in [45]): Consider the set-up of Figure 4, where the ECDQ is as in (30) and has a finite quantization step ?. Assume that P¯ is a proper real rational transfer function, that the open-loop transfer function from w to v is single-input single-output and strictly proper, and that the signal d¯is a white noise sequence jointly second order with the initial state x¯o of P¯. If the dither dh is i.i.d., independent of (¯xo,d¯) and uniformly distributed on (-?/2,?/2), then
  
 Fig. 4. Entropy coded dithered quantizer inside a feedback loop.
 w--v is i.i.d., independent of¦ (¯xo,d¯) and uniformly distributed in ( ?/2,?/2).
 It follows that any coding scheme described by (29) and (30), with dither as in Lemma 5.1, is a linear source coding scheme. Any such coding scheme will be referred to as an ECDQ-based linear source coding scheme. Figure 5 depicts an ECDQ-based linear source coding scheme where we have made explicit the fact that, since the channel is error-free and has zero delay, sˆ = s and, thus, w can be obtained at the encoder side without making use of any additional feedback channel.
 The next lemma gives an upper bound on the (operational) average data rate in an ECDQ-based linear source coding scheme.
 Lemma 5.2: Consider the NCS of Fig. 1 and suppose that that Assumption 2.1 holds. Then, there exists an ECDQ-based linear source coding scheme such that the resulting NCS is SAWSS. For any such coding scheme,
 R 
 where sv2 is the steady-state variance of the auxiliary signal v, and sq2 = ?2/12 is the linear source coding scheme noise variance (see (29)).
 Proof: Consider the NCS of Fig. 1 and assume that the source coding scheme is linear. Since Assumption 2.1 holds, there exist proper LTI filters L and F such that the resulting NCS is internally stable and well-posed (one possibility is to choose L such that v = y and to pick any F which internally stabilizes P). For any such choice of filters, the open loop system linking w with v is stabilizable with unity feedback. Our claim now follows immediately upon using Corollary 5.3 in [45] and the description for the coding noise in Lemma
 5.1.	 
 We are now in a position to prove the main results of this section:
 Theorem 5.1: Consider the NCS of Fig. 1 and suppose that Assumption 2.1 holds. If D ? (Dinf,8), then there exists an ECDQ-based linear source coding scheme satisfying Assumption 2.2 such that the resulting NCS is SAWSS, se2 = D, and
 R 
 where ?'(D) is as in (25).
 Proof: Since D > Dinf, the problem in (25) is feasible (see Appendix A). Thus, there exist proper LTI filters L and
 F rendering the feedback system of Fig. 2 internally stable and well-posed, and sq2' ? R+, such that, in the scheme of Fig. 2, and for any ? > 0, se2' = D and
 	 	(33)
 Denote the above choices for L, F and sq2' by L?,F? and s?2', respectively. Given Lemma 4.1 and Jensen’s inequality, F? can be assumed to be biproper without loss of generality.
 Consider the NCS of Fig. 1 and assume that the link between y and u is given by an ECDQ-based linear source coding scheme with parameters (L,F,?) = (L?,F?,(12s?2')1/2), and set the initial states of L?, F? and of the channel feedback delay to zero. The definition of L?, F? and s?2', together with Lemma 5.1, guarantee by construction that the NCS that results from the above choice of coding scheme is SAWSS and that, in addition, the plant output e and the auxiliary signal v in (29) have steady-state variances satisfying
 	 	(34)
 By Lemma 5.2 we also conclude that, for the above described ECDQ-based linear source coding scheme, the average expected length of the channel input yc satisfies, for some suitable d > 0,
 R 
  
 where we have used (34). Thus, inequality (32) follows upon choosing a sufficiently small ? > 0.
 To complete the proof, we now show that the proposed source coding scheme satisfies Assumption 2.2. Except for the invertibility of the decoder, the properties of dh guarantee that Assumption 2.2 holds (note that, in our case, SE = SD = dh). Since F? is biproper and its initial state is deterministic, knowledge of uk is equivalent to knowledge of wk. If one now proceeds as in the proof of Corollary 5.1 in [45], it follows that one can recover ukc from wk upon knowledge of . Thus, upon knowledge of , one can recover ukc from uk and the decoder is invertible as required.  
 Remark 5.2: The proof of Theorem 5.1 is constructive. Indeed, it suggests a way to build a source coding scheme that renders the resulting NCS of Fig. 1 SAWSS, and achieves se2 = D while incurring an average data rate that is upper¦ bounded by the right-hand side of (32).
  
 Fig. 5. Proposed source coding scheme. If the channel is noiseless and delay free, then sˆ = s and wˆ = w.
 Theorem 5.1 shows that the lower bound on R(D) derived in Corollary 4.1 is tight up to   nats per sample (i.e., tight up to approximately 1.254 bits per sample). Whilst the lower bound in Corollary 4.1 was derived by using an information-theoretic argument, the upper bound in (32) hinges on a specific source coding scheme that uses suitably chosen LTI filters in conjunction with an ECDQ. It follows from the discussion in Section V-B in [45] that the gap between the derived upper and lower bounds on R(D) arises from two facts: First, ECDQs introduce a coding noise which is uniform and not Gaussian (this amounts to the additional  nats per sample). Second, the proposed coding scheme works on a sample-by-sample basis and practical entropy-coders are not perfectly efficient [54, Chapter 5] (this amounts to an additional log2 nats per sample). We emphasize, however, that the above gap corresponds to a worst case gap and it can be significantly smaller in practice (see Section VII).
 A key aspect of our results is that they are stated in terms of the solution to the constrained SNR minimization problem in (25). As such, they highlight the role played by SNR constraints in networked control systems, and thus complement, e.g., [63] where the connection between SNR constraints and other communication constraints has been explored. As already mentioned before, a way of obtaining a solution to the problem in (25) will be discussed in Section VI below.
 Remark 5.3: It is well-known [15] that, when causal source coding schemes of arbitrary complexity are employed, it is possible to mean-square stabilize an LTI plant if and only if the corresponding average data rate R is larger than , where pi denotes the ith unstable plant pole. On the other hand, it is straightforward use Theorem 17 in [50], in conjunction with the proof of Theorem 5.1, to show that any plant satisfying Assumption 2.1 can be stabilized in the sense of Definition 2.1 by incurring an average data rate that satisfies
 	R .	(36)
 The above observation shows, for plants satisfying Assumption 2.1, that it suffices to use an ECDQ-based linear source coding scheme to achieve stability at rates which are at most   nats per sample away from the absolute minimal average data rate compatible with stability (see also
 [45]). ¦
 VI.	COMPUTATIONS AND APPROXIMATE IMPLEMENTATION
 A. Computing the bounds on R(D)
 The bounds on R(D) presented in Corollary 4.1 and Theorem 5.1 are functions of the minimal SNR ?'(D) in (25). In this section, we show that the problem of finding ?'(D) is equivalent to an SNR constrained optimal control problem previously addressed in [35], [47], [48].
 To proceed, we first note that a straightforward manipulation based on Fig. 2 yields, for any sq2' ? R+ and any proper LTI filters F and L that render the LTI feedback system of Figure
 2 internally stable and well-posed,
  
 where S is as in (23), K ? FLy (1 - Lwz-1)-1, Lw and
 Ly are such that L = [Lw Ly ] (see (15)), and we have used that fact that, since F and L are internally stabilizing, Lw is proper and P22 is assumed to be strictly proper, S is stable,
 S(8) = 1 and hence.
 We now define, for the feedback system of Figure 2, the auxiliary problem of finding
 J'(G) ? inf se2',	(39) ?'=G
 where the minimization is performed with respect to all proper LTI filters F and L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of Fig. 2 internally stable and well-posed. Given our assumptions, if the plant P is unstable, then the problem in (39) is feasible if and only if G > Ginf, where Ginf denotes the infimal SNR ?' that is compatible with mean-square stability in the feedback system of Fig. 2 (see [50], [64]). If P is stable, then the problem in (39) is feasible if and only if G = Ginf = 0.
 Lemma 6.1: Consider the problems of finding both ?'(D) and J'(G) in (25) and (39), respectively. Assume, in addition, that the plant P is such that P21 = 0? and P12 = 0? .
 1)	If D ? (Dinf,8), the plant is unstable, or stable with
  , then ?'(D) is a strictly decreasing function of D and the inequality constraint in (25) is active at the optimum.
 2)	If G > Ginf, then J'(G) is a strictly decreasing function of G and the inequality constraint in (39) can be assumed to be active at the optimum without loss of generality.
 Proof:
 1) Consider the definition of ?'(D) in (25) and define
 . We first show
 	D 	? > 0 at the
 optimum. (Since D > Dinf, D-? is always non negative for any feasible set of parameters F,L and sq2' > 0.) Indeed, assume on the contrary that D - ? = 0 at the optimum. Given (38), this would imply that sq2' = 0 or P12FS = 0 at the optimum. Given our assumptions and the definition of ?'(D), only F = 0 is possible. However, F = 0 is not compatible with internal stability when the plant is unstable. In the stable plant case, F = 0 implies
   (note that F = 0 =? K = 0). The latter equality is however unfeasible since, by assumption,
  .
 Given the above, if D > Dinf, and the plant is unstable or is stable with  , then D - ? > 0 at the optimum. Hence, the performance constraint se2' = D in the definition of ?'(D) is equivalent to
 	 	(40)
 at the optimum (see (25) and (38)). Since ?' is a nondecreasing function of sq-'2, it follows that the optimal choice for sq2' is such that the inequality constraint is active at the optimum.
 We now show that ?'(D) is strictly decreasing in D. Our assumptions guarantee that K = 0? and hence FLy =? 0. By using (40) in (37), and the fact that the optimal choice for sq2' achieves equality in (40), the result follows immediately.
 2) Consider the definition of J'(G) in (39). By using an argument similar the one used in Part 1 above, it follows that our assumptions imply that one can assume, without loss of generality, that G + 1 - ||S|| > 0 at the optimum (see also [48, pages 103–104]). Our claims now follow by proceeding as in Part 1) above.
  
 Lemma 6.1 shows, for almost all cases of interest,7 that the inequality constraints in the optimization problems defining both ?'(D) and J'(G) can be assumed to be active at the optimums, without loss of generality. This fact is exploited below to relate the solutions to these problems.
 Theorem 6.1: Consider the optimization problems defining both ?'(D) and J'(G) in (25) and (39), respectively. Assume that G > Ginf, D > Dinf, that the plant P is such that P12 = 0? and P21 = 0? , and that, if P is stable, then   holds.
 Then,
 	D = J'(?'(D)),	G = ?'(J'(G)).	(41)
 Proof: We will only prove that G = ?'(J'(G)). Our remaining claim follows by using a similar argument. Since G > Ginf, the problem of finding J'(G) is feasible. Thus, for any ? > 0, there exist proper LTI filters F? and L?, and s?2' ? R+, that render the system of Figure 2 internally stable and well-posed, and guarantee that
 (42)
 (43)
 where we have used Lemma 6.1 to write an equality in the SNR constraint. Since the inequality in (42) is valid for any ? > 0, it follows that there exist a feasible point for the problem of finding ?'(J'(G)) and, in addition, that ?'(J'(G)) = G. The proof of our second claim would follow if we show that ?'(J'(G)) < G is impossible. Assume that ?'(J'(G)) < G is indeed true. Then, there exist decision variables such that ?' = ? <? G and se2' = J'(G). (Again, we
 7If G = Ginf, then either the problem of finding J'(G) is unfeasible (unstable plant case) or G = 0 (stable plant case). In the latter case, no information can be conveyed through the channel. On the other hand, if the plant is stable and  and it is optimal to leave the plant in open loop. The above cases are clearly uninteresting and have thus been omitted from the discussion in Lemma 6.1.
 use Lemma 6.1 to write an equality in the constraint defining ?'(J'(G)).) Thus, we conclude that
 	 ,	(44)
 where the first equality follows from the fact that the constraint is active at the optimum when calculating J'(??). The above inequality contradicts the fact that, given our assumptions, Lemma 6.1 guarantees that J'(G) is a strictly decreasing function of G. The proof is thus completed.  
 Theorem 6.1 shows, for almost all cases of interest, that the problem of finding ?'(D) in (25) is equivalent to that of finding J'(G) in (39). The latter problem was shown to be equivalent to a convex problem in [47]. For doing so, [47] showed that the problem of finding J'(G) is equivalent to the open-loop causal rate-distortion problem which was shown to be convex in [35]. Shortly thereafter, the convexity of the SNR constrained optimal control problem in (39) was re-derived independently in [48], where a formulation more amenable for numerical computations is presented. We will thus not delve into the details on how to numerically find ?'(D) here, and refer the interested reader to Section 3.3 in [48] for details.
 B. Approximating the behavior of an ECDQ in practice
 The previous subsection explained how a numerical characterization of ?'(D) can be obtained. Here, we will briefly comment on the implementation of a source-coding scheme which achieves the desired level of performance D, while incurring an average data rate R satisfying (32). In principle, such coding scheme can be designed as follows (see proof of Theorem 5.1):
 •	Use the procedure in [48] and Theorem 6.1 to find thefilters L and F, and the auxiliary noise variance sq2',
 which solve the problem of finding ?'(D).
 •	Use these filters in the ECDQ-based linear source coding scheme of Fig. 5 and set all initial states to zero. Choose the ECDQ quantization step as ? = (12-sq2')1/2, an i.i.d. dither signal uniformly distributed on ( ?/2,?/2) and independent of (xo,d), and appropriate entropy coder and decoder mappings Hk and Hk-1 (using, for instance, the Huffman algorithm [54]).
 Implementing an ECDQ requires the availability of the dither at both the encoder and the decoder sides. Additionally, the entropy coder Hk needs to generate a binary word for each input value according to the conditional probability of that input, given the current dither value. The above requirements are impossible to meet exactly in practice. Indeed, the first one is tantamount to requiring an additional perfect channel for being able to communicate the dither from the encoder to the decoder. The second one would require an uncountable number of dictionaries [54], one for each dither value.
 Leaving finite range and precision issues aside, the behavior of an ECDQ can be approximated in practice by using synchronized uniformly distributed pseudo-random dither sequences, generated at both the encoder and the decoder from the same seed, and using entropy-coders and decoders which work conditioned upon a uniformly quantized version of the dither. By using such an approach, all signals in the NCS of Figure 1, except for the channel input and output, will have the same statistics as if an ideal ECDQ was employed. For each possible quantized dither value, one can build the corresponding conditional dictionary in Hk by using, for example, the Huffman coding algorithm [54]. The conditional statistics of the quantizer outputs needed for this purpose, can be approximated by the corresponding stationary statistics which can be estimated empirically by simulation.
 VII.	A NUMERICAL EXAMPLE
 that using more than 11 quantized dither values brings only negligible benefits in terms of rate reduction.) The curve “Measured entropy of quantizer output (11 dither values)” corresponds to an empirical estimate of the conditional entropy of the quantizer output s, given the quantized dither values.
 Our results show that our upper bound is loose. This is consistent with the fact that our upper bound was derived by using worst case considerations. The gap between the measured rate (with conditioning) and our lower bound is about 0.45 bits per sample, which is smaller than the worst case gap   nats per sample (about 1.254 bits per sample). On the other hand the estimated conditional entropy of the quantizer output, given the dither values, is about 0.25 bits per sample above the lower bound. This implies that, in our simulations, the 0.46 bit per sample gap is composed by about 0.21 bits per sample due to the inefficiency of the considered entropy coders, and by about 0.25 bits per sample due to the fact that the ECDQ generates uniform and not Gaussian noise.
 1.4588rates, converge rapidly as D ? 8. Thus, whilst achieving an average data-rate arbitrarily close to the minimal rate
 1.2518for stabilization severely compromises performance [15], our
 Our results show that, as expected, achieving a closed loop performance arbitrarily close to the best non networked performance Dinf requires arbitrarily high data rates. Interestingly, however, for this example, it suffices to use less than 3 bits per sample to achieve a performance that is essentially identical to the best non networked performance. It is also interesting to observe that our bounds, and the measured average data-
  
  
 	Assume that the plant0.2091	P in Fig. 1 is such that
 	 	(45)
 and that (xo,d) is Gaussian with d being unit-variance white noise. By using the results of Sections III and V we computed upper and lower bounds on R(D) for several values of D > Dinf = 0.2091. We also simulated an actual ECDQbased linear source coding scheme for each considered value for D. To that end, we followed the suggestions at the end of Section VI-B and simulated ECDQs where the dither is uniform and perfectly known at both ends of the channels, and where the entropy-coders work conditioned upon a quantized version of the dither. The results are presented in Figure 6. In that figure we plot our upper and lower bounds, and several other curves which report simulation results. All simulation results (referred to as “Measured” in Fig. 6) are averages over twenty 104-samples-long realizations. In particular, “Measured rate (no conditioning)” corresponds to the average data rate in a case where an empirically-tuned entropy-coder is employed which does not make use of the knowledge of the dither values. Even in this case our upper bound proves to be rather loose. The curve “Measured rate (cond. using 11 dither values)” corresponds to the rate achieved when using and entropy coder that works conditioned upon 11 uniformly-quantized dither values. As expected our results show that conditioning reduces the incurred average data-rate. (Simulations suggested results suggest that the performance loss incurred when forcing the average data rate to be low might be modest in some cases.
 VIII.	CONCLUSIONS
 This paper has studied networked control systems subject to average data rate constraints. In particular, we have obtained a characterization of the minimal feedback directed information rate I8(y ? u) average data rate (to within 1.25 bits/sample) that guarantees a prescribed level of quadratic performance, over all encoder-controller-decoder combinations. Our results have been derived for LTI plants that have one scalar control input, one scalar sensor output, and that are subject to Gaussian stationary disturbances and Gaussian initial states. The digital feedback channel considered was assumed to be free of errors and delays. Where no constraints besides causality are imposed on the considered coding schemes, the directed information rate between the encoder input y and decoder output u (a known lower bound to the mean operational data rate) was shown to be minimized only when y and u are jointly
 Gaussian. This, together with the introduction of the notion of strong asymptotic wide-sense stationarity (SAWSS), allowed us to define a wide class of coding schemes (namely, SAWSS schemes) for which a data-rate lower bound in terms of the asymptotic power spectral densities of y and u was derived. We then showed that, for a given stationary performance, the infimum of this lower bound coincides with that of the directed information rate obtained when encoder, channel and decoder are replaced by an AWGN channel surrounded by LTI filters,
  
 Fig. 7. Standard one-degree-of-freedom feedback loop around the plant P.
 taking the infimum over all causal filters which satisfy the same performance constraint. Such insight was then used as motivation for building a source coding scheme capable of achieving rates which are less than   nats per sample away from our derived lower bound, while satisfying the desired performance level constraint. Such coding schemes are based upon entropy dithered quantizers and constitute conceptually simple coding schemes. A numerical example has been include to illustrate our proposal.
 Future work should focus on multiple input and multiple output plant models, multichannel architectures, and on ways of reducing the gap between the derived upper and lower bounds on the minimal average data-rate that guarantees a given performance level.
 IX.	ACKNOWLEDGMENTS
 We would like to thank the AE and the anonymous referees for their careful reviews as well as for their helpful comments, which contributed to improve the technical presentation of this paper.
 APPENDIX
 A. Three consequences of assuming D > Dinf
 In this appendix we show that D > Dinf is sufficient for the optimization problems in (7), (16) and (25) to be feasible. We will make extensive use of the definition of ?'(D) in (25), the related equations in (37) and (38), the conventions regarding the feedback scheme of Fig. 2 made on the paragraph preceding Theorem 4.1, and the definitions of ECDQs and ECDQ-based linear source coding schemes made in Section V.
 Consider the feedback system of Fig. 7, where P, d and xo satisfy Assumption 2.1, and the controller K is such that u(k) = Kk(yk) for some arbitrary mappings Kk. Since P is LTI and all the involved random variables are Gaussian, it follows from well-known results [65] that
 	Dinf = inf se2,	(46)
 K?S
 where S is the set of all proper LTI filters which render the the feedback system of Figure 7 internally stable and well-posed. Our assumptions on P guarantee that the above problem is feasible.
 The fact that D > Dinf and that the problem of finding Dinf is feasible, implies that for every ? ? (0,D - Dinf), there exists K0 ? S such that, in Figure 7,  =
 Dinf + ? < D. Consider the feedback scheme of Fig. 2 with
 F = 1 and L = L0, where L0 is such that v = K0 y. Since K0 ? S, the above choice renders the feedback system of
 Fig. 2 internally stable and well-posed for any additive noise variance sq2' ? R+. This means that the resulting variance of v', say sv20, will be finite. It also means that if q' in Fig. 2 is zero-mean AWGN with variance sq', then the variances of e' and of v' will increase to  , and sv20 +
  , respectively, for some finite factors  which depend only upon K0. As a consequence, for every D > Dinf, there exists K0 ? S such that in Fig. 2 and for the above choice of filters,, q'
 by picking ? = (D-Dinf)/3 and q' as zero mean AWGN with variance  . It immediately follows that the above choice of parameters is also such that, in Figure 2,
 	 .	(47)
 The latter inequality shows that the problem of finding ?'(D) in (25) is feasible for any D > Dinf (indeed, feasible while yielding all signals in the system jointly Gaussian). Now, from Jensen’s inequality and the concavity of log, it also follows from (47) that the problem of finding  in (16) is feasible for any D > Dinf.
 We end this section by showing that the problem of finding R(D) in (7) is also feasible when D > Dinf. To that end, it suffices to consider an ECDQ-based linear source coding scheme to link y and u in Fig. 1, with parameters
  . Indeed, by exploiting
 the properties of the latter choice of parameters (see preceding paragraph), our claim follows by proceeding as in the proof of Theorem 5.1 to show that the above defined ECDQ-based linear source coding scheme satisfies Assumption 2.2, renders the NCS of Figure 1 SAWSS, and achieves se2 < D at a finite average data rate R.
 B. Auxiliary information-theoretic definitions and results
 The following definitions and facts are standard and, unless otherwise stated, can be found in [54]. We assume all random variables to have well defined (joint) probability density functions (PDFs). The PDF of x (x,y) is denoted f(x) (f(x,y)). f(x|y) refers to the conditional PDF of x, given y. Ex {·} denotes mean with respect to the distribution of x.
 The differential entropy of x is defined via h(x) ? -Ex {logf(x)}. The conditional differential entropy of x, given y, is defined via h(x|y) ? -Ex,y {logf(x|y)}. The mutual information between two random variables x and y is defined via I(x;y) ? -Ex,y {log(f(x)f(y)/f(x,y))}. The conditional mutual information between x and y, given z, is defined via I(x;y|z) ? I(x,z;y)-I(z;y). The following are properties of the above quantities:
 (Property 1) I(x;y|z) = h(x|z) - h(x|y,z).
 (Property 2) If f is a deterministic function, then h(x + f(y	h	|
 (Property 3) If	are independent, then h(x|y) = h(x).
 (Property 4)  , where x-1 can be taken to be a deterministic constant, in which case h(x0|x-1) = h(x0).
 Lemma B.1: Assume that (x,z) are jointly second-order random variables, x is Gaussian, and z is arbitrarily distributed.
 Then, I(x;z) = (x;zG), where zG is such that (x,zG) are jointly Gaussian and have the same first- and second-order (cross-) moments as (x,z).
 Proof: Immediate from the proof of Lemma 1 in [66].  
 Lemma B.2: Let x,y,z be arbitrarily-distributed random vectors satisfying the Markov chain
 	x ?? y ?? z.	(48)
 Let xG,yG,zG be jointly Gaussian random variables with the same first- and second-order statistics as (x,y,z). In addition, suppose that E[x|y] = Ly, for some matrix L of appropriate dimensions. Then the Markov chain
 	xG ?? yG ?? zG.	(49)
 holds.8		?
 Proof: The shared statistics between (xG,yG,zG) and (x,y,z) imply that E[yGzG] = E[yz]. The conidtion E[x|y] = Ly implies that E[x|y = y] = E[xG|yG = y] = Ly. With this we obtain that
 	E[(xG - E[x|yG])zG] = E[(xG - LyG)zG]	(50a)
 	= E[xz - Lyz] = E[(x - E[x|y])z] = 0,	(50b)
 where the last equality is due to (48). But for jointly Gaussian variables (xG,yG,zG), E[(xG - E[x|yG])zG] = 0 if and only if (49) holds, completing the proof.  
 Lemma B.3: Let the random variables (uk,yk,xo,dk) be such that, for all k ? N0 and i = k,
 i) yi = Li(xo,di)+Miui-1 for some linear and deterministic mappings  .
 ii ui	= gi(si,yi) for some sequence of deterministic
 functions {gi} and for some exogenous sk ? (xo,dk). Then the Markov chains
 	 	(51)
 hold for all k ? N0.	?
 Proof: The sequences uk, yk are recursively generated as y(0) = L0(xo,d(0)) u(0) = g0(s(0),y(0)) = g0(s(0),L0(xo,d(0))) y(1) = L1(xo,d1) + M1u(0)
 = L1(xo,d1) + M1 g0(s(0),L0(xo,d(0))) u(1) = g1(s1,y1)
 	= g1(s1,{L0(xo,d(0)) ,	} )
 	...	L1(xo,d1) + M1 g0(s(0),L0(xo,d(0)) )
 This reveals that (ui,yi) = g˜i(si,Li(xo,di)) for some deterministic functions {g˜i}, from where (51) follows (since sk ? (xo,dk)). 9	 
 8Lemma B.2 is a stronger version of Lemma 3.2 in [67], since the statement of the latter requires the existence of a linear operator L such that the estimation error x - Ly is independent of both z and y, while Lemma B.2 only requires E[x for all y.
 9It suffices to have in the argument of g˜i instead of	because
 L0(xo,d(0)),L1(xo,d1),...,Li-1(xo,d -1) are (nested) sub sequences of the longest sequence Li(xo,di).
 Theorem B.3: Let the random variables (uk,yk,xo,dk) correspond to those in the feedback system shown in Fig. 1.
 Let   be jointly Gaussian with the same firstand second-order moments as (uk,yk,xo,dk). Then, for evey k ? N0,
 uG(i) ?? uiG-1,yGi ?? xo,dk,	?i = k uG(i) ?? uiG-1,xo,di-1 ?? dki ,	?i = k	(52a)
 (52b)
 ?
 Proof: In the scheme of Fig. 1, the LTI plant causally maps its inputs (ui-1,xo,di) to its output yi. This means that that yi = Li(xo,di) + Miui-1 for some linear mappings
  . Therefore, we can invoke Lemma B.3 to conclude that
 	 	(53)
 holds. The linearity of the {Li} operators implies that
 Li(xo,di) and (xo,dk) are jointly Gaussian, and thus E[xo,dk|Li(xo,di)] = FiLi(xo,di) for some sequence of linear mappings  . Hence, we can directly apply Lemma B.2 and obtain from (53) that
 uiG,yGi ?? Li(xo,di) ?? xo,dk,	?i	(54) (a)
 Hence	0	=	I(xo,dk;uiGLi(xo,di))	=
 (b)
 =
 , which is equivalent to (52a). The inequality (a) is due to the chain rule and the nonnegativity of mutual information (which also implies (c)), and (b) holds since  . Starting
  ,xo	,uG-1)	=
 , which is equivalent to (52b). The
 inequality follows from the chain rule and the non-negativity of mutual information. This completes the proof.	 
 	1,o
 d1	y¯
  
 S2
 x
 2 x2,o Fig. 8. Generic feedback system.
 Lemma B.4: Consider the generic feedback system of Figure 8, where S1 is an arbitrary (hence possibly non-linear and time-varying) causal dynamic system with initial state x1,o and disturbance d1, such that  for some (possibly non-linear and time-varying) deterministic mapping S1,k, and S2 is an arbitrary causal dynamic system with disturbance d2 and initial state x2,o, such that   for some (possibly non-linear and
 time-varying) deterministic mapping S2,k. If (x2,o,d2) are jointly independent of (x1,o,d1), then
  .	(55) Proof: Immediate from [68, Theorem 1].	 
 Lemma B.4 corresponds to a stronger version of Theorem 5.1 in [44]. Indeed, the latter result makes use of additional assumptions on system S2, does not take side information into account, and only shows that the left hand side of (55) is lower
 bounded by the corresponding right-hand side. [3]–[6], [53]
 	Lemma	B.5: 	be	an	SAWSS	process	with
  , for some d > 0. Then
 where Su¯(ej?) is the stationary PSD of u.
 Proof: The definition of an SAWSS process implies that   and   are sequences of asymptotically equivalent Hermitian matrices. This, together with the fact that d > 0, and that log(x)
 is continuous for all x = d > 0, allows one to apply [53, Corollary 2.4] to obtain
 	 	(57)
 Since the matrices  are positive-definite Hermitian Toeplitz with each row being absolutely summable for all n, it follows from the fundamental eigenvalue distribution theorem of Szego (see, e.g., [53, Corollary 2.4 or Theorem 5.4]) that¨
 	 	(58)
 The	proof	is	completed	by	noting	that	 
  .	 
 ﻿This paper studies discrete-time control systems subject to average data-rate limits. We focus on a situation where a noisy linear system has been designed assuming transparent feedback and, due to implementation constraints, a source-coding scheme (with unity signal transfer function) has to be deployed in the feedback path. For this situation, and by focusing on a class of source-coding schemes built around entropy coded dithered quantizers, we develop a framework to deal with average data-rate constraints in a tractable manner that combines ideas from both information and control theories. As an illustration of the uses of our framework, we apply it to study the interplay between stability and average data-rates in the considered architecture. It is shown that the proposed class of coding schemes can achieve mean square stability at average data-rates that are, at most, 1.254 bits per sample away from the absolute minimum rate for stability established by Nair and Evans. This rate penalty is compensated by the simplicity of our approach.
 Index Terms—Average data-rate, networked control systems, perfect reconstruction, signal-to-noise ratio.
 I. INTRODUCTION
 P
 RACTICAL control systems often use nontransparent communication links and, thus, communication constraints arise [1]. Such constraints include random delays, data-loss and data-rate limits (quantization) [23], [36], and [41]. This paper focuses on average data-rate constraints.
 It might be argued that the communication capacity of modern networks is in general sufficiently large, so as to make quantization issues irrelevant (see, e.g., [23] and [41]). However, there exist situations where the communication resources assigned to a particular relevant control signal are limited and, hence, quantization effects become important [36]. Quantization is a highly nonlinear operation on signals and, accordingly, it is hard to analyze [21].
 In control theory, quantization has usually been treated as an undesirable effect that should be compensated for (see, e.g., [32]). This stands in contrast to the perspectiveadopted by information theory, where quantization is considered as an integral part of systems (see, e.g., [8] and [45]). This line of reasoning has recently been brought to the control arena (see, e.g., [34], [39], [40], [57], and [62]). (An alternative view of quantization, closerto nonlinearcontroltheorythan to informationtheory, has also been developed inside the control community; see, e.g., [7], [9], [13], and [37].)
 In a quantized discrete-time control framework, a key problem is being able to characterize the minimal average data-rate (in, e.g., bits per sample) that allows one to achieve a given control objective. In the context of noiseless digital channels, i.e., channels that transmit data without errors or delays at a constrained rate, this question is related to rate-distortion theory (i.e., lossy source-coding problems; see, e.g., [4], [8], [17], and [46]). The associated design problem is how to quantize a signal, with the smallest average data-rate, whilst achieving a prescribed degree of fidelity or performance. A typical performance measure is the mean square error, but other measures are also possible. For instance, [54] suggests discrete measures to address black-and-white control problems such as, e.g., stabilizability and observability.
 Standard results in information theory (and in particular in rate-distortion theory) rely upon coding arbitrarily long sequences which incur arbitrarily long time delays. In addition, most of the general results on rate-distortion theory do not take stability nor causality into account [4], [46]. It, thus, becomes clear that standard rate-distortion theory is not useful to deal with control problems. Some progress has been made in the information theory community towards a causal rate-distortion theory, but most results use coding schemes that, even though causal, allow for arbitrary delays [25], [38]. Only recently, [11] established upper bounds on the zero-delay causal rate distortion function for Gaussian stationary sources. However, the best bounds provided in [11] are of algorithmic nature, and derived for open loop systems. Thus, stability issues are not addressed in [11]. This is also the case of the results in [5], where sequential quantization of Markov sources is addressed.
 0018-9286/$26.00 © 2010 IEEE
 The discussion in the previous paragraph makes the work documented in [34], [55], and [60] specially relevant, even though the focus in those works lies only on stability. The first results that pointed out that there exists a data-rate under which (memoryless) quantized control cannot keep the state of a noiseless plant bounded were presented in [3] and [60]. The results were later extended in [33] and [55] using encoders and decoders with memory, and adaptive quantizer scaling policies (so-called zooming techniques [7] and [60]). A landmark result was published in [34], where the authors focus on noisy plant models subject to mild conditions on the noise sources statistics. It was shown in [34] that it is possible to find causal coders, decoders and controllers such that the resulting closed loop system is mean square stable if and only if the average data-rate (in bits per sample), say  , satisfies
 	 	(1)
 where   denotes the  th unstable plant pole. The above result establishes a fundamental limitation in networked control systems (NCSs) closed over digital channels, when the problem of interest is mean square stabilization. Bounds similar to (1) arise as solutions to different problems (e.g., observability, deterministic stability, etc.) and under different assumptions on the channels and coding schemes (see, e.g., [14], [33], [36], [54], and [55]). Indeed, the quantity on the right hand side of (1) is a fundamental measure of the difficulty of stabilizing a system, as discussed in [35] and [40].
 All constructions known to date that achieve stability at average data-rates arbitrarily close to (1) use complex nonlinear time-varying coding schemes that, in principle, have infinite memory. The consideration of coding schemes with limited (or no memory) is much more involved [36] and no explicit solutions are currently available (see, also, [55, Section VI]). An alternative simpler approach has been presented in [61], although for the scalar plant case only.
 Almost all the work referred to above focuses on stability questions only. A performance-oriented approach has been pursued in [36] and [57]. In that work, conditions for separation and certainty equivalence have been investigated in the context of quadratic stochastic problems for fully observed plants with data-rate constraints in the feedback path. If the encoder has a specific recursive structure, then certainty equivalence and a quasi-separation principle hold [36]. This result is interesting, but [36] does not give a computable characterization of the optimal encoding policies. A similar drawback is shared by the results reported in [57]. In that work, performance related results are expressed in terms of the so-called sequential rate-distortion function (a rate-distortion function with causality constraints), which is difficult to compute in general. For fully observed Gaussian first-order autoregressive systems, [57] provides an expression for the sequential rate-distortion function. However, it is not clear from the results in [57] whether or not the sequential rate-distortion function is operationally tight (see [57, Section IV-C]). Related work can be found in [62], where estimation problems are addressed.
 The main contribution of this paper is a novel, though restricted, bridge between information theory and control theory. The link is restricted inthat it holds for a specific class of sourcecoding schemes based on entropy coded dithered quantizers (see, e.g., [63], [64], and [66]). Nevertheless, the link is useful, enabling one to address control system design problems subject to average data-rate constraints in a systematic manner [47]. Our approach is constructive and based upon standard building blocks. As such, it yields bounds on average data-rates that are guaranteed to be achievable with conceptually simple sourcecoding schemes. An additional feature of our approach is that it does not rely on asymptotic approximations (e.g, high-rate or high vector dimensions assumptions).
 As both a motivation for our approach, and also to illustrate a possibleapplication,we considera problemwhere anoisy linear system has been designed assuming transparent feedback and, due toimplementation constraints,a source-coding schemewith unity signal transfer function has to be deployed in the feedback path. For this situation, we discuss how to obtain bounds on the minimal average data-rate that allows one to attain a certain performance level, and also provide a detailed characterization of the interplay between stability and average data-rates. It is shown that the proposed class of coding schemes can achieve mean square stability at average data-rates that are guaranteed to be at most 1.254 bits per sample away from the absolute minimum in (1). This rate penalty is compensated by the simplicity of our approach.
 A key enabling result in the paper is that, when the proposed class of source-coding schemes is employed, average data-rate constraints can be enforced by imposing signal-to-noise ratio (SNR) constraints in a related analog additive noise channel (see, also, [6] and [51]). Our results, thus, establish a formal relationship between SNR constraints and average data-rates constraints in noiseless digital channels. As such, our work goes beyond [6] and [51] where no such relationship is presented. Early versions of the results reported in this paper can be found in [47] and [50].
 The remainder of the paper is organized as follows. Section II presents notation. Section III describes the setup considered in the paper. Section IV presents a lower bound on average datarates that motivates the remainder of the paper. Section V introduces the class of source-coding schemes considered in the paper and relates average data-rate limits to SNR constraints. Section VI focuses on the interplay between stability and average data rate constraints. Section VII draws conclusions. For ease of reference, the Appendix presents basic information-theoretic facts.
 II. NOTATION
   stand for the reals, the non-negative reals,
 the strictly positive reals, and the non-negative integers, respectively.   denotes the magnitude of the complex scalar  denotes the conjugate transpose of the matrix   is the set of all strictly proper and stable real rational transfer functions, and   is the set of all stable, biproper and minimum phase real rational transfer functions. The usual norm in   is written
 [58]. If is an asymptotically wide sense stationary (wss) process [2], then , , denote its stationary variance, its stationary power spectral density (PSD), and the corresponding spectral factor, respectively. If is a discrete time signal, then is the th sample, and is shorthand for .
 If	is a family of sets, then	.
 If	is a set that does not depend on any index, then
 ( times), as usual. We write if and only if (iff) and are independent. We write iff , and form a Markov chain (see Appendix). and stand for the expectation and probability of , respectively. Definition of information-theoretic quantities and related notation is given in Appendix.
  
 Fig. 1. NCS closed over a digital channel.
 III. PROBLEM SETUP
 This paper focuses on the NCS of Fig. 1, where   is a given linear time-invariant (LTI) system such that
 models exogenous signals, is a signal related to closed loop performance, is a signal available for measurement, and is a control input. We assume that has been designed so as to achieve satisfactory performance when . The feedback path, however, comprises an error free digital channel. Hence, the quantization of the signal   becomes mandatory. This task is performed by an encoder that outputs the sequence of binary symbols  . Once these symbols are available at the receiving end, a decoder generates the signal   that is fed back to  . The situation described above arises naturally if, for example,   corresponds to the interconnection of an LTI plant and an LTI controller that has been designed without taking into account data-rate limits in the feedback path.
 Throughout this paper we assume that the following holds.
 Assumption 3.1:
 (a)	  is a proper real rational transfer function, single-input single-output and strictly proper,
 and  , then the feedback system of Fig. 1 is internally stable and well-posed.
 (b)	The initial state of , say , is a second-order random variable, and is a second-order wss process with spectral factor	.
  
 Assuming that the loop is stable when   is consistent with our setup where   has been designed supposing transparent feedback. Assuming   to be strictly proper guarantees the well-posedness of the NCS of Fig. 1 for all causal mappings between and . This assumption can be removed at the expense of additional care. However, removing the constraint of   being single-input single-output, requires additional effort
 for our approach to be useful.
 The remainder of this paper aims at building a framework to study the interplay between the average rate at which the channel symbols   are transmitted, and the performance and stability of the NCS of Fig. 1. To that end, we begin by first establishing a general lower bound on average data-rates in feedback systems.
 IV. AVERAGE DATA-RATE LIMITS
 A. Background
 The use of digital communication systems requires the coding of analog signals [8]. Shannon’s separation theorem states that this coding process can be separated into two problems [45]: source coding and channel coding (see, also, [17]). Source coding deals with the representation of continuous symbols using a countable alphabet and, as such, involves quantization [21]. On the other hand, channel coding focuses on the reliable and efficient communication of digital data over an underlying analog channel. We note that separation holds, and is useful, for point-to-point communications where causality and delays are not an issue. If causality constraints are imposed, then separation does not hold in general (see, also, [57]). Nonetheless, the study of causal source-coding problems in isolation constitutes a key open problem in information theory [5], [11], [25].
 The study of optimal source-coding (or quantization) problems is the subject of rate-distortion theory. Rate-distortion theory does not take channel coding into account, and assumes an idealized digital link between the sending and receiving ends [4], [21]. In this paper, we adopt a purely source-coding perspective  and consider source encoders whose output symbols   have a variable instantaneous length, but a bounded average length (see, also, [21]). We note, however, that guaranteeing bounded average data-rates does not guarantee bounded instantaneous data-rates [8], [21]. (Conditions for this to happen are explored in [22].)
 Without loss of generality, we consider source-coding schemes with the structure depicted in Fig. 2 [21]. In that figure,   is a lossy encoder,   a reproduction decoder, and the blocks EC and ED form a lossless encoder-decoder pair (also called entropy coder (EC)—entropy decoder (ED) pair; see, e.g., [8, Chapter 5]). The lossy encoder maps continuously valued random variables into a countable set of symbols. These symbols are then mapped by the EC into a countable set of prefix-free binary words that, in general, changes at every time instant [8].  At the receiving end, the ED recovers the lossy encoder output symbols from the binary words generated by the EC, and the reproduction decoder maps the recovered symbols back into real numbers. A precise characterization of  , , EC and ED is provided below.
 B. General Source-Coding Schemes
 In this paper we focus on single-input single-output sourcecoding schemes within feedback loops, as depicted in Fig. 2. Accordingly, we consider lossy encoders  , reproduction decoders   and EC-ED pairs that are causal and, moreover, operate on a sample by sample basis, without delay. We also assume that side information is available at both the encoder and decoder sides. The new side information that becomes available at time instant is denoted by , for the source encoder, and by for the source decoder. Such side information is contained in suitably defined sets and , where and . We also define the set
 , which contains the common side
  
 Fig. 2. General source-coding scheme used within a feedback loop.
 information that becomes available at both the encoder and decoder sides at instant  .
 In Fig. 2, the dynamic system   is assumed to be such that its (scalar) output   satisfies
 	 	(2)
 where	is a (possibly time varying) deterministic mapping, is the initial state of , is the (scalar) input, and , with , is an exogenous random process. We also characterize the output of the lossy encoder via
 	 	(3)
 where	is the input to the encoder,	is as before,  
 is a (possibly time varying) deterministic mapping, and is a fixed countable set. The symbols   are then used by the EC to construct the binary words   via
 	 	(4)
 where	,	is as before,
 is a time varying deterministic mapping, and is a countable set of prefix-free binary words. The output of the EC is transmitted to the receiving end assuming ideal digital communication (consistent with the source-coding point of view adopted in this paper). Once   becomes available at the receiving end, the ED recovers   via
 	 	(5)
 where   is a time varying mapping that
 satisfies
  
 (6) for any  . Condition (6) reinforces the fact that the EC-ED pairs considered here operate in real time, without delay.  Finally, the reproduction decoder constructs its output   via
 	 	(7)
 where   is as before, and   is a
 (possibly time varying) deterministic mapping.
 Before the reception of   are available at the decoder side. It, thus, follows that the expected length
  , measured in nats,4 of any binary description of the lossy encoder output symbol satisfies (see [8, Chapter 5], [45], and [24])
 	 	(8)
 where   denotes conditional entropy (see Appendix). The gap between both sides of (8) depends on how efficient the EC is at encoding  . It is known that there exist ECs such that [8]
 	 	(9)
 That is, the gap in (8) is smaller than   nats (1 bit) when suitable encoding policies are employed (e.g., Huffman5 coding
 [8]).
 In this paper we focus on the time average of  .
 Definition 4.1: The average data-rate of the source-coding schemedescribedabove,measuredinnats persample, isdefined via
  	(10)
 C. Lower Bounds on Average Data-Rates
 We will now study a lower bound on   that depends only on the joint statistics of the source encoder input   and its output  .
 This bound will play a key role in the remainder of this paper.
 Our derivations require the following assumption.
 Assumption 4.1: The systems  , EC, ED and in Fig. 2 are causal, described by (2)–(7), and such that .
 Assumption 4.1 can be thought as being a “fairness” assumption. Indeed, itis consistentwith the reasonablerequirement that the source decoder uses only past and present symbols, and side information not related to the message being sent, to construct the current output value. In other words, we assume that the channel is the only link from to .
 Definition 4.2: The source-coding scheme described by (2)–(7) is said to have an invertible reproduction decoder
 (in short, an invertible decoder) iff, , there exists a deterministic mapping   such that .
 If a source-coding scheme has an invertible decoder, then knowledge of   is equivalent to knowledge of .
 The generality of source-coding schemes with invertible decoder is examined next.
 Lemma 4.1: Consider any source-coding scheme described by (2)–(7). Define . If, ,
 ,	, and the corresponding decoder is not invertible, then there exists another causal source-coding
 41 nat equals	bits.
 5For Huffman coding the gap is actually upper bounded by	 
 , where   is the conditional probability of the most likely symbol of the alphabet of , given   [16].
 scheme, with an invertible decoder, such that   and
  .
 Proof: Assume that the encoder-decoder pairis such that it has been possible to use to recover from , for all (such assumption is not needed at time instant ). If at time instant cannot be inverted
 (i.e., if there exists no deterministic mapping	such that
 ), then there exist	,	, such that	. Denote
 by the conditional probability of the output of being at time instant , given . Consider now another encoder-decoder pair that behaves like , except for the fact that outputs instead of either or at time instant . At time instant , outputs the value with conditional probability , given . Thus
  
 wherefollows from the definition of and that of entropy,follows from Jensen’s inequality, and follows from the definition of   and that of .
 By repeating the above procedure until there are no two symbols of	that are mapped into the same value	at time instant	, one constructs a source-coding scheme where knowing is equivalent to knowing	,	, and
 , . The result follows upon repeating the above for every .
 Given (9), it follows from Lemma 4.1 that one can always focus on source-coding schemes with invertible decoders,
 without loss of any generality. 
 The next result will be used to prove the main result in this section.
 Lemma 4.2: Consider a source-coding scheme inside a feedback loop, as depicted in Fig. 2. If Assumption 4.1 holds and the decoder is invertible, then the Markov chain holds and, conditioned upon true.
 Proof: Given , it follows from (2) that there exists a deterministic mapping such that . Since
  , it immediately follows that	and	are independent upon knowledge of	, thus proving our first claim. The second claim is immediate upon noting that	depends deterministically upon	.	 
 We are now in a position to state the main result of this section.
 Definition 4.3: The directed mutual information rate across
 (or between
 (11)
 Appendix).	 
 Theorem 4.1: Consider a source-coding scheme inside a feedback loop, as depicted in Fig. 2. If Assumption 4.1 holds and the decoder is invertible, then  .
 Proof: Using (8) we have 
   (12)
 where follows from Property in Appendix and the fact that , follows from the fact that the decoder is invertible, (c) follows from , the fact that the decoder is invertible, and the second claim of Lemma 4.2, and   follows from   and the first claim of Lemma 4.2. The result now follows using (10)–(12).  
 Theorem 4.1 states that, when causality constraints are imposed, directed mutual information rate across a source-coding scheme serves as a lower bound on the associated average datarate. The result relates a physical quantity (average data-rate) to an information-theoretic quantity (directed mutual information rate). Theorem 4.1 also suggests that the appropriate information-theoretic definition of average data-rates in causal sourcecoding schemes is the directed mutual information rate. However, showing that the infimum, over all joint input and output distributions that satisfy a causality constraint, of the directed mutual information rate across a source-coding scheme provides an operationally tight lower bound on the corresponding average data-rate, remains an open problem (see, also, [11]).
 To our knowledge, Theorem 4.1 provides, for the first time, a characterization of the relationship between directed mutual information rate and the operational rate of source-coding schemes within feedback loops. The result in the literature that is closest to Theorem 4.1 is [66, Theorem 2]. However, that result is derived for entropy coded dithered quantizers only (see Section V-B), as opposed to the general causal source-coding
  
 Fig. 3. (a) Independent source-coding scheme and (b) equivalent rewriting.
 schemes considered here. Related results are [53, Lemma 4.8.1] and [27, Theorem B.1.1] where feedback data processing inequalities are presented. However, those results do not focus on operational data-rates, and assume no feedback between the signals at the physical ends of the processing chains.
 Other relevant and related works are [28] and [56]. In [28], the authors study fundamental inequalities involving directed mutual information rates across channels within feedback loops, and presents Bode-like fundamental limitations that arise due to finite capacity communication (see, also, [29]). On the other hand, [56] establishes a relationship between operational data-rates and directed mutual information rate from a channel coding perspective. In that work, the authors show that the supremum, over all joint channel input and output distributions that satisfy a causality constraint, of the directed mutual information rate across a channel equals Shannon’s capacity with feedback for that channel. Despite all the work referred to above, no relationship (besides that of Theorem 4.1) between average operational data-rates and directed mutual information rate from a source-coding perspective, and valid in general settings, is currently available in the literature.
 V. A CLASS OF SOURCE CODING SCHEMES
 This paper aims at establishing a bridge between control and information theories, when a specific class of source-coding schemes is employed. This section presents such class.
 A. Independent Source-Coding Schemes
 In order to obtain a simple (yet useful) framework for the study of the NCS of Fig. 1, we will focus on the following class of source-coding schemes.
 Definition 5.1: A source-coding scheme is said to be independent iff Assumption 4.1 holds, its reproduction decoder is invertible, and the (coding or quantization) noise sequence  , defined via
  
 obeys , where   is a second-order zero-mean i.i.d. sequence, has finite differential entropy, , and the filterhas a deterministic initial state; see Fig. 3(a).
 The class of independent source-coding schemes is restrictive. However, it is a sensible choice when data-rate constraints arise in systems that have already been designed to perform satisfactorily in the absence of quantization. In such cases, which include the situation of interest in this paper (see Section III),
  
 Fig. 4. Considered NCS closed over an independent source-coding scheme.
 it is desirable to introduce quantization effects in an additive fashion so as not to alter the nominal design relations (see, also, [20]). We will describe a practical independent source-coding scheme in Section V-B.
 We begin our study of independent source-coding schemes by noting that the following holds.
 Lemma 5.1: Any independent source-coding scheme can be written as shown in Fig. 3(b), where and are auxiliary signals,   is as in Definition 5.1, and  
 are auxiliary filters with deterministic initial states, such that
  . Moreover, in Fig. 3(b),  
 	Proof:	Our first claim follows upon defining  
 . To prove our second claim, we note that the assumptions on and imply that there exist deterministic mappings , with and invertible, such that (see
 Fig. 3(b))	,	,	,
 	,	and	,
 . Hence
 	 	(13)
 The proof is completed upon using (13) in (11).	 
 Since Lemma 5.1 holds, the system that arises when an independent source-coding scheme is employed in the feedback loop of Fig. 1 can be written as shown in Fig. 4. (Note that the error free digital channel of Fig. 1 is embedded in the independent source-coding scheme of Fig. 4. In Section V-B, we will
 make the channel explicit again.)
 A key feature of independent source-coding schemes is that the directed mutual information rate across them can be bounded by the directed mutual information rate that would arise if all random sources were replaced by Gaussian ones. To be precise, we introduce the following definition.
 Definition 5.2: Consider an LTI system with random inputs and random initial state. If   is a state, input, or output variable of the system, then   refers to the signal that would arise in the place of  , when all inputs and initial states are replaced by jointly Gaussian random variables (or processes) having the same first- and second-order (cross-)moments, and maintaining the same statistical dependence relationships, as in the original situation;   is called the Gaussian counterpart of  .  
 Lemma 5.2: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then
  
 	 	(14)
 where and are the auxiliary variables introduced in Fig. 3 (see Lemma 5.1), denotes entropy rate, and denotes relative entropy (see Appendix). Equalities in (14) hold iff and are Gaussian.
 Remark 5.1: If the conditions of Lemma 5.2 hold and, in addition,   is jointly Gaussian, then (see [47, Chapter 5] and [10])
  
 Thus, if is jointly Gaussian, then is minimized by choosing   to be Gaussian.  
 The relevance of Lemma 5.2 lies in that the characterization of directed mutual information rate under Gaussianity assumptions is straightforward.
 Theorem 5.1: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then
  
 to as the stationary signal-to-noise ratio (SNR) of the sourcecoding scheme.)
 Proof: Proceeding as in the proof of Lemma 5.2 (see (15)) we conclude that
  
 5.2) completes the proof.	 
 Theorem 5.1 provides explicit upper bounds on the directed mutual information rate across any independent source-coding scheme embedded in a stable and causal feedback loop. These bounds are, essentially, expressed in terms of the spectral characteristics of the auxiliary variables and in the scheme of Fig. 3(b). Interestingly enough, there exists a one-to-one correspondence between the SNR   of an independent source-coding scheme, and upper bounds on the directed mutual information rate across it. (Given Theorem 4.1, we can infer that there exists a link between the SNR of an independent source-coding scheme and the associated average data-rate. A precise characterization of such link will be given in Section V-B.)
 We note that [12] also presents a relationship between directed mutual information rate and Bode-like integrals, when Gaussiandistributionsareassumed.Ourresultextends[12,Theorem 4.6] to feedback loops with arbitrary disturbance and initial state distributions. Indeed, [12, Theorem 4.6] can be recovered from the inequality in (16) upon assuming   to be jointly Gaussian distributed.
 We end this section by showing that, for any given independent source-coding scheme, there exists another independent source-coding scheme, with the same noise color   and the same directed mutual information rate across it, such that the gap between the right-hand side of (16) and (17) can be made arbitrarily small.
 Theorem 5.2: Consider the NCS of Fig. 1, where the source-coding scheme is independent and has a fixed noise source . Suppose that Assumption 3.1 holds and define Since and , we have from the Bode integral Theorem [44] and the definition of that
  .
 (19). Proceeding as in [58, proof of Lemma 10, p. 171], we conclude that the last term in (21) can be made arbitrarily small with
  . Our last claim now follows from the last claim of Theorem 5.1. (If for some , then is not admissible since it implies and, thus, the representation of the independent source-coding scheme of Fig. 3(b) would be internally unstable.)	 
 B. Entropy Coded Dithered Quantizers
 Entropy coded dithered quantizers (ECDQs) are devices that have convenient properties that make them suitable for use as building blocks when dealing with average data-rate constraints [63], [64], and [66]. In particular, we show below that ECDQs can be used to construct the noise source   that defines an independent source-coding scheme.
 The structure of an ECDQ is shown in Fig. 5. In that figure,   is a dither signal which is assumed available at both the sending and receiving ends, the EC-ED pair is as described in
  
 where   is the quantization step (a designer’s choice). The output   of the quantizer satisfies
  
  
 Fig. 5.	Entropy coded dithered quantizer.
  
 Fig. 6. Entropy coded dithered quantizer inside a feedback loop.
 where  , and the output   of the
 ECDQ is given by
  
 Modelling quantization noise as an independent i.i.d. source is common in the signal processing literature [42]. In general, this model is not exact [26], [52] but becomes exact in ECDQs when the dither   is appropriately chosen (see, e.g., [43] and [63]). The next theorem shows that this key property remains valid when ECDQs are embedded in strictly causal feedback loops.
 Theorem 5.3: Consider the setup of Fig. 6, where the ECDQ is as described above and has a finite quantization step  . Assume that   is a proper real rational transfer function, that the open-loop transfer function from to is single-input singleoutput and strictly proper, that the closed loop system is internally stable and well-posed when , that the signal is a second-order wss process, and that the initial state of , say , is a second-order random variable. If is such that 	, then
 the noise	is such that
 .
 Proof: Similar to the proof of Theorem 1 in [64] (see [47, Chapter 5] for details).
 Remark 5.2: The definition of ECs and EDs implies that Theorem 5.3 holds irrespective of how the EC-ED pair is chosen. (In particular, it holds if the EC-ED pair is omitted; see [43].) It is also worth noting that, if the dither is not subtracted at the decoder side, then only moment-independence is achieved [59]. For the remainder of the paper, the following consequence of Theorem 5.3 is relevant.
 Corollary 5.1: Consider the system of Fig. 3(b) with and as in Lemma 5.1. If an ECDQ, with dither chosen as in Theorem 5.3 and finite quantization step  , is used as the link from to  , then the system of Fig. 3(b) becomes an independent sourcecoding scheme.
  
 Fig. 7. Explicit rewriting of an independent source-coding scheme that employs an ECDQ as the link between	and	.
 Proof: Given Lemma 5.1 and Theorem 5.3, it suffices to show that the resulting source-coding scheme satisfies Assumption 4.1 and has an invertible decoder. Since in the present situation ,theassumptionson implythat
 Assumption 4.1 is satisfied. Also, since , its initial state is deterministic and, by definition of ECDQs,
 and , we conclude that knowledge of is equivalent to knowledge of . The invertibility of
 the decoder thus follows, and the proof is completed.
 When an ECDQ is used as the link between and in the system of Fig. 3(b), the resulting scheme can be rearranged as illustrated in Fig. 7 (cf. Fig. 2). From that figure, it is clear that feedback from   to the input of   in Fig. 3(b) does not require explicit feedback around the digital channel. (Recall that we
 consider error-free digital channels.)
 Remark 5.3: Insource-codingschemes using ECDQs,theencoder and decoder share a common source of randomness (the dither). In principle, this implies that both the encoder and decoder must share information about the dither. This imposes an additional degree of implementation complexity, but to the best of our knowledge, there is no other simple way of satisfying Assumption 4.1. In practice, one can use synchronized pseudorandom number generators initialized with the same seeds.  
 1) Average Data-Rates in Independent Source-Coding Schemes That Use ECDQs: We are now in a position to present an upper bound on the average data-rate in independent source-coding schemes that use ECDQs. We start with the following result.
 Theorem 5.4: Consider the NCS of Fig. 1, where the source-coding scheme is independent. Suppose that Assumption 3.1 holds, and that the link between the auxiliary signals and (see Fig. 3 and Lemma 5.1) is an ECDQ with dither chosen as in Theorem 5.3 and finite quantization step . Then, and there exists an
 EC-ED pair such that	.
 Proof: By definition of ECDQs
 . Also, from Theorem 5.3 and its proof we have that	and	. Using the above, our first claim follows from the proof of Theorem 2 in [66]. The second claim follows from the first, (9) and (11). [A direct proof can be constructed by using the definition of ECDQs, and the fact that knowledge of   is equivalent to knowledge of   (see proof of Corollary 5.1), to show
 equality in all but the first inequality of (12).]	 
 Theorem 5.4 shows that using ECDQs inside independent source-coding schemes allows one to achieve average data-rates that are close to the absolute lower bounds established in Theorem 4.1. The worst case gap, which originates in the inefficiency of the EC and is smaller than   nats, is intrinsic to any scalar lossless coder and cannot be removed, unless one assumes   (high rate regime; [21]), uses block entropy coding (which may introduce unbounded delays; [8], [45]), or allows the coding scheme to operate in a nonstationary fashion by using time-varying policies [34], [55]. In practice the gap may be smaller than   nats [21, p. 2333], [16].
 A useful corollary of Theorems 5.1 and 5.4 is presented next.
 Corollary 5.2: Consider the setup and assumptions of Theorem 5.4. There exists an ECDQ such that
  
 Remark 5.4: Theorem 5.4 and Corollary 5.2 are only existence-type results. The implementation of EC-ED pairs inside ECDQs falls outside the scope of this paper, and we refer the reader to [47, Remark 5.10] for related remarks.  
 Corollary 5.2 provides a closed form upper bound on the average data-rate in an independent source-coding scheme that uses an ECDQ. The bound is given in terms of spectral properties of the ECDQ output , and two additional constant terms. The second term in (22), i.e., nats per sample (i.e.,   per sample), corresponds to the divergence of the ECDQ quantization noise distribution from Gaussianity and arises because ECDQs generate uniform quantization noise (not Gaussian noise; see, also, [63]–[65]). This term can also be given an alternative interpretation in terms of the space filling loss incurred when using a finite-dimensional quantizer instead of an infinite-dimensional one, with spherical quantization cells. We refer the interested reader to [18], [19], and [65] for further details.  As mentioned before, the term   (1 bit) arises due to the inefficiency of the EC inside the ECDQ. Interestingly, our result holds without Gaussianity assumptions on the external signal   nor on the initial state  .
 Remark 5.5: If the conditions of Corollary 5.2 hold and, in addition,   is Gaussian, then a lower bound for   in (22) is given by the first term on the right hand side of (22). That is, (22) becomes tight up to   nats per sample (see Remark 5.1 and Theorem 5.4).  2) Independent Source-Coding Schemes With Memoryless ECDQs: So far, we have considered ECDQs where the EC-ED pair is allowed to exploit all past and present symbols  , binary words  , and side information  . Such ECDQs have unrestricted memory and its implementation requires the knowledge of the conditional distribution of  , given  . That distribution can be difficult to characterize. In order to simplify implementation, it is common to consider ECDQs without memory (see, also, [64]).
 Definition 5.3: An ECDQ is said to be memoryless iff the associated EC-ED pair is such that
 and	, for all	.
 When using a memoryless ECDQ, the EC can only exploit the knowledge of to encode . Thus, (8) must be replaced by . Again, it is possible to design coding policies such that [8] (compare with (9))
 We now present a definition and two results that allow one to state the counterpart of Corollary 5.2 for the case of independent source-coding schemes that use memoryless ECDQs.
  
 Theorem 5.5: Consider the setup and assumptions of Theorem 5.4. If the ECDQ is memoryless, then and there exists an EC-ED pair such that
 .
 Proof: The result follows from the proof of Theorem 2 in [64] and (23).	 
 Theorem 5.6: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then
  
 where all symbols are as in Lemma 5.2 and Theorem 5.1.
 Proof: The result follows proceeding as in the proof of Theorem 5.1 (see details in [47]).  
 Corollary 5.3: Consider the setup and assumptions of Theorem 5.4. There exists a memoryless ECDQ such that
 	 	(24)
 	Proof: Immediate from Theorems 5.5 and 5.6.	 
 Remark 5.6: In analogy to Remark 5.5, we note that if the conditions of Corollary 5.3 hold, and is Gaussian, then
 (24) becomes tight up tonats per sample.	 
 The EC inside a memoryless ECDQ uses less information to encode   than the EC inside an ECDQ with unrestricted memory. As a consequence, average data-rates achievable with memoryless ECDQs will be, in general, larger than those achievable with ECDQs that have unrestricted memory. This conclusionis consistentwithCorollaries 5.2and 5.3,as Jensen’s inequality reveals. However, the conclusion is pessimistic when ECDQs are used inside independent source-coding schemes. In this case, Theorem 5.2 guarantees that the rate penalty, as measured by the gap between the right hand sides of (22) and (24), incurred when using a memoryless ECDQ instead of an ECDQ with memory, can be made arbitrarily small if an appropriate choice for the auxiliary filters and is made. Hence, without being unduly conservative, it suffices to study the SNR   to give upper bounds on the average data-rate of independent source-coding schemes, irrespective of whether they use ECDQs with unrestricted memory or not.
 C. Discussion
 Consider the NCS of Fig. 1 when the source-coding scheme is independent and uses an ECDQ as the link between the auxiliary signals and (see Fig. 7). For this setup, the results of Sections V-A and V-B allow one to restate control problems involving average data-rate constraints as control problems involving stationary SNR constraints. This enables one to design NCSs subject to average data-rate constraints in a systematic fashion that uses standard quadratic optimization methods. For instance, if performance is measured by means of the stationary variance   of the error signal  , then, irrespective of whether the ECDQ has unrestricted memory or not, the minimal average data-rate needed to achieve a performance level , say , satisfies (see Corollaries 5.2 and 5.3)
 (25)
 In (25), the optimization is carried out with respect to all	,
 , and EC-ED pairs that guarantee stability in an appropriate sense, and is the minimal stationary SNR of the source coder that allows one to achieve the performance level  , i.e.,
  
 Once   is characterized (which, in principle, is a standard quadratic control problem), one readily obtains a bound on  (see, also, Section VI).
 The framework provided here is conservative since it is based on inequalities that are not tight in general.  However, the framework provides bounds that are guaranteed, by construction, to be achievable with conceptually simple building blocks, and do not rely on any asymptotic approximation. As illustrated above, the framework also opens the door to use standard synthesis methods to deal with average data-rate limits in control system design, as explored in [47]–[49]. These are the main features that distinguish our approach from the literature surveyed in Section I.
 VI. EXAMPLE: MEAN SQUARE STABILIZATION
 We now illustrate the approach outlined in Section V-C by studying bounds on the minimal average data-rate for stability,
 i.e., we consider   in (25). The case of finite   is omitted due to space constraints (see [48]).
 A. Mean Square Stability Subject to SNR Constraints
 We start by studying the interplay between constraints on the source coder SNR   and stability. By virtue of Theorem 5.3, we focus on the feedback system of Fig. 4, and use the following notion of stability.
 Definition 6.1: Consider the linear system
 , where , is the system state at time instant , , where is a second-order random variable, and are constant matrices of appropriate dimensions, and is a second-order wss process. The system is said to be mean square stable  (MSS) iff there exist finite   and a finite and positive semi-definite both not depending on , such that
 small with a sufficiently large (and cannot be made equal to zero since, by assumption,). Well-known results [58, Lemma 10, p. 171] allow one to conclude that there exists   such that the gap in   is arbitrarily small. The
 result, thus, follows. (The existence of the family also guaranteed by the results in [58].)
 Theorem 6.1 states that, for independent source-coding schemes, the minimal SNR that is compatible with MSS, i.e.,
  , is only a function of the unstable poles of	. Hence, for any given  , the condition	is necessary
 and sufficient to be able to find , and a noise variance  (equivalently, a quantization step ) such that the resulting NCS is MSS and the SNR satisfies .
 We conclude this section with a simple corollary of Theorem
 6.1.
 Corollary 6.1: Consider the setup and assumptions of Theorem 6.1. If  , then the stationary variance   of the error signal   grows unbounded.
 Proof: The result is immediate since, by Assumption 3.1,   and, by Theorem
 We see from Corollary 6.1 that the study of conditions on  for MSS are insufficient to give performance guarantees. This fact is unsurprising, and consistent with results in [15] and [51].
 B. Mean Square Stability Subject to Average Data-Rate Constraints
 We now return to the NCS of Fig. 1 when the source-coding scheme is independent and uses an ECDQ. Given the definition of an independent source-coding scheme, it follows that the notion of MSS described in Definition 6.1 is still valid in this setting. It is also clear that, provided Assumption 3.1 holds,
 , and regardless of the EC-ED pair, the considered NCS is MSS iff	and	.
 Corollary 6.2: Consider the setup and assumptions of Theorem 5.4. Then, irrespective of whether the ECDQ has memory or not, the following holds.
 (a) The minimal average data-rate compatible with MSS, i.e.,   as defined in (25), satisfies
 (28) guaran-
 .
 (a)	Consider the case of ECDQs with unrestricted memory.Equation (22) and Theorem 5.2 yield
  
 The upper bound in (28) follows upon using Theorem 6.1 in (29). The lower bound follows from [34, Theorem 2.1].
 The case of memoryless ECDQs follows similarly.
 (b)	For any , and sufficiently large and	,
 . Also, irrespective of the memory in the ECDQ, there exists such that
 (see (17), (22) and (24)). Thus,
 and the result
 	follows upon choosing	.
  
 Corollary 6.2 establishes lower and upper bounds on the minimal average data-rate that is compatible with MSS in the considered NCS, when an independent source-coding scheme is employed. A feature of our proposal is that ECDQs with unrestricted memory do not provide any advantage over memoryless ones (at least from a MSS point of view; see, also, discussion at the end of Section V-B). This is relevant in practice since the implementation of ECDQs with unrestricted memory is computationally prohibitive. Indeed, in order to design an independent coding scheme that achieves stability at an average data-rate smaller than  , it suffices to use a memoryless ECDQ with sufficiently large quantization step  , an EC-ED pair designed using Huffman coding, and filters and that, for the situation studied in Section VI-A, guarantee MSS at SNRs sufficiently close to  . By doing so, however, the performance of the NCS will be compromised (see Corollary 6.1). This is consistent with results in [34] and [36].
 The results of Corollary 6.2 show that, when an independent source-coding scheme is employed in the NCS of Fig. 1, MSS can be achieved at average data-rates that are guaranteed to be no larger than the absolute bound identified in [34] and [54] plus  nats per sample. This
 extra rate is, in our view, a fair price to be paid if one constrains oneself to the conceptually simple source-coding schemes considered in this paper. We note however that the upper bound in (28) is a worst case upper bound. As mentioned earlier, in practice one can expect to achieve MSS at rates no larger than   (see [16] and [21]).
 Our results can also be used to provide upper bounds on the average data-rate that is needed to achieve MSS, when memoryless source-coding schemes are employed in the considered
 NCS. Indeed, if one chooses	 ,	and a memoryless
 ECDQ, then the resulting source-coding scheme has no memory and it is easy to show that   and the EC-ED pair can be chosen so as to achieve MSS at an average data-rate satisfying
 	 	(30)
 Relation (30) can be contrasted with the results of [55, Section VI] (even though [55] focuses on a different notion of stability). That work shows that there exist memoryless encoders that guarantee stability with bounded (but otherwise unknown) data-rates, whist (30) provides a computable upper bound on the minimal average data-rate compatible with MSS.
 VII. CONCLUSION
 Thispaper hasstudied acontrol problemwherean LTIsystem is designed assuming transparent feedback and, at a later design stage, a unity signal transfer function source-coding scheme is tobe usedsoastominimizetheeffectsthatdata-rate limitsinthe feedback path have on closed loop performance. To address this problem, we have focused on a class of source-coding schemes and, by doing so, we have established a bridge between information and control theories. A key result of our work is that, for the considered class of coding schemes, average data-rate limits can be enforced by imposing signal-to-noise ratio constraints in a related additive noise communication channel. As an application of our results, we studied the interplay between stabilityand average data-rates in the considered setup. For that problem, the proposed class of coding schemes was shown to achieve mean square stability at average data-rates that are guaranteed to be less than 1.254 bits per sample above the absolute minimum established in [34].
 We refer the reader to [48] and [49] for applications of the framework presented in this paper to problems beyond stabilization. A key open problem not addressed in this work is how to incorporate average data-rate limits into control problems using causal (but otherwise unrestricted) source-coding schemes.
 APPENDIX
 The following definitions and results are standard and can be found in [8]. Unless otherwise stated, all variables are assumed to be random variables (RVs) with well defined (joint) probability density (or mass) functions (PFs). The PF of is denoted . refers to the conditional PF of , given . denotes mean with respect to ; stands
 for	independent of	.
 	The differential entropy of	is defined via
 . The conditional differential entropy of , given , is defined via	.
 (equality iff	), iff	.
 .
 	If	is Gaussian with finite variance,
 .
 If and are discrete RVs, then we use to denote the (conditional) entropy of (given ). The definitions are analogue to the continuous case.
  .
  
 (Conditional) mutual information between discrete RVs is defined as in the continuous RV case.
 The relative entropy between and (or divergence of the distribution of with respect to that of ) is defined via . Given joint distributions
 and	, the conditional relative entropy is defined via	.
 	(equality iff	),
 .
 If is the Gaussian counterpart of (see Definition 5.2), then . If , are the Gaussian counterparts of , , then
 .
 If is uniformly distributed on , and is zero mean Gaussian with variance , then
 .
 	The RVs	,	,	form a Markov chain (denoted	) iff
 . If, conditioned upon , , then we write	.
 	If	is a deterministic function of	, then	.
 	If	, then	(equality iff
 ).
 	If	, then
 	(equality iff	as well).
 The entropy rate of a stochastic process is defined via . A useful fact is the following.
 	If	is an asymptotically wss process with stationary
 PSD , then	(equality iff, in addition, is asymptotically Gaussian; see [30, Lemma 4.3]).
 
 ﻿ We present an based model for the gain of an indoor antenna array . This to the of the model , as applied to outdoor indoor links . Our model is applicable to a few adjacent in an system such as . We find that as much as out of of antenna gain are attainable even in rich indoor scattering . At the same time , no fade margin is when the array . the antenna . To provide a reference for the we consider a simple propagation model , which is theoretically and via simulation . This model is found to match our empirical very well . 
 Index Channel , directive , gain reduction factor , outdoor indoor wireless links . 
 I . INTRODUCTION 
 D 
 have been used to improve received signal power since the beginning of . However the specification of an antenna effectiveness in such as improvement of signal strength and reduction of interference is typically for propagation devoid of or reflecting , i . e ., under free space . The more recent appearance of cellular wireless service , in propagation where the presence of is the rule rather than the exception . 
 Relatively little been on the interaction between propagation and the antenna gain . The decrement in achievable antenna gain due to local scattering was originally defined in as the Gain Reduction Factor . This factor was as a location dependent random variable and its statistical were for a specific environment , namely suburban outdoor to outdoor links . The very fast growth in wireless traffic in urban forced base coverage to those of , , for which the may no longer hold . 
 While short range outdoor to indoor links for static or nomadic have been , the effect of directional on those links received very little attention . In fact , most have been based on the use of . Moreover as stated in , path loss prediction are much greater when directional instead of . 
 Recent in antenna design the use of have made it practical to consider the application of in relatively small , such as those used in wireless local area . Such will allow taking advantage of propagation where the dominant lie in an angular range that is much narrower than the array . To the best of our knowledge , there is no general model available to describe angular spread when indoor wireless service is provided from outdoor bases . The actual gains achievable with directional in this setting therefore an empirical study similar to what was in for suburban links . This is the main focus of the work we describe here . We provide the first statistical description of the for this type of environment . 
 The Mean Effective Gain , was in as a useful single parameter to describe the impact of the antenna on the link budget for , primarily for mobile links . This is defined as the ratio between the spatial average power received by the antenna and the sum of the average that would have been received in the same environment by two isotropic , vertically and horizontally . A theoretical method is in to analyze the of a mobile antenna . A general expression based on a statistical model of incident is and empirical data was at to validate the model . A statistical model for incident is also , where model derived and measured of are for street links at . . An analysis of some fundamental of the and the corresponding physical is in for theoretical Rice channel . In contrast to the , as a random variable , the is an average value . 
 . Personal use is permitted , but republication redistribution permission . See : index . for more information . 
 An approach based on modeling from the gain pattern of an antenna as a result of propagation is in . This is defined as the Effective Directivity Antenna Model , which a bin fitted stochastic model to correct the nominal antenna pattern . The based model in the above was for specific directive in outdoor outdoor and indoor indoor links , . and . are carried out to show that the model more accurate path loss prediction than the when directive are used . The however do not focus on the reduction of available gain for beam indoor when are from outdoor bases , as we discuss here . 
 The originally defined in , was subsequently in but the corresponding statistical model is based on empirical data collected only in suburban links , . The approach is based on the arrival distribution of power density which from our methodology , as will be in the next section . 
 Short range outdoor indoor wireless are links for which there is still a lack of empirical , . Several such as and the therein have the subject of path loss characterization for that in some are similar to ours . However , links with directional are only considered in and in . A comparison of maximum received power directive and dipole is in , but this work does not include a statistical characterization of the gain difference between both antenna . In , and of outdoor indoor links that include directive are , but this work does not include a comparison of received power for different antenna . 
 The main objective of this paper is to evaluate the effectiveness of steerable directional at the indoor user terminal in typical short range , fixed wireless , outdoor indoor service . We propose a statistical model for the of links , applicable for instance to a group of within the coherence in transmission . These are important since antenna gain may compensate for the propagation loss disadvantage of indoor outdoor as to indoor indoor links , opening up a much application space for cellular service . The were carried out at . a transmitter and a carefully power measuring receiver . The frequency used is the same as in the fixed wireless service . A total of different links were tested in the urban in Section . The links included in the relative position of outdoor and indoor and variation in path , ranging from to . 
 The remainder of this paper is organized as : Section the theoretical background . Section the measurement hardware and methodology . Section statistical based on our empirical data . Finally , . 
 . CONCEPTUAL BACKGROUND 
 For convenience we assume the outdoor base to be the transmitter and the indoor user the receiver . The receiver is connected to a directional antenna , capable of searching in azimuth for maximum power . The received power at the antenna in a wireless link with properly can be expressed as 
  
 , 
  
 where is the transmit power , and are the transmit and receive antenna gains in free space , for feeder cable between the , the transmitter and the receiver and is the path loss associated with the propagation environment . Expression if the angular spread of the is much smaller than the antenna , as may be the case for a base antenna above clutter . For an indoor antenna surrounded by clutter this directivity gain will in general no longer hold . While the antenna can be in the direction of the arrival , its radiation pattern will attenuate from other , which may be by a beam antenna , in particular an element . Further , as in , we define the as the decrease in gain advantage of a directive over an antenna , due to the presence of clutter . Let G and G be the nominal free space antenna gains of a directive and an antenna respectively . We denote as the actual in clutter gain advantage of the directive antenna when for maximum received power . Then , 
 G G 
 where and are location dependent random . 
 To estimate for a given link , we measure the received power an in azimuth antenna . Let this power be . For exactly the same antenna and transmit power , we repeat the measurement with the directive antenna , rotating it in search of the maximum power . The difference in received power is then the in clutter gain advantage of the directive antenna , , at that spatial position . 
 Then from we can write the as : 
 G G 
 . 
 Since and are the position dependent random received by the , the is by the difference between the spatial of a directive and an antenna , both at the same position , by the difference in free space gains . 
 To establish a basis that can serve as a reference for comparison with our empirical , we use a very simple model for our propagation environment , which nevertheless proved to be quite accurate , as we will see in Section . This model , in that at each indoor measurement location a very large number of equal power at the receive antenna . These are distributed uniformly over the complete azimuth angular range with phases distributed uniformly from to p . In addition to these scattered , a single dominant from one direction , as in Fig . . 
  
 Fig . . Spatial representation of propagation model . 
 The average power ratio of the dominant wave over the total of the scattered is the factor . We now consider a directional antenna rotated in azimuth . For simplicity we assume that the directive and the have the same vertical gain and that the horizontal gain pattern for the directive antenna is constant within the and zero elsewhere . If this antenna is pointed in a direction that does not include the dominant path , then the received voltage will be distributed , provided that one can still assume that a large number of fall within the . On the other hand , when the directive antenna the dominant , then the received voltage will be Rice distributed . However the factor for this 
 distribution will be times that the voltage received by the antenna . This is because the dominant wave is subject to the horizontal antenna gain of the directive antenna , while the scattered power is on average unchanged , as the gain increase is exactly offset by the reduction in number of the received diffuse . 
 With these we can statistically describe the ratio of the power received both of . For the antenna at random in the room the output voltage envelope is a Rice distributed random variable by the local factor . We now consider that at the same position the directive antenna is rotated in search of the maximum power , where antenna , with the antenna expressed in . Then the directive antenna output voltage can be written as , where the collection of random i of random and one Rice distributed random variable with a factor equal to . It is reasonable to assume that these random are independent , since they arrive from non angular . From this we can obtain the statistical characterization of the ratio of between the directive and the . 
 As an example we will consider the limiting situation where . We note that in this case the directive antenna will only be able to choose among distributed random and will thus act as a branch selection combiner . This us to use well known . We characterize the statistics of the ratio , as it from that this will determine the statistics of the . the derived for an branch selection combiner , we see that the Probability Density Function of the output power , 
 is 
 e e , 
 where we have assumed that the average power for each direction is unity . This can be done without loss of generality since we will only consider power . For the antenna the of output power will then have an exponential distribution with unit average . We will also assume that this random variable is independent of the preceding . 
 e . 
 Given the statistics of and we now obtain the ratio distribution for . the version , we find that the of for the case can be calculated as 
  
  
  
 The above integral can in this case be in closed form . again we the u and from it the corresponding Cumulative Distribution Function 
 u N 
 where is the gamma function . 
 In the above example the directive antenna can only offer a diversity gain . The case nonzero can be the same procedure , instead of in and . However the resulting integral corresponding to does not yield a closed form expression and is thus best numerically . We note that when the these must not be , since the model must consider that the average received are not equal as the vary with angular position . We will in general use when to the random power ratio , given that the factor is equal to . As the is 
 equal to by the ratio of nominal antenna gains , the statistics of the are readily from those of 
 . 
 We plot in Fig . the of the for the , , , under the assumption . We have here again assumed that both type of have the same gain in the 
 vertical plane so that . The were calculated for the case , and numerical integration nonzero . In addition we the previously model generating the corresponding random . The were identical . 
 We see that for small of , such as in our , the model that the will be negative with non zero probability i . e ., the array gain advantage can appear to be higher than in free space . This to the condition where the antenna is subject to a large fade while the directive array at the same position can combat such a fade angular selection diversity . Under the we , even for as low as , which a significant fade probability for 
  
 Fig . . of the for the , , , under the assumption . 
 reception , the directive antenna would almost in every case select the dominant direction and as a result only exhibit very shallow . Thus , we have a combination of classic deterministic gain as well as statistical reduction of fade margin , which can result in an overall gain higher than in free space . 
 Throughout this work we consider that the beam scanning of the directive antenna is only in azimuth and that the elevation of the is than the corresponding angular spread and hence the vertical gain is not reduced . 
 Our approach to calculating the is different from the one originally in see in , which is based on a transmission much than the typical coherence of the links considered . The use of the real valued angle of arrival distribution of power and the antenna power gains used in is under such due to the effect over frequency . Instead , for short range links the coherence is much , in the range of . We thus here consider the case of a transmission , for example a range of within the channel coherence . Although the definition of the is not dependent on the relation between the transmission and channel coherence , in the case the approach used in would not be practical . In fact , calculating the received power from the angle of arrival distribution of the electric field would require phase information , and the complex antenna voltage gain . Instead we found it much simpler to directly measure received power with various an dipole used as reference and to apply . 
 . MEASUREMENT EQUIPMENT , AND PROCEDURE 
 The measurement system of a . continuous wave transmitter and a purpose built receiver coupled to a power meter . The outdoor base station into the antenna terminal of a 
  
 Fig . . Measurement location map . 
 . gain dipole . The receiver is . This is wide enough to capture any frequency dispersion that the transmission , induced for example by vegetation movement . 
 The measurement procedure is essentially the same already in , a computer to control antenna position and the synchronized acquisition of power . For each antenna position consecutive were . These were then to remove residual temporal , which were typically less than . . Before each measurement we calibration to assure that the transmitter and the receiver were operating with their nominal and gains . In all field the received power was at least above the noise floor . 
 A . Measurement 
 were carried out at urban type in and , Chile . We selected where the base terminal could be positioned at a range of to from the indoor antenna . This space was in some by with and no higher than . Three were chosen in , one of them on the campus of and two in private . The fourth location was the campus of Maria in , which a wide range of indoor that and relatively large . A schematic of one of them is shown in Fig . where we also specify the outdoor base and indoor subscriber unit . Fig . the setup of the indoor measurement system for the analysis of both small scale and statistics . 
  
 Fig . . Measurement configuration for the case showing the on axis rotating system and the . rotating arm . 
 At these we measured a total of links that to two of , line of sight through a window or non line of sight when the direct path was blocked . This blockage included foliage of , small or a wall . The thickness of the foliated obstruction was typically in the range of . to . The construction was of the brick and mortar type , with non . Window sizes varied in width in the range of . to . and in height from . to . . The minimum window area was m . All were made by the outdoor antenna that the wireless at . height . 
 B . Measurement of Path Loss and Spatial Fade Statistics 
 We firstly at fully our test . This involved the receiver to the dipole antenna on a . length rotating arm stepwise in azimuth , which a displacement of approximately between successive antenna . The power in a rotation at each placement were used to calculate the spatially path loss at that range and to generate the statistics of with respect to that average . The size of the region for the spatial average is well within the shadow fade correlation distance in previous work , , . In fact the in the literature , corresponding to indoor , are in the range of to m . This was repeated over a variety of in our test . The our statistical data with that of typical path loss . We note that these were only at the propagation environment and that spatial was not used in the , as will be below . 
 C . Measurement of the 
 To directly measure the of the as in the previous section , we used a platform capable of rotating the about their axes in with no displacement other than rotation . Power were acquired as before at each angular position . Four of were used indoor , an . gain dipole used as reference and three different directional . The first was a . gain patch antenna with elevation and azimuth half power of and respectively . The second was a . gain horizontal linear array of four vertically polar 
  
 Fig . . horizontal gain for the directive . 
 with elevation and azimuth half power of and respectively . The third was a . gain array of four vertically with elevation and azimuth half power of . The corresponding horizontal gain for the directive , measured in an chamber , are in Fig . . At each measurement location , the nominally antenna was rotated exactly as the directive to average out minor gain less than . Care was taken to place the of in exactly the same position at the beginning of each rotation and this was repeated for a wide range of as will be later . We power for the full rotation , which in the case of the directional us to obtain the maximum signal strength as well as the direction of arrival of the diverse resolvable . The computer measurement procedure is accurate to of one degree and us to verify that were repeatable , i . e ., that successive at any given location would yield the same result . 
 . MEASUREMENT AND ANALYSIS 
 This section the of the and the statistical data derived from them . 
 A . Characterization of the Test Environment , Path Loss 
 We start by our measurement environment from a propagation point of view . To this effect we the data collected with the rotating arm in our to model average path loss and small scale fade statistics . The extensively used log normal model that the spatially path loss in may be as : 
 , 
 between the and , d is the free space path loss at a distance d in , is the 
 TABLE I 
 SUMMARY OF PATH LOSS STANDARD THE AND 
  
 path loss exponent and is a zero mean distributed random variable in with standard deviation . 
 We applied this model to our data after out over one rotation of the arm that the antenna . As conventionally done , we chose d . of the model in , multiple or choosing the intercept point d different from the free space path loss did not provide a measurably better fit . In addition , we the of the small scale with respect to the local . As , we found that the fit to was very good . We show in Table I the model . The were below . in of the for both and links to a very rich environment . It that our test environment is not by large shadow fade related power , particularly for the case , but that it is rich in propagation , as for an indoor setting . 
 B . Characterization of the Test Environment , Angular Spread 
 Consistent with the above , we found the indoor propagation environment to be rich in over a broad angular range . The dominant energy in general from the direction of the nearest window . Moreover , when pointing the antenna in the general direction of the dominant path , angular in the range of a typically in a received power pattern that closely that of the specific antenna used . 
 This may be seen in Fig . , where we plotted received power . angle for the case of the patch array in . To avoid a very figure we only show chosen at random among the total of . These were to their maximum power and the angle at which this power was was set to zero . We also plot the antenna gain pattern and the average per angle from all . It can be seen that the dominant path is not resolvable into separate by the we used and that the angular spread of these dominant is narrower than the of the directive . Other were typically to arrive as wall from than the used , as seen in the above figure . This can be with of the propagation model in Section , assuming the rotation of a directional antenna with the same gain pattern as that used in our . Since in this case our are exclusively based on simulation , we chose to also include the randomness of the factor , which for our links was approximately uniformly distributed in the range to . . Thus in Fig . we show from . to . and the average of . As seen , the model match the behavior quite well , although in contrast with the , the average power for the 
  
 Fig . . Measured received power . angle for . 
  
 Fig . . Model received power . angle for . . . 
 is not constant at angular outside the dominant direction . It may be that in our , the angular distribution of the the direction of the dominant path . 
 To further describe the angular of the , we the angle of arrival of the signal , with the angle of the straight line with . The of this difference is shown in Fig . for both the and . As seen , for links the dominant on a close to direct path from the base . We found that for of the angular deviation was no than . , i . e ., about half a . Instead , for links the angular range for of was . . The latter value is by the angular difference of between the direct path and that of the nearest window in our , which for of was less than . Alternatively , we found that when considering of , aiming along a direct path in a power reduction reaching up to of . for and for links . 
  
 Fig . . of the difference between the angle of arrival of the signal and the angle of the straight path between and . 
  
 Fig . . of the Gain Reduction Factor for the array . 
 C . Statistical Description of the 
 Our show that , as previously in suburban , the may be quite accurately as a random variable . Fig . the cumulative distribution for the case of the array in and . In the same figure we also show the when the model in Section , considering of . and . for and respectively . 
 Here , the model based need to take into account that the vertical gains of the and the directive are not equal . As a result the increase in the factor that the directive antenna voltage only to the horizontal gain advantage over the antenna or equivalently its horizontal reduction , a factor of for this case . On the other hand the total gains of each antenna were considered when calculating the they receive . As seen , despite the simplicity of the model , it measurement accurately , the error being negligible at the 
 TABLE 
 SUMMARY OF MEAN AND STANDARD DEVIATION OF THE 
 BEST FIT MODEL FOR THE GAIN REDUCTION 
 FACTOR FOR VARIOUS 
  
 TABLE 
 SUMMARY OF THE BEST FIT FOR A 
 THE MODEL OF IN 
  
 and probability , while at probability it is . for and to . for . We have the corresponding for the , as they are quite similar . In contrast with , the exhibit a considerable likelihood of negative i . e ., the array gain advantage would appear to be higher than in free space . As already , this at where the is subject to a large fade , which the angular selection diversity gain of the directive antenna can significantly reduce . In negative are not possible since the small scale have been out in frequency . 
 The of the best fit random the for all is by their mean and standard deviation in Table . 
 Although our propagation are different , we fitted the model in to these average . The expression used is 
  
 where is the half power azimuth antenna in . The best fit for A in Table . As in Fig . , the fit is quite good , although the actual average in our case are considerably lower than those measured in suburban . 
 Furthermore , we measured an increasing average with , but this is much less than for the outdoor case . This may be due to the scalar integration of for the case . vector addition of in our case . Therefore in the case the monotonically the total received power , whereas in our case excluding secondary reflected can in some be of benefit . Another manifestation of this phenomenon is the fact that we measured a significant fraction of negative , which contribute to lowering the average . 
 We also found a relatively large standard deviation of the with no statistically significant variation with as to . This is consistent with our angular spread , which is typically dominated by a strong component that is narrow with the directive antenna . Therefore when aiming these towards 
 Fig . . Average Gain Reduction Factor versus antenna . 
 the signal , there will be little difference among them resulting from the effect of secondary . Equivalently , all directive see the same strongly dominant , from which they extract power in accordance with their respective effective . The variation of the , which is similar for all directive , may instead reflect the fact that in our measurement the antenna used as the common reference is subject to considerable Rice spatial fading . 
 We also study the possible correlation between the and the shadow plus small scale fade at the location , as measured by the dipole . It cannot be assumed a that the is a random variable independent of these . We calculated the total of the dipole with respect to the average path loss regression as well as the for the same measurement point . The correlation coefficient between these random is then easily . We found that for all used and both and , the ranged between . and . . The negative in all the fact that deeply faded for the dipole antenna will be those where the directive can offer the benefit , due to angular diversity gains . The negative correlation between the spatial and the , that a directive array is capable of partially that would affect a dipole at the same position . We discuss this further in what . 
 D . Achievable Power Gains 
 To illustrate the actual gains that were with directive we plot in . and the received power for our with the array and the dipole , considering the and separately . We recall that for the dipole these do not include spatial , while for the directive they represent the peak received power over a rotation at the same measurement position of the dipole . This is consistent with a scenario where the user the antenna at a random position , without searching to optimize 
 Fig . . Received power power of . Straight and dashed represent the best fit linear regression for a dipole and for a array respectively . 
  
 Fig . . Received power power of . Straight and dashed represent the best fit linear regression for a dipole and for a array respectively . 
 location and hence , small scale Rice fade must be included in the link budget . 
 In these we also include the linear for both of , considering an intercept at based on free space propagation with the respective . The least fit are in Table . As seen , the are consistent with those for when out small scale . In the case , which is dominated by small scale , the negative slope is marginally , as the lack of spatial deep when measuring power in . The of the difference of received power , with respect to the regression , were in both very close to . The difference between the two regression in the distance range where our were concentrated is as almost identical to the nominal gain difference of the after the average . 
 TABLE 
 SUMMARY OF RECEIVED POWER STANDARD THE AND 
  
 We observe that the standard deviation of received power is about the same for the directive and the dipole . While the antenna array may benefit from fade reduction due to diversity effects , this is more or less by the variability of the . As a result , similar link fade can be used for both antenna . 
 In summary , we found that indoor able to search for the signal with a directive antenna instead of an one , can achieve very significant . For example a nominally . gain antenna will in a indoor setting be able to provide an average . power gain . At the same time , the choice of a directive over an antenna over does not increase the . As already noted , part of the gain is attributable to classic and part to the angular diversity advantage of the directive antenna . As a reference we note that for a purely fading environment , the average diversity gain of the antenna , would have been only . , corresponding to a branch selection combiner . 
 V . CONCLUSION 
 The of an extensive measurement campaign , covering a variety of short range outdoor indoor links , show that significant power gains are available to indoor a steerable directive antenna . For the case that we considered , the antenna gains that should be used for link budget are within . of the nominal , free space gains for example . instead of . for the array . Moreover , while the directive exhibit considerable randomness in the , they also benefit from angular diversity and the combination of both effects in fade statistics that are very similar to those of the antenna . Thus , the use of a directive antenna indoors an increase in average received power , without the need for increasing the fade margin . 
  
 The wish to acknowledge the help provided by . Hector in the design and calibration of the used in this work . 
  
 ﻿ This paper closed form for the space frequency second order statistics for the power gain of indoor . These hold for channel in which path are clustered , both in their arrival and their arrival times . Assuming that the arrival times are independent of the of arrival , we derive a closed form expression for the covariance between the channel power gains at two , at any pair of and . This expression that same covariance asymptotic reduction attainable by increasing , only if , and that the covariance is symmetric in its dependence with and with . Thus , the channel power gain at higher frequency less variation than at lower . Finally , we show that the variable part of the covariance approximately as . 
 INTRODUCTION 
 The propagation of radio within indoor is a complex phenomenon , depending on the operating frequency , the type and location of the and the presence of a usually large number of . Assuming linearity and that the channel vary slowly , the channel behaviour is fully by its frequency response ,, corresponding to the transform of its time invariant channel impulse response . 
 Due to multiple in the surrounding , is typically composed of a sequence of short with random arrival times and , which tend to decrease with arrival time . It been that , in indoor , both the arrival times and arrival of these path distribute in see and , respectively . As a result , given any two known to belong to the same cluster , one cannot assume their times to be independent . The same is true for their of arrival and for their corresponding . 
 The analysis of the space frequency statistics of the indoor wireless channel commonly been by the second order statistics of the complex frequency response or of its magnitude . The former case the complex correlation 
 c , , E , 
 This work was by the Research Council under , and Basal project , and by the Research Direction of Mar ´ ´ . 
 . 
 where H and H are at receiver or transmitter antenna x and x , respectively , and where complex conjugation . This complex correlation , with , been studied in , and in , analytically . In both , it is found that with the x and x more rapidly when the operating frequency is . The analysis in closed form for c , , for non isotropic three and two dimensional diffuse assuming independent , i . e ., that the complex channel gain associated with from one direction are independent of those from another direction . Under this assumption , it was shown in that for the dimensional case , 
  
 the speed of light , is the complex series coefficient of the distribution function of scattered power , is the th order function of the first kind , and f is the angle of the vector x x with respect to a reference azimuth direction . 
 The second case envelope correlation is concerned with E H H and the corresponding envelope correlation e , , . The dependence of the latter expectation on the studied in , , from measured data and for . These reveal that , as , e , , as the , although it to approach a minimum , asymptotic value in the range . . , which is significantly than the lower asymptote of c , , . 
 Our focus in this paper is on the channel power gain and its second order statistics , associated with E H H . Considering the latter expectation instead of that of the complex gains or their is relevant for two main . First , the instantaneous channel power gain at a given frequency the associated capacity . Hence , in the second order statistics of would allow for a better estimation of the performance of spatial diversity based upon , for example , maximum ratio combining with associated to different antenna and or carrier . Second , in based upon selection combining , as well as in multiple access subject to the power capture effect , the performance on the joint statistics of two or more power , from different and carrier . 
 In this paper , we find closed form for the second order statistics of the channel power gain , associated with the 
  
 These depend explicitly upon the , not necessarily equal and the separation distance . Our analysis is valid for an extended model , with with clustered arrival times and , as in the considered in , . The reveal how c , , are , but predict a strictly positive lower asymptote , which is by go to infinity , for any , and by , but only if . 
 In other , there a dependent additive term in c , , and , in the limit , increasing frequency or spatial separation asymptotically this term to zero , yielding the same asymptotic reduction . A second finding is that c , , in exactly the same way with respect to as it does with . The latter , by considering the case , that the power gain of with higher central frequency have smaller variability . On the other hand , the show that , beyond of a few central frequency , the dependent component in c , , as . 
 In the next section , we present the channel model and . The main are derived in Section , by . All are in the Appendix . 
 CHANNEL MODEL 
 As in , , the wireless indoor considered here are by their impulse . Each impulse response of a sequence of path with random , at random times and , and where arrival times and are clustered . At a given reference location x in space , such random impulse response h can be written as 
  
 where ti , is the random arrival time of the th in the i th cluster and ai , is the corresponding real valued amplitude . Our associated with the arrival times , and of are next . 
 A . Arrival Times 
 The arrival times can be decomposed as ti , Ti ti ,, i , , ,... 
 where Ti is the random arrival time of the i th cluster , and ti , is the random delay of the th in the i th cluster relative to Ti , where T T and ti , ti , , for all i . By definition , the cluster with its first ray , and thus 
 ti , , i , ,.... 
 In the forthcoming analysis , it will be useful to describe the overall distribution of arrival times by their corresponding arrival density . This one to introduce the following preliminary and notation . For an infinite sequence of random arrival times , with x possibly 
 being equal to zero , let 
 denote the number of from falling inside 
 , . Define , for i , , the arrival 
  
 i i , if 
 , if z 
 We can now define the single and joint arrival density 
 of as 
 if x is random 
 , if x is random 
 , if x 
 B . 
 The random ai , in are assumed to be of the form 
 ai , pi , i , , ,.... 
 In this expression , is a random variable the amplitude of the i th cluster . The real valued random variable ai , is the amplitude of the th within the i th cluster relative to that cluster amplitude . The polarity pi , take from , with equal probability . 
 The cluster are independent given their arrival times . As in , the dependency of the cluster upon their arrival times can be by the cluster order moment delay profile 
  
 if T , and E , , ,... 
 if T is random . Likewise , the relative are independent given , and we define 
 their moment delay profile as 
  
 where , , for every i . As we shall see , here we will only need to consider the ,,..., . 
 C . Arrival 
 Let i , be the angle of arrival , measured with respect to a reference the horizontal plane , of the with arrival time ti ,, As in , , the of arrival of the within the th cluster can be decomposed as 
  
 In this expression , is the angle of arrival of the th cluster , while , for each are the of arrival of the 
 relative to the th cluster angle of arrival . 
 If the transmitter and receiver and the scattering environment are chosen at random , then the cluster will be uniformly distributed , as in , . Likewise , the absolute will also be uniformly distributed . Moreover , the in , reveal that , in indoor , the distribution of the difference between cluster and the angle of the first cluster in each location is also uniform . This that , conditioned to a given location room and building , cluster arrival can be assumed independent . In contrast , we shall not assume that the are independent , since the within each cluster have been found to arrive from a relatively small range of . Instead , we will assume that , for all , the relative are independent and identically distributed . More precisely , we will work based upon the following assumption : 
 Assumption Statistics of arrival : 
 The cluster arrival are independent and uniformly distributed over , p . 
 The relative arrival i , are independent , identically distributed with probability density function : , p , . 
 Cluster arrival and relative arrival are independent , i . e ., , . 
 All , and are independent of and of . 
 Let us momentarily write in single index notation as 
 . 
 In any given indoor scenario , the impulse response from transmitter to receiver will depend on their corresponding antenna . Suppose the impulse response is when the antenna is at a given point x on the horizontal plane . If we consider two additional receiver , x x , x x , then their arrival times will increase or decrease , depending on their arrival . More specifically , if the separation is small relative to the distance from the , then the arrival can be considered to be the same at x and x . In such case , an arrival time at position x becomes and , at x and x , 
 respectively , where 
  
 is the random delay increment , in , for the th produced by moving from x to x or from x to x and is the speed of light . Thus , the channel frequency for receiver x and x are 
  
 For the above model and with only the given within it , we will next derive closed form for the second order statistics of , over both space separation and frequency . 
 SECOND ORDER STATISTICS OF 
 In this section we will derive a closed form general expression for the space frequency covariance c , , defined in . This result is formally stated as : 
 Theorem : Under the in Section , be two random distributed as 
 Then , 
  
 where , 
 d 
  
 , g g , , , b b and where denote , respectively , the transform and the L norm of the function . 
 Proof : We first note that 
 E 
  
 where a from the fact that ak and that the are zero mean , i . i ., and independent of all the other . On the other hand , 
 E 
  
 Since each amplitude ak and the are i . i .. with zero mean , it that all of in which a given index is different from all the will be zero . This leaves only the following index : 
  
 Thus , 
 E 
  
 Notice that and and to the double index notation for and within , the function can be written as , 
 E 
  
 From Assumption we have that the random variable is identically distributed for for every 
 ,. Therefore , E , for all . 
 Likewise , it from Assumption that if 
 E e , for all ,. Substituting these in our previous expression 
 for we obtain 
 The corresponding to T , T and the T and T have been in , proof of Theorem . Proceeding as in the latter proof , it is easy to show that these are as in please see for . Substituting , and and into , we obtain , the proof . 
 Theorem some interesting of the space frequency covariance c , , : 
 The difference T T is the variance of , i . e ., the variance of the impulse response energy . This is the only term which does not depend upon , . As already in , Remark , under mild on the impulse response statistics , the T , T on the right hand side of vanish as . The latter that , in our case , the space frequency covariance does not go to zero as the frequency separation unbounded . Interestingly , the expression for c , , here that this covariance asymptote given by the term T T is the same regardless of the separation . 
 The of the suggest that , as , increasing the separation between the two receiver antenna the covariance of the channel power gain between any pair of such that 
 and . Moreover , if the E , is which , then and E should vanish as d 
 the resulting asymptotic covariance reduction would be exactly the same that would be by . 
 We now show that , as , the frequency dependent in do vanish as , if O non zero . However , it turns out that the decay is not faster than . 
 Proposition : Suppose Assumption and let the distribute as in Theorem . Then , 
 e E 
 where is function of the first kind and 
  
 are the complex series of . It is well known that the are oscillating and that their decay as . Hence , Proposition that the envelope of E as . The decay rate of E with respect on the transform of . For instance , if a deterministic value , then , 
 , and we obtain E e the last equality from theorem and that are the of the series of cos , see in the proof of Proposition below . On the contrary , if a uniformly bounded probability density function , then , when , E goes to zero as or d , but not faster than that . These are formally stated and proved in the following proposition : 
 Proposition : Suppose the of Proposition hold . Then , for every , there such that 
  
 If , in addition then for any , there such that 
  
  
 As a consequence of Proposition , we have that the frequency dependent part of c , , see with increasing , asymptotically , as . Given the behaviour of the and if the series of decay sufficiently fast with , then c , , will decay approximately as a few of the frequency difference O the latter being given by O , since O for , see . Likewise , for a fixed separation , the two of this term decay as O and , respectively , for relatively large of O and . 
  
 We have derived closed form for the second order statistics of the power gain of indoor . These were for the extended channel model , wherein path arrival times and arrival are clustered . Our reveal that the c , , , associated with two antenna x , x with channel power gains H and H , respectively , is symmetric with respect to and O . Sine the when O is , it that the variance of also as . We have also shown that the dependent part of c , , goes to zero as O to infinity , which also if O non zero and we let . In addition , we have that the reduction of this dependent term approximately proportional to , for a few of the frequency difference O . 
 APPENDIX 
 Proof : Proof of Proposition From the distribution see the statement of Theorem , we can as 
  
 where and are uniformly distributed over , p , and and are independent and identically distributed according to the cluster arrival angle distribution with probability density function . From this , 
 E , 
 which . In order to characterize E , we first recall the identity ,. 
  
 e 
 from where it that , for any , 
 E . 
 this result we can express E as : 
 E 
 E 
 which to . This the proof . 
 Proof : Proof of Proposition For real valued , it that see , section . 
 cos p p O . 
 Thus , 
  
 Substituting into , we obtain 
 E 
 where the last equality because , by theorem , 
 , and because is bounded . in directly to , the proof . 
  
 ﻿This paper presents an empirical study of the achievable data rates of network multiple-input multiple-output (MIMO) techniques including zero-forcing (ZF), zero-forcing dirty paper coding (ZF-DPC) and dirty paper coding (DPC) using actual 4-by-4 indoor wireless channel measurements at 3.5 GHz. Their performances are contrasted with those of conventional techniques, in which either the base stations are not coordinated (NC), or their interference is avoided using frequency division (FD) multiplexing. The measurements were taken in aisle-to-office and large unobstructed hall scenarios. The study of these results reveals that, at high signal-to-noise ratios (SNRs), DPC and ZF-DPC can yield more than a three-fold increase in attainable data rates when compared to NC and FD. The gains obtained using ZF are smaller, but still significant. At low SNRs the system is noise-(rather than interference-) limited, and only DPC exhibits gains. The evaluations in this paper also show that collaborative systems such as DPC can benefit from interference-prone environments to yield increased transmission capacity. With regard to the propagation channel, the classical log-normal plus Rayleigh/Ricean fading model, with parameters fitted to the scenario type, was found to be good at predicting the statistics of the achievable data rates of all the strategies considered.
 Index Terms—Wireless communication, MIMO, channel characterization and modeling, performance analysis.
 I. INTRODUCTION
 T
 HERE is an ever increasing demand for high data rates in homes and offices with all the mobility provided by wireless technology. Since growing numbers of users are to be served within confined spaces, the performance of practical indoor wireless communications systems is expected to be limited, increasingly often, by interference [1]–[3]. Therefore, in order to improve (or even maintain) high data rates, it will become necessary to make use of communication techniques capable of exploiting the coupling between the various propagation links within a given service scenario so as to reduce interference and increase received signal power.
 This management of the interference produced by access points (APs) and users can be achieved by means of coordination. Having all APs operating in a coordinated fashion turns the set of users and APs into a network multiple-input multiple-output (MIMO) system, and the downlink communication medium into a MIMO broadcast channel (MIMO-BC). Network-MIMO systems hold the potential of eliminating interference in a MIMO-BC, greatly increasing bandwidth efficiency in multi-AP wireless systems [3], [4].
 Several strategies have been proposed in the literature to take advantage of the high interconnectivity between several APs and a group of users that is often encountered in MIMOBC scenarios. The technique called dirty paper coding (DPC) [5], [6] has been shown to be capacity-achieving in this setting, and thus it is optimal [7], [8]. DPC is a nonlinear precoding technique that requires full knowledge of channel state information (CSI) at the transmitter.
 The high implementation complexity of DPC suggests the need to consider other simpler, near-optimal MIMO-BC transmission schemes. One such technique, commonly referred to as zero-forcing-DPC (ZF-DPC) [9], [10], achieves sum rates close to those of DPC with smaller computational complexity. In ZF-DPC, part of the interference is removed by linearly combining the signals intended for all users so as to effectively obtain, from end to end, a lower triangular channel matrix. The triangularization of the channel matrix, which requires perfect CSI at the transmitter side, makes it possible to avoid part of the interference. The effect of all remaining interference is eliminated by applying DPC in a sequential fashion.
 Another suboptimal technique, simpler than DPC and ZFDPC, is zero-forcing (ZF) beamforming as described in [11]– [13]. Zero-forcing is an entirely linear preprocessing strategy. In this case, APs have perfect CSI and cooperate to eliminate interference for all users, yielding an effectively diagonal channel matrix between transmitted data flows and users.
 
 Simpler strategies to attain downlink communication in a MIMO-BC are obtained if each AP serves a single user and its CSI is limited to the radio link to that user only. In our work, we consider two possible approaches. The first one is frequency division (FD), that is, splitting the available spectrum into disjoint frequency bands, each one allocated to a single AP-user pair, with each AP transmitting at full power. The second approach, which will be referred to as the noncoordinated (NC) strategy, is to let each AP transmit at full power using all the spectrum available, thus accepting that interference from other users will limit data rates as transmission power increases. The simpler FD and NC schemes in general have inferior performances than those of DPC and ZF-DPC. On the other hand, the former strategies impose the minimum possible backhaul load, since each AP needs to know only the data intended for (and the CSI related to) the user it is serving. There exist other strategies for downlink communication in a MIMO-BC that do not require coordination among APs and whose performance is under certain conditions better than that of FD and NC, such as fractional frequency reuse (FFR) [14]. Although it is known that in some specific cases FFR outperforms FD (see, e.g., [14], [15]) our objective here is only to provide a few simple baseline systems as a reference for comparison. It is understood that whichever is the choice, different propagation conditions may favor different baseline strategies.
 The performance evaluation of diverse network-MIMO techniques for simulated channels has been treated in various works [11], [16]–[19]. This has included the assumption of log-normal plus Rayleigh fading, and the use of the Wyner model [20]–[25]. The effect of limited backhaul capacity between APs in overall performance is assessed in [26]–[28].
 Regarding the channel model, it may be reasonable to assume that small-scale fades are independent for each channel when antennas are several wavelengths apart, but this may not hold for shadow fades. Several articles report on the correlation of fades in different locations of transmit and receive antennas [29]–[34]. To what extent such correlation may affect the network-MIMO channel, based on widely separated base station antennas, is not self-evident. To the best of our knowledge, no empirically based results in support of the assumption that a network-MIMO channel is equivalent to a collection of independent SISO channels have been published.
 The fact that in network MIMO all APs can act in a coordinated fashion taking advantage of CSI, generates two related sources of capacity improvement. Interference between AP-user pairs can be reduced and, at the same time, the total received signal power at each user terminal can be increased. Since these effects depend on the degree of connectivity (or, conversely, isolation), this raises the question of how much performance improvement can be expected for a given type of deployment scenario.
 In this paper, we evaluate and compare the maximum data rates achievable by the DPC, ZF-DPC, ZF, FD and NC strategies, for measured as well as simulated 4-by-4 MIMO indoor channels, under a per antenna power constraint (PAPC). These rates were all calculated under the assumption of perfect CSI at the transmitter. Although this would be hard to attain with currently available technology, the results serve as a comparison basis, being the best performance achievable under equally idealized conditions for all schemes. Channel measurements were obtained for two representative scenarios: aisles with offices alongside them, and large halls. A single slope path-loss model with log-normal shadow fading and Rice/Rayleigh small scale fades [35], was found to be adequate in all the scenarios for the distances at which measurements were taken. With regard to achievable data rates, we found that at high SNRs (in the range of 30 dB), DPC and ZFDPC achieve about a three-fold gain when compared to NC or FD. The gains of ZF are somewhat smaller, depending on the scenario, but still quite significant. A particularly interesting finding is that both DPC schemes not only are able to effectively mitigate the effect of interference but also in some cases can exploit low isolation between AP-user pairs, to yield higher data rates than in scenarios with greater natural isolation. As would be expected, at low SNR, where interference is not the dominant limitation on capacity, the gain over the non-coordinated systems decreases. Our study also reveals that the log-normal plus Rice/Rayleigh fading model is accurate at predicting the statistics of the maximum rates achievable by each of the network-MIMO strategies considered, when the model parameters are selected according to the type of scenario. Thus, an indoor MIMO channel described as a collection of independent SISO channels, would yield similar conclusions to those of our empirically based study, provided that the proper model parameters are used. However, rather than justifying what could have been deemed as somewhat arbitrary choices of parameters to illustrate the achievable benefits of a coordinated system, we decided to use actual measured channel data from typical indoor scenarios to validate our comparison. In the following sections we describe our measurement procedures and the empirical results obtained from them.
 II. MEASUREMENT SETUP AND SCENARIOS
 A. Measurement Setup
 In all measurements, the custom-built channel sounder used consisted of a single channel transmitter and a 4-channel receiver operating at 3.5 GHz for narrowband measurements. The single transmit antenna was placed at locations that would in practice correspond to possible positions of a user. Four receive antennas, one per AP, were placed at the positions typical of AP locations. The antennas used for transmission and reception were omnidirectional coaxial dipoles, vertically polarized. The transmit antenna was mounted on a 50-cm long rotating arm and moved stepwise under computer control in 6-degree increments.
 In all scenarios, measurements were taken at night in the absence of pedestrian movement. For each of the 5 scenarios (3 aisle-to-office and 2 halls), more than a 1000 complex 1by-4 channel vectors were collected, each vector containing the complex gains from a user location to the four APs.
 The single conversion receiver generates 4 channel outputs at 10 kHz. The sampled receiver output sequences were converted into a vector of 4 complex channel gains using the Fast Fourier Transform (FFT). Phase coherence was achieved by locking the transmitter and receiver oscillators to GPS disciplined sources. Exact synchronization of sampling intervals produces data sequences corresponding to an integer number of periods of the 4 sinewave outputs, which allowed FFT processing without the need for windowing. For each position of the transmit antenna, the 4 channel outputs were simultaneously sampled for 1 second, and the total interval was partitioned into 100 non-overlapping subintervals of 10 ms each. This allowed us to verify the consistency of our measurements which should only differ as a consequence of receiver noise. Since the MIMO capacity depends on the
  
 Fig. 1. Diagram of scenario aisle-to-office 1.
 relative rather than the absolute phases of the channels, we calculated phase differences with respect to one of the channels, arbitrarily chosen as a reference. In all our measurements, the ratio between the average gain (magnitude) and its rms fluctuation, calculated over the sequence of 100 measurements, exceeded 20 dB. Thus by averaging our measurements, we further reduced the effect of the measurement noise.
 B. Measured Scenarios
 We performed measurements in two types of scenarios which we considered relevant and where, as our data subsequently confirmed, different propagation behaviours are to be expected. These scenario types are as follows:
 1)	Aisle-to-office Scenarios: Channel measurements were carried out in three different aisle-to-office scenarios. In all cases the building was a steel reinforced concrete structure with interior divisions made of wood and particleboard. For these scenarios, the four receive antennas were wall-mounted at a 2.5 m-height along straight aisles, with a 10-cm separation from the particleboard interior walls. Adjacent AP antennas were separated by approximately 8 m. The transmit antenna was placed at desktop height inside various offices and laboratories adjacent to the aisle. These are non-line-of-sight (NLOS) scenarios. The distances between the transmit antenna and the 4 receive locations ranged from 3 to 25 m. The locations of transmit and receive antennas for scenario aisle-to-office 1 is indicated schematically in Fig. 1. In this figure, the rectangles and circles represent AP and user locations, respectively. The other two scenarios of this type had similar topologies. In each office, measurements were obtained placing the rotating arm on three or four different user locations, more than 1-m apart from one another, as space would allow (for simplicity, only one of these user locations per office is shown in Fig. 1).
 2)	Large Hall Scenarios: We obtained measurements from two large halls. The first one, hall-1, was a 53-m × 14-m, glass-roofed central hall, about 7-m high, with concrete floor and several lateral aisles leading to offices and classrooms on two floors. This hall is built with steel beams with concrete walls separating the hall from the adjacent spaces. The four receive antennas were located at the corners of an imaginary rectangle, with each antenna at approximately 10 cm from a wall on the hall, at heights of about 2.5 m. The transmitter rotating arm was placed in 38 different positions over a grid of locations within the rectangle formed by the receive antennas. This is a line-of-sight (LOS) scenario, in which link lengths ranged from 4 to 26 m.
 The second hall scenario, hall-2, was a 17-m × 23-m gymnasium with wooden floor and concrete walls. The four receive antennas were placed as in hall-1, while the rotating arm was placed in 43 different positions over a grid inside the gymnasium. Transmitter-receiver distances varied from 5 m to nearly 24 m.
 III. STATISTICS OF MEASURED DATA AND MODEL FITTING
 Before using the obtained channel measurements to calculate the data rates achievable with network-MIMO techniques, we describe the observed statistical properties of the fades in each scenario.
 The measured channel gains proved to be consistent with what has been reported in the literature for similar scenarios [35]–[42]. We found that log-normal shadow fading combined with Rayleigh or Ricean small-scale fades resulted in an excellent model fit to our narrowband measurements, provided the model parameters are adjusted to the measured data. The statistics of each type of fading are described below.
 A. Small-scale fading
 For each wireless link, small-scale fading statistics were analyzed by studying the 60 channel gain measurements corresponding to a single turn of the rotating arm. As expected, in all aisle-to-office scenarios, these channel gains showed fading statistics that were well described by a Rayleigh distribution. This is consistent with a NLOS situation [35].
 In the scenarios hall-1 and hall-2, these gains fit a Rice distribution with K-factor between 0 and 3. It is worth noting that despite the fact that these scenarios are LOS, they are, as the previous ones, characterized by strong multipath propagation, although to a lesser extent. In all scenarios, the channel phases associated with each angular position of the rotating arm were uniformly distributed and uncorrelated.
 B. Large-scale fading
 The large-scale fading statistics were obtained by calculating, at different locations, the averages of the channel gains over an arm rotation. The statistics of these average gains were well described by a log-normal distribution. More precisely, for each link distance ??, the path-loss, ??(??) in dB, corresponding to each rotation-averaged channel gain followed the behaviour of a random variable of the form
 ??(??) = ???????? + 10??log10(??/????????) + ????, (1) where ?? ? R+ is a path-loss exponent, ???? is a zero-mean Gaussian random variable with variance ??2 and ???????? is the path loss in dB at ?? = ????????, which we chose as 1 m. For each measured scenario, the model described by (1) was fitted to 80 measured large-scale path losses, finding ???????? and ?? by linear regression, and then setting ??2 as the empirical variance of ??(??) - ???????? - 10??log10(??/????????). The values of ?? and ?? so obtained are listed in Table I. For each scenario type, a
 TABLE I
 PARAMETERS OF (1) FITTED TO MEASURED SCENARIOS.
 Scenario	???????? [dB]	??	?? [dB]
 Aisle-to-office 1	39.8	3.7	3.1
 Aisle-to-office 2	50.3	2.9	3.5
 Aisle-to-office 3	42.9	3.0	2.9
 All aisle-to-office scenarios	45.6	3.1	4.1
 Hall 1	50.3	1.3	2.1
 Hall 2	51.5	1.2	2.2
 All hall scenarios	50.8	1.3	2.1
 row in boldface contains the model parameters resulting from combining all data of its corresponding scenarios.
 It is worth mentioning that no evidence of a “break point” in the path-loss exponent was found from the measurements. More precisely, no multi-slope models [43] were found to provide a better match (in a least-squares sense) to our empirical data. This is consistent with what has been reported before for short-range indoor scenarios [44], [45].
 It was also found that the correlation between the shadow fades in the links between a transmitter antenna and any two receiving antennas was, in all cases, below 0.26 (in modulus). This was true even when the two receiving antennas (in all cases several meters apart) were seen by the transmitter at an angle narrower than 10 degrees.
 IV. BRIEF REVIEW OF NC, FD, ZF, ZF-DPC AND DPC
 In this section we present a brief review of the transmission techniques to be evaluated in Section V and their relation to the channel model.
 A. Channel Model
 A general MIMO wireless downlink narrowband channel between ?? single-antenna APs and ?? single-antenna users can be represented by a ?? × ?? complex valued matrix H. Denoting the vector of signals transmitted by the ?? access points by x ? C??, the vector of received signals y ? C?? can be written as
 	y = Hx + n,	(2)
 where the noise vector n ? C has i.i.d. circularly symmetric complex Gaussian elements with variance ??. Notice that h??,??, the element in the ??-th row and ??-th column of H, is the narrow-band channel gain between the ??-th AP and the ??-th user. Notice also that no joint processing of the output of the MIMO channel is allowed, since it is assumed that cooperation between users is not practical.
 For the three network-MIMO strategies considered in this work, x can be constructed as
 x	= WTu,	(3)
 where the ??-th element in the vector u ? C?? is the information-bearing signal intended for the ??-th user, W ? C??×?? is a linear preprocessing matrix, and T : C?? ? C?? is either a non-linear transformation, in the case of DPC schemes, or the identity matrix, in the case of ZF. We note that an element of x can be a function of two or more user signals only if the APs are able to operate cooperatively. Following standard practice, we will assume that the elements of u are independent zero-mean complex Gaussian random variables with variances
 ???? ? E[??????*??]
 for ?? = 1,2,...,??. Thus, (2) can be written as
 y	= HWTu + n.	(4)
 We will assume that the APs are subject to a PAPC of the form
 	 	(5)
 where ??x2?? is the variance of the ??-th element of x, for some maximum power ?? = 0. All channel matrices considered in the sequel are 4-by-4, that is, ?? = ?? = 4, restricting our analysis to four users served simultaneously by four APs.
 B. Non-Coordinated System (NC)
 We include NC and FD as baseline strategies, against which to compare the other three (network-MIMO) schemes. In the case of NC, each AP serves a single user, and all APs operate without coordination, transmitting at maximum power over the same frequency band. In terms of the model (4), this amounts to restricting W to be a diagonal matrix, or any row or column permutation of it. Since the user signal variances {????} can be chosen freely, there is no loss in generality in assuming for this case that W = I, where I is the identity matrix. With this, the sum rate achievable with an NC system is readily found to be
   [bps/Hz].
 (6) In this expression, ??|h??,??|2 is the power of the signal ???? as received by its intended user. On the other hand,
 ??user, produced by the APs serving all the other users.???/=?? |h??,??|2 represents the interference affecting the ??-th
 C. Frequency Division Scheme (FD)
 In this case, each AP transmits at full power using a fraction of the available spectrum. We will assume that this spectrum is partitioned into ?? adjacent non-overlapping equal-width bands. Therefore, assuming the same total bandwidth as for NC, the maximum achievable rate of FD is trivially given by
  	[bps/Hz].	(7)
 D. Zero-Forcing (ZF)
 The idea in ZF is to eliminate, in the downlink, all interference produced by the APs. To do this, the matrix W in (4) is chosen as  W = H-1. This requires all APs to have CSI and each of them to process the signals intended to all users.
 In this case, the highest sum rate (bits per second per Hz) achievable under a PAPC ?? is readily found to be [46]
 	 	[bps/Hz],	(8)
 where the signal powers {????} are subject to
 	 	(9a)
 	0 = ?|????,??| ???? = ??,	?? = 1,2,...,??	(9b)
 and where   denotes the entry on the ??-th row and ??th column of W. The optimization problem defined by (8) and (9) has been shown to be convex in [11]. Therefore, a global maximum for the right-hand side of (8) can be obtained numerically using standard convex optimization methods [47]. Notice that the solution to (8) subject to (9) will, in general, yield antenna transmit powers  for one or more ?? ? {1,2,...,??}. ZF is known to perform poorly at low SNR but it is easy to improve its achievable rate under such conditions by choosing W as a regularized inverse of H instead of its true inverse [48]. We denote this variant of ZF as regularized zero-forcing (RZF).
 E. Zero Forcing Dirty Paper Coding (ZF-DPC)
 This MIMO-BC technique was first proposed in [9] and then further studied in [10]. The idea behind ZF-DPC is to utilize the linear preprocessing matrix W to assure that the ??-th user receives no interference from the signals ????, for all ?? > ??, and then use DPC to avoid the effects of the remaining interference. More precisely, W is chosen so as to obtain
 ??-1
 	???? = ????,?????? + ???? + ?????,??????,	(10)
 ??=1
 where ????,?? are the elements of some lower-triangular matrix G and ???? is the ??-th element of the vector
 v?Tu,
 see (3). In [10] and [49], this is achieved by choosing
 	W = Q,	(11)
 where Q is the unitary matrix in a QR decomposition of H??, i.e., H = (QR)??, with R being an upper triangular matrix and (·)?? denoting the conjugate transpose operator. Choosing
 W as in (11), we have that
 	y = HWv + n = R??v + n	(12)
 which attains the interference-avoidance outcome described by (10) with G = R??. If H is invertible (as is the case in all the channel matrices considered in this work), then all matrices R satisfying this factorization differ only by a sign inversion in any subset of their rows.2 Therefore, in our case, the absolute value of each element of R (and thus of G as well) is fixed once H is known.
 In ZF-DPC, the effect of the interference represented by the sum at the right end of (10) is avoided by employing dirty paper coding (DPC) [5]. The latter requires having, at the transmitting end, perfect CSI as well as full knowledge of all user signals. After applying DPC and transmitting the result through the channel (10), each ???? is decoded as if there had been no interference at all [5]. Therefore, the achievable sum-rate of ZF-DPC is given by
 ??ZF-DPC  [bps/Hz],
 (13)
 where ????,?? are the elements in the diagonal of R and where the maximization is over all row permutations of H and power allocations {????} that satisfy the PAPC (9) for each permutation. Notice that in this case the scalars ????,?? in (9) are the elements of preprocessing matrix W defined in (11) for the row-permuted matrix H.
 It is easy to show that the optimization problem defined by (13) and (9) is also convex for each permutation. To see this, it suffices to notice that the change of variables ??¯?? ? ????/|????,??|2 and ??¯??,?? ? ????,??|????,??|2 yields the optimization problem defined by (13), (9) equivalent to (8), (9).
 The performance of ZF-DPC has been evaluated using simulated channel data in [10], under a sum power constraint, and in [49], under a PAPC. In both cases, ZF-DPC was shown to be superior to ZF and various non-cooperative MIMO strategies.
 F. Dirty Paper Coding (DPC)
 Although the practical implementation of DPC still awaits specific code designs, we use this technique as an ultimate upper bound for the achievable data rates in each scenario.
 In DPC, no a-priori restriction is imposed on the preprocessing matrix W. Here, the signal intended for user 1, ??1, is transmitted by (possibly) all APs as if there were no other signals being transmitted, i.e., ??1 = ??1. In contrast, the data aimed at user 2 is encoded using DPC into ??2, exploiting the fact that all APs have full CSI, which together with having knowledge of ??1, allows avoiding its interfering effect. The resulting signal is then sent using (possibly) all APs as well. Notice that, by doing this, ??2 is added to the signal received by user 1 as interference. A similar process is employed to successively encode the data for the remaining users, utilizing previously encoded signals as known interference. In this setting, the signal ???? with power ???? reaches all APs through the ??-th column of  , thus arriving at the corresponding user with power(as before, ??l,?? denotes the entry on the l  -th row, ??-th column of W). Therefore, the maximum rate achievable with this technique is [51]
 [bps/Hz],
 (14)
  
  
 Rate per user [bps/Hz]
 Fig. 2. CDF plots of rate per user, obtained when NC, FD, ZF, ZF-DPC and DPC achieve their maximum sum-rates, under the power constraint ?? = 20 dBm, from empirical channel measurements taken in scenario aisle-tooffice 1.
 where the maximum is taken over all permutations of the rows of H and over all matrices W ? C??×?? and powers
 ???? satisfying (9) for each permutation. An efficient method to numerically solve the optimization problem posed by (14) has been proposed in [52]. This is the method we used to evaluate ??DPC(??) for the measured and simulated channel data.
 In the next section we use the above expressions to compare the performance of the various schemes in realistic indoor environments. We use both actual measured channel matrices as well as channel matrices generated by the model obtained from our measurements.
 V. RATE EVALUATION AND COMPARISONS
 In this section we evaluate the maximum sum rates of NC, FD, ZF, ZF-DPC and DPC as described respectively by (6), (7), (8), (13) and (14), subject to the PAPC (9), for the measured channel matrices obtained in all scenarios. We then repeat the sum-rate evaluation using channel matrices generated by the model (1), using the appropriate parameters as derived from our measurements. For the aforementioned transmission schemes, we calculate the per-user rates at specific availabilities, i.e., the maximum per-user rate that is guaranteed to be met or exceeded with some given probability.
 We further assume that the scheduler in this system is perfectly unbiased in that, on average, all users are served an equal amount of time. At the same time, our procedure of AP-to-user associations is designed to avoid combinations that will yield high interference when better choices exist. To achieve both goals we proceed, on each time slot, as follows:
 1)	Assign each user the access point providing the strongest signal (strongest AP) and group the terminals with the same strongest AP.
 2)	For each AP group, draw randomly one user.
 It is assumed that the number of users in each list is comparable. Over a long period of time this assures fairness as all users have an equal chance of being served, while precluding the possibility of choosing in the same time slot more than one user having the same “preferred” AP. More precisely, the channel matrix H = [h??,??] obtained in every time slot is such that
 	argmax|h??,??|2 =/	argmax|h??,??|2,	??? =/	??.	(15)
 	??	??
 This assures that interference-prone schemes such as NC are not unfairly penalized in our comparison by particularly poor associations. In addition, users and APs are ordered so that the ??-th AP is the smallest path-loss AP for the ??-th user. This yields a channel matrix in which the largest magnitude element in each row lies on the main diagonal of the matrix.
 The fact that the above described procedure resulted in equal likelihood of service for all users was confirmed in our simulations by verifying that when the scheduling algorithm was repeated many times, any specific choice of position received service the same number of times. While it may be possible to find scheduling algorithms that further benefit the performance of the NC scheme, it should first be verified that these gains are not achieved at the expense of fairness.
 As is to be expected, unobstructed hall scenarios provide less natural isolation between AP-user pairs than aisle-to-office scenarios. This results in the fact that the hall scenarios were characterized by a significantly smaller path-loss exponent, see Table I. As a consequence, for any given distribution of distances between APs and users, the channel matrices associated with hall scenarios will tend to be less diagonally dominant, i.e., the off-diagonal terms of H will be larger on average. The effect of this on the rate achievable by each transmission technique will be discussed later in this section.
 In all the evaluations, the noise variance in the receiver, for all full-band schemes (i.e., excluding FD), was chosen to be -90 dBm, which roughly corresponds to the thermal noise in a receiver with a noise figure of 10 dB, operating at room temperature, over a bandwidth of 20 MHz.
 A. Aisle-to-office Scenarios
 For each aisle-to-office scenario, 500 4-by-4 channel matrices satisfying (15) were selected by randomly choosing from the set of measured channel data for the corresponding site.
 Fig. 3. User rates at different availabilities for scenarios aisle-to-office 1 (first column) and hall 2 (second column).The CDF of the per-user rates, attained when W and {????} are chosen so as to maximize the sum rate in each scheme, is shown in Fig. 2 for scenario aisle-to-office 1, under the PAPC (9b) with ?? = 20 dBm. The results obtained in the other aisle-to-office scenarios were quite similar. It can be seen directly from the graph that at this transmit power limit, the three network-MIMO techniques outperform NC and FD for all availabilities. We also observe that the performance of DPC and ZF-DPC are very close, which is consistent with results obtained before for Rayleigh i.i.d. channels [10]. It should also be noted that the optimization methods used only assure that the sum capacity of DPC is the best of all coordinated systems. While this was observed in all our calculations, it does not necessarily imply that at all availability levels, the per-user rate of DPC will exceed that of the other options. In fact we found that for some users, the ZF-DPC rates would exceed those of DPC, as can be observed from the CDFs at low availabilities. At high availabilities however, DPC invariably proved to be best. As the transmit power limit is reduced, e.g. to values below -20 dBm, we found that an increasing number of users have a non-zero probability of being assigned zero rate when using DPC, ZF-DPC or ZF. This is a consequence of the fact that we are considering the per-user rates when each of the network-MIMO systems is configured for maximum sum rate, not fairness. As a consequence, the best strategy may include not serving some users at all in some channel realizations. This behavior will be discussed in more detail below.
 The corresponding per-user rates of each scheme in aisleto-office scenario 1, as a function of ??, are shown in the first column of Fig. 3, for availabilities 50% and 90%. On the upper edge of each of these graphs we have included an additional horizontal axis, labeled SNR, to provide an algorithm-independent measure of the received signal-to-noise ratio for each site. This SNR corresponds to the average received SNR in the NC setting excluding interference (i.e., supposing interfering APs are turned off), that is
 	SNR   ,	(16)
 where h(??,????) is the ??-th diagonal entry of the  -th channelmatrix instantiation. This would also be the true SNR for all the full-band schemes considered here if the off-diagonal terms of the channel matrix were zero. Since the latter condition never holds, this notion of SNR is not the actual per-user signal-to-noise ratio, which is algorithm dependent. Instead, it is the average ratio of the power received by a user from its “strongest AP,” to the receiver noise. This ratio depends only on the environment, specifically on the average path loss between AP-user pairs, as can be seen from (16).
 Figure 3 reveals that, in scenario aisle-to-office 1 and for a power limit ?? = 5 dBm (SNR= 31 dB), DPC and ZFDPC attain a data-rate gain in excess of three times, for both 50% and 90% availabilities, when compared with NC or FD. Similar gains are observed at higher power limits. In relation to this, we note that, loosely speaking, for large values of ??, a gain over FD of at most four times would be expected, since for large values of SNR and assuming perfect interference cancellation, the improvement in capacity will be dominated by the ratio of transmission bandwidths used, which is 4 in our case.
 	For	small	values	of	??,	the	interference	terms
 ??variance, which implies that the performance of NC is???/=?? |h??,??|2 in (6) are small compared to the noise
 noise-limited. In the specific case of ZF, the power of some APs will be reduced below ?? to meet the PAPC (9) while inverting the channel by choosing W = H-1. The cost of such interference avoidance effort is larger than the benefit stemming from having zero interference, yielding a sum-rate smaller than that obtained with APs at full power without coordination. The poor performance in low SNR regimes is a well-known shortcoming of pure ZF, which can be improved upon by using regularized zero-forcing (RZF), as discussed in [48]. We illustrate this in Fig. 4 which describes in greater detail the behavior of all techniques at low power for the case of 90% availability. Under such conditions the performance improvement of RZF over ZF becomes evident. As previously discussed, it can also be seen in this figure that at low power, the objective of maximizing sum rate may be achieved by
 SNR [dB]
 -14 -9 -4 1 6 11 16 21 26
  
 -40 -35 -30 -25 -20 -15 -10 -5 0
 P [dBm]
 Fig. 4. User rates at 90% availability for scenarios aisle-to-office 1.
  
 Rate per user [bps/Hz]
 Fig. 5. CDF plots of rate per user, obtained when NC, FD, ZF, ZF-DPC and DPC achieve their maximum sum-rates, under the power constraint ?? = 20 dBm, from empirical channel measurements taken in scenario hall 2.
 not serving some users at all, and as a consequence the rate that can be guaranteed to 90% of users may drop to zero.
 B. Hall Scenarios
 A procedure similar to the one described before was followed to generate 4-by-4 channel matrices in the two hall scenarios where measurements were taken. The CDF of the peruser rates obtained with each network-MIMO technique when optimizing for sum rate under power constraint ?? = 20 dBm are shown in Fig. 5, for scenario hall 2. In this figure we see again that the three network-MIMO techniques considered here provide higher per-user data rates than NC and FD.
 The corresponding per-user rates for several availabilities as a function of ?? for this scenario type are shown in the right column plots of Fig. 3, where scenario hall 2 has been chosen. The results for the other hall scenario are very similar and are omitted for the sake of brevity.
 In this scenario, in the high SNR range, DPC and ZF-DPC attain roughly the same gain, in per-user rates over FD, as that observed in the aisle-to-office scenarios. On the other hand, in this more interference-prone environment, the NC maximum rates are significantly reduced when compared to those of the aisle-to-office scenarios, characterized by higher natural isolation between AP/user pairs.
 The gain of the DPC schemes over ZF is also larger when compared to the aisle-to-office scenarios. Loosely speaking, this can be attributed to the fact that the aisle-to-office channel matrices are more diagonally dominant than the hall channel matrices. As a consequence, and in view of the Gersgorin-? disc theorem [50], the determinant of H will, at times, be smaller in hall scenarios. Since for ZF the preprocessing matrix equals the inverse of H, it will be possible for W to have larger elements in the hall scenarios. In view of the power constraint (9), this means smaller signal variances ???? and thus smaller sum-rates for ZF (see (8)). In contrast, ZFDPC is less sensitive to the smaller-determinant matrices that arise in the hall scenarios. The reason for this behaviour can be found in the fact that the determinant of H is equal to the product of all the diagonal elements of R. Thus, a given decrease in the determinant of H will, in general, entail a smaller reduction of rate for ZF-DPC (see (13)).
 As can be observed from Fig. 3, high-interference scenarios do not always result in decreased per-user rates. While this certainly occurs for a non-coordinated system as clearly seen from the graphs, DPC, ZF-DPC and to a lesser degree ZF may yield an improved performance in the hall scenarios in comparison to that obtained in the better isolated aisle-to-office environments. The improvement is particularly significant (a factor of around 2) at low power, as seen at the left extreme of the graphs. We note that the performance improvement is still evident when the comparison is carried out at equal SNR, i.e., when the differences in average path losses between AP/user pairs for the scenarios has been compensated. This behaviour can be explained by recalling that, at low power, noise rather than interference is the relevant factor in limiting transmission rates. Since the channel matrices of the aisleto-office scenarios are more strongly diagonal-dominant, less power reaches a user from access points other than the one that is dominant. In contrast, in an environment with less isolation (such as the unobstructed halls), the DPC schemes can take advantage of the fact that significant power from all bases will reach each user, while managing to turn most of this power into signal, not interference. (Indeed, it can be seen from (14) that for sufficiently large noise power, increasing the magnitude of the off-diagonal elements of H may produce a relatively larger increase in the numerator of the fraction within the log(·) than on its denominator.) Thus the signal-tonoise-plus-interference ratio is improved. As transmit power increases however, interference becomes dominant, and the hall scenarios suffer from the increased need to compensate for this impairment.
 C. Accuracy of the channel model at predicting achievable network-MIMO rates
 In order to assess the accuracy of the log-normal plus Rayleigh/Rice model at predicting the maximum rates achievable by the techniques described in Section IV, we obtained those rates from channel matrices generated by the general model for each scenario type (aisle-to-office or hall), that is, using the parameters in the boldface rows of Table I. The data rates predicted by the model were, in general, in good agreement with those obtained from empirical data, for each scenario. Figure 6 shows plots of per-user rate with respect
  
 SNR [dB]
 5 10 15 20 25 30 35 40 45 50 55
  
 -20 -15 -10 -5 0 5 10 15 20 25 30
 P [dBm]
 Fig. 6. 90% availability empirical and simulated per-user rates for scenario aisle-to-office 3. Simulated rates were obtained from the model using the scenario-type parameters in row “All aisle-to-office scenarios” of Table I.
 to maximum transmit power constraint ?? for 90% availability for the scenario aisle-to-office 3. It can be seen that, for this scenario, the rates from simulated channels are smaller than those obtained from empirical data. This may be attributed to the fact that the scenario-specific parameter ???????? for aisle-tooffice 3 (in the third row of Table I) is smaller than that of the corresponding general model used for the scenario type (contained in the fourth row of Table I). Thus, in this case the actual received powers will be larger (on average) than those predicted by the model.
 VI. CONCLUSIONS
 In this paper we have evaluated the data rates achievable in an indoor wireless 4-by-4 system by network-MIMO techniques such as zero-forcing (ZF), zero-forcing dirty paper coding (ZF-DPC) and dirty paper coding (DPC). We compared these rates with those achievable with frequency diversity (FD) and no coordination (NC), using measured and simulated indoor channel matrices at 3.5 GHz. Our results show that a single-slope log-normal plus Rayleigh fading model with properly chosen parameters is good at predicting the sumrate performance statistics of these techniques. By numerical evaluation based on empirical data we have shown that in the scenarios tested, at high SNR, the network-MIMO techniques are able to achieve about a three-fold increase in per-user data rates over NC or FD, when considering 50% and 90% availabilities. It was also found that schemes employing dirty paper coding can attain higher per-user rates in hall scenarios, where interference is greater than in aisle-to-office scenarios. This difference was greater for high availability data rates and at small SNRs. Our evaluations also revealed that while ZF in general provides higher data rates than NC and FD, this advantage is lost at low SNR, particularly when high availability is considered. In contrast, DPC based schemes provide significant benefits under virtually all practical conditions.
 ACKNOWLEDGEMENT
 The authors would like to thank Francisco Silva, Dragan Samardzija and Sivarama Venkatesan for their valuable help.
 
 ﻿ The problem of lower on data for closed loop stability been in a variety of . However , the available lead to which are very complex and , thus , of limited practical interest . In this paper , we show how simple only and memoryless entropy scalar can be used to stabilize strongly plant over error free bit rate limited feedback . Despite the simplicity of the building employed , we prove that the data do not exceed absolute lower by more than . per sample . 
 The study of control , i . e ., control with communication , as an active research field during the past see , e .., the special issue . Key within this framework are related to the way in which network affect the stability and performance of control that employ non transparent communication . Typical channel include data rate , random and data . A framework for the treatment of the general analysis or design problem does not exist . Nevertheless , there been significant progress in the study of . For example , data rate have been studied in , e .., . The issue of data been studied in , e .., and random time have been considered in , e .., . 
 In this paper we focus on feedback closed over delay and error free bit rate limited . Within this context , a key result necessary and sufficient on the channel data rate that one to achieve closed loop stability in an appropriate sense ; see , e .., , , and the many therein . This result is given in of a lower bound on the channel which on the unstable plant only over which can be so as to achieve stability . These are quite complex and are not attractive from a practical point of view . On the other hand , showing that the rate at which a stable control system is data is always greater than the bound i . e ., necessity is fairly simple see also , , whereas an actual scheme that stability at any rate above the absolute limit i . e ., the proof of sufficiency is much more involved see , . Clearly , the need to develop the study of simple that achieve close to the in , , , and of that allow one to exploit the when the necessity of the in the construction of . 
 by the discussion above , this paper how simple only and entropy see , e .., can be used to stabilize strongly plant in data which exceed the minimal established in , , by no more than . per sample . The excess rate of our simple scheme is given by the sum of two : a first term due to the divergence of the quantization noise distribution from , and a second term that in the inefficiency of the loss less coder that the binary . 
 In our , the coder and architecture an essential role , which , given the in , e .., , is by no surprising . Our work also light into the why the in are not always consistent with the lower bound on data for stability studied in . 
 The remainder of this paper is organized as : Section the notation used in the paper . and present the considered setup and scheme , respectively . a simplified setting where are . Section the in derive upper on the data that allow one to achieve with the scheme . Section . Due to space , all have been and can be found in or via e mail from the . 
 We define N , , , and . We both the argument of the transform and as the forward shift operator , where the meaning is clear from the context . Given any scalar , magnitude . 
 The set of all discrete time strictly proper real rational transfer is by . 
 Unless otherwise stated , random are always scalar and defined for N , we abbreviate N , , by , and define , . 
 E the expectation operator . The variance , at time instant , of a via ; similarly , a random variable , then its variance . We define , , provided the limit . 
 a wide sense stationary asymptotically process , then its stationary power spectral density and any spectral factor of , a second order one if and only if it finite mean and finite i . e . We say that a random variable is 
 If ,, are continuous resp . discrete random , then resp . the differential resp . discrete entropy of ; resp . the conditional differential discrete entropy of , given ; I ; the mutual information ; I ; the conditional mutual information , given . We recall that , then 
 . In this paper we use in base e . Thus , information is measured in nat . For and basic of the above we refer the reader to . 
 In this paper we are concerned about the stability of a closed loop system built around a discrete time plant model , which a delay and error free data rate limited channel in the feedback path , as shown in Fig . . In that figure , is the plant model , is an controller , is the plant output , u is the plant input , is a reference signal , . In order to make use of the bit rate limited channel , the feedback path an E which binary based on information regarding the plant output . These , which we will denote by , are then sent delay and through the channel . At the end , received to generate the is fed back into the controller . 
 Assumption Plant model : is unstable , no or on the unit circle , is strongly , and its initial state is an independent second order random variable . N 
 Assumption Reference and disturbance : second order zero mean that admit rational , and that are mutually uncorrelated . Moreover , 
 Assuming that the plant is strictly proper that the feedback loop in Fig . is well for all causal , and . The assumption regarding 
 the plant initial state for most of interest . The on are not essential , but we have made them to maintain a straightforward presentation see for the general case . Assumption is standard except for the fact that we be non zero . We avoid the case for brevity . 
 For future reference , we define p as the set of unstable plant , and 
 Throughout this paper we adopt the following notion of stability see also , e .., : 
 Definition Mean square stability : Consider a system by ,,, where N ,: , is the system state at time instant , , where is a second order random variable , and the a second order process independent of . We say that the system is mean square stable if and only if there exist finite and finite , , such that 
 Significant work been devoted to the study of the of the control system in Fig . , when arbitrarily complex are employed see , e .., , . However , the study of simple and efficient received much less attention . This is indeed one of the main for the present work . 
 We seek simple , though effective , and . We propose to use the scheme shown in Fig . . In that figure , F is an filter , and , are for the moment abstract that are to exploit the history of their to generate their corresponding . More precisely , for every N , 
 where and are possibly stochastic that may depend explicitly upon time , and where the range of is a collection of prefix free binary see , e .., . Note that the resulting control loop is well if F . 
 The scheme defined above is a restricted instance of much more general . Indeed , it is a special case of a scheme that the history of the plant output and of the received in an arbitrary causal fashion see , e .., , . 
 Assumption scheme : The scheme in Fig . is such that , N , 
 a knowing and does not provide more knowledge about than just knowing , 
 the sequence of i ,, is known at the and is such that can be exactly from and vice . Also , 
 F and the corresponding initial state is an independent second order random variable . At each time the symbol a word of length measured in . We will be interested in the average rate and , accordingly , we define the average data rate as 
 provided the limit . is the average rate at which the are sent trough the channel and , as such , is a measure of the information flow at the physical level . 
 A key question that when considering any scheme is how to characterize lower on the achievable average data . To that end we define the following : Definition : Consider two . We define if the limit the average directed mutual information see also 
 We can now characterize lower on the average data rate for the scheme : 
 Theorem Lower on data : Consider the control architecture in Fig . and the specific scheme in Fig . , where and are as above . If a hold , then I . 
 Theorem a physical quantity , namely average data rate , to an information theoretic quantity , namely average directed mutual information . It is important to note that a different bound on the average data rate would have arisen if we had considered and with different information available . 
 Theorem is a key one . However , it is not straightforward to characterize I unless one suitable on and or , equivalently , on the signal 
 In the next section we will make some . Later , in Section , we will consider a simple instance of and and we will exploit the for the I is sometimes directed mutual information rate . 
 case to provide upper on the associated to that specific scheme , without the . 
 As at the end of Section , we will assume in this section that the following : 
 Assumption : The noise see is an independent second order zero mean i . i .. sequence . The initial of all in Fig . plant and controller are independent second order random , jointly . 
 Consistent with the fact that the the way in which and are chosen , we will consider the variance of , say , as design parameter . Assumption , when combined with the control system in Fig . and the scheme in Fig . , the linear model in Fig . . 
 As long as and hold variance , the control system in Fig . is if and only if is such that the feedback loop in Fig . is well internally stable in the standard deterministic sense see , e .., , . Thus , if we define 
 then is equivalent to provided and hold ; see . It is easy to see that if and only if F is stable , and is an admissible controller for see , e .., . 
 Theorem we can now characterize average directed mutual information in the situation under study : 
 Consider the feedback loop in Fig . and assume that , and hold . If and , 
 where is the stationary of , is the stationary variance of , and is the stationary scheme signal to noise ratio . Equality in a if and only if is constant for every . 
 Corollary an explicit expression for the average directed mutual information across the scheme in the considered situation . Also , it a relationship between the average directed mutual information across the scheme and the corresponding signal to noise ratio . In particular , it from that provided , and hold the scheme signal ratio to an upper bound on the average directed mutual information across it . Moreover , if is a constant , then we have equality in . Hence , under the constraint of constant , the scheme signal to noise ratio also the average directed mutual information across it . We will show below that , in our setting , one can restrict to be constant without loss of generality . Towards that goal , we start with the following theorem : 
 Theorem Directed mutual information for : Suppose that , and . Then : 
 There such that the feedback loop in Fig . is if and only if 
 Any I can be in a loop if one a stable and strictly proper F such that F is minimum phase , is any stable admissible controller for , and is made sufficiently large but finite . 
 Theorem a bound on the directed mutual information across the considered whose satisfaction is necessary and sufficient to achieve . It also a characterization of the controller , the filter F , and the noise variance , that allow one to achieve any average directed mutual information arbitrarily close to the bound established . 
 We can use Theorem to immediately infer that , provided the in Theorem hold , 
 is a necessary condition that average data need to satisfy in order to guarantee when the scheme . We note that we have , for the class of considered plant , the bound on average data for derived in . However , we have not provided a proof of the of this bound . This issue will be in Section . 
 We next explore the minimal stationary scheme signal to noise ratio compatible with . This will enable us to state the main result of this section . 
 Theorem Signal to noise ratio for : Suppose that , and hold . 
 There such that the feedback loop in Fig . is if and only if the stationary scheme signal to noise ratio 
 Any can be in a loop if one as any stable admissible controller for , F is chosen as 
 F , where , , and is made sufficiently large but finite . 
 Corollary : Consider the feedback loop in Fig . and suppose that , and hold . A solution to the problem of finding the minimal stationary scheme signal to noise ratio that a solution to the problem of finding the minimal average directed mutual information across the scheme that . 
 Corollary is one of the main in this paper . Indeed , by virtue of Theorem , Corollary one to conclude that the minimal signal to noise ratio for an immediate lower bound on the average data that allow one to achieve . The key to these in that we consider a architecture with sufficient of freedom . This one to make constant in the limit as , without constraining the minimum achievable directed mutual information across the scheme , or the corresponding minimal achievable signal to noise ratio a simple calculation based on the of Theorem this . We feel that the provided by these are of fundamental importance and , to the best of our knowledge , new . 
 Remark : The work also minimal signal ratio for stability . However , the of constrain themselves to the particular case F , which does not , in general , allow one to achieve the absolute minimal scheme signal to noise ratio in Theorem . Hence , it is not surprising that the in do not always suggest on average data that are consistent with the in , where that exploit the history used . N 
 a necessary lower bound on the average data rate that one to guarantee . We have , however , provided no proof of the of this bound . It is well known see , that there exist that allow one to achieve with average data arbitrarily close to the bound in Section . However , since we have chosen a specific scheme that is much simpler than those in , , there no guarantee that we will be able to actually achieve the bound . This the study of upper on the average data rate for applicable to the scheme . In particular , we will use the in study achievable , when a memoryless entropy is used to generate the binary that are sent over the channel . 
 Fig . the architecture of an and its relationship to and in Fig . see , e .., . The is such that , 
 where is the dither signal which is known at both of the channel , a uniform defined via , i for , i , where is the quantization step . At each time instant , is memory and loss by the entropy coder explicit information about and the corresponding binary word is sent over the channel . Upon reception , the , which , the received symbol . Accordingly , the output becomes equal to . 
 Lemma see : Consider an as above . If is an independent sequence of i . i .. random , uniformly distributed on , then the in is distributed according to the distribution of , and the scalar mutual information between and I ; . 
 Remark : The proof in no feedback around the . If there is strictly causal feedback around it as in our case , then the same result see detailed proof in . N 
 The in Lemma state that , if one an appropriately , then the difference between the input and output of the , namely , becomes just an i . i .. source , uniformly distributed on each quantization interval as in the classical additive white noise model for quantization ; see , e .., , . This that the use of an one to achieve a the noise source considered in Section except for a different distribution . Therefore , it is sensible to expect that use of an appropriately designed average data close to the in Section . We will show below that this is indeed true . We note that , given the fact that the dither signal is known at both of the channel , choosing as in Lemma that the a i . e ., an is a valid instance of and in Fig . . 
 Theorem Achievable : Consider the control system in Fig . with the scheme in Fig . , where and form an as above . Suppose that the memoryless entropy coder pair inside the see that , and hold , that the controller initial state is an independent second order random variable , and that the dither signal is an independent sequence of i . i .. random , uniformly distributed on . Then , if one as in Theorem Part ; see and 
 , with , then the resulting control system will be and the corresponding average data rate will be upper bounded as 
 where is a continuous and decreasing function of such that lim , lim . 
 Remark are conservative : The upper bound for the average data rate in Theorem is conservative . Indeed , the by are usually closer to the entropy of the source than to the entropy of the source plus see , e .., .. As a consequence , the actual data will be usually close to the expression in 
 Theorem is the main result in this paper . It upper on the average data that guarantee and that are achievable with the scheme in Fig . , when and form a memoryless and when the plant model is strongly . If one as in Part of Theorem , and is chosen sufficiently large but finite , then one will be able to achieve a rate that is no more than per sample i . e ., . per sample away from the absolute bound in , . This additional rate is composed by two : the first one is due to the divergence of the distribution of quantization noise from , and the second one in the inefficiency of the loss less scheme employed to generate the binary . We feel that this extra rate is a fair price to be if one constrain oneself to the conceptually simple considered in this paper . 
 Remark : Note that the in Theorem rely on the in Section . In other , the when lower on the average data for stabilization were key when an actual scheme that a close to that bound . It is also worth that the quantization scheme considered in this paper a well studied building block in the Information Theory literature see , e .., , , . All the above in stark contrast to the approach in , e .., . N 
 In this paper , we have studied the mean square stabilization of strongly plant over bit rate limited . We have a simple scheme which only and an entropy . Within this setup , we have shown that the excess data , which derive from this reduced complexity scheme instead of an arbitrarily complex one , are no than . per sample . to more general and to performance related can be found in . 
  
 ﻿ We analyze the behavior of the mean squared error achievable by , uniform scalar quantization feedback , and post of unrestricted order , when wide sense stationary discrete time random possibly unbounded support . Our are based upon the use of uniform scalar . We consider the number of quantization to be given and fixed , which itself to fixed rate , and focus on the in which is insufficient to avoid overload . In order to guarantee the stability of the closed loop , we consider the use of a clipper before the scalar . Our are valid for zero mean independent whose satisfy some mild , which are met by infinite support such as and . We show that , for fixed , the can be made to decay with the ratio as when to infinity , where . We note that the latter bound is asymptotic in but not in , and that it clipping . 
 Index , quantization 
 I . INTRODUCTION 
 I 
 T is well know that can reduce the magnitude of the reconstruction error that from the of an source , see ., e .., . This reduction is by to digital such as sigma delta , which have been successfully in audio and image quantization . 
 It was shown in that the of modulation is 
 , as , where is the ratio and the order of the feedback filter assumed fixed for all of . In their analysis , the of an additive noise model , in which quantization are assumed to form a wide sense stationary ... random process , white and uncorrelated with the input . Also the , it was recently shown in that by different of unrestricted order for each value of , the 
 can be made to decay as , where the signal to noise ratio of the scalar . The analysis in and restrict to the where the effect of overload is negligible , which cannot be if when the source unbounded support unless infinitely many quantization are available . Indeed , an important body of literature related to quantization overload either by careful design of the 
  
  
 Fig . . Scalar feedback quantization scheme with subtractive dither . 
 or by simply assuming there exist enough quantization to avoid overload , see e .., , and the therein . 
 of bit two level in which the , , by following a deterministic approach . The in yield a continuous time reconstruction error that can be term proportional to , where 
 is independent of . In turn , the continuous time reconstruction error with the in can be uniformly bounded as when . This immediately to an that as , when . To the best of the author knowledge , the latter is the decay rate of the reconstruction error with available in the literature . 
 However , the in and have not been extended to with more than two quantization , and rely upon the input being uniformly bounded . On the other hand , available on the quantization of unbounded the effects of overload do not consider , see , e .., and the therein . 
 In this letter , we study the behaviour of the with increasing ratio when the source is a possibly unbounded wide sense stationary ... band limited process . Our analysis is based upon the use of a uniform scalar , by a clipper , together with feedback , and post of unrestricted order see Fig . . We focus on the in which the number of quantization is insufficient to avoid overload . We show that , for this architecture , the 
 can be made to decay with as , where 
 , provided the following . 
 Assumption : The source process independent , with zero mean and symmetric probability density function . Moreover , there a constant such that the th of each satisfy 
  
 This letter the work in by taking account of clipping in the analysis . 
  
 . 
 SIGNAL , VOL . , NO . , JUNE 
  
 . AND PROBLEM STATEMENT 
 Our are related with the feedback quantization architecture shown in Fig . . In this scheme , the 
 form a zero mean ... process , from sampling a ... band limited signal . For each ratio 
 , the power spectral density of can be 
 written as , where 
  
 In and is the square root of the of the input process when . It is assumed that the input process finite power , i . e ., that . For simplicity , we shall further restrict the analysis to the in which . Notice also from that the total power of in of variance per sample , remains constant for all . 
 . interval and reconstruction . is a random process with i . i .. independent of and uniformly distributed over the interval dither to the input of the the range for the input signal over which overload cannot occur . Since the dither is distributed over , this range is . It is well known that such a dither signal a quantization error process with i . i .. which are also independent of the source , , provided 
 , i . e ., as long as does not 
 overload . Quantization error appear in the output as the stationary process . In order to keep as shown in Fig . . The clipper the value of the input signal so that , if , or 
 , if , 
 thus stability , see . The key point here is that , unlike overload , clipping , given by , are not into the feedback loop . Instead , clipping appear in the output after being by , to yield the process . Unless the source 
 is a stationary process , one cannot guarantee that the of the clipping error will form a stationary , or even a ..., random process . In order to quantify the contribution of clipping to the for not necessarily stationary , we define the average power of clipping in the output as 
 where 
 Two important the under which the combination of clipper and operate are the signal to noise ratio 
  
 and the loading factor 
  
 It from and that , if and are kept fixed , then can only be at the expense of reducing the at 
 which the clipper and operate . 
 For the scheme of Fig . , it was shown in that the reconstruction due to granular quantization only , which here to , can be upper bounded as 
  
 where and the scalar function the unique value of that 
  
 when . Upon 
 , it is also shown in that , for to hold , the frequency 
 of and must satisfy 
  
  
 i . e ., on , for every , where . In and , can be any bounded , nonzero gain . With the optimal in , to and the variance via 
  
 Notice the upper bound on the due to granular quantization in exponentially with . However , the behaviour of the average power of clipping with increasing is unknown . Therefore , in view of , the exponential decay of given by does not necessarily hold for , for sufficiently large . In the next section we find an upper bound to the total average power of the reconstruction error , clipping . 
 . MAIN RESULT 
 We start with the following technical lemma : 
 . If there 
 where 
 Proof : From one of , given in , Sec . . , we have that 
  
 such that . For 
 every , the bound for the first inequality in is with . Substituting this into 
 . The latter , together with the fact that directly to , the proof . 
 The following theorem an upper bound for applicable but not restricted to in which the source unbounded support . 
 Theorem : Suppose there a scalar such that see . Suppose that Assumption 
  
 : A BOUND ON THE OF QUANTIZATION WITH FEEDBACK 
  
 SIGNAL , VOL . , NO . , JUNE 
 Thus , we have an upper bound on the due to clipping that linearly with and exponentially with provided the product does not tend to zero as , see . 
 then to granular quantization in becomes 
  
  
 Upon substituting and in , we obtain the following upper bound : 
  
 The above upper bound for does not tend to zero with increasing unless one the loading factor grow with fast enough . Substituting and into 
 . From the latter , we have that , where . Thus , the term due to clipping in 
 can be reduced only at the expense of operate at a lower . This , in turn , the term due to granular decay more slowly with increasing . 
 For example , if one the loading factor grow with as 
 , where and are to be chosen , then the of becomes 
  
 The optimal decay rate when is by choosing and so as to make granular and clipping error decay at the same asymptotic rate . This is if and only if and are chosen so that 
  
 . Before the above limit , note that from we obtain since , being a random variable uniformly distributed over , standard deviation and with . rule to twice and substituting by , we obtain that 
 , where 
 . CONCLUSION 
 We have studied the asymptotic behaviour of the reconstruction of fixed rate quantization with feedback as the ratio to infinity , for ... possibly unbounded support . It was shown that with the proper choice of and loading factor for each , the can decrease with at least as fast as , where does not depend on . 
  
 This paper the performance of a feedback control loop closed via an error free digital communication channel with transmission delay . The system a discrete time noisy linear plant whose single measurement output is into its single control input by a causal , but otherwise arbitrary , and control scheme . We consider a single input multiple output channel between the controller and the controller which is lossless and random time delay . We derive a lower bound on the minimum average feedback data rate that a certain level of average quadratic performance over all possible of the random delay . For the special case of a constant channel delay , we obtain an upper bound by linear source that attain desired performance with that are at most . per sample greater than the lower bound . We give a numerical example that and operational are increasing of the constant delay . In other , to achieve a specific performance level , greater channel delay spending higher data rate . 
 Taking communication into account for analysis and design proved to be an interesting topic within the area of control theory during recent . This interest is by of communication over point to point wiring and , on the other side , by the complexity that communication impose on classical control . Time delay , packet dropout and data rate quantization are among prominent . 
 an information theoretic approach , , report primary related to system performance . In these works , it is shown that the presence of a finite capacity communication channel in a strictly causal feedback loop a new performance limitation which from conventional Bode formula by a constant channel information rate . Moreover , the derive among entropy rate of internal inside the loop and external outside the loop , resulting in a general performance bound which is affected by finite feedback capacity . Inspired by , , lower and upper are derived on the minimum data rate that a level of quadratic performance in . These works consider noisy linear time invariant with , over an error free digital channel without delay . In particular , that over all causal which represent and control , the average data rate is bounded from below by the directed information rate by the that render the sensor input and control output jointly . Moreover , it is proved in that in an auxiliary structure , the minimum signal to noise ratio which stability and meeting a quadratic performance requirement the lower bound on the desired minimal data rate . For the upper bound analysis , entropy . Such a simple scheme is designed based on the constrained optimization giving the lower bound . Inspired by and , the of present a method based upon semidefinite to characterize the trade off between directed information rate and linear quadratic performance in rate constrained control with fully observable multiple input multiple output . In , the derive a lower bound on the zero delay rate distortion function associated with vector valued Gauss and mean square error distortion constraint . Based on the separation principle , this bound is in fact the lower bound on the minimum data rate for performance in control of fully observable . Then the optimal realization that to the class of vector valued to derive an upper bound on zero delay rate distortion function variable length entropy with lattice quantization . Similar are employed in for on minimum mutual , across a delay free channel , that guarantee specific linear quadratic regulator performance . Specifically , the lower bound based on lower bound and power entropy whereas the upper bound is 
 subject to network induced are generally according to two : robustness and adaptation . The aim in the robustness framework is for certain stability or performance by that do not incorporate time stamp information as a variable . For instance , in , stabilization 
 and H performance for a singular cascade are . Fuzzy model based control is another approach in the robustness framework , where the are based on the size of , and the controller is to be robust over the delay range , . In the adaptation framework , one method is as stochastic switched . The recent on stability and H H performance of jump linear are in and , respectively . The second approach in this framework is predictive control ; a method which is currently quite popular in . According to this technique , the actuator among a sequence of control based on the transmission experienced by them . 
 In all the on system performance , either the effect of channel delay is , or the rate limitation is not taken into account . However , looking into the literature , one can find works investigating performance in with both rate and network induced see , e .., . Even so , a few the approach to treat with such . For example , on the minimum individual non asymptotic rate to guarantee meeting an individual performance requirement boundedness of the maximum ` norm of . 
 In this paper , we study the performance of a discrete time plant with initial state in a loop with exogenous and random or constant channel delay on the feedback path . For the setup with random delay in the channel , we seek the average data rate to achieve a performance level . We show that the average data rate over all possible of the delay is lower bounded by the average directed information rate . We prove for the random channel delay case that under certain , the average directed information rate can be stated in of average power spectral of the involved . We obtain a lower bound on the desired minimal average data rate which is stated as the average of a function of the power spectral of feedback path over all of the delay . To establish all these , we utilize the adopted in and . However , to and , the channel is not delay free in our setup . In other , we extend the information in to the case where there a random time delay between the sensor output and the control input . 
 For the setup with known constant delay in the channel , we show that the above lower bound on the average data rate for quadratic is equal to a function of of the channel over comprised of and with feedback and delay that meet the quadratic performance constraint . Our contribution in this case is showing how the presence of the channel delay the scheme yielding the lower bound . This an insight to the interplay between time delay , average data rate and performance in the considered . We also prove that even over a channel with a constant delay , any admissible performance level can be by an based linear scheme which an average data rate at most approximately . per sample away from the corresponding lower bound . We illustrate via a numerical example that lower and upper as well as empirical and are all increasing of channel delay . This in turn that with demand higher average data to allow for a certain system performance . 
 to our previous works in and , first , we here study the case of random channel delay and second , we employ a simpler proof than information and in and . In this work , we also show the effect of a delay at different in the loop on system . The last departure from our previous is that we incorporate some of into this paper . 
 The remainder of the paper is organized as . Section the notation . Section the main problem . Section the lower bound problem for the setup with random channel delay . lower bound on the desired minimal data rate in the case of constant channel delay . The analysis of upper bound problem in the constant delay case is in Section where the equivalence between with different delay is . A numerical example is given in Section . Finally , Section the 
 By , we denote the set of real whose subset the set of strictly positive real . The set N is defined as N , set of natural . The time index of every considered signal , most , to N . E , log , and . k represent for expectation , natural logarithm , magnitude and H norm , respectively . Moreover , min and are respectively the and of the square which the element on the i th row and th column is by i ,. In addition , is shorthand for ,..., where the th sample of a discrete time signal . Furthermore , for the time dependent set a i , i N , ak is defined as ak , a a . However , if a is a fixed set , then ak , a a 
 Random and are vector valued , unless otherwise stated . account as two random with known marginal and joint probability distribution . Their joint is by , while the marginal by and , respectively . The conditional by and . is the operator for the expectation with respect to the distribution of . We define the differential entropy the conditional differential entropy , and , , , respectively . The mutual information by by I ; and 
 defined as I ; , , log ,. Moreover , the definition of the conditional 
 mutual information between the random given by I ; , I ,; I ;. All the information theoretic in this 
 We call the random process asymptotically wide sense stationary if E 
 and , and E E E hold , 
 where is a finite constant . Accordingly , the steady state covariance matrix and the steady 
 state variance of are defined as , trace , respectively . For the scalar random sequence 
 asymptotically equivalent if and only if they satisfy the following expression for finite : 
 We consider the feedback loop of Fig . where the plant is with one control input and one sensor output by u and , respectively . The disturbed by a zero mean white noise which is by and identity covariance matrix , i . e . I . Moreover , as in Fig . , the plant the vector valued signal upon which the performance measure is . The relationship between the set of and is by a transfer function matrix as : 
 where the dimensionality of each is determined by the of corresponding pair of and . So , , and are the for G , G , G 
 Assumption . . Every entry of the transfer function matrix in is proper with no unstable hidden . Moreover , G , which the single input single output open loop system from u to , is strictly proper . The initial of the plant by x ,..., are jointly with and independent of the disturbance a finite differential entropy . 
 As in Fig . , the output of the plant ,, is into a binary word by the E and over the error free channel . Such transmission is with a random delay . Let denote the delay experienced by the binary word at the . We assume that is an independent and identically distributed i . i .. 
 process which a bounded support at each time step , i . e ., h ,..., , N where hi hi i , ,..., . In order to avoid unnecessary notational complexity and without loss of generality , we set h as h and as . The marginal distribution of the delay is assumed to be known and by where . 
 Such introduce a channel with the following input output relationship : 
 for every , i N . the of by , we can imply form that is a vector comprised of binary which the output of the channel at time . Note that is a random variable depending on the channel delay . We assume 
 that i is at the controller side if i . Moreover , under , binary over the considered channel are not necessarily received in the same order they were . It should be also that the channel does not 
 A more detailed presentation of the feedback path in the of Fig . is provided by Fig . . As , the controller is comprised of a and a lossless component . The 
 where e e the side information at the . : 
 instant , the is assumed to know the time experienced by previous binary and the time delay of the current binary word to be sent over the channel . Therefore , is known at the N . This that yE can be reconstructed perfectly later at the if is by yE and only those of which will be already available at the at time . Note that access to at the at the time is not assured . So the lossless O the binary 
 in which is a sequence the of for which the associated binary will have the by the time , i . e ., yE i : i N , i , i 
 deterministic where . So the 
 of the sequence . Note that since no dropout during data transmission , 
 will certainly have been received at the by the time . In addition , A is a countable set of prefix free binary code , which the input alphabet of the channel at each time instant . 
 On the receiver side , is available as the input to the lossless . This , 
 where is a sequence comprised of of that have time indices less than or equal to the time index of in , i . e ., where , N . Such selection of data for lossless is in accordance 
 , where , an arbitrary deterministic . It should be noted that according to the channel model , is a random variable the of and for all N . Moreover , based on the definition of and , the information provided by is enough for the lossless 
 where is defined as in . Indeed for such reconstruction , the knowledge of the delay is at the . Hence , we further assume that the is provided by through 
 for example . Finally , the controller the control input via 
 where the side information available at the at is in the well defined set . So . Moreover , 
 R , tu , is an arbitrary deterministic where tu is the of . It should be noted that o in o o is defined as o , e . We state some additional of the setting above in the following 
 Remark . It can be from that u and are of where for every N . It thus from the definition of and assuming no dropout in the considered channel that . This that the controller access to the and amount of sensor information when the channel delay is zero and , respectively . 
 Remark . It can be from the definition of in that can have at most at each time step . So the number of the that can be received at the at each time instant to the set ,..., . Therefore , since the channel input is a scalar process , a single input multiple output channel . Moreover ,, as a stochastic process , cannot be i . i . because in the considered channel , no binary word is received at the more than once . 
 Assumption . . At each time instant N , the side information pair e , together with , and consequently , are statistically independent of x ,. Therefore , it can 
 be from the dynamics of the system that I u ; hi for any hi ,..., with hi . Moreover , upon knowledge of , di and Si , the is invertible . It that for each i N , there a deterministic such that 
 Remark . In Appendix A , we will prove that for the architecture of Fig . , any and non invertible with E , O , O and , can be by another set of with the same input output relationship and lower average data rate where the is invertible . 
 For the purpose of the information rate in of spectral of the of the system , we use the following notion of stability : 
 Definition . A scalar strongly asymptotically wide sense stationary if its covariance matrix is asymptotically equivalent to the covariance matrix of the wide 
 asymptotically equivalent . Furthermore , in an , all internal are and their cross covariance matrices are asymptotically equivalent to the cross covariance matrices of corresponding to be to . 
 Clearly , ness ness but not vice ; for both and . For each scheme satisfying and rendering the of Fig . , the steady state variance of the a random variable which on the realization of . The same goes for the average data rate . We make explicit such dependence by writing and , and consider the of these over all of as our performance measure and data rate of interest , respectively . Such of performance and rate , 
 support set for possible of the delay . Moreover , and indicate that the average data rate and steady state variance are of delay . 
 Generally speaking , we are interested in finding the minimal Ra for which a bounded 
 is feasible . Let denote the average steady state variance can be , when the random delay , with the , is present in the channel . Hence , is by the average steady state variance all possibly nonlinear and time u that render the of Fig . . Note that is defined as in Remark . Under the condition that Assumption . , 
 where , , and the average state variance of the all of the delay . The feasible set of the optimization problem in is 
 comprised of all controller and controller by , satisfying 
 Remark . It is straightforward to see from that the concatenation of the pair and the channel in the of Fig . is equivalent to a controller pair with the same and side information that a time delay with same as in , on its received data , and that is by a delay free channel . So the system in Fig . is equivalent to the feedback loop of Fig . in which the and the plant are the same and the have the same as in Fig . . 
 The equivalence pointed out in Remark between of Fig . and the of Fig . 
 will assist us a lower bound on the average data rate Ra in the next Section . 
 In this section , we establish a lower bound on Ra . To do so , we derive and that describe the relationship between the flow of information and system performance in the of Fig . . Therefore , we will update fundamental in , for the case where the channel delay is randomly distributed . As the first result , we show how the average data rate Ra is bounded from below in the following theorem . 
 Theorem . Consider the feedback loop in Fig . for which . and . hold . Then 
 where I .;. . conditional mutual information . According to , the average directed information rate across the forward channel u in the of Fig . over all possible of the channel delay . 
 Proof . It can be from , Theorem . that , for each realization h of the , the 
 Based upon the chain rule of mutual information , the bound in can be as 
 where the definition of li is given in Remark . From the dynamics of the plant , we can easily conclude that the sequence is only a function of and , once is given . Furthermore , it from that upon the knowledge of , side and will be 
 the only u i , i N . Latter together with the fact that Assumption . for the system of Fig . yield the conclusion that the rightmost term of to zero , i N . So we have 
 for the of Fig . . Now by both sides of with respect to the delay , as in , and that the feedback loop of Fig . is equivalent to the system of Fig . , based on Remark , our claim immediately . 
 The next lemma that joint of two the directed information rate between them when these are connected through a channel with random delay . 
 Lemma . Suppose that the of Fig . Assumption . and Assumption . . For this system , if ,, u , a jointly second order set of , then the following : 
 where and uG symbolize the u , respectively , in a way that ,, uG , are jointly with the same first and second order cross as ,, u ,. 
 Proof . According to , Lemma . , the directed information rate from sensor output to the 
 where I u and I uG are defined as in . We conclude based on , the dynamics of the plant and the system of Fig . satisfying Assumption . that 
 , , i N . This together with the chain rule of mutual information lead to 
 Now the proof is complete by taking average over all possible of the delay from both sides of and considering that based on Remark , the system of Fig . is equivalent to the feedback loop in Fig . . 
 If the above are stationary as well , then the average directed information rate can be stated in of the average power spectral density of the involved . The next lemma will state such result formally . 
 Lemma . Suppose that the control input u in the of Fig . is for every realization 
 of the channel delay . For each realization , assume that there a in such a way that 
 where is a process that independent . Such a random process is as 
 for each realization of the random delay . Moreover , the steady state power spectral density of u . 
 Proof . It can be from , Lemma . that in the of Fig . , the following 
 in a process with independent and I u is defined 
 As already before , based on the plant dynamics , the knowledge of will render 
 dependent only on and wi for any i N . Moreover , according to , knowing 
 From , it can be that is actually equal to as in for the of Fig . . Now , our claim is given by taking average from both sides of upper and that based on Remark , the in Fig . and Fig . are equivalent . 
 Corollary . Suppose that the of Fig . Assumption . . Then Ra is lower bounded as : 
 where and u are defined as in and the is restricted to all and Assumption . , u with as stated in Lemma . 
 In this section , we consider the same as in Section but with a channel that a known constant delay , say , on the data . The corresponding feedback loop is by Fig . . The problem we investigate here is a special case of the problem in where the channel delay is constant and therefore , there is only one realization for the channel delay . In this case , we consider the notation Ra and . In Appendix A , we prove that finding is feasible if 
 , . We show that in order to obtain a lower bound on , one can minimize the directed information rate over an auxiliary scheme formed of and an channel with feedback and delay . and related to the delay free version of this optimization derived in will be extended to the case with a constant channel delay . We start by a lower bound on the average data the following theorem . 
 Theorem . Suppose that the feedback system of Fig . . and . . Then the average data lower bounded as : 
 where is the directed information rate across the forward channel u with 
 Proof . Considering that at any N for the of Fig . , we can conclude the claim immediately from Theorem . 
 The directed information rate in will be reduced if the involved are jointly . This result is by the following lemma . 
 Lemma . Suppose that . and . hold for the of Fig . . Furthermore , consider ,, u , as a jointly second order set of random . Denote the u by and uG , respectively , where ,, uG , are jointly with the same first and second order cross as ,, u ,. Then 
 Proof . Recall that , N , for the considered case with constant channel delay . The claim immediately from Lemma . 
 It can be from Lemma that by directed information rate over a scheme u jointly , one can obtain a lower bound on . Now , we will 
 show that the directed information rate can be stated in of power spectral of the involved if such meet certain . 
 Moreover , assume that u is jointly and with the sensor output . Then the directed information rate between u expressed as 
 Proof . Immediate from Lemma by that for the of Fig . at every time instant N . 
 It can be from Theorem and Lemma that the rate performance pair by any and control scheme satisfying Assumption . which the of Fig . is attainable with a lower or equal rate if there a scheme that , u jointly with x , while rendering the system . Due to the of x , and the fact that the plant is , a jointly pair , u can be produced by a control scheme comprised of and an noise source . Such a scheme is in Fig . . The of Fig . all of the and that hold for the system of Fig . . However , the arbitrary are by the auxiliary feedback loop of Fig . . 
 In addition , for such an , a channel with noiseless one sample feedback as communication channel . The control scheme in the of Fig . is 
 where is a zero mean white noise with variance and independent of x ,, and By . It should be that Assumption . for the initial x , the the disturbance the of Fig . . Furthermore , the initial of , and the delay are deterministic . As the system in Fig . is a special case of the structure of Fig . , we use for in Fig . that have in the of Fig . . 
 Theorem . If the of Fig . Assumption . and Assumption . and 
 where and represent the steady state variance of z and the steady state power spectral density of u in Fig . , respectively . Moreover , the feasible set for the optimization in is the set comprised of the noise with that render the system of Fig . 
 Theorem that doing the optimization in over the auxiliary system of Fig . , with the channel and delay , will give a lower bound on the minimal data rate to achieve a certain performance level in the arbitrary possibly nonlinear and time structure of Fig . . The following show how the lower bound derived in can be simplified to a bound which is easier to compute . 
 where is fixed and the steady state power spectral density of . Moreover , suppose that the pair , B , J the feedback loop of Fig . internally stable and well . Then for any , there another pair with a filter , say J , and a proper one , say B , that the system of Fig . internally stable and well , and the steady state power spectral density of z in a way that the following : 
 Intuitively speaking , the of Theorem and Lemma imply that can be bounded from below by a logarithmic term as in which is a function of channel in the 
 of Fig . . Such an intuition will assist us with a lower bound which is appealing in the following corollary . 
 Corollary . Take the feedback loop of Fig . into account as an that 
 in which st and symbolize the steady state z in the auxiliary system of Fig . , respectively . For the optimization problem in , a candidate solution is an filter 
 pair , together with noise variance that cause the system in Fig . to become 
 In this section , we show that for any , , one can always find a scheme that with an average data rate which a distance of about . per sample from the theoretical lower bound . For such a scheme , we propose a design approach which the that together with an with feedback and delay , render the directed information rate over the channel equal to the lower bound on . 
 Definition . We call a scheme with input output relationship as in in the constant channel delay case linear if and only if its dynamics can be as : 
 where By proper with deterministic initial condition . Moreover , a zero mean i . i . random sequence independent of x ,. The initial state of the one step delay feedback channel is assumed to be deterministic . 
 The realization of linear source can be carried out by entropy together with . First , an the 
 in which by , we denote a uniform with resolution , : i ; i . Additionally , a dither signal whose access are provided to both and . The and its complementary formalize entropy for the lossless at the and , respectively . The following lemma an interesting property of when being set up in an feedback loop . 
 Lemma . Consider the feedback loop in Fig . and suppose that the plant is by a proper real rational transfer function matrix in which the transfer function from scalar and strictly proper . For such a system , assume that the input output relationship 
 of the in the feedback path is given by with finite and positive quantization step size . Moreover , take the disturbance into account as a white noise process jointly second order with , the initial state of . Then if the an i . i . process with a uniform distribution over , and independent of , , the i . i ., uniformly distributed over , and independent of , . 
 It can be from above that combining the in with the in in a setting as in Fig . will lead to a linear scheme for the of Fig . as 
 long as the same criteria as for the dither in Lemma . If so , the 
 scheme is a linear based scheme . If such a scheme is on the feedback path of the main of Fig . , the average data rate is bounded from above by a certain value which is shown in the following lemma . 
 Lemma . Suppose that Assumption . for the of Fig . . Then the existence of an based linear source scheme rendering the of Fig . is certified in such a way that the average data rate 
 In , the variance of the quantization error noise of the based linear source scheme is set as . Moreover , st the steady state variance of the signal t 
 Now , through the following theorem , we use the result of Lemma to show that based linear can lead to an upper bound on the desired minimal average data rate . 
 Theorem . Let Assumption . hold for the closed loop system of Fig . . Then for each , , one can always find an based linear source scheme satisfying Assumption . and rendering the feedback loop of Fig . in such a way that is and the average data rate is bounded as 
 In the following remark , we state how the upper bound derived in Theorem can be considered as an upper bound on Ra in the case of random channel delay . 
 Remark . The upper bound in will be an upper bound on Ra in the random channel delay case if and control are linear based designed as in the proof of Theorem for the delay where the have at 
 Fig . : Three possible for the delay component in the case with constant channel delay 
 their sending only for at each time instant N . Clearly , this is due to the fact that at every time step N , is available at the . Such an upper bound does not seem to be tight since imposing a delay of on data is actually a worst case scenario . 
 The derived in this section and the previous section limit the desired average data rate in the of Fig . . In this system , the constant delay is induced by the digital 
 communication channel between the controller and the controller . One concern is the effect of delay location on the derived . The following lemma a step in this issue by showing how the system change when the time delay block is to a different location in the feedback loop of Fig . . 
 Lemma . Consider the of Fig . and two other each of which by moving the delay component in the of Fig . to either the measurement path between the sensor and the controller or the actuation path between the controller and the plant . Fig . the where the time delay in these . Then are not necessarily equivalent across the if the only difference between them is the delay location . However , the equivalence can be assured by the side information to change across the 
 Take the following transfer function into account as the model the generalized the of Fig . : 
 Let us set the disturbance initial x in such a way that Assumption . is satisfied . We calculated lower and upper on as derived in and . For these , we made use of the equivalence between the of Fig . and Fig . , shown in the proof of Lemma , in that we adopted the method in which performance optimization similar to the one for such as the of Fig . . The are for three different of channel delay , , , , with respect a range from to for each . Moreover , we designed actual linear based , and for each the latter interval , we the of Fig . . To do so , we the giving the lower bound on according to the procedure in , Theorem . . The are 
 in Fig . . In this figure , the to as and present the lower 
 Fig . : on in and actual data and for different of time delay h 
 and upper on , respectively . We can compare among with different of channel delay as well . As shown , greater is associated with channel delay , as according to . how the change in response to in the delay is one of the main of this simulation study . We can observe from the plotted in Fig . that fixed , increasing the delay will enlarge the on . In other , the greater the delay is , the higher average data rate is to be used in order to achieve a fixed quadratic performance level . Moreover , Fig . that the lower upper bound converge to the minimum data rate for mean square stability as . From , we know that the minimal data rate of the of Fig . is only a function of unstable of the plant . On the other hand , we use the equivalent system of Fig . for the purpose of calculating . So the observation with convergence of to the minimal data rate for stability comes from the fact that time delay into the model of the plant Ga will not affect its unstable . 
 Simulation are in Fig . as well . The to as OR and OE present the average data and by actual linear . Furthermore , sample long have been considered for the dither . The task 
 in all is done by memory less which do not take the past information of the dither into account as prior knowledge for . In addition to the average data rate , the entropy of the output of the been for the setup . The gap of around . per sample between the measured entropy and the lower bound that for each , , , . per sample of the gap between the actual and lower bound is by the with uniform dither and the remainder . per sample to sample by sample . It can be that the actual and have the same as the of in the previous paragraph . The most prominent property is related to the behaviour of the and as a function of channel delay , i . e ., for a system with greater time delay in the channel , 
 In this paper , the trade off between average data rate and performance in control been studied . Two have been , each of which an plant with disturbance and initial , and scalar control input and sensor output . Moreover , both of them have causal , but otherwise arbitrary , on their feedback which are responsible for and control . The only difference between the two considered is the model of the channel that out data transmission between the and the controller . In one case , the digital communication channel is and information to be are exposed to random delay . In the other system , the channel is error free as well but it is and constant delay on data . For the case with random channel delay , we considered for rate and performance which show the average behaviour of the system over all of the delay . We have shown that for such a setup , data rate is lower bounded by average directed information rate from the sensor output to control input , and u are jointly , the average directed information rate would be . Moreover , we have shown that u satisfy certain , the average directed information rate between them is a function of the average power spectral of these over all of the channel delay . We have shown that value of this function over all arbitrary and that cause system have those and lower the average data rate to attain a quadratic performance . 
 For the constant delay case , which is a special case of the system with random channel delay , we by the minimal average data rate that a certain performance level . the fundamental information and derived for the random delay case , we that this desired minimal average data rate is bounded from below when coder and the channel behave as a concatenation of proper and an channel with feedback and delay . Then we that by such with simply linear based , one can achieve any legitimate performance level by actual which are at most . per sample higher than the lower bound . The through the simulation show that and empirical are increasing of channel delay for a fixed performance level . It delay in the channel higher minimal average data rate that is for a certain level of quadratic performance . 
 Future research will concern with finding closed form solution for the lower and upper bound in the case of random channel delay , finding analytic expression for the desired data rate , lower and upper with shorter gap between them , with model 
 Lemma . Consider a scheme through that a non invertible , 
 and let be defined as . For such scheme , assume that u u and f , N , where . Then there another 
 scheme the control input u u with an invertible in such a way that f , N . 
 Proof . Suppose that in represent a non invertible at a way that upon knowledge of and Si , perfect reconstruction of from been possible for all i 
 Let S and S be associated with and respectively . Two possible can occur . In the first case , S and S are unequal , i . e ., S S . known at the at each time step , this case does not contradict the invertibility . That is due to the fact that the knowledge determine whether u is by or . However , the situation is not the same in the case where S S . Since both and are vector valued , that at least one entry of is not equal to the entry with the same dimension in . The corresponding of and that are not equal to each other are by , , , . Since S S , for each , both and have been exposed to the same delay , say , . So if we denote the output of the that to by , then . It should be noted a positive integer which is at most equal to the size of the set As . Let 
 time . The set E , can be defined with exactly the same as E , but different from it in the sense that E only at time with probability . This only at input instead of either or . Let us define , ; . Then 
 in which aa from the definition of entropy and , can be based on the fact that the function is monotonically decreasing , and from the definition of for the scheme E ,. So E , E , for , and consequently f . 
 The above procedure can be for pair with the same as , to make sure that there are no two of the reproduction into one identical u at time instant . Such iteration will then yield an invertible . In other , 
 u i u i and i f i , i . Our main claim now by the above 
 Suppose that in the standard architecture in Fig . ,, x Assumption . . Considering the of x the 
 in which the steady state variance of is the set of all proper which render the system of Fig . internally stable and well . The considered that finding is feasible . Since can be , for every , , there K which for the system of Fig . . K to this system in a stable setting which is a special 
 case of the in Fig . with and where the steady state variance of , is finite . Therefore , since K , it can bring internal stability and well to the feedback loop of Fig . in the presence of any additive noise with steady sate 
 into account as an with finite variance for the system of Fig . . It should be noted that 
 , depend only on K . Now by choosing and the variance 
 for the , there K rendering the of Fig . internally 
 So considering inequality and concavity of logarithm , we can deduce that the problem of finding in is feasible for every . The feasibility of the problem of finding in is immediately from for any . 
 Due to the validity of , one can always find at least one control pair , say and , that while satisfying Assumption . , the of Fig . in such a 
 In and are the of , and u in Fig . , respectively . Moreover , 
 the and in stem from Theorem if in Lemma and Lemma are satisfied . Therefore , , are jointly of , as in Lemma and the steady state power spectral density of as in Lemma . The 
 pair , with stated in Lemma can be by a scheme which 
 in which a noise with zero mean and independent of . Since is a linear and causal , we can redescribe as 
 It from causality in that , and are lower triangular matrices with and on the top left . This together with the fact that , are jointly allow us to conclude that based on transitivity of asymptotic equivalence for and sum of the matrices in , the and are asymptotically equivalent to of lower triangular matrices . Furthermore , as in will bring internal stability and well ness to the corresponding . Now let us set a concatenation of linear with the same behaviour as steady state behaviour of in for the auxiliary system of Fig . . Moreover , suppose that a variance equal to . So based on the asymptotic equivalence between the matrix , choosing , and as above will render the system of Fig . well and internally stable . More specifically , the latter set of and the noise will give to which and converge . Therefore , for the control input u and error signal z in the feedback loop of Fig . , and hold . Then based on Lemma , the directed information rate in the of Fig . can be expressed as 
 First , we can deduce that any pair E , with stated above a counterpart comprised of filter , and the white noise in architecture of Fig . in such a way that I y u I and . Secondly , the main problem is finding the 
 Fig . : The system whose internal stability the internal stability of the auxiliary system in Fig . 
 all E With all of this in mind , it can be from and that the lower bound for would be equal to the rightmost term of which 
 The necessary and sufficient condition for the feedback loop of Fig . to be internally stable and well is that every entry of the transfer function matrix from input ,, , to z , y ,, u in the system of Fig . to . Such a transfer function matrix , which we denote by , is as : 
 Now , let us shift the delay block in the system of Fig . to the plant model in a way that for 
 Such an auxiliary is by Fig . . Except for the plant model , everything 
 Fig . : The auxiliary feedback loop the internal stability of the of Fig . 
 in the feedback loop of Fig . is assumed to be the same as in the system of Fig . . The 
 internal stability and well ness of the feedback loop of Fig . is if and only 
 Fig . to . It is straightforward to see that Ta . So an equivalence between internal stability and well ness of the system of Fig . and the of Fig . . In other , every triplet rendering the feedback loop of Fig . internally stable and well , will bring internal stability and well ness to the of Fig . as well . One other implication of Ta is that an commonly for of Fig . and Fig . will lead to an identical . This is due to the of 
 exposed to and stationary . Furthermore , those lead to the following H norm for and variance of the output z in the of Fig . : 
 in which , . Likewise , the and variance of the the of Fig . is in of H as : 
 and . Therefore , upon the same triplet ,, , the channel and the variance of the output performance will be the same for the of Fig . and Fig . . 
 According to , Lemma . , for any pair , B , J that the feedback loop of Fig . internally stable and well , there another pair with the same as for B , J in this lemma . Then our follow immediately from the above between the of Fig . and the of Fig . . 
 The feasibility of , to , , the existence of a triplet , say , that to for the system of Fig . . In the latter triplet , is assumed to be a proper filter and . This together with the definition 
 Moreover , the triplet with the in Lemma . Therefore , another triplet , say , in such a way that it 
 for the feedback loop of Fig . . Note that is a filter while only needs to be proper . Now the fact that for any , , the definition of in , and the claim of Theorem complete the proof . 
 the transfer function matrix from to in Fig . . Since is related toby , we can conclude of being proper and real rational , and a strictly proper open loop transfer function . Now the via and in mind , we can deduce our claim immediately from , Lemma . . 
 Let us assume that a linear source scheme is in the feedback path of the main system in Fig . . Due to the feasibility of finding , which satisfaction of Assumption . , we can conclude the existence of together with an , say , render the of Fig . . It from some of internal 
 stability that the system will still be stable if one the only as . This that in the case of unity feedback , internal stability and ness are for the open loop system . We come immediately to the conclusion that based on , Corollary . and statistical of the dither in Lemma . 
 Considering the feasibility of finding , of Lemma , lemma , and Lemma , and invertibility of the , we conclude the claim by following the same as in , 
 One of the common feedback loop across the considered in Fig . is the 
 where plant and u ,, and defined as in 
 . Moreover , A , B , B , C , C , D , D , and D are time invariant matrices of appropriate . According to the recursion in , the and of the plant at each time instant i N can be expressed in of initial , disturbance and control as : 
 For the case where the time delay is by the error free digital channel between the controller and the controller , the relationship between the control input and the sensor output is based on . The dynamics by can be 
 where and represent causal , but otherwise arbitrary , at each N . It 
 from that can be stated as an arbitrary function , say , of , i . e ., 
 k N , is a function of is a function of , and is a function of . 
 In the second case , it is the link between the controller and the plant that the 
 time delay . For such a setting , e and yield a scheme with following dynamics : 
 It from that in this case , can be expressed as , N , where is an arbitrary which is by and . 
 Substituting such an expression into an by induction , we can ,, and as 
 As the third case , we focus on a structure in which the delay is by the path between the sensor and the controller . In this situation , the scheme is by causal 
 and based on induction , we come to the conclusion that for the closed loop system considered in 
 According to the above , system , sensor output , and the each time instant that such are not necessarily equal across the three studied above if the share the design for and control and side information and have the same initial and exogenous . So of each signal change by the delay component in the of Fig . . However , it is straightforward to see from the structure of the ,, the equivalence over can be under the condition that everything is the same across 
 the except for side information which can be considered as decision variable . 
 and . Interplay between transmission delay , average data rate , and performance in output feedback control over digital communication , in Control Conference , May . 
 and . A Achievable performance of zero delay variable rate in control with channel delay , in th Annual Conference on Decision and Control . 
 ... Han , and Survey on recent in control , on 
 . and . Feedback control under data rate : An overview , 
 and O Network induced in control a survey , 
 . and . Control and communication in real time , of 
 A .. and A . Estimation and control over communication . Springer Science Business 
 .. and . A Feedback control in the presence of noisy : Bode like fundamental 
 of performance , on Automatic Control , vol . , no . , . 
 .. A and . Fundamental of disturbance attenuation in the presence of side 
 information , on automatic control , vol . , no . , . 
 E . I . Silva ,. and A framework for control system design subject to average data rate 
 , on Automatic Control , vol . , no . , . 
 , An achievable data rate region subject to a stationary performance constraint for , 
 E . I . Silva ,. and . A . Encina , A characterization of the minimal average data rate that a given closed loop performance level , on Automatic Control , vol . , no . , . 
 . and . control with minimum directed information : Semidefinite approach , on Automatic Control , vol . , no . , . 
 . A and . Zero delay rate distortion via filtering for vector valued 
 , Journal of Selected in Signal to appear , . 
 . and Rate cost in control , . . Available : : . . v 
 and infinity stabilization for singular cascade control with state delay and 
 disturbance , on Industrial , vol . , no . , . 
 . A O . Yin , and Adaptive indirect fuzzy sliding mode controller for control subject to time network induced time delay , on Fuzzy , vol . , no . , . 
 . Cheng , and Fuzzy model based cost control of nonlinear , 
 . Lam , and . Mao , Stability analysis of continuous time switched with a random switching 
 signal , on Automatic Control , vol . , no . , . 
 . Shi , and Network based robust H H control for linear with two channel 
 random packet and time , on cybernetics , vol . , no . , . 
 .. Pang ,. and Output control for : A model based prediction 
 approach , on Industrial Electronics , vol . , no . , . 
 . Li and . Shi , Network based predictive control for constrained nonlinear with two channel packet , 
 on Industrial Electronics , vol . , no . , . 
 . Wen , and . Cheng , Wide area damping controller for power system : A predictive control approach , on Control Technology , vol . , no . , . 
 . ` in controller design for with delay and quantization , in th Conference on 
 .. Han , and . Yang , Optimal communication network based H control with packet for a class of discrete time neural with distributed time delay , on neural and learning 
 E . and control under round robin communication protocol , 
 .. A .. Teel ,. Van de , and control with communication : between transmission , and performance , on Automatic control , vol . , no . , 
 . and .. Wang , On the rate cost of linear control with random communication , in 
 International Symposium on Information Theory , June . 
 .. Cover and . A of information theory . Sons , . 
 . E . I . Silva , and Fundamental and mutual and directed 
 in closed loop , . . Available : : . . 
 H H controller design for input delay and preview based on state decomposition approach , 
 .. dissertation , Department of Human , Metropolitan University , Japan , . 
 E Control and communication with signal to noise ratio . dissertation , Department of 
 .. om ,¨ Introduction to stochastic control theory . Courier Corporation , . 
 .. Gray , and circulant matrices : A review , and in and Information 
 . A A course in H control theory . Berlin ; New York : Springer , . 
  
 ﻿ This paper control closed over noiseless digital . We focus on noisy linear with stationary , initial state , scalar valued control and sensor . For this set up , we show that the absolute minimal directed information rate that one to achieve a level of performance not necessarily stationary , over all of controller is when the output is jointly with the other in the system . This directed information rate lower the achievable operational data . When our attention to controller which make the random in the loop strongly asymptotically wide sense stationary , this bound can be expressed in of their asymptotic power spectral . Then we show that the directed information rate and stationary performance of any such scheme can be when the , channel , controller and behave as an channel with . We also present a simple scheme that one to achieve operational average data that are at most approximately . away from the derived lower bound , while satisfying the performance constraint . A numerical example is to illustrate our . 
 Index control ; optimal control ; average data rate ; information theory ; signal to noise ratio . 
 This paper control for linear time invariant where communication place over a digital communication channel . Such have received much attention in the recent literature , . This interest is by the theoretical inherent to control subject to data rate , and by the many practical that the understanding of fundamental in such a set up may have some are in , e .. . 
 The literature on control subject to can be broadly classified into two . A first group , which , that are rooted in non linear control theory . An alternative approach that information theoretic been adopted in , e .., . A key question by the works in the latter group is how to extend , or adapt if necessary , standard information theoretic to reveal fundamental in data rate limited feedback . Related have been in , where the interplay between information and disturbance attenuation is . 
 The most basic question in a data rate limited feedback control framework is whether closed loop stabilization is possible or not . Indeed , stabilization is possible only if the channel data rate is sufficiently large , . These early several works that study minimal data rate for stabilization and observability see , e .., , . A fundamental result was in . For noisy over a noiseless digital , it is shown in that it is possible to find causal , and such that the resulting closed loop system is mean square stable , if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant . A thorough discussion of this and related work can be found in the survey paper . Recent , stabilization over time , are in . 
 It is fair to state that , for , stabilization subject to data rate are well understood . However , the question of what is the best closed loop performance that is achievable with a given data rate is largely open . Such are related to causal and zero delay rate distortion see , e .., . In the latter context , the best are , to our knowledge , algorithmic in nature , derived for open loop and , at times , rely on arbitrarily long , . It thus that the in the above are not immediately applicable to feedback control . 
 In the rate constrained control literature , lower on the mean square norm of the plant state have been derived which show that , when are present , closed loop performance becomes arbitrarily poor when the feedback data rate the minimum for stability , . This result no matter how the coder , and controller are designed . Unfortunately , the in , do not seem to be tight in general . In contrast , for fully observable noiseless with bounded initial state , that one can essentially recover the best non performance with data arbitrarily close to the minimum average data rate for stabilization . Other valid in the noiseless or bounded support noise can be found in , e .., . 
 Relevant work on optimal control subject to rate , and dealing with unbounded support noise , include , . Those works establish for separation and certainty equivalence in the context of quadratic stochastic for fully , when data rate are present in the feedback path . It is shown in that , provided the a recursive structure , certainty equivalence and a partial separation principle hold . The latter result is relevant . However , does not give a practical characterization of optimal . The in share a similar drawback . Indeed , performance related in are in of the sequential function , which is difficult to compute in general . Moreover , even for the where an expression for such function is available , it is not clear whether the sequential rate distortion function is tight , Section . Partial separation in optimal control been recently in . 
 Additional related to the performance of control subject to data rate are in , and . In , noiseless state estimation subject to data rate are studied . The case most relevant to this work an asymptotic in time quadratic criterion to measure the state reconstruction error . For such a measure , it is shown in that the bound established in is sufficient to achieve any asymptotic distortion level . This is , however , at the expense of arbitrarily large estimation for any given finite time . On the other hand , non linear stochastic control over noisy , and a functional i . e ., not explicit characterization of the optimal control is . In turn , a intensive iterative method for and controller design for over noisy discrete memoryless . for separation and certainty equivalence are also in for some specific set . 
 In this paper , we focus on the feedback control of noisy , with one dimensional control and sensor , that are over an error free and delay free digital channel . The plant initial state and the disturbance assumed to be jointly . By considering causal but otherwise unconstrained , we study the minimal operational average data rate , say , that that the steady state variance of an error signal is below a level . Our approach is mainly information theoretic and thus some of our complement previous in , . 
 We have previously this problem in and . In we have shown that is lower bounded by the directed information rate I u , u are the input to the and output from the , respectively . Then , for a restricted class of , independent by as a unit transfer function u plus additive wide sense stationary noise independent of , we that I u is only u are jointly . For the latter case , it was shown that this information rate can be written as where is the asymptotic power spectral density of u and is the asymptotic variance of the error in u i from u u i and i . It was also shown that an operational average data rate exceeding I u by less than approximately . sample was achievable by an entropy uniform . In , and also for the class of independent , we then derived the minimum feedback directed information rate I u for an asymptotic quadratic performance measure . 
 An important departure of this paper from our previous work is that here the is not constrained to exhibit a unit transfer function u i . e ., it is not restricted to be an independent scheme . As a result , the pair also as a controller for the plant , and thus reaching designing the jointly optimal controller system . For this context , the main of this paper can be as : 
 We show that , for any given performance , I u is if and only if u jointly in Section . 
 For the jointly case , we also show that I for the class of 
 yielding strongly asymptotic wide sense stationary u and . This class namely , is much than and the family of independent . For instance , it for controller built any combination of linear and non linear causal , provided the behaviour is . This result is in Section . 
 In Section we show that the asymptotic of any scheme can be also by a set of and an exogenous additive white noise . 
 In use the latter result to show how to minimize I u within the class of by a related stochastic control problem subject to an constraint in the feedback channel Section . This same constrained problem been in and in for different but related control subject to , see , e .., . 
 the fact that the latter problem was shown to be convex in , and the convenient procedure to solve it in , we explain how to design a simple controller scheme which within approximately . sample of within the class of , for any performance . This result is in Section . 
 To illustrate the above , section a numerical example . Concluding are in Section . 
 Notation : the set of real , the set of strictly positive real 
 N , ,. In this paper , log for natural logarithm , and for the magnitude absolute value of . We work in discrete time and the time index . An said to be proper i . e ., causal if its transfer function remains finite when , and it is said if it is proper and . We define the set U as the set of all proper and stable with that are also stable and proper . 
 In this paper , all random are defined for N . All random and are assumed to be , unless stated otherwise . Given a process , we denote its sample by and use as shorthand for ,...,. We say that a random process is a one if it first and second order that are bounded for that also remain bounded as . 
 are , by definition , second order . We use E to denote the expectation operator . A said to be asymptotically wide sense stationary if and only if there exist and a function , both independent of the statistics of , such that E and E E E 
 for . The steady state spectral density of an process is by and defined as the transform of extended for according to . The corresponding steady state covariance matrix is by , and trace . Jointly second order and jointly are defined in the obvious way . The covariance matrix of a random sequence is by E un , where un u u . For a matrix A , the notation A , to the element of A on the th row , th column , while A and A denote the of A with the and magnitude , respectively . Two of matrices , with An , , are said to be asymptotically equivalent see if the following two are met : 
 for some . useful notation and from Information Theory . 
 This paper on the control system of Fig . . In that figure , is an plant , e u is the control input , is a sensor output , e is a signal related to closed loop performance , and is a disturbance . The feedback path in Fig . a digital channel and thus quantization becomes mandatory . This task is carried out by an whose output to a sequence of binary . These are then over the channel , and back into real by a . The and also embody a controller for the plant . 
 where are proper transfer of suitable . We will make use of the following . 
 Assumption . : is a proper plant , free of unstable hidden , such that the open loop transfer function from u to i . e ., P in is single input single output and strictly proper . The initial state of the plant , say , and the jointly , is zero mean white noise with unit variance I , and finite differential entropy i . e ., the variance of is positive definite . 
 We focus on error free zero delay digital and denote the channel input alphabet by , a countable set of prefix free binary . Whenever the channel input symbol to , the corresponding channel output is given by . The length of is by , and the average data rate across the channel is thus defined as 
 We assume the to be an arbitrary hence possibly non linear and time causal system such that the channel input 
 where ak is shorthand for a ,..., a , SE side information that becomes available at the at time instant , and is a possibly non linear and time deterministic whose range is a subset of . Similarly , we assume that the is such that the plant input u is given by 
 where side information that becomes available at the at time instant , and is a possibly nonlinear and time deterministic . 
 Assumption . : The E Fig . are causal , possibly time or non linear , by . The side information SE and are jointly independent of ,, and the is invertible upon knowledge of and , i . e ., i N , there a deterministic i such that . 
 The assumption on the side information is by the requirement that causal and use only past and present input , and additional information not related to the message being sent , to construct their current see also page in . On the other hand , if , for some E and , the is not invertible , then one can always define an alternative and pair , where the is invertible , yielding the same input output relationship as E and , but a lower average data rate , Lemma . . Accordingly , one can focus , without loss of generality , on where the is invertible . 
 Definition . : We say that the of Fig . is asymptotically wide sense stationary if and only if the state of the plant , the output , the control input u , and the disturbance , are jointly second order . 
 Remark . : The notion of stability above is than the usual notion of mean square stability where only N E is to hold 
 In what we will be interested on information in of the asymptotic spectral of the in the system . To do so , it will be necessary to use a notion of asymptotic wide sense : 
 Definition . Strongly Process : An scalar random process u which to a process u is said to be strongly asymptotically wide sense stationary if the are asymptotically equivalent . Likewise , a is said to be if the covariance and cross covariance matrices of all the in it are asymptotically equivalent to their stationary . Clearly an is also , but the converse may not be true , in general . The same for . 
 The goal of this paper is to characterize , for the of Fig . , the minimal average data a given performance level as measured by the steady state variance of the output e . We denote by the steady state variance of e that can be by setting u , with being a possibly non linear and time deterministic , under the constraint that the resulting feedback loop is . With this definition , we formally state the problem of interest in this paper as : Find , for any , and whenever Assumption . , 
 where se trace , is the steady state covariance matrix of e , and the optimization is carried out with respect to all E satisfy Assumption . and render the resulting . 
 It can be shown that the problem in is feasible for every , see Appendix A . If , then the problem is clearly unfeasible . On the other hand , an infinite average data rate , except for very special . We will thus focus on , without loss of generality . 
 The remainder of this paper within a gap smaller than approximately . per sample . Such characterization is given in of the solution to a constrained quadratic optimal control problem . We also propose and which achieve an average data rate within the above gap , while satisfying the performance constraint on the steady state variance of e . 
 This section that a lower bound on can be by the directed information rate across an auxiliary scheme comprised of and an additive white noise channel with feedback . The starting point of our presentation is a result in , which does not require any . 
 The quantity I u to the directed information rate across the source scheme of Fig . i . e ., between the the output u of the source scheme . Note that I u is a function of the joint statistics u only . 
 We will now derive a lower bound on the directed information rate across the considered scheme , in of the directed information rate that would appear if all the involved were . 
 Lemma . : Consider the of Fig . and suppose that . and . hold . If , in addition , ,,, u are jointly second order , then I u I uG , where and uG are such that , uG are jointly with the same first and second order cross as ,,, u . 
 Proof : Our claim is due to the following chain of and : I u i ; a I , ; i 
 x , o , d ,, , u u and x , o , d , SE , and from Lemma . in Appendix . In turn , equality in since 
 where a , and e hold from the chain rule of mutual information , from b in Theorem . , is a consequence of being a deterministic function of , and from a in Theorem . . This the proof . 
 It from Theorem . and Lemma . that , in order to bound from below , it to minimize the directed information rate that would appear if the source scheme of Figure the channel were by a block u which are jointly . Again , this irrespective of the of the involved . 
 Now we can relate the directed information rate from to uG to their associated spectral . For that purpose and from here on , we shall focus on which render the . 
 Lemma . : Assume that u jointly and that u is and such that , for some . Then , 
 where is the steady state power spectral density of u , and is the steady state variance of the sequence of independent random , defined via u , E u , . 
 Proof : We start by that , since u , are jointly , a simple modification of the proof of Theorem . in ,. the conclusion also and . To proceed , we note that 
 where a from Property in Appendix from the definition of , from Property in and the fact that , by construction , i is a deterministic function of , , and from Property in Appendix i is independent of , . Now , and the definition of directed information rate 
 where a from and in Appendix and the fact that , by construction , is independent of , and from Lemma . in the Appendix and the fact and that u is with 
 Lemma . the directed information rate between in of the spectrum of the process towards which the mutual information is directed . Lemma . Theorem . in , where the author directed information between that are linked by an additive white noise channel with unit transfer function . Thus , Lemma . the latter theorem by considering a channel with noise and with an input output transfer function different from unity . 
 Remark . : It is worth that simply u to be instead of with for all may yield a left hand side of smaller than the term on its right . Indeed , to the best of our knowledge , the only result related to available for is , Lemma . , which an inequality in instead of equality . To see why is not enough for equality , consider the following example : 
 an unstable system with zero initial state that can be by unit feedback . a white process with zero mean and variance . Construct u as 
 gain closed loop with feedback noise . In this case , u can be written as will be with asymptotic power spectral density . Then , the entropy rate of the asymptotic process u is 
 where the last equality from formula Bode Integral Theorem and pi are the of . 
 However , it turns out that , for all , for some lower triangular matrix with along its main diagonal . Therefore , 
 Fig . . Auxiliary system that when the and of Fig . are by an additive white noise channel with one step feedback . 
 In this section we show that , for any given performance , the directed information rate by any scheme can also be by an pair built with only and an exogenous source of white noise . To that end , we begin by that Theorem . and Lemma . readily imply that for any and satisfying Assumption . , and rendering the resulting , I uG , where , uG are such that ,, uG , are jointly with the same first and second order cross as ,, u ,. We also note that the adopted performance measure is quadratic . The above imply that one can always match or improve the rate performance trade off of a given pair by choosing , instead , an and a which , besides rendering the , u , jointly with ,. Since the plant initial state and the disturbance are , and the plant is , one possible way of such pair of u , is by the feedback architecture of Fig . . We formalize these below . 
 Define the auxiliary feedback scheme of Fig . , where everything is as in Fig . except for the fact that we have the link between the plant the plant input u by a set of proper , and , and an additive noise channel with one step feedback and noise such that 
 where for the unit delay . In Fig . , we assume that the plant , the the plant initial state satisfy Assumption . , that the initial of , and of the delay are deterministic , and that is zero mean white noise , independent of ,, and constant variance . 
 In Fig . , we have added as in e to all that refer to that have a counterpart in the scheme of Fig . with possibly different statistics . To streamline our presentation , we adopt the convention that , whenever we refer to the auxiliary feedback system of Figure , it is to be understood that we are implicitly working under the stated in the above paragraph . 
 Theorem . : Consider the of Fig . and suppose that . and . hold . If , , then 
 where the optimization u is with respect to all proper , and auxiliary noise , that render the feedback system of Fig . with 
 F internally stable and well , and and se denote the steady state power spectral density of u and the steady state variance e in Fig . , respectively . 
 Proof : Denote by the set of all E that , when at each end of the discrete channel of Fig . satisfy Assumption . , render the of Figure and guarantee that se . Also , define , to be the set of channel which consider a unit gain i . e ., continuous alphabet noisy channel with possibly time linear as which render u and yield se . Since , both and , are non empty ; see Appendix A . The fact existence of at least one pair in , say E ,, and u , such that its average data rate 
 where the last equality is a consequence of Lemma . . u are jointly , their statistics and corresponding variance of e are by an triplet in ,, of a sequence of linear , N , such that 
 . Since is linear and causal , it a relationship between and which can be expressed 
 where and are of lower triangular matrices such that , , and are the top left of and , respectively . Now , since , u are jointly , the covariance matrices , and associated cross covariance matrices are asymptotically equivalent to matrices as well . the transitivity of asymptotic equivalence for and of matrices in , it is possible to show that and must be asymptotically equivalent to a sequence of lower triangular matrices as well see , e .. , Section . . Also , the associated limiting the resulting internally stable and well otherwise the underlying and would not be in ,, and the steady state spectrum of u and the steady state of both and e . Here , we use the fact that is also ; see the proof of Lemma . . 
 Now , consider the auxiliary feedback system of Figure before . Assume that , steady state behaviour of in i . e ., is comprised of two , as impulse the last of 
 variance equal and see previous paragraph . With the , respectively , and that a 
 n for , and , and since the matrix asymptotically equivalent to the sequence 
 , it that the feedback system of Figure is internally stable and well . In particular , it the an set of in the closed loop such that the plant input u a steady state power spectral density that , by construction , in the previous paragraph . Similarly , the error signal e in Figure a steady state variance se that . It thus from Lemma . 
 We thus conclude that , for any and E , in and u , there exist a proper a white noise source such that , when , the mutual information rate I u in Fig . I u while . that is the , it from and that is lower bounded by u as in , the proof . 
 Theorem . that a lower bound on the minimal average data rate that a given performance level , can be by an optimization problem which is stated for the auxiliary feedback system of Fig . , where communication place over an additive white noise channel with feedback . 
 We finish this section by a simpler lower bound on . To that end , we will first state an auxiliary result . 
 Lemma . : Consider the feedback system of Fig . . Fix and define whenever the involved exist 
 where is the steady state power spectral density of . If the pair , , the feedback system of Fig . internally stable and well , then there exist a second pair of , namely , , , with , that also an internally stable and well feedback loop , leaves the steady state power spectral density of e unaltered , and is such that for any arbitrarily small . 
 Proof : Consider Fig . and the partition . Introduce proper transfer Ly and such that Ly see . A standard argument that the feedback system of Fig . is internally stable and well if and only if the transfer n n and e u in Fig . is stable and proper . It is straightforward to see structure shown in the unnumbered equation at the top of the next page , where 
 We will write i to refer to the when , i , i , i , . Similarly , Ly i and wi refer to the of , when i . Set 
 where . Given and the fact thatn is the relative degree U , X , it that U and F 
 , the feedback system of Figure internally stable and well if and only if , does so . It also immediately that , the same stationary spectral density for e than , . 
 To complete the proof , we now propose specific choice for . Denote by i the signal that when , i , i . Write , where is stable and all its in : . Denote by c ,..., the of that lie on the unit circle . Define , for , , 
 By construction , and U for every , . It now , by proceeding as in the proof of Theorem . in , that there , such that setting in that , is such that for any . 
 Corollary . : Consider the of Fig . and suppose that . and . hold . If , , then 
 where the optimization is with respect to all , and auxiliary noise , that render the feedback system of 
 Fig . internally stable and well , and and se denote the steady state of and e in Fig . , respectively . 
 Proof : Consider the feedback system of Figure and recall the definition of both u and in and . Since , the problem of finding u is feasible see Appendix A . Thus , for any , there exist a proper filter , and , such that se and u ,, , 
 where we have used the fact that u whenever . On the other hand , Lemma . that there a pair of proper ,, with , such that the auxiliary feedback system of Figure is internally stable and well , 
 where the inequality from the definition of . Since for any , , our claim is now immediate from Theorem . . 
 Corollary . that a lower bound on can be by first , i . e ., by first , for the auxiliary feedback system of Fig . , the minimal steady state that that the steady state variance of the error signal e is upper bounded by . Section A how to obtain a numerical approximation to . 
 This section that it is indeed possible to achieve any distortion level , while an average data rate that the lower bound on in Corollary . by less than approximately . per sample . 
 Definition . : The source scheme by and is said to be linear if and only if , when used around an error free zero delay digital channel , is such that its output u are related via 
 scalar valued auxiliary , is an independent second order zero mean i . i .. sequence , and the transfer of proper that , together with the unit delay , have deterministic initial . ¦ 
 Remark . : In Definition . , the requirement independent without reference to other random or is to be understood be independent of all exogenous and initial in the feedback system in which the source scheme is . In particular , when an independent source scheme is used in the of Fig . , is to be assumed independent of ,. ¦ 
 The class of linear source is by the of Section and the class of independent source in . We note that independent source do not necessarily satisfy Assumption . . 
 Linear source are defined in of their input output relationship with no regard as to how the channel input is related to the source scheme input . A simple way of making that relationship explicit is by an entropy ; , . When such a device , , and the channel input and output , are related via 
 where is a dither signal available at both the and sides ,: i ; i a uniform with step size , is a an entropy coder i . e ., a loss less whose output symbol is chosen according to the conditional distribution of , given , and is a the entropy that is complementary to the entropy coder at the side . 
 Lemma . Theorem . in : Consider the set up of Figure , where the is as in and a finite quantization step . Assume that is a proper real rational transfer function , that the open loop transfer function single input single output and strictly proper , and that the signal is a white noise sequence jointly second order with the initial state o of . If the dither is i . i .., independent of , and uniformly distributed on , , then 
 w is i . i .., independent of ¦ , and uniformly distributed in , . 
 It that any scheme by and , with dither as in Lemma . , is a linear source scheme . Any such scheme will be to as an based linear source scheme . Figure an based linear source scheme where we have made explicit the fact that , since the channel is error free and zero delay , and , thus , can be at the side without making use of any additional feedback channel . 
 The next lemma an upper bound on the operational average data rate in an based linear source scheme . 
 Lemma . : Consider the of Fig . and suppose that that Assumption . . Then , there an based linear source scheme such that the resulting is . For any such scheme , 
 where is the steady state variance of the auxiliary signal , and is the linear source scheme noise variance see . 
 Proof : Consider the of Fig . and assume that the source scheme is linear . Since Assumption . , there exist that the resulting is internally stable and well one possibility is to to pick internally . For any such choice of , the open loop system with unity feedback . Our claim now immediately upon Corollary . in and the description for the noise in Lemma 
 Theorem . : Consider the of Fig . and suppose that Assumption . . If , , then there an based linear source scheme satisfying Assumption . such that the resulting is , se , and 
 Proof : Since , the problem in is feasible see Appendix A . Thus , there exist 
 F rendering the feedback system of Fig . internally stable and well , and , such that , in the scheme of Fig . , and for any , se and 
 Denote the above for , and by , and , respectively . Given Lemma . and inequality , can be assumed to be without loss of generality . 
 Consider the of Fig . and assume that the link u is given by an based linear source scheme with ,, ,, s , and set the initial of , and of the channel feedback delay to zero . The definition of , and , together with Lemma . , guarantee by construction that the that from the above choice of scheme is and that , in addition , the plant output e and the auxiliary have steady state satisfying 
 By Lemma . we also conclude that , for the above based linear source scheme , the average length of the channel input , for some suitable , 
 where we have used . Thus , inequality upon choosing a sufficiently small . 
 To complete the proof , we now show that the source scheme Assumption . . Except for the invertibility of the , the of guarantee that Assumption . note that , in our case , SE . Since is and its initial state is deterministic , knowledge of is equivalent to knowledge of . If one now proceeds as in the proof of Corollary . in , it that one can recover from upon knowledge of . Thus , upon knowledge of , one can recover from and the is invertible as . 
 Remark . : The proof of Theorem . is constructive . Indeed , it a way to build a source scheme that the resulting of Fig . , and se while an average data rate that is upper ¦ bounded by the right hand side of . 
 Fig . . source scheme . If the channel is noiseless and delay free , then and . 
 Theorem . that the lower bound on derived in Corollary . is tight up to per sample i . e ., tight up to approximately . per sample . Whilst the lower bound in Corollary . was derived by an information theoretic argument , the upper bound in on a specific source scheme that suitably chosen in conjunction with an . It from the discussion in that the gap between the derived upper and lower on from two : First , introduce a noise which is uniform and not this to the additional per sample . Second , the scheme works on a sample by sample basis and practical entropy are not perfectly efficient , Chapter this to an additional log per sample . We emphasize , however , that the above gap to a worst case gap and it can be significantly smaller in practice see Section . 
 A key aspect of our is that they are stated in of the solution to the constrained minimization problem in . As such , they highlight the role by in control , and thus complement , e .., where the connection between and other communication been . As already before , a way of a solution to the problem in will be in Section below . 
 Remark . : It is well known that , when causal source of arbitrary complexity are employed , it is possible to mean square stabilize an plant if and only if the corresponding average data than , where pi the unstable plant pole . On the other hand , it is straightforward use Theorem in , in conjunction with the proof of Theorem . , to show that any plant satisfying Assumption . can be in the sense of Definition . by an average data rate that 
 The above observation , for satisfying Assumption . , that it to use an based linear source scheme to achieve stability at which are at most per sample away from the absolute minimal average data rate compatible with stability see also 
 The on in Corollary . and Theorem . are of the minimal in . In this section , we show that the problem of finding is equivalent to an constrained optimal control problem previously in , , . 
 To proceed , we first note that a straightforward manipulation based on Fig . , for any and any render the feedback system of Figure 
 Ly are such that Ly see , and we have used that fact that , internally , is proper and P is assumed to be strictly proper , is stable , 
 We now define , for the feedback system of Figure , the auxiliary problem of finding 
 where the minimization is with respect to all , and auxiliary noise , that render the feedback system of Fig . internally stable and well . Given our , if the unstable , then the problem in is feasible if and only if , where the that is compatible with mean square stability in the feedback system of Fig . see , . stable , then the problem in is feasible if and only if . 
 Lemma . : Consider the of finding both and in and , respectively . Assume , in addition , that the such that P and P . 
 , then is a strictly decreasing function the inequality constraint in is active at the optimum . 
 If , then is a strictly decreasing function the inequality constraint in can be assumed to be active at the optimum without loss of generality . 
 optimum . Since , is always non negative for any feasible set of , and . Indeed , assume on the contrary that at the optimum . Given , this would imply that or at the optimum . Given our and the definition of , only is possible . However , is not compatible with internal stability when the plant is unstable . In the stable plant case , 
 note that . The latter equality is however unfeasible since , by assumption , 
 Given the above , if , and the plant is unstable or is stable with , then at the optimum . Hence , the performance constraint se in the definition of is equivalent to 
 at the optimum see and . Since is a function of , it that the optimal choice for is such that the inequality constraint is active at the optimum . 
 We now show that is strictly decreasing in . Our guarantee that and hence FLy . By in , and the fact that the optimal choice for equality in , the result immediately . 
 Consider the definition of in . By an argument similar the one used in Part above , it that our imply that one can assume , without loss of generality , that at the optimum see also , . Our now follow by proceeding as in Part above . 
 Lemma . , for almost all of interest , that the inequality in the optimization both and can be assumed to be active at the , without loss of generality . This fact is below to relate the to these . 
 Theorem . : Consider the optimization both and in and , respectively . Assume that ,, that the such that P and P , and that , stable , then . 
 Proof : We will only prove that . Our claim by a similar argument . Since , the problem of finding is feasible . Thus , for any , there exist proper and , and , that render the system of Figure internally stable and well , and guarantee that 
 where we have used Lemma . to write an equality in the constraint . Since the inequality in is valid for any , it that there exist a feasible point for the problem of finding and , in addition , that . The proof of our second claim would follow if we show that is impossible . Assume that is indeed true . Then , there exist decision such that and se . Again , we 
 If , then either the problem of finding is unfeasible unstable plant case or stable plant case . In the latter case , no information can be through the channel . On the other hand , if the plant is stable and and it is optimal to leave the plant in open loop . The above are clearly uninteresting and have thus been from the discussion in Lemma . . 
 use Lemma . to write an equality in the constraint . Thus , we conclude that 
 where the first equality from the fact that the constraint is active at the optimum when calculating . The above inequality the fact that , given our , Lemma . that is a strictly decreasing function of . The proof is thus . 
 Theorem . , for almost all of interest , that the problem of finding in is equivalent to that of finding in . The latter problem was shown to be equivalent to a convex problem in . For doing so , that the problem of finding is equivalent to the open loop causal rate distortion problem which was shown to be convex in . Shortly thereafter , the convexity of the constrained optimal control problem in was re derived independently in , where a formulation more amenable for numerical is . We will thus not delve into the on how to numerically find here , and refer the interested reader to Section . in for . 
 The previous subsection how a numerical characterization of can be . Here , we will briefly comment on the implementation of a source scheme which the desired level of performance , while an average data . In principle , such scheme can be designed as see proof of Theorem . : 
 • Use the procedure in and Theorem . to , and the auxiliary noise variance , 
 • Use these in the based linear source scheme of Fig . and set all initial to zero . Choose the quantization step as , an i . i .. dither signal uniformly distributed on , and independent of ,, and appropriate entropy coder and and , for instance , the algorithm . 
 an the availability of the dither at both the and the sides . Additionally , the entropy coder needs to generate a binary word for each input value according to the conditional probability of that input , given the current dither value . The above are impossible to meet exactly in practice . Indeed , the first one is tantamount to an additional perfect channel for being able to communicate the dither from the to the . The second one would require an uncountable number of , one for each dither value . 
 Leaving finite range and precision aside , the behavior of an can be in practice by synchronized uniformly distributed pseudo random dither , at both the and the from the same seed , and entropy and which work conditioned upon a uniformly version of the dither . By such an approach , all in the of Figure , except for the channel input and output , will have the same statistics as if an ideal was employed . For each possible dither value , one can build the corresponding conditional dictionary in by , for example , the algorithm . The conditional statistics of the for this purpose , can be by the corresponding stationary statistics which can be by simulation . 
 that more than dither only negligible in of rate reduction . The curve Measured entropy of output dither to an empirical estimate of the conditional entropy of the output , given the dither . 
 Our show that our upper bound is loose . This is consistent with the fact that our upper bound was derived by worst case . The gap between the measured rate with and our lower bound is about . per sample , which is smaller than the worst case gap per sample about . per sample . On the other hand the conditional entropy of the output , given the dither , is about . per sample above the lower bound . This that , in our , the . bit per sample gap is composed by about . per sample due to the inefficiency of the considered entropy , and by about . per sample due to the fact that the uniform and not noise . 
 . , converge rapidly as . Thus , whilst an average data rate arbitrarily close to the minimal rate 
 Our show that , as , a closed loop performance arbitrarily close to the best non performance arbitrarily high data . Interestingly , however , for this example , it to use less than per sample to achieve a performance that is essentially identical to the best non performance . It is also interesting to observe that our , and the measured average data 
 and that , is unit variance white noise . By the of upper and lower on for several of . . We also an actual linear source scheme for each considered value for . To that end , we the at the end of Section and where the dither is uniform and perfectly known at both of the , and where the entropy work conditioned upon a version of the dither . The are in Figure . In that figure we plot our upper and lower , and several other which report simulation . All simulation to as Measured in Fig . are over twenty long . In particular , Measured rate no to the average data rate in a case where an tuned entropy coder is employed which does not make use of the knowledge of the dither . Even in this case our upper bound to be rather loose . The curve Measured rate cond . dither to the rate when and entropy coder that works conditioned upon uniformly dither . As our show that the average data rate . suggest that the performance loss when forcing the average data rate to be low might be modest in some . 
 This paper studied control subject to average data rate . In particular , we have a characterization of the minimal feedback directed information rate I u average data rate to within . sample that a level of quadratic performance , over all controller . Our have been derived for that have one scalar control input , one scalar sensor output , and that are subject to stationary and initial . The digital feedback channel considered was assumed to be free of and . Where no besides causality are on the considered , the directed information rate between the output u a known lower bound to the mean operational data rate was shown to be only u are jointly 
 . This , together with the introduction of the notion of strong asymptotic wide sense , us to define a wide class of namely , for which a data rate lower bound in of the asymptotic power spectral u was derived . We then that , for a given stationary performance , the of this lower bound with that of the directed information rate when , channel and are by an channel surrounded by , 
 taking the over all causal which satisfy the same performance constraint . Such insight was then used as motivation for building a source scheme capable of which are less than per sample away from our derived lower bound , while satisfying the desired performance level constraint . Such are based upon entropy and constitute conceptually simple . A numerical example been include to illustrate our proposal . 
 Future work should focus on multiple input and multiple output plant and on ways of reducing the gap between the derived upper and lower on the minimal average data rate that a given performance level . 
 We would like to thank the AE and the anonymous for their careful as well as for their helpful , which to improve the technical presentation of this paper . 
 In this appendix we show sufficient for the optimization in , and to be feasible . We will make extensive use of the definition of in , the related in and , the regarding the feedback scheme of Fig . made on the paragraph preceding Theorem . , and the of and based linear source made in Section . 
 Consider the feedback system of Fig . , where , and satisfy Assumption . , and the such that u for some arbitrary . and all the involved random are , it from well known that 
 the set of all proper which render the the feedback system of Figure internally stable and well . Our that the above problem is feasible . 
 The fact that the problem of finding is feasible , that for every , , there K such that , in Figure , 
 F and L , where L is such that K . Since K , the above choice the feedback system of 
 Fig . internally stable and well for any additive noise variance . This that the resulting variance of , say , will be finite . It also that if in Fig . is zero mean with variance , then the of e and of will increase to , and 
 , respectively , for some finite which depend only upon K . As a consequence , for every , there K such that in Fig . and for the above choice of ,, 
 by and as zero mean with variance . It immediately that the above choice of is also such that , in Figure , 
 The latter inequality that the problem of finding in is feasible for any indeed , feasible while yielding all in the system jointly . Now , from inequality and the concavity of log , it also from that the problem of finding in is feasible for any . 
 We end this section by showing that the problem of finding in is also feasible when . To that end , it to consider an based linear source scheme to u in Fig . , with 
 the of the latter choice of see preceding paragraph , our claim by proceeding as in the proof of Theorem . to show that the above defined based linear source scheme Assumption . , the of Figure , and se at a finite average data rate . 
 The following and are standard and , unless otherwise stated , can be found in . We assume all random to have well defined joint probability density . The of , is ,. to the conditional of , given . Ex mean with respect to the distribution of . 
 The differential entropy defined via Ex . The conditional differential entropy of , given , is defined via Ex , . The mutual information between two defined via I ; Ex , log ,. The conditional mutual information , given , is defined via I ; I ,; I ;. The following are of the above : 
 Property , where can be taken to be a deterministic constant , in which case x x . 
 Lemma . : Assume that , are jointly second order random , is , arbitrarily distributed . 
 Then , I ; ; , where is such that , are jointly and have the same first and second order cross as ,. 
 Lemma . : Let ,, be arbitrarily distributed random satisfying the chain 
 Let be jointly random with the same first and second order statistics as ,,. In addition , suppose that E Ly , for some appropriate . Then the chain 
 Proof : The statistics between and ,, imply that E E . The E Ly that E E Ly . With this we obtain that 
 where the last equality is due to . But for jointly , E E if and only if , the proof . 
 Lemma . : Let the random , be such that , for all N and i , 
 Proof : The , are as L , u g , g , L , L , d Mu 
 This that , i si , Li , di for some deterministic i , from where since 
 Lemma . is a version of Lemma . in , since the statement of the latter the existence of a linear that the estimation error Ly is independent of , while Lemma . only E for all . 
 L , , L , d ,..., Li , are sub of the sequence Li , di . 
 Theorem . : Let the random , correspond to those in the feedback system shown in Fig . . 
 Let be jointly with the same second order as Then , for N , 
 uG i , i uG i di , i a 
 Proof : In the scheme of Fig . , the plant causally its di to its output . This that that Li , di for some linear 
 Li , di and , are jointly , and thus E , Li , di , di for some sequence of linear . Hence , we can directly apply Lemma . and obtain from that 
 , which is equivalent to a . The inequality a is due to the chain rule and the of mutual information which also , and since . Starting 
 inequality from the chain rule and the non negativity of mutual information . This the proof . 
 Lemma . : Consider the generic feedback system of Figure , where S is an arbitrary hence possibly non linear and time causal dynamic system with initial state x , o and disturbance d , such that for some possibly non linear and time deterministic S ,, and S is an arbitrary causal dynamic system with disturbance d and initial state x , o , such that for some possibly non linear and 
 time deterministic S ,. If x , o , d are jointly independent of x , o , d , then 
 Lemma . to a version of Theorem . in . Indeed , the latter result use of additional on system S , does not take side information into account , and only that the left hand side of is lower 
 Proof : The definition of an process that and are of asymptotically equivalent matrices . This , together with the fact that , and that log 
 is continuous for all , one to apply , Corollary . to obtain 
 Since the matrices are positive definite with each row being absolutely summable for all , it from the fundamental eigenvalue distribution theorem of see , e .., , Corollary . or Theorem . that ¨ 
  
 ﻿ In this paper we propose a new approach to robust optimal experiment design . The key departure from work is that we specifically account for the fact that , prior to the experiment , we possess only partial knowledge of the system . We also give a detailed analysis of the solution for a simple case and propose a concave optimization algorithm that can be applied more generally . 
 It is well known that the choice of experimental a strong influence on the accuracy of from system identification . This substantial research on experiment design over almost a century . Early appear in the statistics literature , ; Cox , ; , ; and , ; and , ; , ; Whittle , ; Wynn , . This work was later to the problem of identification of dynamic , ; , ; and , ;.. and , ;.. and , ; and , ; , . Some of the later work is in and , ; , . 
 Our focus here will be on experiment design for dynamic . Early work on this problem predominately on frequency domain . More recently , there been substantial effort devoted to the inter relationship between identification and control together with the associated issue of input signal design and , ; , . However , a major drawback of all the above work is that , generically , the optimal test signal for dynamic system identification is a function of the unknown system i . e . it on the very thing that the experiment is at finding . 
 Indeed , , : It should be noted that , as usual in experiment design , in order to compute the optimal design the true system to be known . that are robust with respect to uncertainty about the system is a wide open research 
 The goal of the current paper is to propose a methodology for this problem . In particular , we formulate a robust optimal experiment design criterion . We also demonstrate the use of this criterion for a simple case . 
 The layout of the remainder of the paper is as : In Section we give a general formulation of the robust optimal experiment design problem . In Section , we focus on a simple one parameter problem so as to give insight into the problem . In Section we convert the problem to an approximate matrix formulation and discuss asymptotic behavior when the prior knowledge is diffuse . In Section we describe an algorithm for the robust optimal experiment and give a numerical example . In Section we describe an extension to parameter . Finally , in Section we draw . 
 Our focus in the current paper is on how to design the experimental so that the information from a particular experiment is in some specific sense . 
 To motivate our approach we consider a single input single output linear discrete time system of the form : 
 G ut G where G , G are rational transfer in the forward shift operator , G and where 
 is white noise of variance . We let where the 
 We recall that the log likelihood function for , is given by 
 Fisher information matrix is by taking the following expectation and , 
 We assume an open loop experiment so that and ut are uncorrelated . We also assume that G , G no common . Taking , as in , we have be partitioned as 
 where M is the part of the information matrix related to and M is independent of the input . Then 
 Notice that M on the full parameter vector . large , it is more convenient to work with the scaled average information matrix for the parameter , i . e . 
 It is also possible to do a parallel development and , for continuous time . In the latter case , is by 
 where G and G are continuous time transfer assumed independently and fu is the continuous time input spectral density . 
 a matrix , we will need a scalar measure the purpose of experiment design . In the nominal case in the literature i . e . when is 
 assumed known , several of the size been . are 
 where , is a frequency dependent vector related to the gap and , . 
 Thus nominal experiment design is at choosing fu to maximize a function of the type shown in , . Note , however , that the optimal input spectrum , inter on the unknown parameter vector . To address this paradox , we propose an alternative robust optimal experiment design procedure . 
 We assume that we do not have complete a prior knowledge of the true parameter value . Instead , we assume that the can take any value in a compact set . We propose that fu be chosen as : 
 any suitable scalar measure of . Note that we also depend explicitly on . This is standard in nominal experiment design , see for example . Also , we can use the explicit dependence on to 
 associate different weighting to , fu depending on the nature of the prior knowledge regarding . 
 We also need to constrain the allowable set of input . A typical constraint used in experiment design is that the input energy is constrained i . e . we define 
 We note that the are independent it only as a scaling factor in . For clarity in the following discussion we assume white noise and hence only refer to the parameter in the sequel . 
 To give insight into the robust optimal experiment design problem , we consider a simple continuous time problem where G and 
 Nominal experiment design that an initial estimate , is available . Based on this information , the function fu is chosen so as to maximize some scalar function of subject to a constraint on the output or the input power . 
 One interesting observation is that and define a convex combination of the set of all single frequency matrices . This to several use 
 , e .. any value of , fu for fu can be by a finite number of . Also , in the scalar parameter case of and it can actually be shown that we need only use a single frequency input for optimal experiment design and , , namely , fu . Moreover , by differentiation it is readily seen that the optimal input frequency is 
 This is an intuitively pleasing result , i . e . one the test signal at the nominal break point . However , equation the fundamental difficulty in nominal experiment design , namely the optimal experiment on the very thing that the experiment is at . 
 Fig . . M , fu as a function of for nominal input , noise solid . 
 To gauge how important the dependence on is , we note our example at the rate of per decade as a function of both and . Hence , given the prior estimate of the . Also , say that the true parameter in say we choose for the input signal 
 proximately th of the nominal value This to suggest that nominal experiment design is limited to those where a good prior estimate is available . A 
 The reason for multiplying by is that is a variance measure and thus relative 
 We next turn to the robust experiment design in Section . For the scalar parameter problem 
 In subsequent , we will give further into the above design problem . 
 Lemma . Consider the problem stated in equation , the optimal input all its energy inside . Namely , 
 Here we develop an algorithm for the robust experiment design problem , as in the last section , where we approximate the integral in equation by a sum . Specifically , Lemma , we 
 We can now state the following discrete alternative to the optimization problem in equation : 
 We next show that if there a that all equal , then E is optimal . 
 negative we must conclude from and that Since by definition , all the of E are nonE which the proof . 
 With this specific choice we have the following result for our original problem , 
 Lemma . The robust optimal test signal for diffuse prior information spectrum approximately given by 
 that for the matrix A in , apart from the first and last few , the row sum is very nearly constant . 
 Namely , the in Lemma are approximately satisfied which the proof . 
 Remark . The immediate consequence of Lemma is that band limited noise is an approximation to the robust optimal input for our example see Figure . 
 We show that the optimization problem can be converted into a standard linear problem . Let us denote 
 then we can readily show that is equivalent to the following optimization problem : 
 We consider the scalar parameter problem in Section where we assume , 
 i A nominal input of frequency Note that this is the optimal input if the initial estimate of the 
 Band limited ‘ f noise input , . , rad sec . The robust optimal input by . 
 We see from Table that noise is approximately an order of magnitude better than a white noise input in of the cost function . Furthermore , going to the true optimum a further improvement . The optimal input energy is shown in Figure and Fig 
 ure the corresponding of M , fu as a function of . It is interesting to note from Figure 
 that M , fu is an almost constant function of . This should be with the result in Lemma . The latter Lemma that if the input that independent of is feasible , then 
 it is optimal . Here , the input which M , fu constant is not feasible but we see from Figure that the optimal of are almost constant . 
 expression for , given in and . We can again convert this into an approximate discrete form as was done in Section . We write 
 as an approximation to the integral in . The the element of the parameter set , the frequency and Em the input energy at the frequency . 
 In this case , we see that is a matrix for each discrete parameter value . Hence , as in Section , we need a measure of the size of . Say we choose ,, then the robust optimal design becomes 
 Obviously , there are many for the scalar function . One possible choice is 
 This cost function is by the following observation : the scalar function E 
 which that E is a concave function of E . Hence , becomes a standard concave maximization . This latter aspect is the subject of the journal version of this paper . 
 This paper a robust optimal experiment design procedure . We have for a simple case , that a near optimal input is band limited noise . We have also an algorithm to design robust in more general and have showing the gains from the use of the algorithm with either the nominal optimal experiment , band limited white noise or band limited noise . 
  
 ﻿ This paper presents novel results on the joint problem of sampling, reconstruction and quantization of analog signals. Existing literature on this topic deals exclusively with band-limited signals in sampled form. Our key departure from earlier results is that we deal with continuous time reconstruction of not necessarily band-limited signals. Our approach utilizes concepts and tools from optimal sampled-data and receding horizon control theory. The key conclusion from the work presented here is that, in the case under study, the optimal quantizer design problem can be partitioned into two sub-problems, namely (i) the design of an optimal analog pre-filter followed by sampling and (ii) an optimal quantizer, which works directly on the pre-sampled signals. Simulation results are presented which illustrate the performance of the optimal A-D converter designed via these principles.
 Key Words: Sampling, quantization, frames, signal processing, sampled-data control.
  
 INTRODUCTION
 In many applications, one needs to convert analog, continuous time signals into quantized discrete time signals. This leads to an important set of questions regarding the best way to represent a signal by a sequence of sampled and quantized values, such that the information loss inherent in the sampling and quantization process is minimized in some sense. In the present work, we are interested in how to quantize a possible non band-limited signal to obtain the lowest possible reconstruction distortion.
 We will show that, for a given sampling rate and reconstruction filters, minimization of reconstruction error, in an L2 sense, can be converted into a discrete time problem. It turns out that if an appropriate pre-filter is used, then all the information required to find the optimal quantized sequence can always be extracted from discrete time samples of its output, even if the continuous time input signal is not band-limited.
 Solving the optimal quantization problem amounts to finding the solution of a combinatorial optimization programme, which is in general computationally intractable. Our proposal is to convert the optimal quantization problem into a sampled-data moving horizon optimization problem with quantized decision variables. The proposed method gives excellent results and incurs only limited computational effort. It generalizes our previous work reported in [1][2][3][4] by concentrating on sampled-data signals rather than merely on discrete-time sequences.
 Background to the work described here arises from distinct streams. The first of these is associated with the problem of sampling in the absence of quantization [5][6].
  
 IEEE Catalog Number: 06EX1310
 The second related field of research is concerned with quantization of signals where the sampling strategy has been pre ordained [7][1][8][9]. The third stream of prior work arises in the area of sampled data control theory. Here, the emphasis has typically been on regulation (zero reference) problems with unconstrained decision variables [10] [11]. In the present work we extend these concepts to account for non zero reference signals and quantized decision variables.
 Our approach differs from the work described above by virtue of the fact that we design the joint optimal sampler and quantizer using sampled data quantized moving horizon optimization. This leads to significant performance gains, compared with alternative approaches which do not take account of the interaction between sampling and quantization.
 The remainder of this work is organized as follows: In Section II we present the continuous time AD-conversion problem and how it can be translated into discrete-time. Section III introduces the continuous time receding horizon quantizer. Simulation studies are included in Section IV. Section V draws conclusions.
 PROBLEM FORMULATION
 The general form of the systems under study is illustrated in Fig. 1.
  
 Figure 1: Block diagram of the general sampler-quantizerreconstruction system.
 The sampler-quantizer  converts the continuous time signal a(·) into a sequence
 u = {u[k]}k?Z , u[k] ? U, ?k ? Z
 where U is the finite and given set of scalars	(1)
 U = {s1,...,snU}
 corresponding to the available quantization levels.	(2)
 The sampling interval is constant and equal to t seconds. Thus, quantized samples are generated at a rate of 1/t samples per second.
 In Fig 1, the reconstruction filter F converts the discrete time sequence u into a continuous time signal. For example, in case of zero-order hold reconstruction, the impulse response of F would be f(t) = µ(t)+µ(t-t). In the classical framework of perfectly band-limited reconstruction, F would be an ideal low-pass filter with cutoff frequency 1/2t [12]. On the other hand, in most practical applications, zero-order hold or some other form of short impulse response filter (sometimes non-causal) is generally used for reconstruction.
 The filter H is the error frequency weighting filter (see, e.g., [4]). It allows one to represent the different impact of the error at different frequencies for a particular application. For example, if the system is employed for audio signals, then H could be designed to model the psychoacoustical response of human hearing [13]. We are interested in designing a quantizer which minimizes the L2 norm of the frequency weighed error  (see Fig. 1), i.e., minimizes the cost function
 	 	(3)
 where  denotes the standard L2 norm over the real line, i.e.
 	 	(4)
 For the analysis below, it is more convenient to rearrange the system of Fig. 1 to the equivalent form shown in Fig. 2. In this figure, the continuous time filter ? is characterized by the transfer function
 	 	(5)
 and its associated impulse response ?(·).
  
 Figure 2: Equivalent block diagram
 In order to represent the continuous time filtering performed by ?, we define the continuous time version of u as
 	 	(6)
 The output of ?, namely w(·), is given by
  	(7) Substituting (6) into (7) one obtains
  
 If we denote the impulse response of the frequency weighting filter H by h(·) ? L2, then its output satisfies
 	 	(9)
 As a consequence, the frequency weighted error  can be written as
  
 (10) Thus, the quest for optimal sampling and quantization can be stated as the optimization problem of finding the sequence u as in (1) that minimizes the L2 norm of the reconstruction error, i.e.:
 	u  arg	 min
  L2 2.1 Reformulation in Discrete Time
 It will now be shown that the L2 (continuous time) optimization problem in (11) is equivalent to an	2 (discrete time) optimization problem, where the weighting values depend on the signal inter-sample behaviour. This is established in the following lemma, originally introduced without a formal proof in [2].
 Lemma 1frame boundsbe a frame forLet the sequence of functions0 span= R{=?P <(· - kt8), i.e.}k?Z = {W ??(· -Lkt2, with)}k?Z
 	 	(12)
 for all w ? W. Let w(·) ? L2 be defined as in (8) for the sequence of scalars . Let the signal a˜ ? L2, and define F[j,k] and Y [k], k,n ? Z via
 	Y [j]   	(13)
 F[j,k] (14)
 (15)
 where   denotes the standard inner product in L2.
 Then:
  
 (16)
 Proof 1 Substituting (10) in (3) we can write
  
  
 We note that the first term in the last line of (17) is well defined since a˜ ? L2 as required by the Lemma. The second term is finite by virtue of the Cauchy-Schwartz inequality   and the fact that w ? W ? L2,
 which in turn implies the third integral is also bounded. Since inner products are by definition linear and a˜ and w are real signals, one obtains by substituting (8) into (17) that
  
 which is equivalent to (16).
 Remark 1 If we make the change of variables
 	 	(19)
 where u is an un-quantized sequence that yields the global minimum of V in (16), namely V , then
 	 	(20)
 Remark 2 From lemma 1 and remark 1, it is clear that to minimize (3) the only information needed is an optimal un-quantized sequence u (or, alternatively, the sequence {Y [k]}k?Z) and the coefficients F[j,k]. The latter corresponds to samples of the autocorrelation function of ?(·), which can be determined off-line after choosing reconstruction and error weighting filters and then incorporated to the quantization algorithm. On the other hand, {Y [k]}k?Z can be obtained from u by differentiating (16) with respect to u and equating to zero, which leads to
 	 	(21)
 2.2	Pre-filter Requirements for Optimal Quantization
 In practice, any quantization algorithm has to work with discrete-time values. Remarks 1 and 2 arise the need to determine whether a quantization algorithm can elaborate or obtain  from samples of the input signals.
 Consider, first, the determination of the series of coefficients {Y [k]}k?Z. From definition (13), we have
  
  
 There might exist more than one optimal sequence if the reconstruction stage is redundant.
 From the last line of (22), it is clear that the series of coefficientsnal through a filter with frequency response{Y [k]}k?Z can be obtained by passing the input sig-GY (j?) given
 by
  
 and then taking the samples every t seconds. i.e., if we denote the impulse response of GY by gY (·), then Y [j] = (a * gY )(jt), ?j ? Z,
 Let us next consider the determination of u, the sequence of samples which minimizes reconstruction error in the absence of quantization. It is known from sampling theory [14][15] that, for any input signal, u can be obtained by sampling the output of a pre-filter GS(j?) matched to the reconstruction filter. From these results, for the system depicted in Fig. 2, the ideal matched pre-filter for a given reconstruction filter ?(j?) is given by
  
 where
 (25)
 is the discrete time Fourier transform of the sampled au-
 [5]. Notice that H(j?) = ?	w	? e
 The above results suggest that all the necessary information about the input signal for optimal quantization to be feasible can be obtained from samples of the filtered input signal, and that the required pre-filter is not unique. We will provide next necessary and sufficient conditions for a pre-filter to yield samples that allow for optimal quantization.
 Consider the discrete Fourier transform of u, and let fˆ(ej?t) and gˆ(j?) denote the discrete and continuous Fourier transforms of any , respectively.
 Since u is a sequence of samples of a filtered by GS(j?), we have
   of the input signal
 pre-filtered by another filter GX(j?). The discrete Fourier transform of such sequence of samples would be
  
 Recovery of  can be achieved in the discrete-time domain by applying a discrete-time filter G(ej?t) to the sequence v, such that
  
 From (26) and (28) it can be seen that a sufficient and necessary condition is the existence of a periodic transfer function G(ej?t), such that
  
 for the quantizer stage to be able to determine u from the samples v (and allow for optimal quantization). As a particular case, if A? = 1, then, from (23) and (24),
 G  (	) = G (	) ?	R	Y [ ] =	[ ] ? Z.
 The latter equality can also be obtained from (21) by noting that A? = 1 if and only if F[j,k] = dj,k,?j,k ? Z.
 Of course the quantizer stage would need to implement the correction filter G(ej?t) based upon knowledge of GS(j?) and GX(j?), according to (29). Two important special cases are to be highlighted:
 If a has no frequency components beyond p/t [rad/s], then any pre-filter GX satisfying
  
 for some constants 0 < C1 = C2 < 8 would make optimal quantization possible from the samples v. This result is not surprising, since by Shannon’s sampling theorem, the samples of a band-limited signal contain all the information about the complete signal [12].
 If ? is band-limited to a frequency a = p/t but a has energy at frequencies greater than a, then any prefilter GX band-limited exactly to a satisfying
  
 for some constants 0 < K1 = K2 < 8 would have a feasible correction filter that makes optimal quantization possible from the samples v.
 The conclusion from the above cases is that quantization for optimal reconstruction of an input signal not bandlimited to p/t is possible, but demands either the use of an appropriate pre-filter satisfying (29) to get the samples from, or, alternatively, the quantizer needs to “know” the signal between sampling instants.
 THE SAMPLED-DATA RECEDING HORIZON QUANTIZER
 For the general case, minimization of (16) would require the evaluation of (16) for every possible sequence
 the optimization programme becomes computationally in-{u[k]}k?Z , u[k] ? U. For sufficiently long sequences, tractable. To overcome this problem, we propose to use concepts from the receding horizon control framework [16] and optimize over a short receding horizon of samples. A quantizer based on this idea has been recently proposed by the current authors in [3][4] for an all discrete-time system, achieving near optimal performance with rather short horizon lengths [17]. In what follows we will extend this idea to the sampled data case, and show, via simulations, that significant distortion reduction is obtained when converting non band-limited signals.
 3.1	Optimal Quantization Over a Finite Horizon
 Consider the system at . Instead of attempting to optimize the cost over t ? R, we will aim to minimize the cost within a finite time interval  
 N)t), where M,N ? Z+ are design parameters. We will only concentrate on the optimal sequence of quantized coefficients to be generated for the interval  ,
 defined as
 	 	(33)
 The number M, therefore, accounts for the non-causality of ?.
 For the purpose of including in the horizon the effect of past errors, it is convenient to describe the continuous time filter ? by its state space representation
 x?(t)	= Ax(t) + Buc(t)	(34) w(t)	= Cx(t + ?)	(35)
 where A ? Rn×n, B ? Rn×1, C ? R1×n and uc(·) is as defined in (6). In (35), ? = 0 accounts for a possible non-causal ?. If ? is causal, then ? = 0.
 The cost function to minimize is
 	 	(36)
 If x  is known, then, from (34) and (35), w(t) can be determined by
  CeA 
 for .
 Notice that the first term on the right hand side of (37) captures the effect of the choice of u within the horizon. This term corresponds to the forced response of ? to the input u, which can be conveniently represented as
 	 	(38)
 On the other hand, the second term on the right hand side of (37) represents the natural response of ? when the initial state is x . We define the difference between this initial state response and the filtered input signal a˜(t) within the horizon as the target function , for the horizon at t = 	t:
  CeA  By using (38) and (39) , the cost (36) can be expressed as the L2 norm of the difference between the zero-initial-state response  and the target function :
  	(40) Substituting (38) into (40) one obtains
  
 Since ?(·) ? L2, one can exchange the order of sum and integration in (41) and rewrite it in matrix form as
  
 where the vector   and the symmetric, positive definite matrix FN ? RN×N are defined element-wise as
 FN[j,k]   
 	  	(44)
 -
 	j,k = 0,1,...N - 1	(45)
 3.2	The Sampled-Data Receding Horizon Quantizer
 The expressions derived above for the cost function over a finite horizon allow us to introduce the sampled-data receding horizon quantizer. The algorithm finds, at a given instant	t, the vector of quantized coefficients u that minimizes the total filtered reconstruction error from (	-M)t to  defined in (36). Then, the first element of u is sent to the output of the quantizer. The horizon is then shifted forward by t, and iteration  begins. The proposed algorithm, beginning at instant	t, can be formalized as follows:
 Step 1.- Calculate the matrix FN in (43)
 Step 2.- Calculate Y
 Step 3.- Find the optimizer  by minimizing (42)
 Step 4.- Output , the first element of  Step 5.- Increment  and go to Step 2.
 The sequence   of step 4 forms the output of the sampled-data receding horizon sampler quantizer. If the input signal is not band-limited to p/t, the algorithm reduces filtered aliasing and frequency weighted quantization noise. For that purpose, it does simultaneous adaptive filtering on the input signal and adaptive noise shaping of the quantization noise, thus respecting the interaction between both phenomena.
 It is interesting to note that, as the horizon is made larger, the output of the sampler-quantizer defined above approaches the optimal feasible output sequence possible defined in (11).
 SIMULATION STUDY
 We will first show an example comparing the performance of the proposed sampled-data receding horizon quantizer (SDRHQ) against the so called all-discrete-time (DTRHQ) receding horizon quantizer introduced in [3] in the following situation:
 The input signal, a, is an audio signal that has frequency components up to 22 [kHz]. Its frequency energy content is shown in Fig. 3.
  
 Figure 3: Spectral composition of the input signal a(·). • The sampling frequency is half the required to avoid aliasing i.e., 1/t = 11 [kHz].
 The filter H performs zero-order hold reconstruction,
 i.e., it has impulse response f(t) = µ(t) + µ(t - t). • The frequency error weighting filter corresponds to the third order model for the psycho-acoustical response of the human ear [13]. Its frequency response is shown in Fig.4
  
 Figure 4: Frequency response of filter H.
 • No pre-filtering is used, i.e., GX(j?) = 1.
 In the simulation, the DTRHQ has full knowledge of the filters H and F, and utilizes the matrix FN defined in (43). However, it operates based on direct samples of the input signal. Thus, since the implicit pre-filter is a unity gain, (29) predicts that the all-discrete-time quantizer will be unable to determine the target function to be approximated by the reconstruction stage. On the other hand, the SDRHQ utilizes the same matrix FN but has access to the intersample behaviour of the input signal.
 Fig. 5 shows the normalized reconstruction error from the outputs of both quantizers, for several horizon lengths. It
  
 Figure 5: Reconstruction error (normalized to DTRHQ distortion for N = 1 ) from the outputs of all-discretetime quantizer (DTRHQ) and sampled-data quantizer (SDRHQ), for N = 1 to 3, for a non band-limited input.
 can clearly be appreciated how, in this case, the sampleddata converter proposed in the present work outperforms the all-discrete-time converter of [3]. It can also be seen that the distortion exhibits a small decrease with the increase of the horizon length N. This suggests that, in this example, the main contributor to the distortion is aliasing noise, for both converters. Notice that the SDRHQ operated without an anti-aliasing filter. This, together with its much lower distortion in comparison with that of the DTRHQ, suggests that the sampled-data receding horizon optimization algorithm accomplishes a form of pre-filtering of the input signal that effectively reduces aliasing.
 CONCLUSIONS
 This paper has shown how receding horizon sampled-data control methods can be utilized to design optimal AD Converters. A key departure from earlier results in this area is that we optimize a filtered version of the continuous time reconstruction error for not necessarily band limited input signals. Inter alia, we show that the optimal design problem can be decomposed into two subproblems, namely the design of an optimal analogue pre-filter together with an optimal quantizer . We showed that the latter is feasible if only samples of the signals are available. The efficacy of the proposed method has been illustrated by an example using an audio signal sampled below its Nyquist rate.
 ﻿ The characterization of ` compressible random is and extended to the case of stationary and ergodic . The main result of this work a check necessary and sufficient condition for a stationary and ergodic sequence to be ` compressible in the sense by Amini , and . Furthermore , for non ` compressible random , we provide a closed form expression for the best term relative approximation error given a rate of as the block length to infinity . 
 of compressibility for a stochastic process , meaning that every realization can be well in some sense by its best term sparse version , been a topic of recent interest , , . compressibility for random can play an important role in regression , reconstruction for instance in the classical compressed analysis and reconstruction setting , Th . , inference and decision making . One important case is such a compressibility notion for i . i .. where the probability is with a density function . In this context , every realization of the process is clearly non sparse almost surely , and conventional ways of compressibility for finite dimensional , based on the power law decay of the best term approximation error or that belong to the weak ` ball , are not applicable either , as shown in , . 
 by this , Amini al . and al . have recently new for compressible random . These are not based on the typical absolute approximation error decay pattern of the , but on a relative ` best term approximation error behavior . In particular , Amini al . formally define the concept of ` compressible process in Section below . This new definition a meaningful way of i . i .. random or in of the probability that almost all the ` relative energy of the process is concentrated in an arbitrarily small sub dimension of the innovation domain , as the block length to infinity . Under this umbrella , they provide two important the theory of order statistics . On the one hand , , Theorem that a concrete family of i . i .. is ` compressible the generalized , ‘ t and log logistic , while on the other side , 
 , Theorem that with exponentially such as Generalized are not ` compressible . Therefore , it is interesting to ask about the compressibility of i . i . not considered in this analysis . In this direction , we highlight the work of al . , which under an alternative notion of relative ` compressibility almost sure convergence instead of convergence in measure criterion adopted in and a different analysis setting fixed rate instead of the variable rate used in , an exact dichotomy between compressible and non compressible i . i . Therefore , it is an interesting direction to connect Amini al . ` with the more refined almost sure convergence analysis of the ` best term relative approximation error in , Prop . , with the idea of the analysis of and . 
 To address this question , as well as to extend the analysis to more general random , this work precisely all ` compressible random , in the sense of Amini al . , within the family of stationary and ergodic . Our main result Theorem a necessary and sufficient condition for an ergodic process to be ` compressible , for any arbitrary . Furthermore , for the case of non ` compressible ergodic , we provide a closed form expression for an achievable rate ` approximation error function . The key element in the proof is the application of the ergodic theorem and the derivation of intermediate almost sure convergence Lemma and in Section that match the approximation result by al . , Prop . for the i . i .. case . A corollary of Theorem a necessary and sufficient condition to categorize i . i .. random in of ` , which the analysis in and . 
 For a finite dimensional vector x ,.., in , let , ,.. denote the ordered vector such that 
 denote the ` norm of the best term approximation of , where by definition ` In addition , 
 the best term ` approximation error of , in the sense that if is the collection of sparse , then , is the solution of minx `. For the analysis of infinite , Amini al . and al . the following relative best term ` distortion indicator : 
 From this , Amini al . a notion of critical dimension for a finite length signal and a notion of ` for infinite . 
 Definition : For and , , let us define , min ,.., :, . 
 Let X ,.... be a random sequence with in , and by its consistent family of finite dimensional : , where X ,.., and the space of probability for the measurable space , . As a short hand , we denote by 
 : the process distribution of . Let us define the measurable set An , 
 where the last equality is by . Then in analogy with Definition , Amini al . the following : 
 is the critical number of with which the set typical with respect to . With this , the process and , respectively is said to be ` compressible , if , , , 
 We say that the rate distortion pair , is ` achievable for with probability , if there a sequence of positive such that and 
 Definition : The rate distortion approximation function of , with probability is given by : 
 In the next section , we will study the class of stationary and ergodic , where the best term approximation measured in of the function in will be in closed form . Furthermore , it will be shown for this class of random that 
 Let be a stationary and ergodic sequence process with distribution : , where we denote by 
 P its shift invariant distribution . For simplicity , we assume that where de measure 
 . Then is with a probability density function and . 
 For a ,, we say that a measurable function : , , is integrable with respect 
 THEOREM : Let be a stationary and ergodic process with marginal shift invariant distribution such that , and let us consider an arbitrary . Then we have the following dichotomy : 
 If L : then is not ` compressible . Furthermore , if we introduce the induced probability measure in , by : 
 Theorem a necessary and sufficient condition for a stationary and ergodic process to be ` compressible in the sense in Definition . 
 In the case of non ` compressible , i . e ., when L , Theorem , what we call , the achievable rate distortion region for the process , given by the set of critical rate distortion : 
 This region solely on the shift invariant measure and its induced measure in . 
 In both i and , the critical rate for a stationary and ergodic is independent of . The justification is that asymptotically to infinity , the characterization of to compute on that belong to the tail field of the process , which is known to be trivial i . e ., their have zero or one probability for the case of ergodic . Therefore , we obtain almost sure convergence that make irrelevant the role of in the characterization of see Section 
 A natural order among stationary and ergodic process can established from Theorem . 
 Proposition : If is ` compressible for some , then is ` compressible for all . Proof : If L , then L for all . 
 Proposition : If is not ` compressible for then is not ` compressible for all . Proof : If L , then L for all . 
 For the emblematic i . i .. scenario , we want to highlight in more detail the by Amini al . related to ` compressibility in the sense of . , Theorem that if is such that for some , EX e then the i . i .. process is not ` compressible . On the other hand , , Theorem that if to the domain of attraction of a stable distribution , Chap . . with a , , then the process is ` compressible . First for , Theorem a refined result , revealing a indeed , the complete family of that are not ` compressible . In fact , in addition to that go to zero exponentially and consequently f L Gamma , heavy tail whose density are lower dominated by a power law decay of the form with for , student ‘ t freedom , are not ` compressible either . On the other hand , concerning , Th . , it is simple to verify that any that is in the domain of attraction of an a stable law with a that EX and consequently , part i of Theorem this family of ` compressible i . i . 
 Corollary : Let be a stationary and ergodic process . If where EX e , then is not ` compressible for any . 
 Corollary : Let be an ergodic process with invariant distribution and density , . If as for some , then . is compressible , if and only if ,. 
 For the proof of Theorem , we derive almost sure convergence see Lemma and in Section . In the case when L : if is such that 
 for some , it that , a ... In the case when , then limn , , a .. These are consistent and extend the result by al . , Prop . , which for the i . i .. case the same convergence limit for the object Their proof was based on the ‘ lemma of order statistics see in , Th . , while ours is based on the application of the ergodic theorem . 
 then the latter also for all and for all , . Likewise , if for some , for some , and for all , the pair , is ` achievable for with probability , then all , with , and , are ` achievable for probability . 
 Proof : Considering as a short hand , by definition in , where from it that . 
 is lower dominated by , if there x and C such that for that then C . 
 Let us first consider the case when L . For the rest , it is important to note that given that , then for all , there such that , and for all 
 d , there such that . For X ,.., , we can define : 
 The second almost sure convergence is from the assumption that L . Then , we can state the following : 
 In order to prove , let us fix , . Then there , such that and from Lemma if is such that , then limn , , a ... 
 Let us consider an arbitrary such that , then again from Lemma , if a sequence is such that , then limn , 
 , a ... Consequently ,, almost surely to a distortion strictly less than , and then for all : 
 Hence from the definition of in , we have that eventually in , which that 
 In general , the condition on for the rate i . e ., , such that and the distortion , 
 The first inequality from Proposition and the last equality from the fact that the function is continuous as . 
 To derive a lower bound , let us consider an arbitrary , . We know that there such that . Again from Lemma , for all such that 
 On the other hand , from , we have that it is necessary that . This last inequality and are valid for any , , then and 
 In other , that limn , a .. Furthermore , we have the following : 
 eventually in . From this , for all , which the result from Proposition . 
 The work of .. Silva is by of Grant and the Basal Grant . The work of .. is by Grant , and internal research grant 
 such that for some . Let us take an arbitrary At . From the zero rate assumption on and the definition of At , such that , which that Therefore considering that , 
 Doing the same process for the collection , we have that , 
 Then from the sigma additivity , which the result in . 
 with this result , for an arbitrary , let us consider such that limn . We know that there to such that , and for this to we consider and as defined in . Then for any it that 
 Furthermore , from definition of the ordered sequence , it is simple to verify that see . : 
 if we consider , from , and the fact that , is o in 
 the last equality in from definition of in . Finally from and , 
 let be such that . Let us consider an where and consequently and are mutually absolutely continuous . Again for this , we use the in , where for all , it that such that . 
 The first equality from the continuity of the function with respect to . Then from the fact that , we have that , , , a .. Finally , proving that , a .. an equivalent symmetric argument and we omit it . 
 Proof : For let us consider , and , such that and such that . Considering the and 
 that At from and . Let us fix an arbitrary At . Considering that , then eventually in , and therefore eventually , which 
 from definition of that limn , . The fact the event surely the result . 
  
 ﻿We consider a networked LTI system subject to an average data-rate constraint in the feedback path. We provide upper bounds to the minimal source coding rate required to achieve mean square stability and a desired level of performance. In the quadratic Gaussian case, an almost complete rate-distortion characterization is presented. 
 This paper focuses on the interplay between average datarate constraints (in bits per sample) and stationary performance for a networked control system comprising a noisy LTI plant and an average data-rate constraint in the feedback path. In such a setup, the results of [8] guarantee that it is possible to find causal encoders and decoders such that the resulting closed loop system is mean square stable, if and only if the average data-rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. This result has been extended in several directions (see, e.g., [7], [9]). However, when performance bounds subject to average data-rate constraints are sought, there are relatively fewer results available. Indeed, to our knowledge, there are no computable characterizations of the optimal encoding policies in networked control scenarios [1], [3], [5], [9], [13]. 
 In this note, we present upper and lower bounds on the minimal average data-rate that allows one to attain a given performance level (as measured by the stationary variance of the plant output). From a source-coding perspective, we are aiming at characterizing the rate-distortion function in closed-loop systems. This extends beyond causal rate-distortion theory [2] due to being subject to a stability constraint. Our results exploit a framework for networked control system design subject to average data-rates developed in [10], [11]. 
 Consider the NCS of Figure 1, where P is an LTI plant with state x ? Rnx and initial state xo, u ? R is the control input, y ? R is a sensor output, e ? Rne is a signal related to closed loop performance, and d ? Rnd is a disturbance. We assume that (xo,d) are jointly second-order and Gaussian (with finite entropies). The feedback path in Figure 1 comprises a delayfree noiseless digital channel, a causal encoder whose output yc is a sequence of binary words, and a causal decoder. 
 Fig. 1. Networked control system. average data-rate across the channel is defined as 
 We do not restrict the complexity of the encoder or the decoder a priori, and only assume them to be causal, and to have access to independent side information SE and SD. Our aim is characterizing 
 where se2  trace{Pe}, Pe is the stationary variance matrix of e, D > 0 is a desired level of performance, and the optimization is carried out with respect to all causal encoders E and decoders D that render the resulting NCS (asymptotically) mean square stable (MSS), i.e., that render (x,u,d) jointly second-order and asymptotically wide-sense stationary processes. 
 R = I8(y ? u) = I8(yG ? uG), (3) where I8(a ? ß) denotes the mutual information rate [6] between a and ß, and (yG,uG) are jointly Gaussian processes with the same second order statistics as (y,u). 
 Thus, in order to bound R(D) from below, it suffices to minimize the directed mutual information rate that would appear across the source coding scheme, when all signals in the loop are jointly Gaussian. 
 Lemma 3.1: Suppose that (yk,uk) in Fig. 1 are second order and jointly Gaussian random sequences. Then uk can be constructed from yk as u(i) = Li(yi,ui-1) + s(i), i = 1,...,k (4) where, for each i = 1,...,k, s(i) is a zero-mean Gaussian random variable such that s(i) ?? (ui-1,yi-1,si-1), and 
 Fig. 2. NCS that arises when, in Figure 1, the encoder E and decoder D form a linear source coding scheme. 
 where Li : Ri×(i-1) ? R is a linear operator such that Li(yi,ui-1) is the minimum mean-square error estimator of u(i) given (yi,ui-1). 
 We conclude from the above that, for a given performance level D, the minimum of I8(yG ? uG) over all causal encoders and decoders is achievable by an encoder/decoder pair which behaves as a linear system plus additive white Gaussian noise sk such that s(i) ?? (yi,ui-1), ?i. 
 We next define the class of linear source coding schemes, which are capable of yielding a relationship between y and u of the form given by (4). 
 Definition 4.1: A source coding scheme is said to be linear if and only if, when used around a noiseless digital channel, is such that its input y and output u are related via 
 where v and w are auxiliary signals, q is a second-order zeromean i.i.d. sequence, both F and K are proper LTI systems, and q is independent of (xo,d). 
 When a linear source coding scheme is used in the NCS of Figure 1, the LTI feedback system of Figure 2 arises. 
 Lemma 4.1: Consider the NCS of Figure 1 and assume that the encoder E and the decoder D form a linear source coding scheme. Under suitable assumptions, I8(y ? u) = I8(v ? w) and 
 where Sw is the stationary power spectral density of w and sq2 is the variance of the auxiliary noise q. 
 Linear source coding schemes have sufficient degrees of freedom to allow one to whiten w without compromising optimality. Thus, our results lead to: 
 Theorem 4.1: Consider the NCS of Figure 1 under suitable assumptions. Define, with reference to the feedback scheme of Figure 2, the infimal signal-to-noise ratio function 
 where sa2, a ? {v,q,e}, is the stationary variance of a in Figure 2, and the optimization is carried out with respect to all sq2 ? R+ and all proper LTI filters F and K which render 
 Theorem 4.1 characterizes the minimal average data-rate that guarantees a given stationary performance level, in terms of ?(D), i.e., in terms of the minimal SNR that guarantees the desired performance level in a related LTI architecture. Interestingly, the upper bound in (9) is valid even if one removes the assumption of (xo,d) being Gaussian 
 To find ?(D), one can resort to the results in [4]. A case where an explicit solution is available is when D ? 8, i.e., when only stabilization is sought. In that case, it follows from Theorem 4.1 and [12] that 
 where p1,...,pnp are the unstable poles of P. If one uses (10) in (8) and (9), then one recovers, within a modest gap, the absolute minimal average data-rate compatible with stability derived in [8]. 
 ﻿ This paper discrete time control subject to average data rate . We focus on a situation where a noisy linear system been designed assuming transparent feedback and , due to implementation , a source scheme with unity signal transfer function to be in the feedback path . For this situation , and by on a class of source built around entropy , we develop a framework to deal with average data rate in a tractable manner that from both information and control . As an illustration of the of our framework , we apply it to study the interplay between stability and average data in the considered architecture . It is shown that the class of can achieve mean square stability at average data that are , at most , . per sample away from the absolute minimum rate for stability established by and . This rate penalty is by the simplicity of our approach . 
 Index Average data rate , control , perfect reconstruction , signal to noise ratio . 
 control often use nontransparent communication links and , thus , communication arise . Such include random , data loss and data rate quantization , , and . This paper on average data rate . 
 It might be that the communication capacity of modern is in general sufficiently large , so as to make quantization irrelevant see , e .., and . However , there exist where the communication assigned to a particular relevant control signal are limited and , hence , quantization effects become important . Quantization is a highly nonlinear operation on and , accordingly , it is hard to analyze . 
 In control theory , quantization usually been as an undesirable effect that should be for see , e .., . This in contrast to the by information theory , where quantization is considered as an integral part of see , e .., and . This line of reasoning recently been brought to the control arena see , e .., , , , , and . An alternative view of quantization , to , also been inside the control community ; see , e .., , , , and . 
 In a discrete time control framework , a key problem is being able to characterize the minimal average data rate in , e .., per sample that one to achieve a given control objective . In the context of noiseless digital , i . e ., that transmit data without or at a constrained rate , this question is related to rate distortion theory i . e ., source ; see , e .., , , , and . The associated design problem is how to quantize a signal , with the average data rate , whilst a degree of fidelity or performance . A typical performance measure is the mean square error , but other are also possible . For instance , discrete to address black and white control such as , e .., and observability . 
 Standard in information theory and in particular in rate distortion theory rely upon arbitrarily long which incur arbitrarily long time . In addition , most of the general on rate distortion theory do not take stability nor causality into account , . It , thus , becomes clear that standard rate distortion theory is not useful to deal with control . Some progress been made in the information theory community towards a causal rate distortion theory , but most use that , even though causal , allow for arbitrary , . Only recently , established upper on the zero delay causal rate distortion function for stationary . However , the best provided in are of algorithmic nature , and derived for open loop . Thus , stability are not in . This is also the case of the in , where sequential quantization of is . 
 The discussion in the previous paragraph the work in , , and specially relevant , even though the focus in those works only on stability . The first that pointed out that there a data rate under which memoryless control cannot keep the state of a noiseless plant bounded were in and . The were later extended in and and with memory , and adaptive scaling so and . A landmark result was in , where the focus on noisy plant subject to mild on the noise statistics . It was shown in that it is possible to find causal , and such that the resulting closed loop system is mean square stable if and only if the average data rate in per sample , say , 
 where the th unstable plant pole . The above result a fundamental limitation in control closed over digital , when the problem of interest is mean square stabilization . similar to arise as to different e .., observability , deterministic stability and under different on the and see , e .., , , , , and . Indeed , the quantity on the right hand side of is a fundamental measure of the difficulty of a system , as in and . 
 All known to date that achieve stability at average data arbitrarily close to use complex nonlinear time that , in principle , have infinite memory . The consideration of with limited or no memory is much more involved and no explicit are currently available see , also , , Section . An alternative simpler approach been in , although for the scalar plant case only . 
 Almost all the work to above on stability only . A performance approach been in and . In that work , for separation and certainty equivalence have been in the context of quadratic stochastic for fully with data rate in the feedback path . If the a specific recursive structure , then certainty equivalence and a quasi separation principle hold . This result is interesting , but does not give a computable characterization of the optimal . A similar drawback is by the in . In that work , performance related are expressed in of the so sequential rate distortion function a rate distortion function with causality , which is difficult to compute in general . For fully first order , an expression for the sequential rate distortion function . However , it is not clear from the in whether or not the sequential rate distortion function is tight see , Section . Related work can be found in , where estimation are . 
 The main contribution of this paper is a novel , though restricted , bridge between information theory and control theory . The link is restricted it for a specific class of based on entropy see , e .., , , and . Nevertheless , the link is useful , one to address control system design subject to average data rate in a systematic manner . Our approach is constructive and based upon standard building . As such , it on average data that are to be achievable with conceptually simple . An additional feature of our approach is that it does not rely on asymptotic e ., high rate or high vector . 
 As both a motivation for our approach , and also to illustrate a , we linear system been designed assuming transparent feedback and , due , a source unity signal transfer function to be in the feedback path . For this situation , we discuss how to obtain on the minimal average data rate that one to attain a certain performance level , and also provide a detailed characterization of the interplay between stability and average data . It is shown that the class of can achieve mean square stability at average data that are to be at most . per sample away from the absolute minimum in . This rate penalty is by the simplicity of our approach . 
 A key result in the paper is that , when the class of source is employed , average data rate can be enforced by imposing signal to noise ratio in a related additive noise channel see , also , and . Our , thus , establish a formal relationship between and average data in noiseless digital . As such , our work goes beyond and where no such relationship is . Early of the in this paper can be found in and . 
 The remainder of the paper is organized as . Section notation . Section the setup considered in the paper . Section a lower bound on average that the remainder of the paper . class of source considered in the paper and average data rate to . Section on the interplay between stability and average data rate . Section . For ease of reference , the Appendix basic information theoretic . 
 the strictly positive , and the non negative , respectively . the magnitude of the complex scalar the conjugate transpose of the matrix is the set of all strictly proper and stable real rational transfer , and is the set of all stable , and minimum phase real rational transfer . The usual norm in is written 
 . If is an asymptotically wide sense stationary process , then denote its stationary variance , its stationary power spectral density , and the corresponding spectral factor , respectively . If is a discrete time signal , then is the th sample , and is shorthand for . 
 times , as usual . We write if and only if and are independent . We write , and form a chain see Appendix . and stand for the expectation and probability of , respectively . Definition of information theoretic and related notation is given in Appendix . 
 This paper on the of Fig . , where is a given linear time invariant system such that 
 exogenous , is a signal related to closed loop performance , is a signal available for measurement , and is a control input . We assume that been designed so as to achieve satisfactory performance when . The feedback path , however , an error free digital channel . Hence , the quantization of the signal becomes mandatory . This task is by an that the sequence of binary . Once these are available at the end , a the signal that is fed back to . The situation above naturally if , for example , to the interconnection of an plant and an controller that been designed without taking into account data rate in the feedback path . 
 a is a proper real rational transfer function , single input single output and strictly proper , 
 The initial state of , say , is a second order random variable , and is a second order process with spectral factor . 
 Assuming that the loop is stable when is consistent with our setup where been designed supposing transparent feedback . Assuming to be strictly proper the well of the of Fig . for all causal between and . This assumption can be removed at the expense of additional care . However , removing the constraint of being single input single output , additional effort 
 The remainder of this paper at building a framework to study the interplay between the average rate at which the channel are , and the performance and stability of the of Fig . . To that end , we begin by first a general lower bound on average data in feedback . 
 The use of digital communication the of . separation theorem that this process can be into two : source and channel see , also , . Source with the representation of continuous a countable alphabet and , as such , quantization . On the other hand , channel on the reliable and efficient communication of digital data over an underlying channel . We note that separation , and is useful , for point to point where causality and are not an issue . If causality are , then separation does not hold in general see , also , . Nonetheless , the study of causal source in isolation a key open problem in information theory , , . 
 The study of optimal source or quantization is the subject of rate distortion theory . Rate distortion theory does not take channel into account , and an digital link between the sending and , . In this paper , we adopt a purely source perspective and consider source whose output have a variable instantaneous length , but a bounded average length see , also , . We note , however , that bounded average data does not guarantee bounded instantaneous data , . for this to happen are in . 
 Without loss of generality , we consider source with the structure in Fig . . In that figure , is a , a reproduction , and the and form a lossless pair also entropy coder entropy pair ; see , e .., , Chapter . The continuously valued random into a countable set of . These are then by the into a countable set of prefix free binary that , in general , at every time instant . At the end , the the output from the binary by the , and the reproduction the back into real . A precise characterization of and is provided below . 
 In this paper we focus on single input single output within feedback , as in Fig . . Accordingly , we consider , reproduction and that are causal and , moreover , operate on a sample by sample basis , without delay . We also assume that side information is available at both the and sides . The new side information that becomes available at time instant is by , for the source , and by for the source . Such side information is in suitably defined and , where and . We also define the set 
 information that becomes available at both the and sides at instant . 
 In Fig . , the dynamic system is assumed to be such that its scalar output 
 where is a possibly time deterministic , is the initial state of , is the scalar input , and , with , is an exogenous random process . We also characterize the output of the via 
 is a possibly time deterministic , and is a fixed countable set . The are then used by the to construct the binary via 
 is a time deterministic , and is a countable set of prefix free binary . The output of the is to the end assuming ideal digital communication consistent with the source point of view adopted in this paper . Once becomes available at the end , the via 
 for any . Condition the fact that the considered here operate in real time , without delay . Finally , the reproduction its output via 
 Before the reception of are available at the side . It , thus , that the length 
 , measured in , of any binary description of the output symbol see , Chapter , , and 
 where conditional entropy see Appendix . The gap between both sides of on how efficient the is at . It is known that there exist such that 
 That is , the gap in is smaller than bit when suitable are employed e .., 
 Definition . : The average data rate of the source via 
 We will now study a lower bound on that only on the joint statistics of the source input and its output . 
 Assumption . : The and in Fig . are causal , by , and such that . 
 Assumption . can be thought as being a fairness assumption . Indeed , the that the source only past and present , and side information not related to the message being sent , to construct the current output value . In other , we assume that the channel is the only link from to . 
 Definition . : The source scheme by is said to have an invertible reproduction 
 in short , an invertible there a deterministic such that . 
 If a source scheme an invertible , then knowledge of is equivalent to knowledge of . 
 The generality of source with invertible is next . 
 Lemma . : Consider any source scheme by . Define . If 
 and the corresponding is not invertible , then there another causal source 
 , where is the conditional probability of the most likely symbol of the alphabet of , given . 
 Proof : Assume that the such that it been possible to use to recover from , for all such assumption is not at time instant . If at time instant cannot be inverted 
 by the conditional probability of the output of being at time instant , given . Consider now another pair that like , except for the fact that instead of either or at time instant . At time instant , the value with conditional probability , given . Thus 
 from the definition of and that of entropy , from inequality , and from the definition of and that of . 
 By the above procedure until there are no two of that are into the same value at time instant , one a source scheme where knowing is equivalent to knowing and 
 Given , it from Lemma . that one can always focus on source with invertible , 
 Lemma . : Consider a source scheme inside a feedback loop , as in Fig . . If Assumption . and the is invertible , then the chain and , conditioned upon true . 
 Proof : Given , it from that there a deterministic such that . Since 
 , it immediately that and are independent upon knowledge of , thus proving our first claim . The second claim is immediate upon that upon . 
 Theorem . : Consider a source scheme inside a feedback loop , as in Fig . . If Assumption . and the is invertible , then . 
 where from Property in Appendix and the fact that , from the fact that the is invertible , from , the fact that the is invertible , and the second claim of Lemma . , and from and the first claim of Lemma . . The result now . 
 Theorem . that , when causality are , directed mutual information rate across a source scheme as a lower bound on the associated average . The result a physical quantity average data rate to an information theoretic quantity directed mutual information rate . Theorem . also that the appropriate information theoretic definition of average data in causal is the directed mutual information rate . However , showing that the , over all joint input and output that satisfy a causality constraint , of the directed mutual information rate across a source scheme an tight lower bound on the corresponding average data rate , remains an open problem see , also , . 
 To our knowledge , Theorem . , for the first time , a characterization of the relationship between directed mutual information rate and the operational rate of source within feedback . The result in the literature that is to Theorem . is , Theorem . However , that result is derived for entropy only see Section , as opposed to the general causal source 
 considered here . Related are , Lemma . . and , Theorem . . where feedback data are . However , those do not focus on operational data , and assume no feedback between the at the physical of the . 
 Other relevant and related works are and . In , the study fundamental directed mutual information across within feedback , and Bode like fundamental that arise due to finite capacity communication see , also , . On the other hand , a relationship between operational data and directed mutual information rate from a channel perspective . In that work , the show that the , over all joint channel input and output that satisfy a causality constraint , of the directed mutual information rate across a channel capacity with feedback for that channel . Despite all the work to above , no relationship besides that of Theorem . between average operational data and directed mutual information rate from a source perspective , and valid in general , is currently available in the literature . 
 This paper at a bridge between control and information , when a specific class of source is employed . This section such class . 
 In order to obtain a simple yet useful framework for the study of the of Fig . , we will focus on the following class of source . 
 Definition . : A source scheme is said to be independent Assumption . , its reproduction is invertible , and the or quantization noise sequence , defined via 
 , where is a second order zero mean i . i .. sequence , finite differential entropy and the a deterministic initial state ; see Fig . a . 
 The class of independent source is restrictive . However , it is a sensible choice when data rate arise in that have already been designed to perform satisfactorily in the absence of quantization . In such , which include the situation of interest in this paper see Section , 
 it is desirable to introduce quantization effects in an additive fashion so as not to alter the nominal design see , also , . We will describe a practical independent source scheme in Section . 
 We begin our study of independent source by that the following . 
 Lemma . : Any independent source scheme can be written as shown in Fig . , where and are auxiliary , is as in Definition . , and 
 . To prove our second claim , we note that the on and imply that there exist deterministic , with and invertible , such that see 
 Since Lemma . , the system that when an independent source scheme is employed in the feedback loop of Fig . can be written as shown in Fig . . Note that the error free digital channel of Fig . is in the independent source scheme of Fig . . In Section , we will 
 A key feature of independent source is that the directed mutual information rate across them can be bounded by the directed mutual information rate that would arise if all random were by . To be precise , we introduce the following definition . 
 Definition . : Consider an system with random and random initial state . If is a state , input , or output variable of the system , then to the signal that would arise in the place of , when all and initial are by jointly random or the same first and second order cross , and the same statistical dependence , as in the original situation ; is the counterpart of . 
 Lemma . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
 where and are the auxiliary in Fig . see Lemma . , entropy rate , and relative entropy see Appendix . in hold and are . 
 Remark . : If the of Lemma . hold and , in addition , is jointly , then see , Chapter and 
 The relevance of Lemma . in that the characterization of directed mutual information rate under is straightforward . 
 Theorem . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
 Theorem . explicit upper on the directed mutual information rate across any independent source scheme in a stable and causal feedback loop . These are , essentially , expressed in of the spectral of the auxiliary and in the scheme of Fig . . Interestingly enough , there a one to one correspondence between the of an independent source scheme , and upper on the directed mutual information rate across it . Given Theorem . , we can infer that there a link between the of an independent source scheme and the associated average data rate . A precise characterization of such link will be given in Section . 
 We note that also a relationship between directed mutual information rate and Bode like , when . , Theorem . to feedback with arbitrary disturbance and initial state . Indeed , , Theorem . can be from the inequality in upon assuming to be jointly distributed . 
 We end this section by showing that , for any given independent source scheme , there another independent source scheme , with the same noise color and the same directed mutual information rate across it , such that the gap between the right hand side of and can be made arbitrarily small . 
 Theorem . : Consider the of Fig . , where the source scheme is independent and a fixed noise source . Suppose that Assumption . and define Since and , we have from the Bode integral Theorem and the definition of that 
 . Proceeding as in , proof of Lemma ,. , we conclude that the last term in can be made arbitrarily small with 
 . Our last claim now from the last claim of Theorem . . If for some , then is not admissible since it and , thus , the representation of the independent source scheme of Fig . would be internally unstable . 
 Entropy are that have convenient that make them suitable for use as building when dealing with average data rate , , and . In particular , we show below that can be used to construct the noise source that an independent source scheme . 
 The structure of an is shown in Fig . . In that figure , is a dither signal which is assumed available at both the sending and , the pair is as in 
 where is the quantization step a designer choice . The output of the 
 quantization noise as an independent i . i .. source is common in the signal literature . In general , this model is not exact , but becomes exact in when the dither is appropriately chosen see , e .., and . The next theorem that this key property remains valid when are in strictly causal feedback . 
 Theorem . : Consider the setup of Fig . , where the is as above and a finite quantization step . Assume that is a proper real rational transfer function , that the open loop transfer function from to is single input and strictly proper , that the closed loop system is internally stable and well when , that the signal is a second order process , and that the initial state of , say , is a second order random variable . If is such that , then 
 Proof : Similar to the proof of Theorem in see , Chapter for . 
 Remark . : The definition of and that Theorem . irrespective of how the pair is chosen . In particular , it if the pair is ; see . It is also worth that , if the dither is not at the side , then only moment independence is . For the remainder of the paper , the following consequence of Theorem . is relevant . 
 Corollary . : Consider the system of Fig . with and as in Lemma . . If an , with dither chosen as in Theorem . and finite quantization step , is used as the link from to , then the system of Fig . becomes an independent scheme . 
 Fig . . Explicit of an independent source scheme that an as the link between and . 
 Proof : Given Lemma . and Theorem . , it to show that the resulting source scheme Assumption . and an invertible . Since in the present situation , 
 Assumption . is satisfied . Also , since , its initial state is deterministic and , by definition of , 
 and , we conclude that knowledge of is equivalent to knowledge of . The invertibility of 
 When an is used as the link between and in the system of Fig . , the resulting scheme can be as in Fig . . Fig . . From that figure , it is clear that feedback from to the input of in Fig . does not require explicit feedback around the digital channel . Recall that we 
 Remark . : , and share a common source of randomness the dither . In principle , this that both the and must share information about the dither . This an additional degree of implementation complexity , but to the best of our knowledge , there is no other simple way of satisfying Assumption . . In practice , one can use synchronized number with the same . 
 Average Data in Independent Source That Use : We are now in a position to present an upper bound on the average data rate in independent source that use . We start with the following result . 
 Theorem . : Consider the of Fig . , where the source scheme is independent . Suppose that Assumption . , and that the link between the auxiliary and see Fig . and Lemma . is an with dither chosen as in Theorem . and finite quantization step . Then , and there an 
 . Also , from Theorem . and its proof we have that and . the above , our first claim from the proof of Theorem in . The second claim from the first , and . A direct proof can be by the definition of , and the fact that knowledge of is equivalent to knowledge of see proof of Corollary . , to show 
 Theorem . that inside independent source one to achieve average data that are close to the absolute lower established in Theorem . . The worst case gap , which in the inefficiency of the and is smaller than , is intrinsic to any scalar lossless coder and cannot be removed , unless one high rate regime ; , block entropy which may introduce unbounded ; , , or the scheme to operate in a nonstationary fashion by time , . In practice the gap may be smaller than ,. , . 
 Corollary . : Consider the setup and of Theorem . . There an such that 
 Remark . : Theorem . and Corollary . are only existence type . The implementation of inside outside the scope of this paper , and we refer the reader to , Remark . for related . 
 Corollary . a closed form upper bound on the average data rate in an independent source scheme that an . The bound is given in of spectral of the output , and two additional constant . The second term in , i . e ., per sample i . e ., per sample , to the divergence of the quantization noise distribution from and because generate uniform quantization noise not noise ; see , also , . This term can also be given an alternative interpretation in of the space filling loss when a finite dimensional instead of an infinite dimensional one , with spherical quantization . We refer the interested reader to , , and for further . As before , the term bit due to the inefficiency of the inside the . Interestingly , our result without on the external signal nor on the initial state . 
 Remark . : If the of Corollary . hold and , in addition , is , then a lower bound for in is given by the first term on the right hand side of . That is , becomes tight up to per sample see Remark . and Theorem . . Independent Source With Memoryless : So far , we have considered where the pair is to exploit all past and present , binary , and side information . Such have unrestricted memory and its implementation the knowledge of the conditional distribution of , given . That distribution can be difficult to characterize . In order to simplify implementation , it is common to consider without memory see , also , . 
 Definition . : An is said to be memoryless the associated pair is such that 
 When a memoryless , the can only exploit the knowledge of to encode . Thus , must be by . Again , it is possible to design such that compare with 
 We now present a definition and two that allow one to state the counterpart of Corollary . for the case of independent source that use memoryless . 
 Theorem . : Consider the setup and of Theorem . . If the is memoryless , then and there an pair such that 
 Theorem . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
 Proof : The result proceeding as in the proof of Theorem . see in . 
 Corollary . : Consider the setup and of Theorem . . There a memoryless such that 
 Remark . : In analogy to Remark . , we note that if the of Corollary . hold , and is , then 
 The inside a memoryless less information to encode than the inside an with unrestricted memory . As a consequence , average data achievable with memoryless will be , in general , than those achievable with that have unrestricted memory . This . and . , as inequality . However , the conclusion is pessimistic when are used inside independent source . In this case , Theorem . that the rate penalty , as measured by the gap between the right hand sides of and , when a memoryless instead of an with memory , can be made arbitrarily small if an appropriate choice for the auxiliary and is made . Hence , without being unduly conservative , it to study the to give upper on the average data rate of independent source , irrespective of whether they use with unrestricted memory or not . 
 Consider the of Fig . when the source scheme is independent and an as the link between the auxiliary and see Fig . . For this setup , the of A one to restate control average data rate as control stationary . This one to design subject to average data rate in a systematic fashion that standard quadratic optimization . For instance , if performance is measured by of the stationary variance of the error signal , then , irrespective of whether the unrestricted memory or not , the minimal average data rate to achieve a performance level , say , see . and . 
 , and that guarantee stability in an appropriate sense , and is the minimal stationary of the source coder that one to achieve the performance level , i . e ., 
 Once is which , in principle , is a standard quadratic control problem , one readily a bound on see , also , Section . 
 The framework provided here is conservative since it is based on that are not tight in general . However , the framework that are , by construction , to be achievable with conceptually simple building , and do not rely on any asymptotic approximation . As above , the framework also the door to use standard synthesis to deal with average data rate in control system design , as in . These are the main that distinguish our approach from the literature in Section I . 
 We now illustrate the approach outlined in on the minimal average data rate for stability , 
 i . e ., we consider in . The case of finite is due to space see . 
 We start by the interplay between on the source coder and stability . By virtue of Theorem . , we focus on the feedback system of Fig . , and use the following notion of stability . 
 , where , is the system state at time instant where is a second order random variable , and are constant matrices of appropriate , and is a second order process . The system is said to be mean square stable there exist finite and a finite and positive semi definite both not depending on , such that 
 small with a sufficiently large and cannot be made equal to zero since , by assumption ,. Well known , Lemma ,. allow one to conclude that there such that the gap in is arbitrarily small . The 
 result , thus The existence of the family also by the in . 
 Theorem . that , for independent source , the minimal that is compatible with , i . e ., 
 , is only a function of the unstable of . Hence , for any given , the condition is necessary 
 and sufficient to be able to find , and a noise variance equivalently , a quantization step such that the resulting is and the . 
 Corollary . : Consider the setup and of Theorem . . If , then the stationary variance of the error signal unbounded . 
 We see from Corollary . that the study of on for are insufficient to give performance . This fact is unsurprising , and consistent with in and . 
 We now return to the of Fig . when the source scheme is independent and an . Given the definition of an independent source scheme , it that the notion of in Definition . is still valid in this setting . It is also clear that , provided Assumption . , 
 Corollary . : Consider the setup and of Theorem . . Then , irrespective of whether the memory or not , the following . 
 a The minimal average data rate compatible with , i . e ., as defined in , 
 a Consider the case of with unrestricted memory . Equation and Theorem . yield 
 The upper bound in upon Theorem . in . The lower bound from , Theorem . . 
 Corollary . lower and upper on the minimal average data rate that is compatible with in the considered , when an independent source scheme is employed . A feature of our proposal is that with unrestricted memory do not provide any advantage over memoryless at least from a point of view ; see , also , discussion at the end of Section . This is relevant in practice since the implementation of with unrestricted memory is prohibitive . Indeed , in order to design an independent scheme that stability at an average data rate smaller than , it to use a memoryless with sufficiently large quantization step , an pair designed , and and that , for the situation studied in Section A , guarantee at sufficiently close to . By doing so , however , the performance of the will be see Corollary . . This is consistent with in and . 
 The of Corollary . show that , when an independent source scheme is employed in the of Fig . , can be at average data that are to be no than the absolute bound in and plus per sample . This 
 extra rate is , in our view , a fair price to be if one oneself to the conceptually simple source considered in this paper . We note however that the upper bound in is a worst case upper bound . As , in practice one can expect to achieve at no than see and . 
 Our can also be used to provide upper on the average data rate that is to achieve , when memoryless source are employed in the considered 
 , then the resulting source scheme no memory and it is easy to show that and the pair can be chosen so as to achieve at an average data rate satisfying 
 Relation can be with the of , Section even though on a different notion of stability . That work that there exist memoryless that guarantee stability with bounded but otherwise unknown data , whist a computable upper bound on the minimal average data rate compatible with . 
 is designed assuming transparent feedback and , at a later design stage , a unity signal transfer function source scheme is tobe rate feedback path have on closed loop performance . To address this problem , we have on a class of source and , by doing so , we have established a bridge between information and control . A key result of our work is that , for the considered class of , average data rate can be enforced by imposing signal to noise ratio in a related additive noise communication channel . As an application of our , we studied the interplay between average data in the considered setup . For that problem , the class of was shown to achieve mean square stability at average data that are to be less than . per sample above the absolute minimum established in . 
 We refer the reader to and for of the framework in this paper to beyond stabilization . A key open problem not in this work is how to incorporate average data rate into control causal but otherwise unrestricted source . 
 The following and are standard and can be found in . Unless otherwise stated , all are assumed to be random with well defined joint probability density or mass . The of is . to the conditional of , given . mean with respect to ; 
 If and are discrete , then we use to denote the conditional entropy of given . The are analogue to the continuous case . 
 Conditional mutual information between discrete is defined as in the continuous case . 
 The relative entropy between and or divergence of the distribution of with respect to that of is defined via . Given joint 
 If is the counterpart of see Definition . , then . If , are the of then 
 The entropy rate of a stochastic process is defined via . A useful fact is the following . 
 , then equality , in addition , is asymptotically ; see , Lemma . . 
  
 ﻿This paper studies discrete-time control systems subject to average data-rate limits. We focus on a situation where a noisy linear system has been designed assuming transparent feedback and, due to implementation constraints, a source-coding scheme (with unity signal transfer function) has to be deployed in the feedback path. For this situation, and by focusing on a class of source-coding schemes built around entropy coded dithered quantizers, we develop a framework to deal with average data-rate constraints in a tractable manner that combines ideas from both information and control theories. As an illustration of the uses of our framework, we apply it to study the interplay between stability and average data-rates in the considered architecture. It is shown that the proposed class of coding schemes can achieve mean square stability at average data-rates that are, at most, 1.254 bits per sample away from the absolute minimum rate for stability established by Nair and Evans. This rate penalty is compensated by the simplicity of our approach. 
 Index Terms—Average data-rate, networked control systems, perfect reconstruction, signal-to-noise ratio. 
 RACTICAL control systems often use nontransparent communication links and, thus, communication constraints arise [1]. Such constraints include random delays, data-loss and data-rate limits (quantization) [23], [36], and [41]. This paper focuses on average data-rate constraints. 
 It might be argued that the communication capacity of modern networks is in general sufficiently large, so as to make quantization issues irrelevant (see, e.g., [23] and [41]). However, there exist situations where the communication resources assigned to a particular relevant control signal are limited and, hence, quantization effects become important [36]. Quantization is a highly nonlinear operation on signals and, accordingly, it is hard to analyze [21]. 
 In control theory, quantization has usually been treated as an undesirable effect that should be compensated for (see, e.g., [32]). This stands in contrast to the perspectiveadopted by information theory, where quantization is considered as an integral part of systems (see, e.g., [8] and [45]). This line of reasoning has recently been brought to the control arena (see, e.g., [34], [39], [40], [57], and [62]). (An alternative view of quantization, closerto nonlinearcontroltheorythan to informationtheory, has also been developed inside the control community; see, e.g., [7], [9], [13], and [37].) 
 In a quantized discrete-time control framework, a key problem is being able to characterize the minimal average data-rate (in, e.g., bits per sample) that allows one to achieve a given control objective. In the context of noiseless digital channels, i.e., channels that transmit data without errors or delays at a constrained rate, this question is related to rate-distortion theory (i.e., lossy source-coding problems; see, e.g., [4], [8], [17], and [46]). The associated design problem is how to quantize a signal, with the smallest average data-rate, whilst achieving a prescribed degree of fidelity or performance. A typical performance measure is the mean square error, but other measures are also possible. For instance, [54] suggests discrete measures to address black-and-white control problems such as, e.g., stabilizability and observability. 
 Standard results in information theory (and in particular in rate-distortion theory) rely upon coding arbitrarily long sequences which incur arbitrarily long time delays. In addition, most of the general results on rate-distortion theory do not take stability nor causality into account [4], [46]. It, thus, becomes clear that standard rate-distortion theory is not useful to deal with control problems. Some progress has been made in the information theory community towards a causal rate-distortion theory, but most results use coding schemes that, even though causal, allow for arbitrary delays [25], [38]. Only recently, [11] established upper bounds on the zero-delay causal rate distortion function for Gaussian stationary sources. However, the best bounds provided in [11] are of algorithmic nature, and derived for open loop systems. Thus, stability issues are not addressed in [11]. This is also the case of the results in [5], where sequential quantization of Markov sources is addressed. 
 The discussion in the previous paragraph makes the work documented in [34], [55], and [60] specially relevant, even though the focus in those works lies only on stability. The first results that pointed out that there exists a data-rate under which (memoryless) quantized control cannot keep the state of a noiseless plant bounded were presented in [3] and [60]. The results were later extended in [33] and [55] using encoders and decoders with memory, and adaptive quantizer scaling policies (so-called zooming techniques [7] and [60]). A landmark result was published in [34], where the authors focus on noisy plant models subject to mild conditions on the noise sources statistics. It was shown in [34] that it is possible to find causal coders, decoders and controllers such that the resulting closed loop system is mean square stable if and only if the average data-rate (in bits per sample), say  , satisfies 
 where   denotes the  th unstable plant pole. The above result establishes a fundamental limitation in networked control systems (NCSs) closed over digital channels, when the problem of interest is mean square stabilization. Bounds similar to (1) arise as solutions to different problems (e.g., observability, deterministic stability, etc.) and under different assumptions on the channels and coding schemes (see, e.g., [14], [33], [36], [54], and [55]). Indeed, the quantity on the right hand side of (1) is a fundamental measure of the difficulty of stabilizing a system, as discussed in [35] and [40]. 
 All constructions known to date that achieve stability at average data-rates arbitrarily close to (1) use complex nonlinear time-varying coding schemes that, in principle, have infinite memory. The consideration of coding schemes with limited (or no memory) is much more involved [36] and no explicit solutions are currently available (see, also, [55, Section VI]). An alternative simpler approach has been presented in [61], although for the scalar plant case only. 
 Almost all the work referred to above focuses on stability questions only. A performance-oriented approach has been pursued in [36] and [57]. In that work, conditions for separation and certainty equivalence have been investigated in the context of quadratic stochastic problems for fully observed plants with data-rate constraints in the feedback path. If the encoder has a specific recursive structure, then certainty equivalence and a quasi-separation principle hold [36]. This result is interesting, but [36] does not give a computable characterization of the optimal encoding policies. A similar drawback is shared by the results reported in [57]. In that work, performance related results are expressed in terms of the so-called sequential rate-distortion function (a rate-distortion function with causality constraints), which is difficult to compute in general. For fully observed Gaussian first-order autoregressive systems, [57] provides an expression for the sequential rate-distortion function. However, it is not clear from the results in [57] whether or not the sequential rate-distortion function is operationally tight (see [57, Section IV-C]). Related work can be found in [62], where estimation problems are addressed. 
 The main contribution of this paper is a novel, though restricted, bridge between information theory and control theory. The link is restricted inthat it holds for a specific class of sourcecoding schemes based on entropy coded dithered quantizers (see, e.g., [63], [64], and [66]). Nevertheless, the link is useful, enabling one to address control system design problems subject to average data-rate constraints in a systematic manner [47]. Our approach is constructive and based upon standard building blocks. As such, it yields bounds on average data-rates that are guaranteed to be achievable with conceptually simple sourcecoding schemes. An additional feature of our approach is that it does not rely on asymptotic approximations (e.g, high-rate or high vector dimensions assumptions). 
 As both a motivation for our approach, and also to illustrate a possibleapplication,we considera problemwhere anoisy linear system has been designed assuming transparent feedback and, due toimplementation constraints,a source-coding schemewith unity signal transfer function has to be deployed in the feedback path. For this situation, we discuss how to obtain bounds on the minimal average data-rate that allows one to attain a certain performance level, and also provide a detailed characterization of the interplay between stability and average data-rates. It is shown that the proposed class of coding schemes can achieve mean square stability at average data-rates that are guaranteed to be at most 1.254 bits per sample away from the absolute minimum in (1). This rate penalty is compensated by the simplicity of our approach. 
 A key enabling result in the paper is that, when the proposed class of source-coding schemes is employed, average data-rate constraints can be enforced by imposing signal-to-noise ratio (SNR) constraints in a related analog additive noise channel (see, also, [6] and [51]). Our results, thus, establish a formal relationship between SNR constraints and average data-rates constraints in noiseless digital channels. As such, our work goes beyond [6] and [51] where no such relationship is presented. Early versions of the results reported in this paper can be found in [47] and [50]. 
 The remainder of the paper is organized as follows. Section II presents notation. Section III describes the setup considered in the paper. Section IV presents a lower bound on average datarates that motivates the remainder of the paper. Section V introduces the class of source-coding schemes considered in the paper and relates average data-rate limits to SNR constraints. Section VI focuses on the interplay between stability and average data rate constraints. Section VII draws conclusions. For ease of reference, the Appendix presents basic information-theoretic facts. 
 the strictly positive reals, and the non-negative integers, respectively.   denotes the magnitude of the complex scalar  denotes the conjugate transpose of the matrix   is the set of all strictly proper and stable real rational transfer functions, and   is the set of all stable, biproper and minimum phase real rational transfer functions. The usual norm in   is written 
 [58]. If is an asymptotically wide sense stationary (wss) process [2], then , , denote its stationary variance, its stationary power spectral density (PSD), and the corresponding spectral factor, respectively. If is a discrete time signal, then is the th sample, and is shorthand for . 
 ( times), as usual. We write if and only if (iff) and are independent. We write iff , and form a Markov chain (see Appendix). and stand for the expectation and probability of , respectively. Definition of information-theoretic quantities and related notation is given in Appendix. 
 This paper focuses on the NCS of Fig. 1, where   is a given linear time-invariant (LTI) system such that 
 models exogenous signals, is a signal related to closed loop performance, is a signal available for measurement, and is a control input. We assume that has been designed so as to achieve satisfactory performance when . The feedback path, however, comprises an error free digital channel. Hence, the quantization of the signal   becomes mandatory. This task is performed by an encoder that outputs the sequence of binary symbols  . Once these symbols are available at the receiving end, a decoder generates the signal   that is fed back to  . The situation described above arises naturally if, for example,   corresponds to the interconnection of an LTI plant and an LTI controller that has been designed without taking into account data-rate limits in the feedback path. 
 (a)	  is a proper real rational transfer function, single-input single-output and strictly proper, 
 (b)	The initial state of , say , is a second-order random variable, and is a second-order wss process with spectral factor	. 
 Assuming that the loop is stable when   is consistent with our setup where   has been designed supposing transparent feedback. Assuming   to be strictly proper guarantees the well-posedness of the NCS of Fig. 1 for all causal mappings between and . This assumption can be removed at the expense of additional care. However, removing the constraint of   being single-input single-output, requires additional effort 
 The remainder of this paper aims at building a framework to study the interplay between the average rate at which the channel symbols   are transmitted, and the performance and stability of the NCS of Fig. 1. To that end, we begin by first establishing a general lower bound on average data-rates in feedback systems. 
 The use of digital communication systems requires the coding of analog signals [8]. Shannon’s separation theorem states that this coding process can be separated into two problems [45]: source coding and channel coding (see, also, [17]). Source coding deals with the representation of continuous symbols using a countable alphabet and, as such, involves quantization [21]. On the other hand, channel coding focuses on the reliable and efficient communication of digital data over an underlying analog channel. We note that separation holds, and is useful, for point-to-point communications where causality and delays are not an issue. If causality constraints are imposed, then separation does not hold in general (see, also, [57]). Nonetheless, the study of causal source-coding problems in isolation constitutes a key open problem in information theory [5], [11], [25]. 
 The study of optimal source-coding (or quantization) problems is the subject of rate-distortion theory. Rate-distortion theory does not take channel coding into account, and assumes an idealized digital link between the sending and receiving ends [4], [21]. In this paper, we adopt a purely source-coding perspective  and consider source encoders whose output symbols   have a variable instantaneous length, but a bounded average length (see, also, [21]). We note, however, that guaranteeing bounded average data-rates does not guarantee bounded instantaneous data-rates [8], [21]. (Conditions for this to happen are explored in [22].) 
 Without loss of generality, we consider source-coding schemes with the structure depicted in Fig. 2 [21]. In that figure,   is a lossy encoder,   a reproduction decoder, and the blocks EC and ED form a lossless encoder-decoder pair (also called entropy coder (EC)—entropy decoder (ED) pair; see, e.g., [8, Chapter 5]). The lossy encoder maps continuously valued random variables into a countable set of symbols. These symbols are then mapped by the EC into a countable set of prefix-free binary words that, in general, changes at every time instant [8].  At the receiving end, the ED recovers the lossy encoder output symbols from the binary words generated by the EC, and the reproduction decoder maps the recovered symbols back into real numbers. A precise characterization of  , , EC and ED is provided below. 
 In this paper we focus on single-input single-output sourcecoding schemes within feedback loops, as depicted in Fig. 2. Accordingly, we consider lossy encoders  , reproduction decoders   and EC-ED pairs that are causal and, moreover, operate on a sample by sample basis, without delay. We also assume that side information is available at both the encoder and decoder sides. The new side information that becomes available at time instant is denoted by , for the source encoder, and by for the source decoder. Such side information is contained in suitably defined sets and , where and . We also define the set 
 information that becomes available at both the encoder and decoder sides at instant  . 
 In Fig. 2, the dynamic system   is assumed to be such that its (scalar) output   satisfies 
 where	is a (possibly time varying) deterministic mapping, is the initial state of , is the (scalar) input, and , with , is an exogenous random process. We also characterize the output of the lossy encoder via 
 is a (possibly time varying) deterministic mapping, and is a fixed countable set. The symbols   are then used by the EC to construct the binary words   via 
 is a time varying deterministic mapping, and is a countable set of prefix-free binary words. The output of the EC is transmitted to the receiving end assuming ideal digital communication (consistent with the source-coding point of view adopted in this paper). Once   becomes available at the receiving end, the ED recovers   via 
 (6) for any  . Condition (6) reinforces the fact that the EC-ED pairs considered here operate in real time, without delay.  Finally, the reproduction decoder constructs its output   via 
 Before the reception of   are available at the decoder side. It, thus, follows that the expected length 
 , measured in nats,4 of any binary description of the lossy encoder output symbol satisfies (see [8, Chapter 5], [45], and [24]) 
 where   denotes conditional entropy (see Appendix). The gap between both sides of (8) depends on how efficient the EC is at encoding  . It is known that there exist ECs such that [8] 
 That is, the gap in (8) is smaller than   nats (1 bit) when suitable encoding policies are employed (e.g., Huffman5 coding 
 Definition 4.1: The average data-rate of the source-coding schemedescribedabove,measuredinnats persample, isdefined via 
 We will now study a lower bound on   that depends only on the joint statistics of the source encoder input   and its output  . 
 Assumption 4.1: The systems  , EC, ED and in Fig. 2 are causal, described by (2)–(7), and such that . 
 Assumption 4.1 can be thought as being a “fairness” assumption. Indeed, itis consistentwith the reasonablerequirement that the source decoder uses only past and present symbols, and side information not related to the message being sent, to construct the current output value. In other words, we assume that the channel is the only link from to . 
 Definition 4.2: The source-coding scheme described by (2)–(7) is said to have an invertible reproduction decoder 
 (in short, an invertible decoder) iff, , there exists a deterministic mapping   such that . 
 If a source-coding scheme has an invertible decoder, then knowledge of   is equivalent to knowledge of . 
 The generality of source-coding schemes with invertible decoder is examined next. 
 Lemma 4.1: Consider any source-coding scheme described by (2)–(7). Define . If, , 
 ,	, and the corresponding decoder is not invertible, then there exists another causal source-coding 
 , where   is the conditional probability of the most likely symbol of the alphabet of , given   [16]. 
 Proof: Assume that the encoder-decoder pairis such that it has been possible to use to recover from , for all (such assumption is not needed at time instant ). If at time instant cannot be inverted 
 by the conditional probability of the output of being at time instant , given . Consider now another encoder-decoder pair that behaves like , except for the fact that outputs instead of either or at time instant . At time instant , outputs the value with conditional probability , given . Thus 
 wherefollows from the definition of and that of entropy,follows from Jensen’s inequality, and follows from the definition of   and that of . 
 By repeating the above procedure until there are no two symbols of	that are mapped into the same value	at time instant	, one constructs a source-coding scheme where knowing is equivalent to knowing	,	, and 
 Given (9), it follows from Lemma 4.1 that one can always focus on source-coding schemes with invertible decoders, 
 Lemma 4.2: Consider a source-coding scheme inside a feedback loop, as depicted in Fig. 2. If Assumption 4.1 holds and the decoder is invertible, then the Markov chain holds and, conditioned upon true. 
 Proof: Given , it follows from (2) that there exists a deterministic mapping such that . Since 
 , it immediately follows that	and	are independent upon knowledge of	, thus proving our first claim. The second claim is immediate upon noting that	depends deterministically upon	. 
 Theorem 4.1: Consider a source-coding scheme inside a feedback loop, as depicted in Fig. 2. If Assumption 4.1 holds and the decoder is invertible, then  . 
 where follows from Property in Appendix and the fact that , follows from the fact that the decoder is invertible, (c) follows from , the fact that the decoder is invertible, and the second claim of Lemma 4.2, and   follows from   and the first claim of Lemma 4.2. The result now follows using (10)–(12). 
 Theorem 4.1 states that, when causality constraints are imposed, directed mutual information rate across a source-coding scheme serves as a lower bound on the associated average datarate. The result relates a physical quantity (average data-rate) to an information-theoretic quantity (directed mutual information rate). Theorem 4.1 also suggests that the appropriate information-theoretic definition of average data-rates in causal sourcecoding schemes is the directed mutual information rate. However, showing that the infimum, over all joint input and output distributions that satisfy a causality constraint, of the directed mutual information rate across a source-coding scheme provides an operationally tight lower bound on the corresponding average data-rate, remains an open problem (see, also, [11]). 
 To our knowledge, Theorem 4.1 provides, for the first time, a characterization of the relationship between directed mutual information rate and the operational rate of source-coding schemes within feedback loops. The result in the literature that is closest to Theorem 4.1 is [66, Theorem 2]. However, that result is derived for entropy coded dithered quantizers only (see Section V-B), as opposed to the general causal source-coding 
 schemes considered here. Related results are [53, Lemma 4.8.1] and [27, Theorem B.1.1] where feedback data processing inequalities are presented. However, those results do not focus on operational data-rates, and assume no feedback between the signals at the physical ends of the processing chains. 
 Other relevant and related works are [28] and [56]. In [28], the authors study fundamental inequalities involving directed mutual information rates across channels within feedback loops, and presents Bode-like fundamental limitations that arise due to finite capacity communication (see, also, [29]). On the other hand, [56] establishes a relationship between operational data-rates and directed mutual information rate from a channel coding perspective. In that work, the authors show that the supremum, over all joint channel input and output distributions that satisfy a causality constraint, of the directed mutual information rate across a channel equals Shannon’s capacity with feedback for that channel. Despite all the work referred to above, no relationship (besides that of Theorem 4.1) between average operational data-rates and directed mutual information rate from a source-coding perspective, and valid in general settings, is currently available in the literature. 
 This paper aims at establishing a bridge between control and information theories, when a specific class of source-coding schemes is employed. This section presents such class. 
 In order to obtain a simple (yet useful) framework for the study of the NCS of Fig. 1, we will focus on the following class of source-coding schemes. 
 Definition 5.1: A source-coding scheme is said to be independent iff Assumption 4.1 holds, its reproduction decoder is invertible, and the (coding or quantization) noise sequence  , defined via 
 obeys , where   is a second-order zero-mean i.i.d. sequence, has finite differential entropy, , and the filterhas a deterministic initial state; see Fig. 3(a). 
 The class of independent source-coding schemes is restrictive. However, it is a sensible choice when data-rate constraints arise in systems that have already been designed to perform satisfactorily in the absence of quantization. In such cases, which include the situation of interest in this paper (see Section III), 
 it is desirable to introduce quantization effects in an additive fashion so as not to alter the nominal design relations (see, also, [20]). We will describe a practical independent source-coding scheme in Section V-B. 
 We begin our study of independent source-coding schemes by noting that the following holds. 
 Lemma 5.1: Any independent source-coding scheme can be written as shown in Fig. 3(b), where and are auxiliary signals,   is as in Definition 5.1, and 
 . To prove our second claim, we note that the assumptions on and imply that there exist deterministic mappings , with and invertible, such that (see 
 Since Lemma 5.1 holds, the system that arises when an independent source-coding scheme is employed in the feedback loop of Fig. 1 can be written as shown in Fig. 4. (Note that the error free digital channel of Fig. 1 is embedded in the independent source-coding scheme of Fig. 4. In Section V-B, we will 
 A key feature of independent source-coding schemes is that the directed mutual information rate across them can be bounded by the directed mutual information rate that would arise if all random sources were replaced by Gaussian ones. To be precise, we introduce the following definition. 
 Definition 5.2: Consider an LTI system with random inputs and random initial state. If   is a state, input, or output variable of the system, then   refers to the signal that would arise in the place of  , when all inputs and initial states are replaced by jointly Gaussian random variables (or processes) having the same first- and second-order (cross-)moments, and maintaining the same statistical dependence relationships, as in the original situation;   is called the Gaussian counterpart of  . 
 Lemma 5.2: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then 
 where and are the auxiliary variables introduced in Fig. 3 (see Lemma 5.1), denotes entropy rate, and denotes relative entropy (see Appendix). Equalities in (14) hold iff and are Gaussian. 
 Remark 5.1: If the conditions of Lemma 5.2 hold and, in addition,   is jointly Gaussian, then (see [47, Chapter 5] and [10]) 
 The relevance of Lemma 5.2 lies in that the characterization of directed mutual information rate under Gaussianity assumptions is straightforward. 
 Theorem 5.1: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then 
 Theorem 5.1 provides explicit upper bounds on the directed mutual information rate across any independent source-coding scheme embedded in a stable and causal feedback loop. These bounds are, essentially, expressed in terms of the spectral characteristics of the auxiliary variables and in the scheme of Fig. 3(b). Interestingly enough, there exists a one-to-one correspondence between the SNR   of an independent source-coding scheme, and upper bounds on the directed mutual information rate across it. (Given Theorem 4.1, we can infer that there exists a link between the SNR of an independent source-coding scheme and the associated average data-rate. A precise characterization of such link will be given in Section V-B.) 
 We note that [12] also presents a relationship between directed mutual information rate and Bode-like integrals, when Gaussiandistributionsareassumed.Ourresultextends[12,Theorem 4.6] to feedback loops with arbitrary disturbance and initial state distributions. Indeed, [12, Theorem 4.6] can be recovered from the inequality in (16) upon assuming   to be jointly Gaussian distributed. 
 We end this section by showing that, for any given independent source-coding scheme, there exists another independent source-coding scheme, with the same noise color   and the same directed mutual information rate across it, such that the gap between the right-hand side of (16) and (17) can be made arbitrarily small. 
 Theorem 5.2: Consider the NCS of Fig. 1, where the source-coding scheme is independent and has a fixed noise source . Suppose that Assumption 3.1 holds and define Since and , we have from the Bode integral Theorem [44] and the definition of that 
 (19). Proceeding as in [58, proof of Lemma 10, p. 171], we conclude that the last term in (21) can be made arbitrarily small with 
 . Our last claim now follows from the last claim of Theorem 5.1. (If for some , then is not admissible since it implies and, thus, the representation of the independent source-coding scheme of Fig. 3(b) would be internally unstable.) 
 Entropy coded dithered quantizers (ECDQs) are devices that have convenient properties that make them suitable for use as building blocks when dealing with average data-rate constraints [63], [64], and [66]. In particular, we show below that ECDQs can be used to construct the noise source   that defines an independent source-coding scheme. 
 The structure of an ECDQ is shown in Fig. 5. In that figure,   is a dither signal which is assumed available at both the sending and receiving ends, the EC-ED pair is as described in 
 where   is the quantization step (a designer’s choice). The output   of the quantizer satisfies 
 Modelling quantization noise as an independent i.i.d. source is common in the signal processing literature [42]. In general, this model is not exact [26], [52] but becomes exact in ECDQs when the dither   is appropriately chosen (see, e.g., [43] and [63]). The next theorem shows that this key property remains valid when ECDQs are embedded in strictly causal feedback loops. 
 Theorem 5.3: Consider the setup of Fig. 6, where the ECDQ is as described above and has a finite quantization step  . Assume that   is a proper real rational transfer function, that the open-loop transfer function from to is single-input singleoutput and strictly proper, that the closed loop system is internally stable and well-posed when , that the signal is a second-order wss process, and that the initial state of , say , is a second-order random variable. If is such that 	, then 
 Proof: Similar to the proof of Theorem 1 in [64] (see [47, Chapter 5] for details). 
 Remark 5.2: The definition of ECs and EDs implies that Theorem 5.3 holds irrespective of how the EC-ED pair is chosen. (In particular, it holds if the EC-ED pair is omitted; see [43].) It is also worth noting that, if the dither is not subtracted at the decoder side, then only moment-independence is achieved [59]. For the remainder of the paper, the following consequence of Theorem 5.3 is relevant. 
 Corollary 5.1: Consider the system of Fig. 3(b) with and as in Lemma 5.1. If an ECDQ, with dither chosen as in Theorem 5.3 and finite quantization step  , is used as the link from to  , then the system of Fig. 3(b) becomes an independent sourcecoding scheme. 
 Fig. 7. Explicit rewriting of an independent source-coding scheme that employs an ECDQ as the link between	and	. 
 Proof: Given Lemma 5.1 and Theorem 5.3, it suffices to show that the resulting source-coding scheme satisfies Assumption 4.1 and has an invertible decoder. Since in the present situation ,theassumptionson implythat 
 Assumption 4.1 is satisfied. Also, since , its initial state is deterministic and, by definition of ECDQs, 
 and , we conclude that knowledge of is equivalent to knowledge of . The invertibility of 
 When an ECDQ is used as the link between and in the system of Fig. 3(b), the resulting scheme can be rearranged as illustrated in Fig. 7 (cf. Fig. 2). From that figure, it is clear that feedback from   to the input of   in Fig. 3(b) does not require explicit feedback around the digital channel. (Recall that we 
 Remark 5.3: Insource-codingschemes using ECDQs,theencoder and decoder share a common source of randomness (the dither). In principle, this implies that both the encoder and decoder must share information about the dither. This imposes an additional degree of implementation complexity, but to the best of our knowledge, there is no other simple way of satisfying Assumption 4.1. In practice, one can use synchronized pseudorandom number generators initialized with the same seeds. 
 1) Average Data-Rates in Independent Source-Coding Schemes That Use ECDQs: We are now in a position to present an upper bound on the average data-rate in independent source-coding schemes that use ECDQs. We start with the following result. 
 Theorem 5.4: Consider the NCS of Fig. 1, where the source-coding scheme is independent. Suppose that Assumption 3.1 holds, and that the link between the auxiliary signals and (see Fig. 3 and Lemma 5.1) is an ECDQ with dither chosen as in Theorem 5.3 and finite quantization step . Then, and there exists an 
 . Also, from Theorem 5.3 and its proof we have that	and	. Using the above, our first claim follows from the proof of Theorem 2 in [66]. The second claim follows from the first, (9) and (11). [A direct proof can be constructed by using the definition of ECDQs, and the fact that knowledge of   is equivalent to knowledge of   (see proof of Corollary 5.1), to show 
 Theorem 5.4 shows that using ECDQs inside independent source-coding schemes allows one to achieve average data-rates that are close to the absolute lower bounds established in Theorem 4.1. The worst case gap, which originates in the inefficiency of the EC and is smaller than   nats, is intrinsic to any scalar lossless coder and cannot be removed, unless one assumes   (high rate regime; [21]), uses block entropy coding (which may introduce unbounded delays; [8], [45]), or allows the coding scheme to operate in a nonstationary fashion by using time-varying policies [34], [55]. In practice the gap may be smaller than   nats [21, p. 2333], [16]. 
 Corollary 5.2: Consider the setup and assumptions of Theorem 5.4. There exists an ECDQ such that 
 Remark 5.4: Theorem 5.4 and Corollary 5.2 are only existence-type results. The implementation of EC-ED pairs inside ECDQs falls outside the scope of this paper, and we refer the reader to [47, Remark 5.10] for related remarks. 
 Corollary 5.2 provides a closed form upper bound on the average data-rate in an independent source-coding scheme that uses an ECDQ. The bound is given in terms of spectral properties of the ECDQ output , and two additional constant terms. The second term in (22), i.e., nats per sample (i.e.,   per sample), corresponds to the divergence of the ECDQ quantization noise distribution from Gaussianity and arises because ECDQs generate uniform quantization noise (not Gaussian noise; see, also, [63]–[65]). This term can also be given an alternative interpretation in terms of the space filling loss incurred when using a finite-dimensional quantizer instead of an infinite-dimensional one, with spherical quantization cells. We refer the interested reader to [18], [19], and [65] for further details.  As mentioned before, the term   (1 bit) arises due to the inefficiency of the EC inside the ECDQ. Interestingly, our result holds without Gaussianity assumptions on the external signal   nor on the initial state  . 
 Remark 5.5: If the conditions of Corollary 5.2 hold and, in addition,   is Gaussian, then a lower bound for   in (22) is given by the first term on the right hand side of (22). That is, (22) becomes tight up to   nats per sample (see Remark 5.1 and Theorem 5.4).  2) Independent Source-Coding Schemes With Memoryless ECDQs: So far, we have considered ECDQs where the EC-ED pair is allowed to exploit all past and present symbols  , binary words  , and side information  . Such ECDQs have unrestricted memory and its implementation requires the knowledge of the conditional distribution of  , given  . That distribution can be difficult to characterize. In order to simplify implementation, it is common to consider ECDQs without memory (see, also, [64]). 
 Definition 5.3: An ECDQ is said to be memoryless iff the associated EC-ED pair is such that 
 When using a memoryless ECDQ, the EC can only exploit the knowledge of to encode . Thus, (8) must be replaced by . Again, it is possible to design coding policies such that [8] (compare with (9)) 
 We now present a definition and two results that allow one to state the counterpart of Corollary 5.2 for the case of independent source-coding schemes that use memoryless ECDQs. 
 Theorem 5.5: Consider the setup and assumptions of Theorem 5.4. If the ECDQ is memoryless, then and there exists an EC-ED pair such that 
 Theorem 5.6: Consider the NCS of Fig. 1, where the sourcecoding scheme is independent. If Assumption 3.1 holds, then 
 Proof: The result follows proceeding as in the proof of Theorem 5.1 (see details in [47]). 
 Corollary 5.3: Consider the setup and assumptions of Theorem 5.4. There exists a memoryless ECDQ such that 
 Remark 5.6: In analogy to Remark 5.5, we note that if the conditions of Corollary 5.3 hold, and is Gaussian, then 
 The EC inside a memoryless ECDQ uses less information to encode   than the EC inside an ECDQ with unrestricted memory. As a consequence, average data-rates achievable with memoryless ECDQs will be, in general, larger than those achievable with ECDQs that have unrestricted memory. This conclusionis consistentwithCorollaries 5.2and 5.3,as Jensen’s inequality reveals. However, the conclusion is pessimistic when ECDQs are used inside independent source-coding schemes. In this case, Theorem 5.2 guarantees that the rate penalty, as measured by the gap between the right hand sides of (22) and (24), incurred when using a memoryless ECDQ instead of an ECDQ with memory, can be made arbitrarily small if an appropriate choice for the auxiliary filters and is made. Hence, without being unduly conservative, it suffices to study the SNR   to give upper bounds on the average data-rate of independent source-coding schemes, irrespective of whether they use ECDQs with unrestricted memory or not. 
 Consider the NCS of Fig. 1 when the source-coding scheme is independent and uses an ECDQ as the link between the auxiliary signals and (see Fig. 7). For this setup, the results of Sections V-A and V-B allow one to restate control problems involving average data-rate constraints as control problems involving stationary SNR constraints. This enables one to design NCSs subject to average data-rate constraints in a systematic fashion that uses standard quadratic optimization methods. For instance, if performance is measured by means of the stationary variance   of the error signal  , then, irrespective of whether the ECDQ has unrestricted memory or not, the minimal average data-rate needed to achieve a performance level , say , satisfies (see Corollaries 5.2 and 5.3) 
 , and EC-ED pairs that guarantee stability in an appropriate sense, and is the minimal stationary SNR of the source coder that allows one to achieve the performance level  , i.e., 
 Once   is characterized (which, in principle, is a standard quadratic control problem), one readily obtains a bound on  (see, also, Section VI). 
 The framework provided here is conservative since it is based on inequalities that are not tight in general.  However, the framework provides bounds that are guaranteed, by construction, to be achievable with conceptually simple building blocks, and do not rely on any asymptotic approximation. As illustrated above, the framework also opens the door to use standard synthesis methods to deal with average data-rate limits in control system design, as explored in [47]–[49]. These are the main features that distinguish our approach from the literature surveyed in Section I. 
 We now illustrate the approach outlined in Section V-C by studying bounds on the minimal average data-rate for stability, 
 i.e., we consider   in (25). The case of finite   is omitted due to space constraints (see [48]). 
 We start by studying the interplay between constraints on the source coder SNR   and stability. By virtue of Theorem 5.3, we focus on the feedback system of Fig. 4, and use the following notion of stability. 
 , where , is the system state at time instant , , where is a second-order random variable, and are constant matrices of appropriate dimensions, and is a second-order wss process. The system is said to be mean square stable  (MSS) iff there exist finite   and a finite and positive semi-definite both not depending on , such that 
 small with a sufficiently large (and cannot be made equal to zero since, by assumption,). Well-known results [58, Lemma 10, p. 171] allow one to conclude that there exists   such that the gap in   is arbitrarily small. The 
 result, thus, follows. (The existence of the family also guaranteed by the results in [58].) 
 Theorem 6.1 states that, for independent source-coding schemes, the minimal SNR that is compatible with MSS, i.e., 
 , is only a function of the unstable poles of	. Hence, for any given  , the condition	is necessary 
 and sufficient to be able to find , and a noise variance  (equivalently, a quantization step ) such that the resulting NCS is MSS and the SNR satisfies . 
 Corollary 6.1: Consider the setup and assumptions of Theorem 6.1. If  , then the stationary variance   of the error signal   grows unbounded. 
 We see from Corollary 6.1 that the study of conditions on  for MSS are insufficient to give performance guarantees. This fact is unsurprising, and consistent with results in [15] and [51]. 
 We now return to the NCS of Fig. 1 when the source-coding scheme is independent and uses an ECDQ. Given the definition of an independent source-coding scheme, it follows that the notion of MSS described in Definition 6.1 is still valid in this setting. It is also clear that, provided Assumption 3.1 holds, 
 Corollary 6.2: Consider the setup and assumptions of Theorem 5.4. Then, irrespective of whether the ECDQ has memory or not, the following holds. 
 (a) The minimal average data-rate compatible with MSS, i.e.,   as defined in (25), satisfies 
 (a)	Consider the case of ECDQs with unrestricted memory.Equation (22) and Theorem 5.2 yield 
 The upper bound in (28) follows upon using Theorem 6.1 in (29). The lower bound follows from [34, Theorem 2.1]. 
 Corollary 6.2 establishes lower and upper bounds on the minimal average data-rate that is compatible with MSS in the considered NCS, when an independent source-coding scheme is employed. A feature of our proposal is that ECDQs with unrestricted memory do not provide any advantage over memoryless ones (at least from a MSS point of view; see, also, discussion at the end of Section V-B). This is relevant in practice since the implementation of ECDQs with unrestricted memory is computationally prohibitive. Indeed, in order to design an independent coding scheme that achieves stability at an average data-rate smaller than  , it suffices to use a memoryless ECDQ with sufficiently large quantization step  , an EC-ED pair designed using Huffman coding, and filters and that, for the situation studied in Section VI-A, guarantee MSS at SNRs sufficiently close to  . By doing so, however, the performance of the NCS will be compromised (see Corollary 6.1). This is consistent with results in [34] and [36]. 
 The results of Corollary 6.2 show that, when an independent source-coding scheme is employed in the NCS of Fig. 1, MSS can be achieved at average data-rates that are guaranteed to be no larger than the absolute bound identified in [34] and [54] plus  nats per sample. This 
 extra rate is, in our view, a fair price to be paid if one constrains oneself to the conceptually simple source-coding schemes considered in this paper. We note however that the upper bound in (28) is a worst case upper bound. As mentioned earlier, in practice one can expect to achieve MSS at rates no larger than   (see [16] and [21]). 
 Our results can also be used to provide upper bounds on the average data-rate that is needed to achieve MSS, when memoryless source-coding schemes are employed in the considered 
 ECDQ, then the resulting source-coding scheme has no memory and it is easy to show that   and the EC-ED pair can be chosen so as to achieve MSS at an average data-rate satisfying 
 Relation (30) can be contrasted with the results of [55, Section VI] (even though [55] focuses on a different notion of stability). That work shows that there exist memoryless encoders that guarantee stability with bounded (but otherwise unknown) data-rates, whist (30) provides a computable upper bound on the minimal average data-rate compatible with MSS. 
 Thispaper hasstudied acontrol problemwherean LTIsystem is designed assuming transparent feedback and, at a later design stage, a unity signal transfer function source-coding scheme is tobe usedsoastominimizetheeffectsthatdata-rate limitsinthe feedback path have on closed loop performance. To address this problem, we have focused on a class of source-coding schemes and, by doing so, we have established a bridge between information and control theories. A key result of our work is that, for the considered class of coding schemes, average data-rate limits can be enforced by imposing signal-to-noise ratio constraints in a related additive noise communication channel. As an application of our results, we studied the interplay between stabilityand average data-rates in the considered setup. For that problem, the proposed class of coding schemes was shown to achieve mean square stability at average data-rates that are guaranteed to be less than 1.254 bits per sample above the absolute minimum established in [34]. 
 We refer the reader to [48] and [49] for applications of the framework presented in this paper to problems beyond stabilization. A key open problem not addressed in this work is how to incorporate average data-rate limits into control problems using causal (but otherwise unrestricted) source-coding schemes. 
 The following definitions and results are standard and can be found in [8]. Unless otherwise stated, all variables are assumed to be random variables (RVs) with well defined (joint) probability density (or mass) functions (PFs). The PF of is denoted . refers to the conditional PF of , given . denotes mean with respect to ; stands 
 If and are discrete RVs, then we use to denote the (conditional) entropy of (given ). The definitions are analogue to the continuous case. 
 (Conditional) mutual information between discrete RVs is defined as in the continuous RV case. 
 The relative entropy between and (or divergence of the distribution of with respect to that of ) is defined via . Given joint distributions 
 If is the Gaussian counterpart of (see Definition 5.2), then . If , are the Gaussian counterparts of , , then 
 The entropy rate of a stochastic process is defined via . A useful fact is the following. 
 PSD , then	(equality iff, in addition, is asymptotically Gaussian; see [30, Lemma 4.3]). 
 ﻿This paper presents novel results on perfect reconstruction feedback quantizers (PRFQs), i.e., noise-shaping, predictive and sigma-delta A/D converters whose signal transfer function is unity. Our analysis of this class of converters is based upon an additive white noise model of quantization errors. Our key result is a formula that relates the minimum achievable MSE of such converters to the signal-to-noise ratio (SNR) of the scalar quantizer embedded in the feedback loop. This result allows us to obtain analytical expressions that characterize the corresponding optimal filters. We also show that, for a fixed SNR of the scalar quantizer, the end-to-end MSE of an optimal PRFQ which uses the optimal filters (which for this case turn out to be IIR)decreases exponentiallywith increasing oversampling ratio. Key departures from earlier work include the fact that fed back quantization noise is explicitly taken into account and that the order of the converter filters is not apriori restricted. 
 Index Terms—Differential pulse code modulation, optimization, quantization, sigma-delta modulation, source coding. 
 HE term feedback quantizer (FQ) refers to a class of analog-to-digital converter (ADC) architectures wherein a scalar quantizer is placed within a linear feedback loop. Well-known examples of FQs include  -modulators, DPCM converters [1] and sigma-delta modulators [2]. The latter schemes have been very successfully applied in a number of areas, including audio compression [1], [3], oversampled A/D conversion [2], [4], subband coding [5], digital image half-toning [6], power conversion [7], and control over networks [8]. 
 Fig. 1 depicts a general FQ configuration. In this scheme,  may take the form of a nonuniform or a uniform quantizer [9], the latter being either dithered or undithered [10]. 
 The filters   in an FQ system allow one to exploit the predictability of the input signal so as to reduce the variance of  . When compared with simple PCM 
 smaller quantization step. The error-feedback filter   opens the possibility of spectrally shaping the effect of quantization errorsontheoutput.Inthisway,onecanallocatemoreofthequantization noise in the frequency bands where it is less harmful from a user’s point of view. Accordingly, it is convenient to use a frequency weighted error criterion, via an error frequency weighting filter  , and to focus on the frequency weighted MSE (FWMSE) (see discussion in [3] and [11]). 
 For the sake of generality, we consider the possible use of a clipper before . This device limits the value of the quantizer input signal so that if , and if 
 , where is the saturation threshold of the clipper. Thisclippingtechniquecanbeusedtokeep fromoverloading, which is helpful in reducing limit-cycle oscillations (idle tones) in an FQ with high order filters, as proposed in [4]. On the other hand, if we chose   to be sufficiently large, then  , and the clipper has no effect on the system. 
 If the characteristics of   and the spectral properties of the input signal   are known, then the design of an FQ converter that minimizes the variance of   amounts to choosing the filters 
 It is often desirable that a converter is transparent to the system in which it is inserted. This corresponds to the widespread paradigm in which the coding scheme adapts to the application that employs it, without need to modify the latter. A transparent converter is one whose signal transfer function (i.e., the transfer function from input   to output  ) is unity at the frequencies of interest. The design of such perfect reconstruction feedback quantizers (PRFQs) constitutes the main topic of the present work. PRFQs are characterized by the property that, in the absence of quantization effects, there is no frequency weighted reconstruction error, i.e.,. If we denote the power spectral density (PSD) of , then it can be seen from Fig. 1 that the latter holds if and only 
 Thus, in the design of an optimal PRFQ converter, only two degrees of freedom are available: the filters   (or, alternatively,  ). 
 To the best of our knowledge, existing results on optimal filter design for PRFQ converters either consider finite order filters [2], [12], [13], assume (or require) that the variance of the signal   is much smaller than that of   [1], [4], 
 [14], [15], or have a heuristic component in the optimization [2], [3],[13],[16]–[19].Theonlyexplicitanalyticalexpressionscurrently available for the optimal performance (and corresponding filter frequency responses) of a PRFQ converter are those given in [14]. However, the assumption of negligible fed back quantization errorsused in [14] makes these filters suboptimal. Indeed, as we will show in the sequel, there exist situations where the filters proposed in [14] yield large fed back quantization error, even when a fine step scalar quantizer is used. In these situations, not only is the main assumption in [14] violated, but also an FWMSE much larger than predicted can result due to excessive quantizer overload (see, e.g., [2] and [13]). 
 In the present paper, we will show how to design optimal PRFQ converters. For this purpose, as in [12], [14], and [16]–[18], we model the scalar quantizer as a linear device that introduces additive white noise whose variance is proportional to that of the signal being quantized. A key departure from [14], however, is that we explicitly take into account fed back quantization noise in the feedback loop. Our main contributions are: 
 i) We derive one-parameter equations that relate the minimum achievable frequency weighted MSE to the signal-to-noise ratio (SNR) of  ; ii) We show, within our model, that the frequency weighted MSE in an optimal PRFQ where the SNR of   is fixed decreases exponentially with oversampling ratio; and iii) We derive equations that characterize the optimal filters for a PRFQ. Our results can be applied to any given number of quantization levels, and to almost arbitrary input spectra and frequency weighting criteria. 
 The remainder of this paper is organized as follows: In Section II, we present our analysis model for PRFQ converters. In Section III, we formulate the associated optimization problem. Section IV presents a one-parameter characterization of the solution. In Section V we discuss the main properties of an optimized PRFQ. The case of oversampled FQ is analyzed in Section VI. Section VII discusses the relationship to previous results and highlights the importance of taking account of fed back quantization noise. Section VIII presents simulation results. Section IX draws conclusions. (For ease of exposition, all proofs of our results are included in the Appendix.) 
 We write “iff” as a short hand expression for “if and only if.” The sets of all complex-valued square integrable and absolutely integrable functions on are denoted by and  , respectively. Given , , we adopt the standard inner product	, where denotes complex conjugation. We denote the corresponding 
 -transform. If is a transfer function, then we use the short hand notation to refer to the associated frequency response 
 . If is a set, then we write “a.e. on  ” (almost everywhere on ) for “everywhere on , except on a zero Lebesgue measure subset of .” We use to denote the variance of a given wide sense stationary (w.s.s.) random process , having PSD . We recall that if has zero mean, then 
 where is any given function and denotes any arbitrary and positive bounded value. For later use, we also recall the following definition. 
 In this section, we discuss some of the main aspects of feedback quantization. We also describe the analysis model and the constraints to be considered later in the search for the optimal filters. 
 We begin by presenting the equations that describe the behavior of the PRFQ shown in Fig. 1. 
 1) Quantization and Clipping Errors: From Fig. 1, the quantization error   is given by 
 such that, if , then is said to be overloaded. When the quantizer is not overloaded, then is only granular quantization error, namely , which can be bounded as , , for some 
 (see, e.g., [9]). For example, if is a symmetric, uniform, nondithered quantizer with levels and quantization interval , then one needs in order to obtain . 
 As outlined in the introduction, the clipper in Fig. 1 can be used to keep   from overloading. For simplicity, we will only consider here two possibilities, namely, that  , or else 
 . The former choice guarantees that   does not overload, since clipping error, defined as 
 using clipping is that, unlike overload errors, clipping errors are not fed back into through . This helps to avoid large limit-cycle oscillations arising from the overload of  , see [4]. Since such oscillations are not part of the analysis model we will use, their occurrence could increase the FWMSE significantly above the value predicted by the model. 
 which reveals that differs from by the sum of the quantization and clipping errors. 
 Notice thatthese equations are exactand require no assumptions on the signals involved. From (8b) one can see that corresponds to the signal transfer function (STF), from to , of the converter. Similarly, the product is the transfer function for quantization errors, usually referred to as the noise transfer function (NTF) of the converter . The term   will play a crucial role in the derivation of the optimal 
 Output (BIBO) stable iff for any input sequence   satisfying allthesignalsinthe converterarebounded. 
 , and, thus, all the other signals in the converter are bounded. On the other hand, if , then can be written as 
 If the quantizer has a finite number of quantization levels, then   is bounded. If is stable and is minimum-phase, then it follows from (9) that is bounded. This, in turn, guarantees that   and all the other signals in the converter are bounded [see (4) and (8)]. Summarizing, if all the filters in Fig. 1 are stable, and if   has no zeros on or outside the unit circle, then the resulting PRFQ is BIBO stable. 
 In addition, if and are stable, then the norm of their impulse responses, namely and , are bounded. 
 Therefore, for a uniform quantizer with quantization interval  , it suffices to have   or more quantization levels in order to avoid clipping or overload errors. 
 1) Input Spectrum and Frequency Weighting: The error weighting filter   in Fig. 1 models the impact that reconstruction errors have at each frequency. This “performance assessment” filter is application dependent, and is assumed to be stable and given. The input signal  is a zero-mean w.s.s. stochastic process  with known PSD and finite power, i.e.,  . In 
 order to simplify our subsequent analysis, we shall further restrict and to satisfy the following: 
 Assumption 1: The product is a piece-wise differentiable function having at most a finite number of discontinuities and satisfying , . In addition, is such that one  of the following conditions holds. 
 such that . Furthermore, if denotes the set of noncontiguous and nonoverlapping intervals in such that 
 We note that the above is a rather weak constraint, since conditions i) and ii) include almost any product of practical or theoretical interest. In particular, condition i) covers all the cases where the product has no zeros on the unit circle. In turn, condition ii) is satisfied if is zero over any interval on   having nonzero measure, or if is rational and has zeros on the unit circle. 
 2) The Quantizer: We shall focus our analysis on the effect that granular quantization errors have on the FWMSE. For this effect to closely represent the actual FWMSE, we need to assume the following: 
 Assumption 2: The variances of overload and clipping errors are negligible, i.e. 
 In addition, and as stated in the introduction, we will adopt an additive white noise model for  . This model is widely used for the analysis and design of data converters (see, e.g., [1]–[5], [12]–[14], [16]–[18], and [20]–[22]), being usually described as follows. 
 Assumption 3: The sequence of quantization noise   is a zero-mean w.s.s. random process, uncor- 
 The above additive white noise model, although not exact, is, in general a good approximation when a signal with a smooth probability density function (pdf) is quantized with many levels and negligible overload (in the sense of Assumption 2 ), see, e.g., [2]. The model can be made exact, even for few quantization levels, by utilizing a uniform scalar quantizer with either subtractive or nonsubtractive dither6, provided quantizer overload does not occur, see [10]. As discussed before, one way to achieve this is to use a quantizer with a sufficiently large number of quantization levels, so as to satisfy (10). In this case, if the quantization interval is   and the dither sequence 
 whitens  , makes uncorrelated to when is not overloaded and is bounded as , then any number of levels greater than or equal to will make Assumption 3 hold exactly. If a smaller number of quantization levels are employed so that  , then the use of dither with the same characteristics as before, together with clipping (i.e., setting  ), will also make   satisfy Assumption 3 exactly. 
 seeFig.1.Thisequationdescribestheeffectof on through the feedback path. However, if the scalar quantizer has a finite and fixed number of quantization levels, then another link between these two variances needs to be considered. In order to model this relationship, we will use the fixed SNR model employed in, e.g., [12], [14], [16], [17], and [21]. 
 Assumption 4: For a fixed number of quantization levels, the variance of quantization errors is proportional to the variance of the signal being quantized, i.e., there exists   such that 
 If no clipping is used (i.e., if ), then corresponds exactly to the SNR of . If , then is a good a approximation of the SNR of when (11b) in Assumption 2 holds. In our model, is assumed fixed and given. Strictly speaking,   depends on the pdf of , on the number of quantization levels of , and on how quantization thresholds and levels are distributed along the dynamic range of  . In practice, for a given number of quantization levels,   should be chosen such that the dynamic range of   is used efficiently, whilst en- 
 6Here and in the sequel, we assume the dither is such that n is white and uncorrelated with x when Q is not overloaded. 
 suring a low probability of quantizer overload or clipping. For example,for the often cited uniform quantizer with levelsand loading factor  equal to 4 we obtain(assuming that   has a uniform pdf and neglecting overload errors). We note that for large  , and provided overload errors are negligible, a quadratic relationship between and holds for most types of scalar quantizers (see, e.g., [9]). This is indeed the well-known rule of “6 [dB] reduction of quantization noise variance per additional bit of quantizer resolution.” 
 In the sequel, we refer to the model of PRFQ determined by Assumptions 2, 3, and 4 as The Linear Model. Summarizing, the Linear Model is exact if the PRFQ uses a dithered quantizer having enough quantization levels to avoid overload. If not enough quantization levels are available and dither is used jointly with clipping, then the model is exact in predicting the effects of granular quantization errors, and is a good approximation in predicting the total FWMSE if Assumption 2 also holds. If the scalar quantizer is undithered, has a small quantization interval (relative to  ) and enough quantization levels to avoid overload, then the Linear Model can be expected to yield a good approximation of the total FWMSE. Perhaps surprisingly, the Linear Model turns out to predict with remarkable accuracy the FWMSE of an optimal PRFQ when few quantization levels and clipping are used with a loading factor big enough to satisfy Assumption 2, even without dither, and even for a 1-bit quantizer. This can be observed from the simulation results presented in Section VIII. 
 We shall restrict the search for the optimal filters to those satisfying the following constraint. 
 As foreshadowed in Section I, the first constraint enforces perfect reconstruction. As discussed in Section II-A-3, the stability constraints on   are a necessary condition for the converter to be BIBO stable. The additional requirement on  , namely strict causality, is needed for the feedback loop in Fig. 1 to be well defined (see, e.g., [2, ch. 4]). Notice that we will not a priori require   to have zeros only inside the open unit disk. Instead, we will show that the latter property arises naturally from the solution of the design optimization problem. 
 An additional constraint on   arises from the value of  , as explained next. The ratio between the variances of and 
 One can see from the above that if , then any pre-filter or scaling of the quantization intervals of will yield   , thus, making large overload (or clipping) inevitable. This would increaseoveralldistortion and, if no clipping is used, may lead to large limit-cycle oscillations. We, thus, conclude that the use of feedback imposes the following constraint. Constraint 2: 
 If the above constraint is met, then	can be found by substituting (13) into (14). This gives 
 Given the model described in the previous section, we can now evaluate the quantity that we aim to minimize, namely, the frequency weighted mean squared error (FWMSE). From (8c), and Assumptions 2 and 3, it follows that the FWMSE is given by 
 of the FWMSE in the Linear Model can be stated as follows. Optimization Problem 1: For given  , and for given and   satisfying Assumption 1, find the frequency responses , and   satisfying Constraints 1 and 2 that minimize 
 The following proposition allows us to further reduce the number of unknowns in (16) by characterizing the optimal 
 Itisconvenienttorewrite(17)morecompactlybyintroducing the following change of variables: 
 proof of Proposition 1 in the Appendix ), Constraint 2 is satisfied iff . In addition, a stable and strictly causal 
 (i.e., one satisfying Constraint 1) always leads to a function	, see (20), which satisfies 
 This result follows directly from Jensen’s formula [23] (see also the Bode Integral Theorem in, e.g., [24]). 
 On the other hand, as we shall see in Section IV, if Assumption 1 holds, then the optimal within the set of functions described by (22) and the requirement turns out to be piece-wise differentiable on , has at most a finite number of discontinuity points, and satisfies 
 Under these conditions, it is always possible to find a stable and strictly causal filter such that approximates   arbitrarily well on , as stated in the following lemma. 
 , that it has at most a finite number of discontinuity points and that it satisfies (23). Then, for every , there 
 Optimization Problem 2: For given and known   and for   satisfying Assumption 1, find 
 characterizes the optimal feedback filter, say  , via (20) (see also Lemma 1). In the following section, we will show how to solve this optimization problem. 
 It would be desirable to provide an explicit analytical solution to Optimization Problem 2. Unfortunately, and as will become apparent in the discussion later, developing a closed form solution, for arbitrary functions , appears infeasible. Nevertheless, we can provide a one-parameter characterization of the optimal function in (24) as follows. 
 Theorem 1: For any given satisfying Assumption 1, and for any , the function in (24) belongs to the one-parameter family of functions , where 
 Here,	, is the lower bound of feasible	’s, and	, if it exists, is the unique scalar such that 
 Note that the above result provides an explicit analytic expression for , once the optimal , defined as 
 Theorem 1 can be used to develop an efficient algorithm to solve Optimization Problem 2. The key point is that substitution of (25a) into (21) changes the search space from the infinite-dimensional set   to the real interval  . More precisely, Optimization Problem 2 is turned into the simpler problem of finding the minimizer of the single variable nonconvex scalar function 
 , (and hence the solution of Optimization Problem 2) is unique. Furthermore,   can be obtained by finding the root of a scalar, convex, and monotonically decreasing function. 
 Moreover, it follows from Theorems 2 and 3 that, for any satisfying Assumption 1, and for any  , the global minimizer of (27) exists and is unique . In addition, these results guarantee that   can be easily found by solving (28), via, for example, the bisection algorithm [25], or any other convex optimization method [26]. 
 We can now express and the minimum achievable FWMSE, namely , in terms of , and . Indeed, combining (28) and (25a) with (21) yields (after some algebraic simplification) that 
 It can be seen from (31a) that   is a monotonically increasing function of. In view of Theorem 3, this implies that, as expected, is monotonically decreasing with increasing  . As a consequence, the converse of Optimization Problem 1, namely, finding the optimal filters and minimum required SNR of   for a given target distortion, can be solved by using (28) and (31). Moreover, since the RHS of (31a) is a concave, monotonically increasing function of  , this parameter can be easily found by using standard iterative algorithms, as in the original optimization problem. 
 It is also interesting to note that (28) and (31a), which relate and via the parameter  , have a structure akin to the well-known reverse water-filling equations (see, e.g., [27, pp. 108–123], and [28]). The latter characterize the rate-distortion function for Gaussian sources. 
 To summarize, we have given an explicit analytic expression for the optimal and , once has been determined. Furthermore, we have shown that the parameter   always exists, is unique, and can be easily found using simple numerical methods. 
 In the following sections, we will provide additional insight into the consequences of theseresults, as well as into some properties of optimal PRFQs, 
 In the sequel, we say that a PRFQ is optimal or optimized if its filters	,	satisfy (19) for negligibly small values of and	, and	is such that  , a. e. on	, with	as defined by (24). 
 It follows from Theorems 2 and 3 that, for any given  satisfying Assumption 1,   in (25a) describes the family of all noise shaping characteristics that are optimal for some  . 
 As we will show, adjusting   from 0 to   (equivalently,  from   to 0) allows one to undergo a smooth progression from “full” noise-shaping to no noise-shaping, in an optimal manner. An example of this progression is shown in Fig. 2. Note in this figure how   (solid lines) approaches a unit transfer function as   (the quantizer SNR for which   in the figure), becomes smaller (and gets larger). It can also be observed that approaches the inverse of   is increased. 
 Such asymptotic convergence does indeed take place in general, as the following theorem shows: 
 Theorem 4: For any   satisfying Assumption 1, the functions   defined in (25a) converge uniformly to 
 as  . Similarly, for any function satisfying condition i) in Assumption 1, the functions defined in (25a) converge uniformly to 
 ( ), which reduces the PRFQ to a PCM converter. In view of (30), this no-noise shaping scenario is asymptotically optimal as  . In turn,   defined in (33) corresponds to the full whitening feedback filters proposed in [1], [14], [15]. From (29) and (33),   is optimal iff  . See also the discussion in Section VII. 
 1) The Output of the Quantizer: By looking at Fig. 1 and using Assumption 3, we find that the PSD of in an optimized PRFQ is given by 
 (31) and (28), we conclude that the variance of the quantization noise in an optimized PRFQ is given by 
 (25a) has been used. Substitution of (49a) and (28) into this expression leads to 
 4, the output of the quantizer in an optimized PRFQ is white. This suggests that near optimal coding of the quantizer output can be achieved with a memory-less entropy coder. 
 2) The Frequency Weighted Reconstruction Error: The PSD of the frequency weighted reconstruction error is given by tion of (18) into the above yields	. 
 (37) Thus, we conclude that the frequency weighted quantization error in an optimized PRFQ is not white. This fact stands in stark contrast to the conclusions reached when the FQ filters are optimized without the perfect reconstruction constraint (1), see, e.g., [22]. It also differs from the result obtained when the feedback filter is optimized ignoring fed back quantization error, as in [14] and [15]. Note that, as   is made larger,   not only becomes smaller, but its PSD asymptotically approaches11 a constant function over the frequencies 
 It is well known that oversampling (i.e., sampling a bandlimited continuous-time signal at a frequency above its Nyquist rate) allows one to achievea smaller MSE error for a given,fixed number of quantization levels. For instance, the MSE of simple scalar quantization (without feedback) is known to decrease as  , see [29], where is the oversampling ratio, given by the order of the feedback filter (see also recent work in [20]). From a rate-distortion viewpoint, the inversely polynomial error decay of this error estimate is ”too slow” to compensate for the increase in the overall bit-rate due to oversampling (which is proportional to ). To be more precise, let us consider a scalar quantizer with quantization levels, where   denotes the quantization resolution in bits per sample. If the additional bitrate caused by oversampling was utilized instead to increase  , then the MSE would decay as  , i.e., exponentially12. 
 A faster decay of the MSE of oversampled FQ with   can be achieved by selecting a different feedback filter (with possibly different order) for each oversampling ratio. An example of such a family (of 1-bit   converters) was given in [31]. Here, the continuous-time reconstruction error can be uniformly bounded by   is independent of . This bound guarantees an MSE that decays with as , which is faster than any inverse polynomial, but still far from 
 12Strictly speaking, this only holds for signals whose pdfs have finite support. Indeed, it has been shown that for several infinite support pdfs, the MSE of uniform quantization decreases asymptotically with b not faster than (ln2) , where a > 0 is a constant independent of b, see [30]. 
 exponential. Based on this result, the family of 1-bit converters reported in [32] achieve an MSE that is , i.e., exponentiallydecayingwith increasing . Notably, the results in [31] and [32] were obtained using an exact, deterministic model of quantization. 
 We will next show that, within the Linear Model, if the optimal infinite order filters characterized in Section IV are used for each value of , then one can achieve an exponential decay of   with the oversampling ratio, provided   is kept constant. If the input sequence is obtained from sampling a band-limited analog signal, oversampling would cause   [defined in (20)] to vary with . To capture this effect, we replace 
 In (38), denotes the square root of the PSD of the frequency weighted input without oversampling, and . Notice that , that is, the total power of (in units of variance per sample), remains constant for all . This ensures a uniform comparison basis for the distortion figures. 
 corresponds to the output-SNR of . Interestingly, it is possible to establish a precise “exchange” formula for and . Indeed, in terms of minimal achievable distortion, the effect of increasing oversamplingis equivalent to an exponential increase in the output-SNR of . This is shown in the next theorem: 
 Theorem 5: Under the Linear Model described in Section II-B, for any function , and for any  , the minimum achievable FWMSE satisfies: 
 If we assume that depends exponentially on the number of bits per sample, then Theorem 5 suggests an FWMSE that decays exponentially with , provided the Linear Model holds and that optimal filters , and (characterized by (18), (25a) and (28)) are employed for each . The following simple example illustrates this idea: 
 is constant , without oversampling. For this setup, the optimal for our model of PRFQ is ( 
 ), i.e., a PCM converter. From (21), the minimum FWMSE without oversampling (i.e., with ) becomes 
 where . To analyze oversampling behavior of in this case, we apply Theorem 5 to the above expression. 
 for all . Note that, to achieve (42), needs to be synthesized according to (31b) and (20). Therefore, for this example, the MSE of an optimized PRFQ with fixed exhibits an exponential decay with the oversampling ratio (since, by definition,  ). 
 being a uniform quantizer with many levels and operating with a loading factor of 4), then (42) becomes 
 posing that Assumptions 3 and 4 hold, we obtain from (43) that is lower and upper bounded by terms proportional to 
 . For loading factor values of 6, 10, and 20, the exponent in the latter expression changes to  , respectively. 
 The next theorem shows that the exponential decay of the FWMSE obtained in the example above can be extended to arbitrary (band-limited) input signals and frequency weighting criteria. 
 Thus, under the Linear Model, we have that the FWMSE of an optimized PRFQ decays exponentially with . 
 Remark1: WerecallthatTheorem6isexactwithintheLinear Model described in Section II-B. Here it is convenient to present some further observations regarding the validity of that model when the oversampling ratio tends to infinity, for different implementations of a PRFQ. 
 As already mentioned in Section II-B, if   is bounded and a sufficiently large number of quantization levels to avoid overload is used together with dither, then the Linear Model is exact. Nevertheless, there is no guarantee that the number of necessary quantization levels to avoid overload remains constant as   increases. If such number increases with , then can only be kept constant by increasing the number of quantization levels in the quantizer. 
 If the number of quantization levels is insufficient to avoid clipping/overload errors, and if dither and clipping are used with a fixed loading factor, then there exists a certain finite value of   beyond which Assumption 2 is violated. This arises from the fact that, for any fixed loading factor, the effect of clipping errors in the output does not decay with  , thus, becoming the dominant component in the FWMSE for sufficiently high oversampling ratios. Further reduction of the FWMSE would then require one to balance clipping and granular quantization errors by increasing the loading factor. If the number of quantization levels is fixed, this would necessarily reduce the value of  , clearly increasing the component of the FWMSE due to granular quantization errors . Nevertheless, if clipping and dither are used (with  ), then the Linear Model and Theorem 6 is exact in describing the FWMSE due to granular quantization errors. 
 If one tried to optimize the filters of a PRFQ neglecting fed back quantization noise, i.e., by trying to minimize 
 Theorem 1]. This corresponds to the result obtained in [14], whichwasrestrictedtothecaseswhere .Forthecase 
 , the noise transfer function magnitude is also equivalent to that derived in [15]. The latter is optimal in the sense of minimizing the ratio , but not in the sense of minimizing for a fixed quantizer SNR . 
 As shown in Theorem 4, , in general, does approach  as . One can then expect to be near optimal in situations where , see (16). The latter is often satisfied at high bit-rates (i.e., when many quantization levels are available). However, for any given number of quantization levels, it is easy to find practical situations where is such that is comparable to (or greater than) . More precisely, from (22), and recalling that (see Appendix B), one can show that, if over a set of frequencies in with measure , where is some positive 
 whose magnitude becomes significantly small (in relative terms) over certain frequency bands. (An example is included in Section VIII.) A direct consequence is that, for these cases, and in view of (16), trying to match  to will yield a performance far from optimal, also increasing the risk of incurring large limit-cycle oscillations if no clipping is employed (see, e.g., [2] and [13]). 
 approaches   was already observed in [12]. Several heuristic solutions have been proposed since then (see, e.g., [2], [3], [13], [15], [17], and [18]). In contrast to these approaches, the method derived in the present paper allows one to characterize the true optimal filters, by explicitly taking into account   in the cost functional to be minimized [see (16)]. Our method not only guarantees that  , but also yields the actual optimal filters. Our proposal also has the advantage of being applicable to arbitrary input spectra and frequency weighting functions, regardless of how small the quantizer SNR 
 To illustrate our results, we have designed the filters of a PRFQ aimed at digitally encoding audio signals in a psychoacousticallyoptimalmanner.Thedetailsofthesimulationmodel, as well as the results of both the simulations and the numerical optimizations are given later. 
 The PSD of audio signals was modeled as unit-variance zero mean white Gaussian noise filtered through 
 nitude of the frequency response of is depicted in Fig. 3 (solid line). The frequency weighting filter considered had a frequency response magnitude which approximated the psychoacoustic curve derived in [3, Table 1], thus, modeling the sensitivity of human hearing to noise . The corresponding frequency response is plotted with dotted line in Fig. 3 (the sampling frequency is 44.1 [kHz]). The resulting 
 for these   is also shown in the same figure (dashed line). For this choice of  , and in view of (46), one could expect the norm of a full whitening feedback filter to be very large. This is indeed the case:  . Thus, the suboptimal feedback filter characterized by (45) requires the use of a scalar quantizer with at least 18 bits in order to become feasible (see Constraint 2). 
 In the simulations, was chosen to be a uniform mid-rise quantizer with quantization interval . Several values were considered for the simulations, calculated as 
 denotes the loading factor. Two different loading factors were considered: 4 and 6. The latter choice yields a slightly lower   than the usual loading factor of 4. However, this regime has the benefit of making overload errors smaller and more infrequent. As the simulation results will show, for our choices of and , this more conservative loading factor yields lower 
 Fig. 3. Frequency response magnitudes for	(z) (solid line), P(z) (dotted line) and g(!) = j	(e	)P(e	)j (dashed line). 
 For each   (and corresponding two values for  , one for each loading factor), the filters of the converter were designed according to the following: 
 These functions were then approximated  with rational IIR transfer functions , (of order 7) and 
 An appropriate value for the parameter in (18) was chosen via , see (35), assuming 
 For each combination of and , the resulting PRFQ converter was simulated utilizing two different architectures. 
 1, with	having (virtually) infinitely many levels. Thus, for all	(neither clipping nor overload er- 
 Overloading and Clipped : Here, has levels, which yields a scalar quantizer with a finite input dynamic range . As a consequence, any value would overload (if ) or produce clipping error (if ). To avoid large limit-cycle oscillations, this variant was simulated using clipping (i.e., 
 Each simulation with the nonoverloading PRFQ comprised 100,000 samples. For the overloading converter, five 100,000 samples simulations were performed for each combination of 
 The results of the numerical optimizations and the simulations are discussed next. 
 Comparison Between   and the Rate-Distortion Function: The information theoretic lower bound (see [28]) for the FWMSE associated with the given source   and filter   is plotted in Fig. 4 (solid line). This corresponds to Shannon’s quadratic frequency weighted Distortion-Rate function when . As the bit-rate is increased, the gap between and this absolute lower bound decreases to approximately 7.5 [dB] for and 11 [dB] for 
 , at . This difference can be attributed to the rate-distortion inefficiency of the uniform scalar quantizer . On the other hand, the larger performance gap observed at lower bit-rates can be attributed to the perfect reconstruction constraint.  Recall that, at low bit rates, the achievement of Shannon’s rate-distortion function demands the suppression of relatively less significant bands of the PSD of the input signal (see, e.g., [27] and [28]). This linear distortion, which a PRFQ cannot achieve, is more severe at lower bit-rates. Thus, the performance gap increases as   is reduced. 
 Nonoverloading  : The FWMSE of this converter variant is presented in four of the plots in Fig. 4, with labels beginning with “  opt. PRFQ, Nonoverloading.” These differ in the loading factor, and in the meaning of   in each case. For the plots whose labels do not have the ending “E.C.” (entropy coding),   is simply the number utilized to generate the value   for which the filters were optimized. The plots whose labels end in “E.C.” correspond to the same simulations, but for each point the value of   is the scalar entropy of the quantized output of the converter. It can be seen in Fig. 4 that the FWMSE obtained for the nonoverloading without entropy coding is remarkably close to the theoretical value predicted by (31a). More importantly, even for bit-rates as small as , each observed ratio   deviates from its nominal value of by less than 2%. (For the extreme situation , the observed   was slightly lower than predicted, while was 55% higher than 1/12 due to the highly nonuniform pdf of the resulting sequence  .) It can also be seen that the scalar entropy of the quantized output of the PRFQ in these cases is very close to Shannon’s   function for a given distortion. This agrees with the observation that the output of   in an optimized PRFQ is white, see the comment at the end of Section V-B-1. The difference between these quantities is bigger for lower values of  , for the same reason discussed in Section VIII-B-1. 
 Overloading  : For the overloading PRFQ using an  of 4, the FWMSE diminished along with the corresponding 
 that range of bit-rates. This performance degradation can be attributed to clipping errors. The fact that overload errors become noticeable only for high bit rates (many quantization levels) might seem, at first, surprising. However, this phenomenon can be easily explained by noting that the size of the tails of the pdf of   that fall outside the dynamic range of remains approximately constant in relation to for all . (This is a direct consequence of the loading factor rule.) In contrast, granular (nonoverloading) quantization error is proportional to 
 (which is held constant in the simulations). Therefore, the ratio between clipping and granular quantization errors grows approximately as   and clipping errors become dominant for sufficiently high bit-rates. 
 Because of the reduced occurrence (and magnitude) of clipping errors, the optimized PRFQ with overloading 
 and   exhibits an FWMSE smaller than that of its counterpart with  . Furthermore, this more conservative loading factor allows the converter to perform almost exactly as predicted by our analytical expression for 
 . For the chosen input PSD and frequency weighting filter, and calculating as , the value of varies with as shown in Fig. 4 (dotted line). As seen in this figure, the gap between and , for each value of , gets smaller as the bit-rate decreases. This agrees with the fact that the optimal PRFQ approaches a PCM converter as  , see Section V-A. It can also be seen in Fig. 4 that the optimized PRFQ with overloading and   exhibits an improvement of 32 [dB] over PCM at  . Equivalently, in order to obtain the same FWMSE as that of PCM at 16 bits, the PRFQ converter with   requires less than 12 bits. At lower bit-rates, the improvement of the optimal PRFQ overPCM is also significant. For example,the overloading PRFQ with   has a lower FWMSE than the PCM converter with  , thus, achieving a data rate compression of 50% (see Fig. 4). 
 This paper has studied perfect reconstruction feedback quantizers based on an additive white noise model for quantization errors. We have derived results that relate the minimum frequency weighted MSE and the SNR of the scalar quantizer embedded in the converter. We have also provided closed form expressions for the optimal frequency responses of the filters in the converter and have derived several properties of optimal PRFQs. In particular, we have shown that the optimal frequency response magnitudes of the filters are unique, that the frequency weighted errors of an optimal PRFQ are nonwhite, and that consecutive samples of the output sequence of the scalar quantizer are uncorrelated. We have also shown that, within our model, thefrequencyweightedMSEofanoptimal,oversampledPRFQ, decreases exponentially with oversampling ratio. 
 The following preliminary results are necessary to prove the theorems stated in the previous sections. We begin by introducing the following definition. 
 We say that two functions   are similarly functionally related iff there exists a monotonically increasing function such that , for all , and write . Similarly, if there exists a monotonically decreasing function such that , for all 
 If and are oppositely functionally related, then the inequality in (47) is reversed. In either case, equality is achieved iff   (and therefore  ) is almost constant. 
 Proof: We will examine the difference between the RHS and LHS in (47). We obtain 
 We now proceed to upper bound the last term in the above inequality. From (63) and (64), we have that 
 where the last inequality stems from (62) and (63). Substitution of (69) into (65) yields 
 Since is bounded, and from (23b), it follows from (70) that for any , one can always choose sufficiently large 
 Denote the squared norm of [see (24)] via, and define the set of all the having the same norm as 
 It is easy to show21 that must belong to . From this, and since	, it follows that 
 The problem described by (72)–(74) falls within the category of isoperimetrical problems, well known in variational calculus (see, e.g., [35] and [36]). The standard solution of these problems is based upon the fact that any   that extremizes   [see (72)] needs to satisfy 
 We note that for the trivial case in which is almost constant (see Definition 1),   is also almost constant. Applying this to constraint i) in (74) yields that, for this case, is such that 
 . Thus, the remainder of the proof addresses only the cases in which	is not almost constant. 
 In order to find , we will next discard the possible solutions of (77) which do not correspond to global minimizers of  in . The unique remaining function, which is obtained with and in (77), will characterize the solution of Optimization Problem 2. 
 The Case : Fore this case, substitution of (77) into (74) yields that needs to satisfy 
 so that can be obtained explicitly from . Note that cannot be zero in the above expression, otherwise would be undefined. From this, the feasible  sign combinations for , the sign before the square root, and   in (77) are 
 Discarding Option a): We show next that any solution obtained by applying option a) in (77), say  , yields a greater 
 follows by applying Theorem 7 to the numerator of(80), together with (48) and the fact that . Both inequalities are strict since is not almost constant (see Theorem 7 and Definition 1). 
 On the other hand . From the above, it follows that [ee (21)], discarding, for all non 
 Discarding Option b): The candidate solutions are now characterized by options b) and c) only. Applying (49a) to (77) and (79), these solutions take the form 
 Combining this result with (84), and considering   to be not almost constant, we obtain 
 quency response magnitude in the absence of fed back quantization noise [recall (45)]. This is not surprising, since taking   amounts to removing constraint i) (which restricts the 
 Since the functions , are continuously differentiable , so is . We, therefore, have that if 
 1, then we obtain = 0. This would imply f (!) = 0 for all ! such that g(!) > 0. Thus, since f must belong to C , the integral of lnf (!) over the remaining frequencies needs to be infinite. Since ln(x) < x;8x 2, this implies that kf k = 1 (infeasible) and D(f ) = 1. 
 We will first elaborate upon (95) to derive (28). Then we will prove that (94) holds. 
 where and are as defined in (48). Application of the identity, [which follows from (48) and (49a)] to the numerator on the RHS of (96) yields 
 The Sign of : Since , this limit needs to be analyzed for two possible scenarios, depending on whether or not is positive. 
 (	) must necessarily hold in order to obtain . Thus, and its first derivatives are continuous. Therefore, in view of (93), we get 
 , it is clear from (97) that there a value for	greater than under which	is small enough to render negative. Therefore,	. 
 Limits: In order to show that the limits (29) and (30) in Theorem 3 hold, we write as 
 We will first prove the validity of . Clearly, if for all (condition i) of Assumption 1), then the RHS of the above equation tends to as 
 . If this wasn’t the case, then the second condition of Assumption 1) must be satisfied, and therefore the conditions of 
 In order to show that [i.e., (30)] holds, we first note from (104) that for all . On the 
 H. Proof of Theorem 6 Applying (49c) to (37) one can write Since	is monotonically decreasing (see Theorem 
 to the minimum FWMSE for a constant , by virtue of (42) we have that	. Substitution of this into the last inequality yields	. 
 ﻿ In this paper we review the previously for successive compute and forward and discuss about their incompleteness through some . Then we present a comprehensive formulation for step successive interference cancellation problem through the asymmetric successive forward strategy . It is shown that the generalized formulation the previous . 
 Interference management in relay is one of the most important in wireless . Among the various for relay forward , compress and forward , amplify and forward , and compute and forward see and therein , the later one interesting attention for the problem of interference in noisy relay , especially when it is more helpful to decode linear of rather than individually them . In , several have been for integer non integer linear of in the with equal unequal power , and with symmetric asymmetric channel gains . 
 In , a compute and forward strategy been for relay with equal power and asymmetric channel gains based on lattice . The receiver given sufficiently many linear , can decode integer linear of and solve for its desired . To obtain higher , can recover those linear which are closer to the channel fading . This strategy simultaneously protection against noise and the opportunity to exploit interference for gains . On the other hand , in , a scheme been for relay with unequal power and symmetric channel gains based on lattice and compute an achievable cast rate within a constant gap to the capacity . There is one point here : in , the assumed equal power and stated that unequal power can be incorporated by scaling the channel appropriately . Here with an example we show that , when one does this , the of a smaller rate region than that found in . 
 In successive strategy been for the relay network with the same assumption as in . In this method after a linear combination of , the receiver can combine it with its channel observation to obtain a new effective channel which is better for the next targeted linear combination . 
 In an asymmetric been for with unequal power and asymmetric channel gains based on lattice . In , relay with unequal power and asymmetric channel gains been considered as a special case of the model in . They also consider successful recovery of the individual at the final destination in addition to the relay recovery . 
 In and a asymmetric been . This method scaling to decode integer linear of . The use of scaling in fact is equivalent to non integer linear of lattice and it different to have different , so by appropriately those , different on the boundary of the rate region can be . Also , in the idea of non integer linear of lattice been . Their method can be considered as a special case of the of and as well . 
 In this correspondence we first give a brief review of the previous works about . Moreover we discuss about their incompleteness of some through some . Then we present our generalized formulation for the successive interference cancellation problem in asymmetric relay through the asymmetric successive forward strategy . More specifically , we extend the method of to step and also we take advantage of scaling to reach to a comprehensive formulation for the asymmetric strategy . It is shown that the generalized formulation the previous . 
 The paper is organized as : In Section the system model is defined . In Section the of are and . In Section our main , the extended asymmetric successive compute approach for general relay is . Some concluding are provided in Section 
 set of the real up to the nearest integer value . are shown with . 
 In this paper we consider a real relay network . It is the same network as been used in , but here we consider the assumption of unequal power and asymmetric channel gains . Each relay in the network , indexed by ;, a noisy linear combination of the through 
 where are the channel gains and is i . i .. noise , , In . Let denote the vector of channel gains from the relay . For all , each channel input length sequence subject to the average power constraint , i . e . 
 The channel gains are assumed to be known and constant . In this document we only consider communication between the transmitter and the . 
 In this section we review and compare the works of , and discuss about their . 
 In and , achievable rate have been for real valued based on the successive compute and forward strategy . The combination of those is in the following theorem . 
 Theorem . For the defined system model with equal equation gain am , , where non zero am am , , with ;, the following computation are achievable 
 As in , the interpretation of is that the relay can first target a linear combination , am , that is easy to decode and then use it to create a better effective channel for the second linear combination 
 In , a class of real with one receiver power P and symmetric channel gains been considered . Channel gains have been assumed to be one . An achievable rate region been based on the compute and forward strategy . It is in the following theorem , 
 is achievable for user , with bounded error probability which goes to zero infinity . 
 In equal power have been to the in the network . It been declared in that in the case of unequal power , channel gains can be scaled in order to get equal power . Next by the aid of two , we show that the that can be by the method of with the scaling of channel gains are smaller than those by the method of . 
 Example : Consider a network with , , unequal power constraint P P , and channel gains h h . Theorem the following achievable rate for : 
 If we do channel gain scaling as , we get to equal power constraint P for both . Substituting these along with a which 
 is than , e .., assume i in , then from the assumption P P , it is that . 
 With another example , we show that the rate region of with equal power and asymmetric channel gains , is smaller than the one from by scaling the power accordingly while leaving unit channel gains . 
 Example : Consider a network with , , P P , and h h . Substituting them in Theorem along with a , 
 If we do scaling in order to obtain symmetric channel gains equal to , the scaled power will be P h and P h . With this , Theorem the following rate for user i , 
 since h h h subject to h h . So as before , it is seen that in a higher rate R in comparison with 
 Thus , the method of , scaling channel gains with power , smaller rate region than that in in the case of unequal power . 
 In and , an asymmetric compute and forward is which the problem by appropriately fine and course for each user . Also in , by the aid of scaling , the equation been which can reach to the rate of for the case of unequal power . 
 In a network been considered which a receiver with which an dimensional channel , the channel gain between th transmitter and the receiver , is i . i .. white noise with distribution , , to the power constraint . Let h and P . The theorem is as , 
 Theorem . For the defined system model and equation coefficient am , with ; where am am , the following computation rate is 
 vector of power of the as P , P . 
 In Section we state the extension of to step asymmetric successive compute and forward by the same method as used in and also we consider scaling as done in and . 
 In and by real scaling and the resulting non integer linear combination of , one additional flexibility for the individual message of different . Corresponding of interest from and can be summed up in the following theorem , 
 Theorem . Let real . For the defined system model in Section with equal power equation coefficient am , where am am , the computation rate is achievable if 
 The main idea behind this result on the observation that the lattice do not have to lie in the lattice which is used for at the . 
 As an example in , the two way relay channel been considered where two have unequal power P and P and the channel gain vector to the relay is unity . The relay power constraint . All are with unit variance . They obtain the following achievable rate region for the , where the relay is assumed to decode a linear combination of user of the form where , 
 , , and for all in . As it is seen , the previously problem for unequal power is here by the aid of , but it is not obviously a straightforward substitution of channel vector and in the equation of the rate region . In the next section we propose a straightforward formulation which this problem . 
 In , the step successive interference cancellation strategy been for more than one linear combination of in multiple access . The method is based on the matrix algebra consider general equation gain which is different from the method of which is based on successive interference cancellation and consider orthogonal equation gain . 
 In a network with relay and been considered . They used the idea the non integer linear combination of instead of scaling and they have critical on the rate region where the rate of one user the cut set bound . For the method of is actually a special case of and by considering . 
 In this section we propose a straightforward and comprehensive equation for step asymmetric successive compute and forward strategy . We assume unequal power and asymmetric channel gains . Our main result for step asymmetric successive compute and forward strategy is in the following theorem : 
 Theorem . For the system model with unequal power constraint P , consider non zero equation coefficient ami , where ami ami , ami , and i ;, and let real . The computation rate R is achievable where 
 Corollary . For , i . e ., single step of integer linear combination of , the rate of 
 Remark : One can see that by scaling channel gains with power , i . e ., substituting with , and also substituting with in , is . But in spite of the formal equivalence of and in the case of unequal power , there is a difference between the conceptual basis of these . As in the proof of Theorem in Appendix , in the strategy of , the of different have equal independent of what the of are , but in the strategy of , the transmit their own real power . 
 Corollary . For , i . e ., step interference cancelation , the rate of to 
 Remark : Equal power and asymmetric channel By considering equal power P P , to , and also by considering for all , and am , 
 Remark : Unequal power and symmetric channel Assume for all , and a I , then . 
 As it is seen from the above , our formulation is capable of both unequal power and asymmetric channel without the aid of scaling . 
 In the following corollary we obtain a more reduced form for and when and , e .., the network with one relay and less than three . 
 Corollary . For the network with , , the rate of Theorem for step successive interference cancelation , i . e . 
 Remark : For , P P , i . e ., equal power , a a , a and a b , b , and to , . 
 In this paper we studied the asymmetric step successive compute and forward scheme for interference management in relay with unequal power and asymmetric channel gains . We first the previous for this scheme and about their through some . Then we a comprehensive formulation for asymmetric successive compute and forward scheme by taking advantage of all the of the previous works together . It is shown that the approach the previously . 
 Proof : Let ,..., which are simultaneously good and the lattice among them . The equivalent noise variance at the , which will be defined later in the proof , determine the order of this lattice chain . noise variance to a lattice . Additionally , are such that which are simultaneously good with second , where is a positive number . At each transmitter , the is , where the region of , and the number of the relay which in lower achievable among the which should decode the message of transmitter . The transmitter , 
 where is a random vector uniformly distributed in which is as a dither . is independent uniformly distributed in , so it average all 
 The receiver to retrieve from ym instead of , for all . Note that there is a one to one between , so 
 is equivalent to . For , the receiver consider lattice which the lattice point ym in in which , i . e ., 
 so the lattice of transmitter , is successful following relation , 
 for all , where the variance of the effective noise m and it is as : 
 In the following , we obtain the for the successive of other possible integer of . To decode the integer sum with coefficient vector ami , where i ;, the relay the projection of onto ym from ym as : 
 At last the relay the linear combination with coefficient vector ami by 
 By the same as shown in and substituting with with , and am with ami , it can be shown that the following rate is achievable for all 
 First we introduce the following lemma which will be used in the proof of Corollary . 
 Lemma . The following identity is satisfied for arbitrary dimensional real a a , a an , b , b and c , c , with . 
 By substituting the of the a ,, the right hand side of , we have 
 . So the total number of in the first and second line of are n n n which is equal to and is zero for . 
 is n , the number of in i i is n n when i is not satisfied , so the number of in is n n n which is equal to zero for . 
 Proof : We must show that and to . It is easy to see that is exactly the same as the first term of . Now we simplify to obtain the second term of 
  
 ﻿There are two major issues which underly the design of an analog to digital converter; namely “when to sample?” and “how to represent the amplitude of each sample?”. In the majority of past work, these two aspects have typically been treated separately. Recently [1] we have introduced a novel algorithm called MSIC which uses moving horizon optimization ideas to determine both when and how to sample. Our earlier work [1] gave a heuristic description of the algorithm and showed, via simulations, that a performance gain was achieved in both the required bit rate and the achieved distortion level. This seemingly paradoxical result is due to the interpolative capabilities inherent in MSIC. The goal of the current paper is to give theoretical support to the MSIC algorithm. In particular, we provide bounds on the probability that beneficial interpolation occurs for the particular case of horizon length 2 with flat, unity gain error weighting filter. 
 The two fundamental questions that arise when dimensioning an analog-to-digital converter are “when to sample?” and “how to represent the amplitude of each sample?”. Several studies deal with the first question (neglecting quantization effects), see, e.g., [2], [3] and gives rise to irregular and or periodic (non-uniform) sampling patterns. Other strategies deal with each sampled value. In particular, S?- conversion has been deployed in a wide range of applications, see, e.g., [4], [5]. Recently, S?- conversion has been extended to incorporate ideas stemming from finite set constrained predictive control, see [6]–[8]. 
 Earlier work by the present authors, reported in [1], uses moving horizon optimization ideas [9] to address the dual question of when to sample and how to quantize. The scheme, which we termed MSIC in [1], selects samples to be marked as “mute” whenever it is desirable to replace them by a linearly interpolated value derived from their neighbouring samples. Notice that this procedure implicitly leads to signal-dependent (adaptive) non-uniform sampling patterns. 
 Our earlier work [1], gave an introduction to the algorithm and showed, via simulations, that distortion introduced in the conversion process can be reduced, when compared to existing schemes, such as straight quantization, S?- conversion and its multi-step extensions [6]–[8]. Additionally, we showed that, in general, the number of bits required for the digital representation of the signal can also be reduced. This seemingly paradoxical result arises from the interpolation inherent in the MSIC algorithm. 
 In the current paper, we will give theoretical support to the MSIC algorithm by examining the conditions under which beneficial interpolation occurs. In particular, a bound on the probability that interpolation occurs is derived for the special case of horizon 2 optimization. 
 The goal behind MSIC is to perform quantization so as to minimize both bit rate and distortion. It is based on the idea of linear interpolation between quantized values. MSIC can mark a sample as “mute”. Later, mute samples, are reconstructed by interpolating linearly between neighbouring quantized values. This concept is illustrated in Fig. 1. 
 Fig. 1 a) and b) show the reconstruction of an analog signal from a (uniformly sampled) discrete-time signal obtained via straight quantization. The dashed line in Fig. 1-a) corresponds to reconstruction using zero-order hold. The benefit of using 
 Fig. 1. Reconstruction of an analog signal from samples obtained via: a) straight quantization; b) linear interpolation 
 linear interpolation is clear in Fig. 1-b), where the values for the samples at times k ? {3,6,9} were obtained by linear interpolation of quantized values of the neighbouring samples. It can be observed that the reconstructed signal in Fig. 1b) fits better the original analog signal. The MSIC algorithm builds on this observation and reduces distortion by creating virtual quantization levels between the original ones every time a sample is interpolated. Additionally, data compression occurs due to the fact that mute samples do not require bits to represent their value thereby reducing the average bit rate needed. 
 Consider a scalar discrete-time signal  with underlying sampling rate fS. The purpose of analog-to-digital conversion is to obtain a quantized representation of ak, i.e., a discrete-time signal, uk, k ? N. Each value uk is restricted to belong to a given finite set of scalars, {s1,s2,...,snu-1}. 
 In the MSIC algorithm, uk is allowed to be assigned as mute. This condition is denoted by the symbol “*”. Thus, the constraints on uk can be expressed as: 
 In order to incorporate mute samples into the conversion problem, we will make use of an interpolator whose output is defined as follows : 
 where f and l are functions of k such that for every k corresponding to a vacant sample ul = closest non-mute sample before uk uf = closest non-mute sample after uk (4) 
 We can see that  is a discrete-time signal formed of all nonmute values of uk and the linearly interpolated mute elements of uk. Notice that  is not constrained to belong to UThe quantization process: 
 can be designed [1] by utilizing a frequency weighted measure of the error, i.e.: 
 where L is the total number of samples of ak. In (5), ek is the filtered error, defined as: 
 1) Finite Horizon Formulation: To ensure that the computations are feasible, it is necessary to restrict the number of decision variables as well as the number of future values of ak considered in the optimization. Thus, at sampling step k we will replace the infinite horizon cost function (5) by the following quadratic cost function defined over a shorter optimization horizon of length N: 
 The finite horizon cost proposed in (7) is determined by only a finite number N of constrained values uj. These decision variables are grouped into the vector 
 Note that the last element of , that is, uk+N-1, must not take the value “*” (mute sample), because, if it did, then the calculation of the corresponding value of the interpolated sample, , would require the value of the successive value, namely uk+N, which falls outside and ahead of the horizon. 
 2) Modified Moving Horizon Approach: Minimization of the multi-step cost introduced in (7) gives rise to the optimizer: 
 Given (8),  contains a feasible sequence over the optimization horizon. If one was to use the standard moving horizon concept as deployed, e.g., in [9]–[11] , then only the first element would be implemented. At the next optimization step the whole procedure would be repeated again with the horizon shifted by 1. While this is in principle possible, the inclusion of mute samples leads to additional aspects that are addressed in [1]. In general terms, the optimization method may give rise to several consecutive mute samples. In that case, if the horizon moves one step, the decision for the next element would have to consider the backwards propagation effect, which is not taken into account in the finite horizon cost (7). This would lead to performance degradation. To overcome this problem, MSIC includes the following modification to the standard moving horizon approach : 
 such that , i.e., uk+M is non-mute. The horizon will then be shifted by M+1. The above procedure ensures that, at every optimization step, the precursor is non-mute. 
 Remark 1: Notice that the use of mute samples gives rise to a non-uniform sampling pattern, and that the average sampling rate will be lower than fS. 
 Remark 2: The MSIC embeds non-interpolative converters such as straight quantization, S?- conversion and the MultiStep Optimized Converter of [8] in a more general setting. Since the conversion process is carried in an optimal manner, see (9), the MSIC will, in general, give lower distortion than these other conversion algorithms. In the sequel, we will analyze the quantized sequences produced by the MSIC algorithm. 
 In [1] it was shown by simulation that MSIC yields lower quantization noise than traditional analog-to-digital conversion methods, and at the same time it utilizes less bits. Clearly, these gains depend on having a high proportion of mute samples p, where: 
 here, I represents the number of interpolated samples after L samples have been converted. 
 Notice that in the case of horizon length N = 2, at most one out of every two samples can be marked as mute, limiting the value of p to 0.5. We also define the probability of muting a sample as: 
 In [1] it was also shown that the Normalized Bit Ratio, defined as the total number of bits required by the output sequence from MSIC, namely R2, divided by the total number of bits required by traditional quantization, RTrad is given by: 
 As a consequence, the data compression of MSIC depends on what proportion of samples are marked as mute and on the size of the constrained set nU, see (2). 
 Towards the goal of deriving expressions for Pint (see (12)), in this section we will elucidate the conditions under which the MSIC chooses interpolation rather than quantization. The following definitions are used: 
 Q() , a standard nearest neighbour scalar quantizer [12], with output set {s1,...,snU-1}. 
 In the sequel, we will restrict our analysis to the simplest case, in which N = 2, W(z) = 1, and where si+1 = si + 
 We will also assume that input signal and output set are scaled such that no overload occurs, i.e., . 
 (interpolated) or quantized. However, having restricted the horizon length to N = 2, the rules of the MSIC algorithm  a 
 imply that (see II-B.2), if a given sample ak is a candidate for interpolation, then the previous converted sample result, , is non-mute. Similarly, if the sample ak was to be marked as mute, then the conversion result for the sample just after it, namely , would be non-mute. As a consequence, when considering the possible interpolation of ak, the expression for   becomes 
 Probabilistic analysis of (17) is non trivial since its left hand side contains a sum of correlated continuous random variables. For our purpose, it is convenient to first assign to each quantization level an integer number in ascending order (see Fig. 2) and then define the Amplitude Band Number (c.f. Voronoi Region) for a sample ak as 
 This integer quantity represents the number of the closest quantization level for sample ak. As an illustration, for the example in Fig. 2, Nk-1 =1 , Nk = 2 and Nk+1 = 3. Given the above, the values ak can be decomposed into : 
 is an integer number that could be interpreted as a “coarse” second difference of ak. 
 Since N¨k is integer valued and, by assumption,  , we only need to distinguish 3 cases that fulfill (20), namely: 
 It can be seen from (18) that the required values for N¨k (22) depend on the values of ak-1, ak and ak+1. For the purpose of probabilistic analysis, it is more convenient to express it in terms of: 
 These expressions can be obtained through careful examination of the definitions given in IV, (18) and Fig. 2. For sake of simplicity, details are omitted. 
 (25) could be solved numerically. Alternatively, it is possible to devise a graphical method to represent (25) and solve (25), as depicted in Fig. 3. In this figure, the range of combinations of variables   which satisfy (25) are shadowed. The horizontal axis is , determining the value for the first difference Nk-Nk-1(see (23)), as denoted by the vertical marked bands. The vertical axis is a?k+1. The horizontal jigsaw bands (horizontal rows of diamonds) mark zones yielding the same result for Nk+1 -Nk (see (24)). Thus, a given value for   defines the ranges of a?k+1 that in (25) yield either -1, 0 or 1 in order to satisfy the conditions for interpolation of sample ak. 
 The diagonal line (-.-.-.) in Fig. 3 lies entirely inside the solution area, and represents the condition . If  , this line corresponds to the case a?k+1 = a?k, that is, a¨k = 0. Since, by assumption,  , we can conclude, in accordance to intuition, that in order to satisfy the condition for interpolation, a¨k must be as close to zero as possible. In general, a given value of a¨k will satisfy the condition of interpolation depending on the value of . It can also be seen from figure 5 that the limits of the required 
 Fig. 3. Graphical solution to equation (25). Shaded areas represent the combinations of  satisfying it. 
 Additionally, if  , the difference between each of these periodic functions and the diagonal line determines the required limits for a¨k. The required limits for a¨k then depend upon . Fig. 4 illustrates the effect of quantization error  in the range for a¨k that satisfies the condition for interpolation, for three different values of a?k. In particular, it can be inferred from figures 5 and 6 that, if |a¨k| > 2?, then 
 We can utilize the framework established in section V to analyze Pint (see (12)). Towards this goal, we interpret , a?k and a¨k as random variables which we denote by , a? and ¨a respectively. Assuming that  is uniformly distributed on 
 the interval   and statistically independent from a? and a¨ , it is possible to obtain (see Fig. 4), for a given value of a? and a¨, the set of intervals to which  must belong in order to satisfy (25). Since  is uniformly distributed, the total length of these intervals, which we will denote here as (a?,¨a), is proportional to the probability that the condition for interpolation is satisfied. Thus, the conditional probability of interpolation given that a? = x and ¨a = y is given by: 
 Note that the function g(x,y) defines a random variable g(a?,¨a). If the joint probability density function for a? and ¨a is fa?,¨a(x,y), then Pint can be calculated via: 
 Note that the -2? and 2? limits in the first integral arise from the fact that g(x,y) = 0?y /? (- , as previously noted. Function g(x,y) can be numerically or graphically obtained. Fig. 5 shows g(x,y) v/s y for three different values of x. 
 Based on the analysis on section 6, the probability of interpolation for a given signal depend on the statistical properties of the first and second differences of the signal. Clearly, if fa?,¨a(x,y) is known, the probability of interpolation can be obtained precisely from (27). However, in practice it is desirable to estimate the probability of interpolation “a-priori”, based on coarse information about the signal. We present next a lower bound for Pint for the case where only the second moment of ¨a, that is, E[¨a], is known. 
 THEOREM: For each y0 ? [0,2?], and provided E[¨a] < 4?, a lower bound for the probability of interpolation can be obtained by 
 where f¨a(y) is the probability density function of ¨a. Equation (29) then leads to 
 REMARK: Clearly, the above theorem depends on the choice of y0. Since, by construction , h?(y0) is monotonically decreasing, the derivative of the right hand side of (28) is zero only at the global maximum. Thus, the tightest value for the bound can be found numerically by optimizing y0. 
 Simulations were obtained by applying the MSIC algorithm to two CD quality music signals each of length 50,000. We grouped the samples into 192 categories, for 3 values of a? and 64 values of a¨, and calculated the statistical frequencies of interpolation within each category. According to the law of large numbers, the greater the number of samples in each category, the closer the statistical frequency will approach the actual value of the probability of interpolation for each category. More specifically, for a category defined as samples ak such that |a?k - x| < dx and |a¨k - y| < dy, it can be expected that for highly populated categories the statistical frequency will approach the conditional probability of interpolation g(x,y) as dx,dy ? 0. 
 Figures 6 - 8 show the results for 8 quantization levels. In these figures, the dashed line represents the function g(x,y) for the corresponding value of x (see Fig. 5). 
 It can be seen that statistical frequency approaches g(x,y). The differences in the heights of the columns from the dashed line are explained from the fact that in some categories the number of samples was rather small. This is particularly true for larger values of a? or a¨ . Despite that, it is important to 
 verify that the zones for which no interpolation is predicted shows indeed no interpolations at all (except an isolated bar on the extreme right in Fig. 6, arising from a category of one sample whose existence is explained by the fact that that category admits samples with a? not strictly equal to zero). Similarly, for the categories where g(x,y) predicts probability of interpolation equal to 1, all samples were in deed interpolated. 
 To illustrate the bounds derived in section VII, we converted 22 different audio signals, each one with 4 and 8 quantization levels. For each signal, the value for E[¨a] was estimated from the entire signal. Fig. 8 illustrates the empirical frequencies of interpolation. Furthermore, it shows the lower bounds obtained from (28) by choosing  (which gives  =0.5) and y0 = ? (which gives  =0.25). It can be observed from this figure that the predicted lower bounds, although weak, are confirmed by simulations. In particular, as suggested by the bounds developed, the (empirical) probability of interpolation decreases as  E?[¨a] increases. 
 This paper has analyzed a recently described algorithm which simultaneously achieves reduced bit rates and lower distortion levels. The algorithm, designated MSIC, uses moving horizon optimization together with interpolation. In this paper, we give a lower bound on the probability that interpolation can be beneficially used for the particular case of horizon length and flat error weighting filter. Simulation results have also been presented which confirms the validity of the lower bound in practical circumstances. 
 ﻿ 
 We study the increase in per sample differential entropy rate of random and after being through a non minimum phase discrete time , linear time invariant filter . For discrete time and random , it long been established that this entropy gain ,, the integral of . It is also known that , if the first sample of the impulse response magnitude , then the latter integral the sum of the logarithm of the of the non minimum phase of i . e ., its outside the unit circle , say . These have been derived in the frequency domain as well as in the time domain . In this note , we begin by showing that time domain , which consider finite length and then to infinity , have significant mathematical and , therefore , are inaccurate . We discuss some of the of this oversight when considering random . We then present a rigorous time domain analysis of the entropy gain of for random . In particular , we show that the entropy gain between equal length input and output is upper bounded by and if and only if there an output additive disturbance with finite differential entropy no matter how small or a random initial state . Unlike what with linear , the entropy gain in this case on the distribution of all the involved . Instead , when the input differential entropy to that of the entire longer output of , the entropy gain irrespective of the and without the need for additional exogenous random . We illustrate some of the of these by their in three different . Specifically : a simple derivation of the rate distortion function for non stationary , for 
 I . INTRODUCTION 
 In his seminal paper , gave a formula for the increase in differential entropy per degree of freedom that a continuous time , band limited random process u after passing through a linear time invariant continuous time filter . In this formula , if the input process is to a frequency range ,, differential entropy rate per degree of freedom u , and the filter frequency response , then the resulting differential entropy rate of the output process is given by , Theorem 
  
 The last term on the right hand side of can be understood as the entropy gain entropy amplification or entropy boost by the filter . proved this result by that an filter can be seen as a linear operator that selectively scales its input signal along infinitely many , each of them an orthogonal component of the source . The result is then by writing down the determinant of the of this operator as the product of the frequency response of the filter overfrequency , logarithm and then taking the limit as the number of frequency to infinity . 
 An analogous result can be for discrete time input u and output , and an discrete time filter by them to their continuous time , which 
  
 where 
 is the differential entropy rate of the process u . Of course the same formula can also be by the frequency domain proof technique that in his derivation of . 
 The rightmost term in , which to the entropy gain of , can be related to the structure of this filter . It is well known that causal with a rational transfer function such that i . e ., such that the first sample of its impulse response unit magnitude , then 
 , 
 where i are the of and , : is the open unit disk on the complex 
 plane . This a straightforward way to evaluate the entropy gain of a given filter with rational transfer function . In addition , that , if , then such gain is greater than one if and only if outside . A filter with the latter property is said to be non minimum phase ; conversely , a filter with all its said to be minimum phase . 
 appear naturally in various . For instance , any unstable system via linear feedback control will yield transfer which are , . Additionally , also appear when a discrete time with zero order hold equivalent system is from a plant whose number of its number of by at least , as the sampling rate , Lemma . . On the other hand , all linear phase , which are specially for audio and , are , . The same is true for any all pass filter , which is an important building block in signal , . 
 An alternative approach for the entropy gain of is to work in the time do 
 main ; a function of un , for every , and evaluate the limit 
 . More precisely , for a impulse response , we can write 
 g 
 y , 
  
 where yn , y y yn and the random vector un is defined likewise . From this , it is clear that 
 h yn un log , 
 where or simply for the determinant of . Thus , 
 , 
  
 regardless of whether i . e ., the polynomial g with magnitude greater than one , which clearly and . Perhaps surprisingly , the above contradiction not only been in previous works such as , , but the time domain formulation in the form of been as a to prove or disprove see , for example , the reasoning in ,. . 
 A reason for why the contradiction between , and can be from the analysis in for an a noisy feedback loop , as the one in Fig . . In 
  
 Figure . Left : a noisy feedback loop . Right : equivalent system when the feedback channel is noiseless and unit gain . 
 this scheme , a causal feedback channel which the output an exogenous noise random process c to generate its output . The process c is assumed independent of the initial state of , by the random vector x , which finite differential entropy . For this system , it is shown in , Theorem . that 
 , a 
 with equality a deterministic function of . Furthermore , it is shown in , Lemma . that if 
 x and the steady state variance of asymptotically bounded as , then 
 , b 
 where pi are the of . Thus , for the case in which , the output y is the result of filtering u by a filter as shown in Fig . right , and the resulting entropy rate of will exceed that of u only if there is a random initial state with bounded differential entropy see a . Moreover , under the latter , , Lemma . that if is stable and x , then this entropy gain will be lower bounded by the right hand side of , which is greater than zero if and only . However , the result in b does not provide under which the equality in the latter equation . 
 Additional and intuition related to this problem can be from in . There it is shown that if is a two sided stationary random process by a state space recursion of the form 
 A , a 
 un , b 
 for some A , , , with unit variance i . i .. u , then its entropy rate will be exactly i . e ., the differential entropy rate of u plus the of with i now being the of A outside the unit circle . However , as noted in , if the same system with zero or deterministic initial state is excited by a one sided infinite 
 i . i .. process u with unit sample variance , then the asymptotic entropy rate of the output process y is just i . e ., there is no entropy gain . Moreover , it is also shown that if is a random sequence with positive definite covariance matrix and , then the entropy rate that of u by the of . This that for an system which a representation of the form , the entropy gain for a single sided i . i .. input is zero , and that the entropy gain from the input to the output plus disturbance is , for any disturbance of positive definite covariance matrix no matter how small this covariance matrix may 
 be . 
 The previous analysis that it is the absence of a random initial state or a random additive output disturbance that the time domain formulation yield a zero entropy gain . But , how would the addition of such finite energy exogenous random to actually produce an increase in the differential entropy rate which asymptotically the of In a sense , it is not clear from the above what the necessary and sufficient are under which an entropy gain equal to the of the analysis in only a set of sufficient and on second order statistics and to derive the previously . Another important observation to be made is the following : it is well known that the entropy gain by a linear is independent of the input statistics . However , there is no reason to assume such independence when this entropy gain as the result of a random signal to the input of the , i . e ., when the by itself does not produce the entropy gain . Hence , it remains to characterize the set of input statistics which yield an entropy gain , and the magnitude of this gain . 
 The first part of this paper to these . In particular , in Section explain how and when the entropy gain in the above , starting with input and output of finite length , in a time domain analysis similar to , and then taking the limit as the length to infinity . In Section it is shown that , in the output plus disturbance scenario , the entropy gain is at most the of . We show that , for a broad class of input not necessarily or stationary , this maximum entropy gain is only when the disturbance bounded differential entropy and its length is at least equal to the number of non minimum phase of the filter . We provide upper and lower on the entropy gain if the latter condition is not met . A similar result is shown to hold when there is a random initial state in the system with finite differential entropy . In addition , in Section we study the entropy gain between the entire output sequence that a filter as response to a shorter input sequence in Section . In this case , however , it is necessary to consider a new definition for differential entropy , effective differential entropy . Here we show that an effective entropy gain equal to the of is provided the input finite differential entropy rate , even when there is no random initial state or output disturbance . 
 In the second part of this paper we apply the in the first part to three , namely , control , the rate distortion function for non stationary , and the channel capacity with feedback . In particular , we show that equality in b for the feedback system in Fig . left under very general even when the noisy . For the problem of finding the quadratic rate distortion function for non stationary auto regressive , previously in , we provide a simpler proof based upon the we derive in the first part . This proof the result stated in , to a class of non stationary . For the feedback capacity problem , we show that capacity based on a short random sequence as channel input and on a feedback filter which the entropy rate of the end to end channel noise such as the one in , crucially depend upon the complete absence of any additional disturbance anywhere in the system . Specifically , we show that the information rate of such capacity to zero in the presence of any such additional disturbance . As a consequence , the relevance of the robust i . e ., in the presence of feedback capacity of , which to be a fairly unexplored problem , becomes evident . 
 Finally , the main of this work are in Section . 
 Except where present , all are in the appendix . 
 A . Notation 
 For any system , the transfer function to the transform of the impulse response g , g ,..., i . e . For a transfer function , we denote by n 
 the lower triangular matrix g as its first column . We write as a shorthand for the sequence x ,..., and , when convenient , we write in vector form as , x x , where transposition . Random are non , such as non and , such as . For matrices we use upper case , such as A . We write i A to the note the i th magnitude eigenvalue of A . If An , then 
 x z 
 u y 
 Figure . Linear , causal , stable and time invariant input and output , initial state and output disturbance . 
 Ai , the entry in the intersection between the i th row and the th column . We write , with i i , to refer to the matrix formed by the i to i of A . The expression m A m to the square sub matrix along the main diagonal of A , with its top left and bottom right on Am , m and Am , m , respectively . A diagonal matrix whose are the as 
 . PROBLEM DEFINITION AND 
 Consider the discrete time system in Fig . . In this setup , the input u is a random process and the a causal , linear and time invariant system with random initial state vector x and random output disturbance z . In vector notation , 
 yn , n ,, 
 where n is the natural response the initial state x . We make the following further the around it : 
 Assumption . is a causal , stable and rational transfer function of finite order , whose impulse 
 response g , g ,... g . N 
 It is worth that there is no loss of generality in considering g , since otherwise one can write as g g , and thus the entropy gain by would be plus the entropy gain due to g , which an impulse response where the first sample . 
 Assumption . The random initial state x is independent of u . 
 Assumption . The disturbance z is independent of u and to a dimensional linear subspace , for some finite . This subspace is by the of a matrix 
 where for the countably infinite size of , such that z . Equivalently , z 
  
 , where the random vector s , z finite differential entropy and is independent of u . 
 As in the Introduction , we are interested in the entropy the presence or absence of the random u , by 
 . 
 In the next section we provide geometrical insight into the behaviour of for the situation where there is a random output disturbance and no random initial state . A formal and precise treatment of this scenario is then in Section . The other are considered in the subsequent 
 . 
 . GEOMETRIC INTERPRETATION 
 In this section we provide an intuitive geometric interpretation of how and when the entropy gain defined in . This understanding will justify the introduction of the notion of an random process in Definition below , which will be shown to play a key role in this and in related . 
 A . An Illustrative Example 
 Suppose for the moment Fig . is an FIR filter with impulse response g , g , , i . Notice that this choice , and thus one non minimum phase zero , at . The associated matrix for is 
  
 G , 
  
 whose determinant is clearly one indeed , all its are . Hence , as in the introduction , u , and thus G and in general does not introduce an entropy gain by itself . However , an interesting phenomenon becomes evident by looking at the singular value decomposition 
 of G , given by , where Q and R are unitary matrices and D , d , d , d . In this case , D . , . , . , and thus one of the singular of G is much smaller than the although the product of all singular , as . As will be shown in Section , for a stable such uneven distribution of singular only when non minimum phase . The effect of this can be by looking at the image of the cube 
 , through G shown in Fig . . If the input u were uniformly distributed over this cube of unit 
  
 Figure . Image of the cube , through the square matrix with , and . 
 volume , then would distribute uniformly over the unit volume parallelepiped in Fig . , and hence u . 
 Now , if we add to a disturbance z , with distributed over . , . independent of u , and with R , the effect would be to thicken the support over which the resulting random vector y z is distributed , along the direction pointed by . with the direction along which the support of is given by q , , the first row of Q , then the resulting support would have its volume significantly , which can be associated with a large increase in the differential entropy of y with respect to u . Indeed , a relatively small variance an still produce a significant entropy gain . 
 The above example that the entropy gain from un to yn as a combination of two . The first of these is the uneven way in which the random vector is distributed over . The second factor is the alignment of the disturbance vector with respect to the span of the subset , i i of of , associated with singular of , indexed by the in the set . As we shall discuss in the next section , non minimum phase , then , as , there will of going to zero exponentially . Since the product of the singular of for all , it that , i must grow exponentially with , where , i is the i th diagonal entry of . This that the span of , i i , compensating its shrinkage along the span of , i i , thus keeping un for all . Thus , as , any small disturbance distributed over the span of , i i , added to , will keep the support of the resulting distribution from shrinking along this subspace . Consequently , the expansion of the span of , i i is no longer , yielding an entropy increase proportional to log , i . 
 The above analysis one to anticipate a situation in which no entropy gain would take place even when some singular of tend to zero as . Since the increase in entropy is made possible by the fact that , as , the support of the distribution of along the span of , i i , no such entropy gain should arise if the support of the distribution of the input un accordingly along the pointed by the , i i of . 
 An example of such situation can be easily as : Let in Fig . have phase and suppose that u is as , where u is an i . i .. random process with bounded entropy rate . Since the determinant of n for all , we have that un u n , for all . On the other hand , yn nu n u n . Since for some finite 
 recall Assumption , it is easy to show that , 
 and thus no entropy gain . 
 The preceding discussion that the entropy gain produced the situation shown in Fig . on the distribution of the input and on the support and distribution of the disturbance . This in stark contrast with the well known fact that the increase in differential entropy produced by an invertible linear operator only on its , and not on the statistics of the input . We have also seen that the distribution of a random process along the different within the space which it a key role as well . This the need to specify a class of random which distribute more or less evenly over all . The following section a rigorous definition of this class and a large family of belonging to it . 
 B . Entropy Balanced 
 We begin by formally the notion of an entropy balanced process u , being one in which , for every finite , the differential entropy rate of the orthogonal projection of un into any subspace of dimension the entropy rate of u . This idea is precisely in the following 
 definition . 
 Definition . A random process is said to be entropy balanced if , for every , 
 , a 
 for every sequence of matrices , with . Equivalently , a random process is entropy balanced if every unitary transformation on 
 a random sequence yn such that . This property of the resulting random 
 sequence yn that one cannot predict its last with arbitrary accuracy by its previous , even to infinity . 
 We now characterize a large family of entropy balanced random and establish some of their . Although intuition may suggest that most random such as i . i .. or stationary should be entropy balanced , that statement rather difficult to prove . In the following , we show that the entropy balanced condition is met by i . i .. with per sample probability density function being uniform , piece wise constant or . It is also shown that to an entropy balanced process an independent random independent of the former another entropy balanced process , and that filtering an entropy balanced process by a stable and minimum phase filter an entropy balanced process as well . 
 Proposition . Let u be a i . i .. random process with positive and bounded per sample 
 variance . Then u is entropy balanced . N 
 Lemma . Let u be an i . i .. process with finite differential entropy rate , in which each is distributed according to a piece wise constant in which each interval where this is constant measure greater than o , for some bounded away from zero constant o . Then u is entropy balanced . N 
 Lemma . Let u and v be mutually independent random . If u is entropy balanced , then 
 w , u is also entropy balanced . N 
 The working behind this lemma can be intuitively by that to a random process another independent random process can only increase the spread of the distribution of the former , which to balance the entropy of the resulting process along all in space . In addition , it from Lemma that all i . i .. a per sample which can be by uniform , piece wise constant or as many times as are entropy balanced . It also that one can have non stationary which are entropy balanced , since Lemma no for the process v . 
 Our last lemma related to the of entropy balanced that filtering by a stable and minimum phase filter the entropy balanced condition of its input . 
 Lemma . Let u be an entropy balanced process stable and minimum phase filter . Then the also an entropy balanced process . N 
 This result that any stable moving average auto regressive process from is also entropy balanced , provided the of the and regression correspond to a stable filter . 
 We finish this section by pointing out two of which are non entropy balanced , namely , the output of a filter to an entropy balanced input and the output of an unstable filter to an entropy balanced input . The first of these a central role in the next section . 
 . ENTROPY GAIN DUE TO EXTERNAL 
 In this section we formalize the which were qualitatively outlined in the previous section . Specifically , for the system shown in Fig . we will characterize the entropy gain defined in for the case in which the initial state x is zero or deterministic and there an output random disturbance of possibly infinite length z which Assumption . The following will be instrumental for that purpose . 
 Lemma . Let A be a causal , finite order , stable and minimum phase rational transfer function with 
 impulse response a , a ,... such that a . Then and 
 . N 
 Lemma . Consider the system in Fig . , and suppose z Assumption , and that the input process u is entropy balanced . Let the of , where , ,... are the singular of , with , , ,, such that . the number of these singular which tend to zero exponentially as . Then 
 . N 
 The proof of this Lemma can be found in the Appendix , page . 
 The previous lemma precisely the geometric idea outlined in Section . To see this , notice that no entropy gain is if the output disturbance vector is orthogonal to the space by the . If this were the case , then the disturbance would not be able fill the subspace along which is shrinking exponentially . Indeed , if for all , then 
 , and the latter sum out the one on the of , while limn n nun since u is entropy balanced . On the contrary and loosely speaking , if the projection of the support of onto the subspace by the is of dimension , then remains bounded for all 
 n , and the entropy limit of the sum on the of the possible entropy gain . Notice that because , and thus 
 this entropy gain from the uncompensated expansion of along the space by the 
 of . 
 Lemma also the following corollary , which that only a filter with outside the unit circle i . e ., an transfer function can introduce entropy gain . 
 Corollary Minimum Phase do not Introduce Entropy Gain . Consider the system shown in Fig . and let u be an entropy balanced random process with bounded entropy rate . Besides Assumption , suppose that is minimum phase . Then 
 . 
 N 
 Proof : Since is minimum phase and stable , it from Lemma that the number of singular of which go to zero exponentially , as , is zero . Indeed , all the singular vary with . Thus and Lemma directly that the entropy gain is zero since the of is zero . 
 A . Input Do Not Produce Entropy Gain 
 In this section we show that random satisfying Assumption , when added to the input u i . e ., before , do not introduce entropy gain . This result can be from Lemma , as stated in the following theorem : 
 Theorem Input do not Introduce Entropy Gain . Assumption . Suppose that u is entropy balanced and consider the output 
 y . 
 where b a , with a being a random vector satisfying a , and where . Then , 
  
 Proof : In this case , the effect of the input disturbance in the output is the forced response it . This response can be as an output disturbance . Thus , the argument of the differential entropy on the of is 
  
  
 . 
 Therefore , 
 u . 
 The proof is by substituting this result into the of and that 
 . 
  
 Remark . An alternative proof for this result can be given based upon the of an sequence , as . Since , , we have that un un . 
 Let and be a matrices with which satisfy and such that is a unitary matrix . Then 
 , 
 where we have applied the chain rule of differential entropy . But 
  
 which is upper bounded for an and , the latter due to u being entropy balanced . On the other hand , since is independent of un , it that un un , 
 for all . Thus , where the last equality 
 from the fact that u is entropy balanced . N 
 B . The Entropy Gain by Output when 
 We show here that the entropy gain of a transfer function with outside the unit circle is at most the sum of the logarithm of the magnitude of these . To be more precise , the following assumption is . 
 Assumption . The and its transfer , of which are . the number of distinct , given by , i . e ., such that , with li being the multiplicity of the i th distinct zero . We denote by i , where : ,.. ,...,, the distinct zero of associated with the i th non distinct zero of , i . e ., 
 . 
 N 
 As can be from the previous in this section , we will need to characterize the asymptotic behaviour of the singular of . This is accomplished in the following lemma , which these singular to the of . This result is a generalization of the unnumbered lemma in the proof of , Theorem in the appendix as Lemma , which for FIR 
 transfer , to the case of infinite impulse response transfer i . e ., transfer . 
 Lemma . For a transfer Assumption , it that 
  
 an ,, otherwise , 
 where the in the sequence an , are positive and increase or decrease at most 
 with . N 
 The proof of this lemma can be found in the appendix , page . 
 We can now state the first main result of this section . 
 Theorem . In the system of Fig . , suppose that u is entropy balanced and that and z satisfy and , respectively . Then 
 , 
 where , min , and is as defined in Assumption . Both are tight . The upper bound is if limn n n , where the unitary matrices the 
 left singular of . N 
 Proof : See Appendix , page . 
 The second main theorem of this section is the following : 
 Theorem . In the system of Fig . , suppose that u is entropy balanced and that 
 Assumption . Let z be a random output disturbance , such that i , i , and that . Then 
 . 
 N 
 Proof : See Appendix , page . 
 V . ENTROPY GAIN DUE TO A RANDOM INITIAL SATE 
 Here we analyze the case in which there a random initial state x independent of the input u , and zero or deterministic output disturbance . 
 The effect of a random initial state in the output as the natural response it , namely the sequence n . Thus , yn can be written in vector form as 
 yn n . 
 This that the effect of a random initial state can be as a random output disturbance , which us to apply the from the previous . 
 Recall from Assumption that is a stable and rational transfer function with . As such , it can be as 
 G , 
 where is a filter only all the of , and is a FIR filter , all the of . 
 We have already established recall Theorem that the entropy gain by the minimum phase system is zero . It then that the entropy gain can be only by the of and an appropriate output disturbance . Notice that , in this case , the input process w to i . e ., the output sequence to a random input u is independent of since we have the natural response after , hose initial state is now zero . This condition us to directly use Lemma in order to analyze the entropy gain that u after being 
 by , which with . This is by the next theorem . 
 Theorem . Consider a stable th order filter , and with a random initial state x , such that x . Then , the entropy gain due to the existence of a random initial 
 state is 
 . 
 Proof : Being a and stable rational transfer function , can be as 
 G , 
 where is a stable transfer function only all the of and with all its at the origin , while is stable and FIR filter , all the of . Let and be the natural of their common random initial state x , respectively , where , . Then we can write 
 y . 
 , 
 Since is stable and , it from Corollary that un for all , and therefore 
 h yn un yn . 
 Therefore , we only need to consider the entropy gain by the possibly non minimum to a random output disturbance n , which is independent of the input . Thus , the of Lemma are met considering , where 
 the for , and , , ,. Consequently , it to consider the differential entropy on the of , whose argument is 
  
  
 , 
 where , un bounded entropy rate and is entropy balanced since is the natural response of a stable system and because of Lemma . We remark that , in , is not independent of x , which one from the proof of Theorem directly . 
 On the other hand , since is FIR of order at most , we have that , where 
 is a non singular upper triangular matrix independent of . Hence , can be written as 
 , where and , . According to , the entropy gain in as long as is lower bounded by a finite constant or if it sub linearly as . Then , we need m n to be a full row ranked matrix in the limit as . However , 
 , 
 where m the the . We will now show that these 
  
 do not go to zero as . Define the matrix such that m . Then , it that , 
  
  
 . 
 Hence , the minimum singular value of m is lower bounded by the singular value of , for all . But it was shown in the proof of Theorem see page that limn min . this result in and taking the limit , we arrive to 
 . 
 Thus 
  
 is upper and lower bounded by a constant independent v is entropy balanced , m , and , which that the entropy rate in the of to zero . The proof is finished by Lemma . 
 Theorem us to formalize the effect that the presence or absence of a random initial state on the entropy gain similar to those in Section . Indeed , if the random initial state x finite differential entropy , then the entropy gain , since the alignment between x and the is . This us to characterize the behavior of the entropy gain due only to a random initial state , when the initial state x can be written as pst , with , which that x an undefined or differential entropy . 
 Corollary . Consider an FIR , order filter , such that its random initial state can be written as x , where and . Then , 
 , 
 where , min ,. The upper bound in is when is a non singular matrix , with defined by n as in Theorem . 
 Proof : The effect of the random initial state to the output sequence y can be written as yn , 
 where . Therefore , an , it that 
 h 
 remains bounded , for , if and only if limn . 
 Define the rank of as ,... If m , then the lower bound is by in . Otherwise , enough such that , 
 . 
 We then proceed as the proof of Theorem , by considering a unitary matrix , and a matrix An such that 
 H 
 This procedure us to conclude that , and 
 that the lower limit in the latter sum when is a full row rank matrix . the latter into the proof . 
 Remark . If the random initial state x is with , then the entropy gain by an FIR minimum phase at least log . Otherwise , the entropy gain could be identically zero , as long as the of fill only the orthogonal space to the span of the row in m , where En , and m are defined as in the proof of Theorem . 
 Both , Theorem and Corollary , reveal that the entropy gain as long as the effect of the random initial state with the first of , just as in the of the previous section . 
 . EFFECTIVE ENTROPY GAIN DUE TO THE INTRINSIC OF THE FILTER 
 If there are no and the initial state is zero , then the to an input un is given by . Therefore , the entropy gain in this case , as defined in , is zero , regardless of whether 
 or . 
 Despite the above , there is an interesting question which , to the best of the knowledge , not been before : Since in any filter the entire output is longer than the input , what would happen if one the differential of the complete output sequence to that of the shorter input sequence As we show next , a proper definition of this question recasting the problem in of a new definition of differential entropy . After providing a geometrical interpretation of this problem , we prove that the new entropy gain in this case is exactly . 
 A . Geometrical Interpretation 
 Consider the random u , u u and , y y y related via 
 . 
 , 
 Suppose u is uniformly distributed over , , . the conventional definition of differential entropy of a random sequence , we would have that 
 h y , y , y y , y y y , y , 
 because y is a deterministic function of y and y : 
 y . 
 In other , the problem in that although the output is a three dimensional vector , it only two of freedom , i . e ., it is restricted to a dimensional subspace of R . This is in Fig . , where the set , , is shown with the u plane , together with its image through as defined in . 
 As can be seen in this figure , the image of the square , through is a dimensional rhombus over which y , y , y uniformly . Since the intuitive notion of differential entropy of an ensemble of random such as how difficult it is to compress it in a fashion to the size of the region by the associated random vector , one could argue that the differential entropy of y , y , y , far from being , should be somewhat than that of u , u since the rhombus , a area than , . So , what does it mean that and why should y , y , y Simply put , the differential entropy to the volume by the support of the probability density function . our example , the latter three dimensional volume is clearly zero . 
  
 Figure . Support of u laying in the u plane to that of u the rhombus in R . 
 From the above discussion , the comparison between the differential of R and u R of our previous example should take into account in a two dimensional subspace of R . Indeed , since the multiplication by a unitary matrix does not alter differential , we could consider the differential entropy of 
  
 y 
 ,, 
 the matrix with in the singular value decomposition of 
 G T . 
 and is a unit norm vector orthogonal to the of and thus orthogonal well . We are now able to compute the differential entropy in R for , corresponding to the rotated version that its support is now with R . 
 The preceding discussion the use of a version of the notion of differential entropy for a random vector which the number of actually 
 of its length . 
 Definition The Effective Differential Entropy . Let be a random vector . be written as a linear transformation , for some u , , then the effective differential entropy defined as 
 h , Ay , 
 where AT is an for , with . N 
  
 It is worth that differential entropy of a vector , whose support is greater than zero , from considering it as the difference between its absolute entropy and that of a random variable uniformly distributed over an dimensional , unit volume region of . More precisely , if in this case the probability density function of y y is integrable , then . . , 
 , 
 where is the discrete valued random vector resulting an dimensional uniform with cubic quantization with volume . However , if we consider a support to an dimensional subspace of , i . e ., AT , as in Definition , then the entropy of its version in , say , is distinct from Ay , the entropy of Ay in . Moreover , it turns out that , in general , 
 , 
 despite the fact that A . Thus , the definition given by does not yield consistent for the case wherein a random vector a support dimension i . e ., its number of of freedom smaller that its length If this were not the case , then we could redefine by , in a spirit similar to the one behind dimensional entropy . To see this , consider the case in which u uniformly over , and . Clearly , uniformly over the unit length segment the origin with the point . Then 
  
 On the other hand , since in this case Ay u , we have that 
 . 
 Thus 
  
 The latter example further why the notion of effective entropy is appropriate in the setup considered in this section , where the effective dimension of the random does not coincide with their length it is easy to verify that the effective entropy not change if . Indeed , we will need to consider only which can be by multiplying some random vector u , with bounded differential entropy , by a tall matrix , with as in , which are precisely the by Definition . 
 B . Effective Entropy Gain 
 We can now state the main result of this section : 
 Theorem . Let the entropy balanced random sequence u be the input of an filter , and let y be its output . Assume that is the transform of the length sequence . Then 
  
 N 
 Theorem that , when considering the full length output of a filter , the effective entropy gain is by the filter itself , without the presence of external random or initial . This may seem a surprising result , in view of the made in the previous , where the entropy gain only when such random exogenous were present . In other , when observing the full length output and the input , the maximum entropy gain of a filter can be in of the volume expansion by the filter as a linear operator , provided we measure effective differential instead of differential entropy . 
 Proof of Theorem : The total length of the output , will grow with the the input , FIR , and will be infinite , . Thus , we define the output length function 
 is FIR with i .. length , 
 l , length input is u , . 
 It is also convenient to define the sequence of matrices , with 
 h 
 G ni , i , ni , i . This one to write the entire output of a causal 
 i , i , j 
 impulse response to an input u as 
 y . 
 Let the , where , is 
 diagonal with positive , and is unitary . 
 The effective differential entropy one of un by 
  
 nun un 
 , 
 where the first equality from the fact that un can be written as , which that un un . But 
 G . 
 unitary , it that , which that . 
 The product , is a symmetric matrix , with its first column , h h , given by 
 . 
 Thus , the sequence to the to of those resulting from the complete convolution , even when the , where the time reversed perhaps infinitely large response . Consequently , the and ¨ theorem , it that 
  
 where is the discrete time transform of . 
 In order to finish the proof , we divide by , take the limit as , and replace in the 
 latter . 
 . SOME 
 A . Rate Distortion Function for Non Stationary 
 In this section we obtain a simpler proof of a result by Gray , and , which the rate distortion function of a non stationary auto regressive process x of 
 u 
 w y 
 Figure . Block diagram representation of how the non stationary source x is built and then reconstructed as u . 
 a certain class to that of a corresponding stationary version , under distortion . Our proof is based upon the in the previous , and the class of non stationary for which the in are valid . 
 To be more precise , let and be the impulse of two linear time invariant A and A with rational transfer 
  
 , 
 where pi , i ,...,. From these it is clear that A is unstable , A is stable , and 
 A A , ,. Notice also that lim A and lim A pi , and thus 
  
 M 
 a 
 Consider the non stationary random source x and the asymptotically stationary source by passing a stationary process w through A and A , respectively , which can be written as 
 x 
 A block diagram associated with the construction in Fig . . Define the rate distortion for these two as 
 , lim ,, 
 n , 
 , lim ,, 
 n , 
 where , for each , the are taken over all the conditional probability density 
 and yielding E and E , respectively . 
 The above rate distortion have been in for the case in which w is an i . i .. process . In particular , it is explicitly stated in , that , for that case , 
 . 
 We will next provide an alternative and simpler proof of this result , and extend its validity for general not necessarily stationary w , the entropy gain of non minimum phase established in Section . Indeed , the approach in is based upon asymptotically equivalent matrices in of the covariance matrices . This w to be and i . i .. and A to be an all pole unstable transfer function , and then , the only non stationary is that from unstable . For instance , a innovation by an unstable filter A would yield a source which cannot be Gray and approach . By contrast , the reasoning behind our proof w be any process , and then let the source be Aw , with A unstable and possibly and stable as well . 
 The statement is as : 
 Theorem . Let w be any stationary process with bounded differential entropy rate , and let 
 x and be as defined in and , respectively . Then . N 
 Thanks to the in the previous , it is possible to give an intuitive outline of the proof of this theorem given in the appendix , page by a sequence of block . More precisely , consider the shown in Fig . . In the top diagram in this figure , suppose the for the non stationary source . The sequence u is independent of , and the linear filter is such that the error a necessary condition for minimum . The filter is the product of A see in the appendix a stable , filter with unit frequency response magnitude such that . 
 If one now the filter towards the source , then the middle diagram in Fig . is . By doing this , the stationary source with an additive error signal u that the same asymptotic variance as u , reconstructed as u . From the invertibility of , it also that the mutual information rate between and that . Thus , the channel u the same rate and distortion as the . 
 However , if one now a short the error signal u as in the bottom diagram 
  
 Figure . Block diagram representation of the of in the proof of Theorem . 
 of Fig . , then the resulting additive error term u u will be independent of and will have the same asymptotic variance as u . However , the differential entropy rate of u will exceed that of u by the of . This will make the mutual information rate between and to be less than that between and by the same amount . Hence , be at most . A similar reasoning can 
 be to prove that . 
 B . Control 
 Here we revisit the setup shown in Fig . and in Section I . Recall from b that , for this general class of control , it was shown in , Lemma . that 
 , 
 where are the of the plant in Fig . . 
 By the in show next that equality in b provided the feedback channel the following assumption : 
 Assumption . The feedback channel in Fig . can be written as 
 w , 
 where 
  
  
 Figure . Top : The class of feedback by Assumption . Bottom : an equivalent form . 
 A stable rational transfer such that is , the same unstable as , and the feedback the plant . 
 F is any possibly non linear operator such that , , for all , and 
 . N 
 An illustration of the class of feedback satisfying this assumption is on top of Fig . . Trivial of satisfying Assumption are a additive channel and by linear . Indeed , an system with a strictly causal transfer function , the feedback channel that Assumption is widely known as a noise shaper with input and post filter , used in , e .. . 
 Theorem . In the control system of Fig . , suppose that the feedback channel Assumption and that the input u is entropy balanced . If the random initial state of the plant , with , x , then 
 . 
 N 
 Proof : Let and , A . Then , from Lemma in the appendix , the output yn can be written as 
 y u , 
 , state x , s 
 where s is the initial state of and 
 u , u . 
 see Fig . Bottom . Then 
 I x ; yn yn yn x 
 yn nu n 
 nu n nu n 
 nu n nu n , 
 where the initial state s to yn , the initial state x to the output of , and the initial state x of to yn . Since u is entropy balanced and c finite entropy rate , it from Lemma that u is entropy balanced as well . Thus , we can proceed as in the proof of Theorem to conclude that 
 This the proof . 
 C . The Feedback Channel Capacity of non white 
 Consider a non white additive channel of the form 
 , 
 where the subject to the power constraint 
 . 
  
 and z is a stationary process . 
 The feedback information capacity of this channel is by a input , and is given by 
 lim , 
  
 Figure . Block diagram representation a non white the scheme considered in . 
 where is the covariance matrix of and , for every , the input is to depend upon the channel since there a causal , noise less feedback channel with one step delay . 
 In , it was shown that an auto regressive moving average process of th order , then can be by the scheme shown in Fig . . In this system , is a strictly causal and stable finite order filter and v is with for all such that is with a positive definite covariance matrix . 
 Here we use the in Section to show that the information rate by the capacity scheme in to zero if there any additive disturbance of length at finite differential entropy affecting the output , no matter how small . To see this , notice that , in this case , and for all n , 
  
 yn In 
 yn In 
 yn In yn 
 I z v z , 
 n 
 since In . From Theorem , this gap between differential is precisely the entropy gain by In to an input when the output is affected by the disturbance . Thus , from 
 Theorem , the capacity of this scheme will correspond to , where are the of , which is precisely the result stated in , Theorem . . 
 However , if the output is now affected by an additive disturbance d not passing through such that , , with , then we will have 
 yn In . 
 In this case , 
  
 yn In 
 yn In 
 yn In . 
 But limn n In In , which directly from Theorem to each of the differential . Notice that this result irrespective of how small the power of the disturbance may be . 
 Thus , the capacity scheme in and further studied in , although of theoretical importance , would yield zero rate in any practical situation , since every real signal is unavoidably affected by some amount of noise . 
 . 
 This paper provided a geometrical insight and rigorous for the increase in differential entropy rate to as entropy gain by passing an input random sequence through a discrete time linear time invariant filter such that the first sample of its impulse response unit magnitude . Our time domain analysis us to explain and establish under what the entropy gain with what was by , who a approach to a related problem in his seminal paper . In particular , we that the entropy gain only if outside the unit circle i . e ., it is non minimum phase , . 
 This is not sufficient , nonetheless , since the input and output be u and , the difference 
 is zero for all , yielding no entropy gain . However , if the distribution of the input process 
 u a certain regularity condition defined as being entropy balanced and the output the form , an output disturbance with bounded differential entropy , we have shown that the entropy gain can range from zero to the sum of the logarithm of the of the of , depending on distributed . A similar result is if , instead of an output disturbance , we let have a random initial state . We also considered the difference between the differential entropy rate of the entire and longer output of and that of its input , i . e .,, where is the length of the impulse response of . For this purpose , we the notion of effective differential entropy , which can be applied to a random sequence whose support dimensionality smaller than its dimension . Interestingly , the effective differential entropy gain in this case , which is intrinsic to , is also the sum of the logarithm of the of the of , without the need to add or a random initial state . We have some of the of these in three . Specifically , we used the fundamental here to provide a simpler and more general proof to characterize the rate distortion function for non stationary and distortion . Then , we applied our to provide sufficient for equality in an information inequality of significant importance in control . Finally , we that the information rate of the capacity scheme in for the channel with feedback to zero in the presence of any additive disturbance in the channel input or output of sufficient finite length , no matter how small it may be . 
 APPENDIX 
 A . of Stated in the Previous 
 Proof of Proposition : Let be the per sample variance of u , thus . Let 
 yn , . Then , where In is the identity matrix . 
 As a consequence and thus . 
 Proof of Lemma : Let be the the sample is constant . Let be the of these . Define the discrete random process c , where i if and 
 only if . . Then 
  
 , 
 where the inequality is due to the fact that un and yn are deterministic of un , and hence un yn . un from we obtain 
 h yn un yn I ; un un 
 c u c . 
 n 
 Hence , 
  
 where the last equality from Lemma see Appendix whose are met because , given , the sequence un independent each of them distributed uniformly over a possibly different interval with bounded and positive measure . The opposite inequality is by following the same as in the proof of Lemma , from onwards , which the proof . 
 Proof of Lemma : Let yn , , where is a unitary matrix and where and have . Then 
 h yn yn yn 
 yn 
 We can lower bound as : 
 h yn nun 
 nun , 
 nun , 
 nun , 
 nun . 
 Substituting this result into , dividing taking the limit as , and that , since 
 u is entropy balanced , then , lead us to 
 . 
 The opposite bound over can be from 
 , 
 where n is a jointly sequence with the same second order moment as . Therefore , 
 , with i being the variance of the sample i . The fact 
 that a bounded second moment at each entry i , and the latter inequality in , 
 satisfy , which the proof . 
  
 Proof of Lemma : Let yn , where is a unitary matrix and where and have . Since , we have that 
 . 
 Let be the of , where An is an orthogonal matrix , and is a diagonal matrix with the singular of . Hence 
 h . 
 It is straightforward to show that the diagonal in are lower and upper bounded by the and singular of , say and , respectively , which 
 . 
 But from Lemma , limn limn , and thus 
 , 
 where the last equality is due to the fact that u is entropy balanced . This the proof . 
 Proof of Lemma : The fact that limn is upper bounded directly from 
 the fact that A is a stable transfer function . On the other hand , An is positive definite with all its equal to , and so is positive definite as well , with limn . Suppose that limn . If this were true , then it would hold that limn A nA . But A n is the lower triangular matrix associated with A , which is stable since A is minimum phase , that limn A nA , thus leading to a contradiction . This the proof . 
 Proof of Lemma : Since is unitary , we have that 
  
 , 
 n 
 where 
 , n , , , 
 , . 
 the chain rule of differential entropy , we get 
 . 
 Notice that . Thus , it only remains to determine the limit of as . We will do this by a lower and an upper bound for this differential entropy and show that these converge to the same expression as . 
 To lower bound we proceed as 
  
  
  
  
  
  
  
  
 , 
 where a from as well to the set , while and stem from the independence between u and z . Inequality is a consequence of , and e from to the set in the second term , and that is not reduced upon the knowledge of . 
 On the other hand , 
 , 
 then , by and in , dividing by , and taking the limit , we obtain 
 m 
 n un Xi , i m un 
 m 
 , 
 where the last equality is a consequence of the fact that u is entropy balanced . 
  
 We now derive an upper bound for . the random vector 
 x , 
 we can write 
  
 where 
 m , , ,.... 
 Therefore , 
  
 . 
 Notice that by Assumption and thus is restricted to the span of of dimension , for all . Then , for , one can construct a unitary matrix , such that the of An span the space by the of and such that B 
 . Therefore , from , z 
 An z 
  
 KA 
  
 where and KAn z are the covariance matrices of A and A , 
 respectively , and where the last inequality from . The fact that and are bounded and remain bounded away from zero for all , and the fact that min either sub exponentially since singular decay exponentially to zero , with , imply in that 
 . 
 But the fact that that . This , together with the 
 assumption that u is entropy balanced 
 , 
 which with the lower bound found in , the proof . 
 Proof of Lemma : The transfer function can be as , where is stable and minimum phase and is stable with all the non minimum phase of , both being rational . From Lemma , in the limit as , the of are lower and upper bounded by min and T , respectively , where min 
 . Let and the , respectively , with , , , and , , , being the diagonal of the diagonal matrices respectively . Then 
 G 
 the i th row of by , i be , we have that , from the Courant theorem that 
  
  
 v span , : 
 , id n , 
 Likewise , 
 i 
 min 
 v span , i : 
 , id n , 
 Thus 
 . 
 The result now directly from Lemma in the appendix . 
 Proof of Theorem : In this case 
 m . 
 Notice that the of the matrix m n span a space of dimension , ,.., which that one can have m n if . In this case i . e ., if limn m n then the lower bound is by the latter expression into and Lemma . 
 We now consider the case in which limn m n . This condition that there large such that for all . Then , for exist unitary matrices 
 , 
  
 where An and An have , such that 
  
 H 
 Thus 
  
  
  
 The first differential entropy on the of the latter expression is uniformly upper bounded because u is entropy balanced , m , and . For the last differential entropy , 
  
 notice that m . Consider the An m , being unitary , being diagonal , . We can then conclude that 
 . 
 Now , the fact that 
 AST 
 one to conclude that 
 . 
 that A and that is unitary , it is easy to show by the 
 theorem that 
 , 
 with equality if and only if A . Substituting this into and then the latter into we arrive to 
 . 
 Substituting this into , the fact that u is entropy balanced and Lemma the upper bound in . Clearly , this upper bound is if , for example , n n is non singular for large , since , in that case , and we can choose An I 
  
 and An . This the proof . 
 Proof of Theorem : As in , the transfer function can be as , where is stable and minimum phase and is a stable FIR transfer function with all the phase of in total . u n , nun , we have that yn nu n , u n un , and that is entropy balanced from Lemma . Thus , 
 h yn un un nu n u n . 
 This that the entropy gain of due to the output disturbance z to the entropy gain due to the same output disturbance . One can then evaluate the entropy gain of by Theorem to the filter instead of , which we do next . 
 Since only the z are non zero , it that in this case see Assumption . Therefore , m n m n and the sufficient condition given in Theorem will be satisfied for if limn , where now is the left unitary matrix in the . We will prove that this is the case by a 
 contradiction argument . Thus , suppose the contrary , i . e ., that 
 lim Q 
 n . 
 Then , there a sequence of unit norm , with for all , such that 
  
 For each , define the length image , and decompose them as 
 t 
 such that an and . Then , from this definition and from , we have that 
 , , a 
 lim b 
 n 
 c 
 As a consequence , 
 , 
 where the last equality from the fact that , by construction , is in the span of the , together with the fact that is unitary which that . Since the top m 
 in decay exponentially as , we have that 
 , 
 where is a finite order polynomial of from Lemma , in the Appendix . But 
  
  
  
 Taking the limit as , 
  
 where we have applied and the fact that m is bounded and does not depend on . Now , notice that is a matrix with the convolution the impulse response its time reversed version , respectively on its first row and column . It then from , Lemma . that 
  
 the inequality is strict because all the of are strictly outside the unit disk . Substituting this into we conclude that 
 , 
 which . Therefore , to a contradiction , the proof . 
 Proof of Theorem : Denote the product of A as 
 , 
 which clearly 
 , , 
 , 
 where b is the first sample in the impulse response of . Notice that that limn n limn n kunk for every sequence of random u with uniformly bounded variance . Since only stable and its coincide exactly with the of A , it that A is a stable transfer function . Thus , the asymptotically stationary process defined in can be as 
 x n , , 
 where is a lower triangular matrix with its main diagonal equal to b . 
 The fact that is with b as in that for any un with finite differential entropy 
 , , 
 G 
 which will be next . 
 For any given , suppose that is chosen and and un are distributed so as to minimize I ; un subject to the constraint E E I E kunk i . e ., , un is a realization of ,, yielding the reconstruction 
 y . 
 Since we are considering mean squared error distortion , it that , for rate distortion , un must be jointly with . From these , define 
 u n , , 
 y n , x n u n , n , n n u n . 
 where is a zero mean vector independent of u n , n with finite differential entropy such 
 that , k . Then , we have that , I ; yn a I ; I n ; n 
  
  
  
  
  
  
  
  
 where a from being invertible , is due to the fact that n n u n , because un . The equality from u n un see . Equality in e because n u n , and in because of . The last inequality because n n and n . But from Theorem , limn n u n un , and thus , limn n n ; n . 
 At the same time , the distortion for the source n when reconstructed as n is 
  
 where a because is bounded , and is due to the fact that , in the limit , is a unitary operator . the of and , we conclude that limn n n ; n ,, and therefore 
 . 
 In order to complete the proof , it to show that . For this 
 purpose , consider now the asymptotically stationary source n , and suppose that n un ,. Again , n and un will be jointly , satisfying un the latter condition is for minimum . From this , one can propose an alternative realization in which the error The change of and the in this chain of is by the block shown in Fig . . sequence is u , , yielding an output n n u n with n u n . Then 
 , I n ; n n 
 a un 
 u n 
 u n n 
 n n 
 I n ; n 
 I ; 
 e ; yn , I 
 where a by that n un and because un , from , is a consequence of n u n , from the fact that n n u n . Finally , e because is invertible for all . Since , asymptotically as , the distortion by yn for the non stationary source is the same which is when n is reconstructed as recall , we conclude that 
 , the proof . 
 B . Technical 
 Lemma . Let u be a random process with independent , and where each element is uniformly distributed over possible different , such that ai amin , i , 
 for some positive and bounded amin . Then u is entropy balanced . N 
 Proof : Without loss of generality , we can assume that ai , for all i otherwise , we could scale the input by amin , which would scale the output by the same proportion , increasing the input entropy by amin and the output entropy by log amin , without the result . The input 
 vector un is confined to an box Un the support of un of volume and entropy 
 . This support is an box which of different volume . Each of 
 these is determined by un to ai , and freely over . Thus , the volume of each box is the product of sizes ai of the associated selected free sweeping . But that ai for all i , the volume of each box can be upper bounded by . With this , the added volume of all the in the original box can be upper bounded as 
 . 
 We now use this result to upper bound the entropy rate of . 
 is a unitary matrix and where and 
 have . From this definition , will distribute over a finite region 
 , corresponding to the projection onto the dimensional span of the of . Hence , is upper bounded by the entropy of a uniformly distributed vector over the same support , i . e ., by , where is the dimensional volume of this support . In turn , 
 is upper bounded by the sum of the volume of all dimensional in 
 the box in which un is confined , which we already by , and which is upper bounded 
 as in . Therefore , 
 Dividing taking the limit as 
  
 On the other hand , 
 , 
 where a because is an orthogonal matrix . correspond to the jointly sequence with the same second order as , and that the distribution differential entropy for a given covariance , we obtain the upper bound 
  
 where a since the are independent , and from the fact that and from the Courant theorem . Since is bounded for all , 
 we obtain by substituting into that . The combination of this with , the proof . 
  
 We re state here for completeness and convenience the unnumbered lemma in the proof of , Theorem as : 
 Lemma . Let the function be as defined in but for a transfer function with no and only a finite number of , of which lie outside the unit circle . Then , 
  
 an ,, otherwise , 
 where the in the sequence an , are positive and increase or decrease at most 
 with . N 
 Lemma . Let be rational transfer function of relative degree , with initial a rational transfer function of initial state s . Let 
 y , u , 
 where u is an exogenous signal . Then 
  
 where the initial state of is x and the initial state of can be taken to be x s . 
 Proof : Let . Define the following : 
 Then the recursion corresponding to is 
 , 
  
 This that the initial state of to 
 x x x x . 
 Let and . be written as 
 X 
 t 
 i i , 
 X 
 t 
 i , , i 
 which that the initial state of can be taken to be 
 s , s s s . 
 Since , it that 
 . 
 Combining the above , it is found related to the input u by the following recursion : 
 , 
 t p 
 i Xi i , i 
 p 
 Xi i , , 
 which to , 
  
  
 . state x 
 . state x , s 
  
 ﻿ We present empirical on the achievable gains stemming from the use of wireless remote radio in a typical urban environment . Our work is based on simultaneous path loss of the base station and links to outdoor street level . We statistically characterize the increase in received power , when a is added to improve the coverage by a base station . We consider diverse coverage for the mobile terminal , the effect of position with respect to the intended . We also compare the power gains that would be in practice from combining the from the base with those of the , such as selection combining and maximum ratio combining . We conclude that under practical , the of will depend very strongly on the existence of line of sight links between the and the intended . For at low , below the clutter , only in a street canyon position with respect to the will obtain a benet . Our data also that the gains in noise ratio when maximum ratio combining are only marginally better than those of the much simpler selection combining . 
 One of the great for today wireless communication is to provide adequate spatial coverage in a cost effective way , while and interference adequate for high frequency re use . To meet this challenge , during the last few there been a growing interest in the study of , relay and low complexity , . These may constitute relatively simple , low cost and easy to install when to the deployment of an additional base station to serve mobile within the cell . 
 While are around a small coverage base station with low transmit power connected to a wired network , a is connected to a through a wireless link , being able to repeat or re code the data by amplify or decode and forward , among . A repeater may be thought of as an amplify and forward relay with no or ability . In this context , there are two basic of : wireless , and remote radio , also known as ber , connected to a by an optical ber . 
 will suffer large and small scale fading at both and links . In contrast , for wireless , achievable gains will only depend on the quality of the connection , which it an attractive solution to provide connectivity to wireless in dense urban , where the links to the may experience shadowing and ber connectivity for the may be readily available . 
 Proper placement of a is obviously a fundamental factor in the compromise between the desired gains and the cost associated with its installation . In this regard , in an urban environment it is reasonable to expect that while coverage will improve with antenna height , this will at the same time have a negative impact on deployment and on channel interference , thus affecting frequency reuse in a large system , . If the is to cover an area of a size comparable to a small cell , then it is reasonable to assume that this is best by its antenna in normally used by base i . e above the clutter height . In such well established propagation such as those of and al . will be adequate to predict the coverage by the . Alternatively , they may be positioned at lower with the aim of in more limited , i . e . generating within a large cell . Coverage in growing interest as it can provide local in signal to noise ratio and thus higher data , . This may be without generating excessive interference in neighboring and at lower implementation when to a base station , , , particularly when below the surrounding clutter height . Numerous empirical have been for the statistical characterization of path loss in relatively radio links . Analytical based on optical geometry have also been for in urban , . Goldsmith al . , based on a collection of , a mathematical description of the radius of coverage of a . Such may be used to characterize the link portion of a repeater . al . based on outage capacity for a very short range outdoor indoor environment . In , the different of and in cellular , concluding that careful placement can improve capacity substantially by transferring from heavily loaded to lightly loaded . 
 The . Relay Task Group and the WINNER Consortium have the use of both empirical and theoretical to predict path for wireless links with transmission below the clutter of , also applicable to links . While the accuracy of for diverse been the subject of extensive , their use in wireless more than treating each link individually , since the joint statistics of and links may not correspond to those of independent random . Statistics for power gains stemming from the use of or based on simultaneous in urban , as here , have to the best of our knowledge not been . 
 In this work , we report on path loss of the and links in a typical outdoor urban environment . We evaluate diverse related to the performance improvement that can be when a in an area by a base station . We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user . Our will be useful to calculate the achievable in coverage and in transmission experienced by mobile . were carried out in an area within a range of of the , which is at lamp post height , considering that this type of setting will be typical for a practical deployment of low complexity . To avoid our statistics we randomly chose and for the at various from the . The maximum range was by the requirement that at all , particularly those , the received power would be within the by our channel sounding system . We note that our system was capable of measuring path that would considerably exceed those compatible with a typical link budget in a wireless network . We also considered two of . One was chosen so as to maximize the likelihood of to the and the other chosen in close proximity to the , but from direct street view by construction . In this way we were able to consider the effect of blocking a , as may occur when a surrounding construction is after the placement of the ber repeater . Thus our statistics allow us to quantify the of a for randomly mobile within a given range considering in coverage and in transmission . From the joint data of path loss for the and links we were able to compare the effectiveness of Selection Combining and Maximal Ratio Combining at the mobile terminal . To this effect we considered typical transmit and antenna gains at the and and calculated received signal power at the under the condition of equal noise power for both . It was found that the statistical gain of over is less than under all tested , due to the random power imbalance between the links . 
 Our also us to calculate between for the and links . The low validate an approach based on considering them as independent random . 
 The measurement campaign extended over a period of during summertime . The urban area used as test bed in a Mar , Chile , a mix of high rise and two story with ranging between and , built on a plane region at sea level . of received power at were carried out at street level . The area is by nearby that a transmitter at a location typical for a base station covering a relatively large urban area . The transmitter height was above the measurement region at a distance of to the position . The was mounted in two on the exterior of a high building . The streets in the measurement area are lined with with ranging between and . All links were non line of sight , with due to the surrounding . All were within a radius of centered at the low height . Regarding the distance of these to the , were within , between and , and between and , which us to also obtain range dependent statistics . A schematic description of the terrain and of our measurement scenario are in . and . 
 At the position , a continuous wave transmitter based on a oscillator at . with output power through a sector type antenna . The antenna used had azimuth and elevation at . All were within this . At the , the antenna used was a vertically dipole with gain , a signal at . with output power . Power at the were carried out an model NA spectrum analyzer that simultaneously tracked the at . and . . A noise of in a was low noise . The antenna used was a gain vertically dipole at a . height . The measurement system included a computer that acquired the spectrum analyzer at a rate of per second . Within the selected area all power the noise by at least . 
 The was at an altitude of at two different : one at a street intersection so as to simultaneously illuminate two streets and the other one within of the placement but with no direct path to the adjacent streets . In the case there are and user , while in the second all are . The measurement campaign for both involved moving the along a straight path of about . at each of the that were selected at random within of the , as in Fig . . This was done a very slow moving vehicle in such a way that approximately equally spaced power were collected . At the frequency , this more than received power at spaced at least half a apart . From these we the path loss at each receiver position , which the calculation of average path loss and small scale fade statistics at each of the . 
 To evaluate achievable power gains under realistic we calculated the received power at the mobile station assuming typical for transmit and antenna gains . the path for all measured we then the cumulative distribution function of the received power at the , considering the alone , the alone , the with the under a selection combining scheme and the with the considering a maximum ratio combiner . We assumed the existence of a with of transmission power . The and are assumed to be the same as the used in our with gains of and , respectively . For the we assumed that in most practical the would be attached to a wall and consequently only illuminate a sector of in azimuth . All our are in fact within a sector of such angle . We thus considered that a typical , sector antenna would be used , consistent with in . For the transmit at the we considered two that cover what could be considered the high and low of a practical deployment : and . 
 To quantify the gains we dene as the increase in received power that can be to at a given availability level . Let be the received power at the . We denote the of power received from the alone as and the of the power received under as . The gain in received power when is then : 
 The same type of of course to the gain through . We will use to calculate , at various availability , the power gain resulting from the use of a . 
 In this section discuss the that can be when a low altitude height . We consider a random selection of within a radius of the . In order to verify that our selected urban do not deviate from those by for wireless , we our path loss with those by such . For the links between and , we found a good match to the COST model and to the alternative WINNER path loss model for the case in an operating frequency of . . With respect to these , our measured were slightly higher , and on average , with ... of and respectively . The measured for the street canyon links were far lower than those for links at the same distance , from to , with an average of . They were however higher by about on average than those by the advanced model for Type with both and below . For the collection of mobile that we considered , we also empirical for both and links according to a log distance model , with free space at . The resulting were and log respectively . We note that most of our links were partially by relatively large . To evaluate the effect of choosing a position , we considered two . One had an uncluttered path to two streets , while the other was in close proximity but by construction . The placement is here as Non and the second as O . The position links to some of the chosen . For a setting such as this , where both are below clutter , links are as being in a . Correspondingly , we use this notation and refer to the links as non street canyon . In the radius , half of the links are of the street canyon type while all O links are non street canyon . When considering a radius all links to the become of type . This according to the link is at which can be in practice to benet most from a given placement . 
 As an example of the achievable gains we present in Fig . the of the received power at the within the radius , for the case where the at for both . All other are as in section . As seen , the for and are virtually indistinguishable , the shift being less that . We observe that the resulting from the use of a very much depend on its placement . For example , at coverage , the will guarantee a power of at least . an will only provide a slight improvement , the same power to of , while an will increase this coverage to . this calculation under the assumption of a transmit power of in coverage to and respectively . In what we summarize the power gains for various the importance of proper placement of the with respect to the intended . 
 Figure . of received power at for coverage radius and power . height was . 
 To evaluate the effect of coverage range we divided the where the power were carried out into three with of up to , and , to the . Table I the power gain as in with a at three availability for two transmit and the two . We recall that for radii of or less the placement street canyon type links to all , while only of the are in that condition for the radius . The consequence , clearly in the table , is that the gains available to of the sharply when the coverage radius is reduced to , i . e . when all of them are in a street canyon with respect to the . Both Fig . 
 Figure . of received power at street canyon for coverage radius and power . 
 and Table I illustrate the importance of proper of the . We further explore this below . 
 Based on our above we divided the for all links with up to length into street canyon and canyon , rather than by distance as before . Table the for the power gain under this , again the previously detailed in section . The illustrate that very considerable power gains are available for street canyon links , even at low . In contrast , the row corresponding to the non street canyon links very modest gains regardless of the position . The latter were considering only that are of non street canyon type under both , so that are based on a common set of data . 
 To illustrate the gains achievable under the most favorable and , we present in Fig . the for received power considering all street canyon within the range of , at a power of . When this with Fig . , it becomes clear that the street canyon are those that contribute most to the in received power . For example the will provide at least to of . This same minimum power will be available to of if a is included . this for an assumed transmit power coverage to virtually . 
 Finally we calculated correlation of for the and links , for O and as 
 where and represent respectively the path , the small scale , from the and 
 the to a position . E and E are the statistical of these path at the distance under consideration , from a linear regression of path loss . distance . and are the standard of the with respect to these . 
 In both O and these were found to be below . . When small scale are out , this to . . 
 Our empirical suggest that a important gains in received power even when as low as and operating at the relatively low transmit power of . However , these gains are basically associated with the existence of or , equivalently , street canyon links to , and thus heavily depend on site and user . In all , the gains achievable are only marginally than those by a , to the fact that , in practice , the likelihood of the and the base providing comparable is very small . 
 Virtually no correlation . was between the of the user link and the base user link . This treating them as independent random when modeling a wireless system by , the appropriate statistical description of the individual links . 
  
 ﻿This paper presents novel results on the optimal design of Noise-Shaping Differential Pulse-Coded Modulation coders. The main contribution resides in the derivation of explicit analytic formulas for the optimal filters and the minimum achievable frequency weighted reconstruction error. A novel aspect in the analysis is the fact that we account for fed-back quantization noise and that we make no restrictions on the order of the filters deployed. 
 Analog-to-Digital converters which utilize a scalar quantizer and linear, time invariant filters in a feedback loop have been extensively employed as a source coding method since the concept was first introduced in the ’s. The generalized form of this architecture, which we denote Noise Shaping Differential Pulse Code Modulation  NS-DPCM, can be represented as in Fig. . The filters in a NS-DPCM system allow one to account for the correlation between consecutive input samples, and to spectrally shape the quantization noise in the output, so as to minimize the frequency weighted mean square reconstruction error FWMSE. Special cases of the NS-DPCM architecture include ?-Modulators, DPCM converters [], and noise-shaping converters, such as one and multi-bit Sigma-Delta modulators []. NS-DPCM converters are extensively used in the context of audio compression [], digital image half-toning [] and oversampled A/D conversion []. 
 Provided that the input power spectral density PSD, frequency weighting error criterion, and scalar quantizer characteristics are known, the design of an NS-DPCM converter that achieves minimum FWMSE amounts to finding the corresponding optimal filters. This has been an intense area of research for at least  years. However, available to date results on optimal filter design for NS-DPCM encoders have been obtained assuming either fixed, finite order filters [], [], []– [], negligible fed back quantization noise [], [], or have relied upon heuristic design methods [], []. Since optimal performance can, in general, only be attained by arbitrary order filters designed accounting for fed back quantization noise, an exact characterization of the optimal performance and filters for NS-DPCM converters has remained an open problem. In this paper we derive an explicit analytic expression for the optimal performance and filter frequency responses for NS-DPCM converters. We characterize the scalar quantizer via its signal-to-noise ratio, and adopt a white quantization noise model []. The performance bound obtained corresponds to the minimum FWMSE that can be achieved by an NS-DPCM encoder-decoder with any linear, time-invariant filters. A key departure from [] which, to the best of our knowledge, gives the only currently available explicit analytic solutions to the problem, is that we account for fed back quantization noise. This allows us to derive exact expressions. 
 Our results show that an optimal NS-DPCM converter exhibits several interesting properties. These include a spectrally flat frequency weighted error spectrum, and a white signal at the input of the scalar quantizer. We also show that, for AR Gaussian sources, the rate-distortion efficiency with the optimal filters depends only on how efficient the embedded scalar quantizer is at quantizing nearly Gaussian samples. 
 We use standard vector space notation for signals. For example, x is used to denote {xk}k?Z. We also use z as the argument of the z-transform. Given two square integrable complex valued functions f? and g? defined over [-p,p], we adopt the inner product where * denotes complex conjugation. We denote the usual 
 -norm as  is a transfer function, then we use the short hand notation F to refer to the associated frequency response Fej?, ? ? [-p,p]. If I is a set, then we will write “a.e. on I” almost everywhere on I as a short hand notation for “everywhere on I except at most on a zero Lebesgue measure set of points”. 
 We use sx to denote the variance of a given wide sense stationary w.s.s. random process x, having PSD Sxej?. Note that, where 
 allows one to describe the Kolmogorov’s minimal prediction error variance [] of a w.s.s. process x via ?x , ?Sx = ??x. The spectral flatness measure of a w.s.s. process x is denoted . It is easy to show that  = ?x = , and that ?x =  if and only if Sxej? is constant a.e. on [-p,p]. 
 As foreshadowed in the introduction, we consider the general form of an NS-DPCM architecture shown in Figure . In our model, the input sequence x is assumed to be a zero mean, w.s.s. random process, with known PSD Sx = |?x| satisfying Sxej? > ,a.e. on [-p,p]. The element denoted by Q describes a scalar quantizer, with given and known characteristics. For each input vk, k ? Z, it outputs wk and generates the quantization error nk , wk-vk. The three discrete-time filters Az, Bz and Fz in Fig.  are design choices. 
 To asses performance, we introduce the delay-compensated frequency weighted error 
 ² , Pzx˜ - z-tx,  where t = . The error weighting filter Pz models the impact that reconstruction errors have on each frequency. Thus, it is application dependent. 
 ¯ In this paper, we restrict attention to the cases in which¯ ¯Pej?¯ > , ?? ? [-p,p], i.e., Pz has no zeros on the unit circle. Additionally, we require: 
 Constraint : Az,Bz, Fz and Pz are stable. In addition, Fz is strictly causal i.e., limz?8 Fz = . 
 The first part in the above constraint is required in order to avoid unbounded signals in the NS-DPCM converter. The additional requirement on Fz is needed for the feedback loop in Fig.  to be well defined see, e.g., [, Chap. ]. 
 Since the NS-DPCM architecture embeds a nonlinear element a scalar quantizer within a feedback loop, exact analysis of quantization errors is, in general, a formidable task []. This has motivated the widespread use of an additive noise model for quantization errors []–[], []–[]. This model allows one to study the converter via linear analysis tools. It is usually formulated as follows: 
 Assumption : The quantization errors are i.i.d. random variables, uncorrelated with the input signal. 
 In order not to limit our subsequent analysis to a specific type of scalar quantizer, the following is also assumed: 
 Assumption :	The probability density function PDF of v is not affected by the filters in the converter other than via its second moment  . 
 Under Assumption , any given type of scalar quantizer with a fixed number of quantization levels leads to quantization errors whose variance is proportional to the variance of its input. This can be stated as 
 where ? is the signal-to-noise ratio of the scalar quantizer not to be confused with that of the NS-DPCM encoder-decoder system. ? depends on the number of quantization levels, the PDF of the signal being quantized and the companding characteristics of the scalar quantizer itself . 
 Our ultimate goal is to find the frequency responses of the filters A, B, and F that minimize the variance of ² under Assumptions  and , and for given and known ?x, P and ?. The quantity s² so obtained will constitute the achievable lower bound on the FWMSE for the NS-DPCM converter. 
 Towards the above goal, we first derive an expression that relates the decision variables to the error measure that we wish to minimize. From equation, Assumption , and recalling we have where sn is the variance of the quantization error, and is a delay compensated version of AB, the frequency response from x to x˜. The first term on the right hand side of  corresponds to the variance of the frequency weighted quantization error in ². The second term in  accounts for the frequency weighted linear distortion introduced by the filters in the encoder-decoder pair . 
 The variance is related to via . From Assumption , the latter is given by . Combining this result with  gives 
 The above expression relates the filters Az,Bz,Fz, and the quantizer signal-to-noise ratio ?, to the FWMSE. Minimization of this cost functional will yield expressions for the optimal filters and performance. 
 For comparison, we note that the cost functional , together with Assumptions  and , is also part of the analysis in [], [] and [], wherein equivalent optimization problems are addressed  . 
 In this section we derive explicit analytic expressions for the optimal filters and the associated optimal performance for the NS-DPCM scheme, subject to a mild restriction. The analysis is based on a set of equations that the optimal filters must necessarily satisfy. To facilitate the flow of ideas, all proofs are given in the Appendix. 
 Minimization of  is simplified by noting that, for stable and strictly causal Fz, it holds that kFk = k - Fk - . Substitution of this equality into  yields 
 Notice that the cost functional in  involves only two unknown functions, namely W and F. This makes it simpler to work with than the functional in . 
 The optimization problem can be further simplified by writing the optimal W in terms of | - F|. Unfortunately, the relationship between F and the optimal W, for the general case, can only be stated implicitly, as shown next. 
 Remark : Notice that, from , W is a positive, symmetric and real valued function of ?. It then follows from  that the product of the optimal filters Az, Bz must exhibit linearly decreasing phase. 
 In general, the presence of |W| in the inner product on the right hand side of  makes it difficult, if not impossible, to express the optimal W explicitly in terms of F. However, under specific conditions on ?, ?xP and | - F|, an analytical explicit solution to  can be obtained, as follows: 
 Lemma : Provided , a.e. on [-p,p], then, for a given frequency response F, the optimal W satisfies , a.e. on [-p,p]. 
 To summarize our results so far, we have shown that, provided  holds, F determines the optimal W through . 
 These two, in turn, determine the optimal A and B via  and , respectively. We can now state the main result of this paper: 
 Theorem :If a.e. on [-p,p], then the minimum achievable frequency weighted reconstruction MSE of an NS-DPCM converter is 
 The results stated in Theorem  have very interesting consequences. Some of these consequences are discussed below. 
 Optimality of Scalar Quantization Without Feedback:It is easy to verify from the results in Theorem  that scalar quantization without feedback is optimal if and only if |?xP| is constant. In particular, it follows from  that if |?xP| = , a.e. on [-p,p], then the optimal NS-DPCM converter reduces to a PCM converter with a fully whitening pre-filter and a post-filter satisfying |A| = ?|?x|- and |B| = |A|- ?/? + . 
 Comparison with []: The minimum FWMSE for an NS-DPCM system derived by Noll in [], neglecting fed back quantization noise, is. Perhaps surprisingly, Theorem  shows that the optimal performance is slightly better compare with . Moreover, the corresponding optimal filters AN, BN and FN derived in [] 
 respectively. Substituting these expressions into  actually yields an FWMSE , where is the spectral flatness 
 Total Frequency Weighted Distortion is White: It follows from  and  that, in an optimized NS-DPCM system, the PSDs of frequency weighted quantization noise and linear distortion are, respectively 
 Theorem  holds, the noise shaping effected by an optimal NS-DPCM system is not “complete”, i.e., frequency weighted quantization noise is not white. However, the PSD of the total frequency weighted error is white, since 
 Relation with the Reverse Water-Filling Paradigm:The parametric Rate-Distortion formula for a Gaussian w.s.s. process and FWMSE as the distortion measure is given by the well known reverse water-filling paradigm see, e.g., []. 
 For, it predicts total frequency weighted distortion to be equally distributed over frequency. It also predicts the input signal to appear at the output with PSD 
 , i.e., less significant spectral components of x suffer higher attenuation. Interestingly,  is equivalent to Sx |P| = s?², a.e. on [-p,p]. Furthermore, S² is flat, as discussed in c above, and  yields Sx |W| |P| = Sx |P| - s?², in full agreement with the above prediction. 
 Output of the Scalar Quantizer is White: It can be seen from a that, unless |?xP| is constant, the optimal A is not a full whitening filter for ?x. Interestingly, however, it is straightforward to verify that the optimal filters in  render a sequence w see Fig.  with flat PSD. More precisely, Sw , |?x| |A| + sn | - F| = ?, where ? is the same arbitrary constant that appears in a. A remarkable implication is that the quantized output of the optimized NS-DPCM converter can be efficiently translated into bits by means of a first-order entropy coder. 
 Rate-Distortion Analysis: The rate-distortion efficiency of any source encoding scheme with quadratic error as distortion measure can be established by comparing its 
 against the upper bound derived by O’Neal in []. For the case s² = min? Sxej?Pej?, and restricting to Gaussian inputs, this bound [, eq. ] can be written as 
 SNRmaxdB , R - log?x - log?P ,  where R denotes the bit-rate in bits per sample, ?x is the spectral flatness measure of x and ?P is the minimum variance associated with P see Section I. Notice that R is Shannon’s upper bound [] for the SNR in decibels of encoding a Gaussian memoryless source. 
 On the other hand, under the conditions of Theorem  and using , the best achievable SNR of an NS-DPCM system is given by: 
 By comparing   and , we see that the SNR of the NS-DPCM converter optimized via Theorem  departs from the information-theoretic upper bound  as follows: 
 The difference ?SNR , log?-R for of Gaussian sources has long been known for a variety of scalar memoryless quantizer types see, e.g. [] and the references therein. Assuming v to be Gaussian in the optimized NS-DPCM converter, ?SNR can be approximated by -., -., -. and -. for a uniform quantizer with entropy coding E.C., non-uniform quantizer with E.C., non-uniform quantizer optimized for MSE without E.C., and uniform quantization without E.C. and a loading factor of , respectively  see []. 
 VI. CONCLUSIONS This paper has derived explicit analytic expressions for the best achievable performance and optimal filters for noiseshaping DPCM encoders. These expressions, which we believe to be novel, were found by accounting for fed back quantization noise in the optimization. The results presented in this paper simplify the analysis and design of NS-DPCM converters, and provide valuable insight into the trade-offs inherent in linear feedback quantizers. 
 Proof: SinceRln· is a monotonically increasing function,¡R ¢ minimization of fg is equivalent to minimizing ln fg . From Jensen’s inequality and the constraint f ? B+, we obtain 
 Equality is obtained in a if and only if f = ?g-,a.e. on [-p,p], for some ? > . Inequality b becomes equality if and only if. This completes the proof. 
 Substituting this into  yields . Notice that  guarantees that the denominator on the right hand side of  is strictly positive. The proof is completed by noting that substitution of  into  gives the inequality , thus validating our initial supposition. 
 Proof: [Theorem ] Suppose the optimal F is such that  in Lemma  holds. Then, one can substitute  into  to obtain 
 Requiring Fz to be stable and strictly causal from Con-¯ ¯ straint  is equivalent to requiring the function ¯ - Fej?¯ to belong to the set of non-negative log-integral functions defined in , see, e.g. [, Theorem ..] and []. Then, it follows from Lemma  that the optimal | - F| is as in a. Substitution of the latter into  yields . It also follows from a that the inequality in Lemma  is equivalent to the condition required by the theorem. This validates our initial supposition. Notice also that the latter inequality also guarantees that k-Fk < ?+, as required by Condition . Finally, substituting a into ,  and  yields the remaining equalities of . This completes the proof. 
 ﻿In this paper we propose a new approach to robust optimal experiment design. The key departure from earlier work is that we specifically account for the fact that, prior to the experiment, we possess only partial knowledge of the system. We also give a detailed analysis of the solution for a simple case and propose a concave optimization algorithm that can be applied more generally. 
 It is well known that the choice of experimental conditions has a strong influence on the accuracy of models obtained from system identification experiments. This has motivated substantial research on experiment design over almost a century. Early results appear in the statistics literature (Wald, 1943; Cox, 1958; Kempthorne, 1952; Kiefer and Wolfowitz, 1960; Karlin and Studden, 1966; Federov, 1971; Whittle, 1973; Wynn, 1972). This work was later adapted to the problem of identification of dynamic systems (Levadi, 1966; Gagliardi, 1967; Goodwin and Payne, 1973; G.C. Goodwin and Murdoch, 1973; G.C. Goodwin and Payne, 1973; Arimoto and Kimura, 1973; Mehra, 1974). Some of the later work is summarised in (Goodwin and Payne, 1977; Zarrop, 1979). 
 Our focus here will be on experiment design for dynamic systems. Early work on this problem focused predominately on frequency domain designs. More recently, there has been substantial effort devoted to the inter-relationship between identification and control together with the associated issue of input signal design (Hilderbrand and Gevers, 2003; Hjalmarsson, 2005). However, a major drawback of all the above work is that, generically, the optimal test signal for dynamic system identification is a function of the unknown system (i.e. it depends on the very thing that the experiment is aimed at finding). 
 Indeed, quoting (Hjalmarsson, 2005): “It should be noted that, as usual in experiment design, in order to compute the optimal design the true system has to be known. Methods that are robust with respect to uncertainty about the system is a wide open research 
 The goal of the current paper is to propose a methodology for addressing this problem. In particular, we formulate a robust optimal experiment design criterion. We also demonstrate the use of this criterion for a simple case. 
 The layout of the remainder of the paper is as follows: In Section 2 we give a general formulation of the robust optimal experiment design problem. In Section 3, we focus on a simple (one parameter) problem so as to give insight into the problem. In Section 4 we convert the problem to an approximate matrix formulation and discuss asymptotic behavior when the prior knowledge is diffuse. In Section 5 we describe an algorithm for computing the robust optimal experiment and give a numerical example. In Section 6 we describe an extension to multi-parameter systems. Finally, in Section 7 we draw conclusions. 
 Our focus in the current paper is on how to design the experimental conditions so that the information obtained from a particular experiment is maximized in some specific sense. 
 To motivate our approach we consider a single input single output linear discrete time system of the form: 
 yt = G1(q)ut + G2(q)wt (1) where G1(q), G2(q) are rational transfer functions in the forward shift operator q, G2(8) = 1 and where 
 {wt} is Gaussian white noise of variance s. We let   where ? denotes the parameters 
 We recall that the log likelihood function for data Y given parameters ß, is given by 
 Fisher’s information matrix is obtained by taking the following expectation (Goodwin and Payne, 1977) 
 We assume an open loop experiment so that wt and ut are uncorrelated. We also assume that G1(q), G2(q) and s have no common parameters. Taking expectations, as in (4), we have that M can be partitioned as 
 where M1 is the part of the information matrix related to ? and M2 is independent of the input. Then 
 Notice that M1 depends on the full parameter vector ß. Assuming N is large, it is more convenient to work with the scaled average information matrix for the parameter ?, i.e. 
 It is also possible to do a parallel development (Goodwin and Payne, 1977) for continuous time models. In the latter case, (11) is replaced by 
 where G1 and G2 are continuous time transfer functions (assumed independently parameterized) and fu(?) is the continuous time input spectral density. 
 Since M is a matrix, we will need a scalar measure of M for the purpose of experiment design. In the nominal case treated in the literature (i.e. when ß is 
 assumed known), several measures of the “size” of M have been proposed. Examples are 
 where f(ß,?) is a frequency dependent vector related to the ?-gap (Hilderbrand and Gevers, 2003). 
 Thus nominal experiment design is aimed at choosing fu(·) to maximize a function of the type shown in (13), (14). Note, however, that the optimal input spectrum depends, inter-alia on the unknown parameter vector ß. To address this paradox, we propose an alternative robust optimal experiment design procedure. 
 We assume that we do not have complete a-prior knowledge of the true parameter value ß. Instead, we assume that the parameters can take any value in a compact set T. We propose that fu*(·) be chosen as: 
 where J is any suitable scalar measure of M. Note that we also allow J to depend explicitly on ß. This is standard in nominal experiment design, see for example (14). Also, we can use the explicit dependence on ß to 
 associate different weighting to M(ß,fu) depending on the nature of the prior knowledge regarding ß. 
 We also need to constrain the allowable set of input signals. A typical constraint used in experiment design is that the input energy is constrained i.e. we define 
 We note that the results are independent of s as it appears only as a scaling factor in (8). For clarity in the following discussion we assume white noise and hence only refer to the parameter ? in the sequel. 
 To give insight into the robust optimal experiment design problem, we consider a simple continuous time problem where G2 (s) = 1 and 
 Nominal experiment design assumes that an initial estimate, is available. Based on this information, the function fu (·) is chosen so as to maximize some scalar function of  subject to a constraint on the output or the input power. 
 One interesting observation is that equations (18) and (19) define a convex combination of the set of all single frequency matrices. This leads to several use- 
 ful facts, e.g. any value of M (?,fu) for fu ? S can be generated by a finite number of frequencies. Also, in the scalar parameter case of equations (18) and (19) it can actually be shown that we need only use a single frequency input for optimal experiment design (Goodwin and Payne, 1977), namely, fu (?) = d (? - ?*). Moreover, by differentiation it is readily seen that the optimal input frequency is 
 This is an intuitively pleasing result, i.e. one places the test signal at the (nominal) 3dB break point. However, equation (20) reinforces the fundamental difficulty in nominal experiment design, namely the optimal experiment depends on the very thing that the experiment is aimed at estimating. 
 Fig. 1. ?2M (?,fu) as a function of ? for nominal input (dots), “1/f” noise (solid) . 
 To gauge how important the dependence on ? is, we note thatin our example decays at the rate of 40dB per decade as a function of both ? and ?. Hence, given the prior estimate of the paramefrequency. Also, say that the true parameter lies in ter, ?, say we choose for the input signal 
 proximately 1/100th of the nominal value! This seems to suggest that nominal experiment design is limited to those cases where a good prior estimate is available. A 
 The reason for multiplying by ?2 is that M-1 is a variance measure and thus   gives relative 
 We next turn to the robust experiment design described in Section 2. For the scalar parameter problem 
 In subsequent sections, we will give further insights into the above design problem. 
 Lemma 1. Consider the problem stated in equation (22), the optimal input has all its energy inside T. Namely, 
 Here we develop an algorithm for the robust experiment design problem, as formulated in the last section, where we approximate the integral in equation (22) by a Riemann sum. Specifically, utilising Lemma 1, we 
 We can now state the following discrete alternative to the optimization problem in equation (22): 
 We next show that if there exists a feasiblesuch that all entries ofare equal, then E is optimal. 
 negative we must conclude from (34) and (35) that ??? Since by definition, all the entries of E are nonE  which completes the proof. 
 With this specific choice we have the following result for our original problem (22), 
 Lemma 3. The robust optimal test signal (for diffuse prior information) has spectrum approximately given by 
 that for the matrix A in (28), apart from the first and last few rows, the row sum is very nearly constant. 
 Namely, the conditions in Lemma 2 are approximately satisfied which completes the proof. ??? 
 Remark 4. The immediate consequence of Lemma 3 is that band limited “ ” noise is an approximation to the robust optimal input for our example (see Figure 1). 
 We show that the optimization problem (30) can be converted into a standard linear programming (LP) problem. Let us denote 
 then we can readily show that (30) is equivalent to the following optimization problem: 
 We consider the scalar parameter problem described in Section 3 where we assume  , 
 (i)	A nominal input of frequency 1 radsec (Note that this is the optimal input if the initial estimate of the 
 (iii)	Band limited ‘f1’ noise input, [0.1,10] rad/sec. (iv) The robust optimal input generated by LP. 
 We see from Table 1 that “1/f” noise is approximately an order of magnitude better than a white noise input in terms of the cost function (30). Furthermore, going to the true optimum gives a further 40% improvement. The optimal input energy is shown in Figure 2 and Fig- 
 ure 3 shows the corresponding values of ?2M (?,fu) as a function of ?. It is interesting to note from Figure 
 3 that ?2M (?,fu*) is an almost constant function of ?. This should be compared with the result in Lemma 2. The latter Lemma predicts that if the input that generates  independent of ? is feasible, then 
 it is optimal. Here, the input which gives ?2M (?,fu) constant is not feasible but we see from Figure 3 that the optimal values of  are almost constant. 
 expression for M (ß,f) given in (11) and (12). We can again convert this problems into an approximate discrete form as was done in Section 4. We write 
 as an approximation to the integral in (12). The index k refers to the (discretized) element ?k of the parameter set T, the index m denotes the frequency and Em denotes the input energy at the mth frequency. 
 In this case, we see that Qk is a matrix for each discrete parameter value. Hence, as discussed in Section 2, we need a measure of the “size” of Qk. Say we choose J (Qk,k), then the robust optimal design becomes 
 Obviously, there are many choices for the scalar function J. One possible choice is 
 This cost function is motivated by the following observation: Defining the scalar function f (E) = 
 which shows that f (E) is a concave function of E. Hence, solving (44) becomes a standard concave maximization. This latter aspect is the subject of the journal version of this paper. 
 This paper has proposed a robust optimal experiment design procedure. We have argued for a simple case, that a near optimal input is band-limited “1/f noise”. We have also proposed an algorithm to design robust experiments in more general cases and have presented results showing the gains obtained from the use of the algorithm compared with using either the “nominal optimal experiment”, band limited white noise or band-limited “1/f” noise. 
 ﻿ This paper an empirical study of the achievable data of network multiple input multiple output zero forcing , zero forcing dirty paper and dirty paper actual by indoor wireless channel at . . Their are with those of conventional , in which either the base are not , or their interference is frequency division . The were taken in aisle to office and large unobstructed hall . The study of these that , at high signal to noise , and can yield more than a three fold increase in attainable data when to and . The gains are smaller , but still significant . At low the system is noise rather than interference limited , and only gains . The in this paper also show that collaborative such as can benefit from interference prone to yield transmission capacity . With regard to the propagation channel , the classical log normal plus fading model , with fitted to the scenario type , was found to be good at the statistics of the achievable data of all the considered . 
 Index Wireless communication channel characterization and modeling , performance analysis . 
 HERE is an ever increasing demand for high data in and with all the mobility provided by wireless technology . Since growing of are to be within confined , the performance of practical indoor wireless is to be limited , increasingly often , by interference . Therefore , in order to improve or even maintain high data , it will become necessary to make use of communication capable of the coupling between the various propagation links within a given service scenario so as to reduce interference and increase received signal power . 
 This management of the interference produced by access and can be by of . all operating in a fashion turns the set of and into a network multiple input multiple output system , and the communication medium into a broadcast channel . Network hold the potential of interference in a , greatly increasing efficiency in wireless , . 
 Several have been in the literature to take advantage of the high between several and a group of that is often in . The technique dirty paper , been shown to be capacity in this setting , and thus it is optimal , . is a nonlinear technique that full knowledge of channel state information at the transmitter . 
 The high implementation complexity of the need to consider other simpler , near optimal transmission . One such technique , commonly to as zero forcing , , sum close to those of with smaller computational complexity . In , part of the interference is removed by linearly combining the intended for all so as to effectively obtain , from end to end , a lower triangular channel matrix . The of the channel matrix , which perfect at the transmitter side , it possible to avoid part of the interference . The effect of all interference is by in a sequential fashion . 
 Another suboptimal technique , simpler than and , is zero forcing as in . Zero forcing is an entirely linear strategy . In this case , have perfect and to eliminate interference for all , yielding an effectively diagonal channel matrix between data and . 
 Simpler to attain communication in a are if each a single user and its is limited to the radio link to that user only . In our work , we consider two possible . The first one is frequency division , that is , splitting the available spectrum into disjoint frequency , each one to a single user pair , with each at full power . The second approach , which will be to as the strategy , is to let each transmit at full power all the spectrum available , thus that interference from other will limit data as transmission power . The simpler and in general have inferior than those of and . On the other hand , the former impose the minimum possible load , since each needs to know only the data intended for and the related to the user it is serving . There exist other for communication in a that do not require among and whose performance is under certain better than that of and , such as fractional frequency reuse . Although it is known that in some specific see , e .., , our objective here is only to provide a few simple as a reference for comparison . It is understood that whichever is the choice , different propagation may favor different . 
 The performance evaluation of diverse network for been in various works , . This included the assumption of log normal plus fading , and the use of the model . The effect of limited capacity between in overall performance is assessed in . 
 Regarding the channel model , it may be reasonable to assume that small scale are independent for each channel when are several apart , but this may not hold for shadow . Several report on the correlation of in different of transmit and receive . To what extent such correlation may affect the network channel , based on widely base station , is not self evident . To the best of our knowledge , no based in support of the assumption that a network channel is equivalent to a collection of independent have been . 
 The fact that in network all can act in a fashion taking advantage of , two related of capacity improvement . Interference between user can be reduced and , at the same time , the total received signal power at each user terminal can be . Since these effects depend on the degree of connectivity or , conversely , isolation , this the question of how much performance improvement can be for a given type of deployment scenario . 
 In this paper , we evaluate and compare the maximum data achievable by the , and , for measured as well as by indoor , under a per antenna power constraint . These were all calculated under the assumption of perfect at the transmitter . Although this would be hard to attain with currently available technology , the serve as a comparison basis , being the best performance achievable under equally for all . Channel were for two representative : with alongside them , and large . A single slope path loss model with log normal shadow fading and Rice small scale , was found to be adequate in all the for the at which were taken . With regard to achievable data , we found that at high in the range of , and achieve about a three fold gain when to or . The gains of are somewhat smaller , depending on the scenario , but still quite significant . A particularly interesting finding is that both not only are able to effectively mitigate the effect of interference but also in some can exploit low isolation between user , to yield higher data than in with greater natural isolation . As would be , at low , where interference is not the dominant limitation on capacity , the gain over the non . Our study also that the log normal plus Rice fading model is accurate at the statistics of the maximum achievable by each of the network considered , when the model are selected according to the type of scenario . Thus , an indoor channel as a collection of independent , would yield similar to those of our based study , provided that the proper model are used . However , rather than justifying what could have been as somewhat arbitrary of to illustrate the achievable of a system , we decided to use actual measured channel data from typical indoor to validate our comparison . In the following we describe our measurement and the empirical from them . 
 In all , the custom built channel sounder used of a single channel transmitter and a channel receiver operating at . for . The single transmit antenna was at that would in practice correspond to possible of a user . Four receive , one per , were at the typical of . The used for transmission and reception were coaxial , vertically . The transmit antenna was mounted on a long rotating arm and stepwise under computer control in degree . 
 In all , were taken at night in the absence of pedestrian movement . For each of the aisle to office and , more than a complex by channel were collected , each vector the complex gains from a user location to the four . 
 The single conversion receiver channel at . The receiver output were converted into a vector of complex channel gains the Fast Transform . Phase coherence was by locking the transmitter and receiver to . Exact synchronization of sampling data corresponding to an integer number of of the , which without the need for . For each position of the transmit antenna , the channel were simultaneously for second , and the total interval was partitioned into non of each . This us to verify the consistency of our which should only differ as a consequence of receiver noise . Since the capacity on the 
 relative rather than the absolute phases of the , we calculated phase with respect to one of the , arbitrarily chosen as a reference . In all our , the ratio between the average gain magnitude and its fluctuation , calculated over the sequence of , . Thus by our , we further reduced the effect of the measurement noise . 
 We in two of which we considered relevant and where , as our data subsequently confirmed , different propagation are to be . These scenario are as : 
 Aisle to office : Channel were carried out in three different aisle to office . In all the building was a steel concrete structure with interior made of wood and . For these , the four receive were wall mounted at a . height along straight , with a separation from the interior . Adjacent were by approximately . The transmit antenna was at height inside various and adjacent to the aisle . These are non line of sight . The between the transmit antenna and the receive ranged from to . The of transmit and receive for scenario aisle to office is schematically in Fig . . In this figure , the and represent and user , respectively . The other two of this type had similar . In each office , were the rotating arm on three or four different user , more than apart from one another , as space would allow for simplicity , only one of these user per office is shown in Fig . . 
 Large Hall : We from two large . The first one , hall , was a , glass central hall , about high , with concrete floor and several lateral leading to and on two . This hall is built with steel with concrete separating the hall from the adjacent . The four receive were at the of an imaginary rectangle , with each antenna at approximately from a wall on the hall , at of about . . The transmitter rotating arm was in different over a grid of within the rectangle formed by the receive . This is a line of sight scenario , in which link ranged from to . 
 The second hall scenario , hall , was a gymnasium with wooden floor and concrete . The four receive were as in hall , while the rotating arm was in different over a grid inside the gymnasium . Transmitter receiver varied from to nearly . 
 Before the channel to calculate the data achievable with network , we describe the statistical of the in each scenario . 
 The measured channel gains proved to be consistent with what been in the literature for similar . We found that log normal shadow fading combined with or small scale in an excellent model fit to our , provided the model are to the measured data . The statistics of each type of fading are below . 
 For each wireless link , small scale fading statistics were by the channel gain corresponding to a single turn of the rotating arm . As , in all aisle to office , these channel gains fading statistics that were well by a distribution . This is consistent with a situation . 
 In the hall and hall , these gains fit a Rice distribution with factor between and . It is worth that despite the fact that these are , they are , as the previous , by strong propagation , although to a lesser extent . In all , the channel phases associated with each angular position of the rotating arm were uniformly distributed and uncorrelated . 
 The large scale fading statistics were by calculating , at different , the of the channel gains over an arm rotation . The statistics of these average gains were well by a log normal distribution . More precisely , for each link distance , the path loss , in , corresponding to each rotation channel gain the behaviour of a random variable of the form 
 log , where is a path loss exponent , is a zero mean random variable with variance and is the path loss in at , which we chose as . For each measured scenario , the model by was fitted to measured large scale path , finding and by linear regression , and then setting as the empirical variance of log . The of and so are listed in Table I . For each scenario type , a 
 row in the model resulting from combining all data of its corresponding . 
 It is worth that no evidence of a break point in the path loss exponent was found from the . More precisely , no slope were found to provide a better match in a least sense to our empirical data . This is consistent with what been before for short range indoor , . 
 It was also found that the correlation between the shadow in the links between a transmitter antenna and any two was , in all , below . in modulus . This was true even when the two in all several apart were seen by the transmitter at an angle narrower than . 
 In this section we present a brief review of the transmission to be in their relation to the channel model . 
 A general wireless channel between single antenna and single antenna can be by a complex valued matrix . the vector of by the access by , the vector of received can be written as 
 where the noise . i .. circularly symmetric complex with variance . Notice that ,, the element in the th row and th column of , is the narrow band channel gain between the th and the th user . Notice also that no joint of the output of the channel is , since it is assumed that between is not practical . 
 For the three network considered in this work , can be as 
 where the th element in the vector u is the information bearing signal intended for the th user , is a linear matrix , and : is either a non linear transformation , in the case of , or the identity matrix , in the case of . We note that an element be a function of two or more user only if the are able to operate . Following standard practice , we will assume that the of u are independent zero mean complex random with 
 where x is the variance of the th element of , for some maximum power . All channel matrices considered in the sequel are by , that is , , our analysis to four simultaneously by four . 
 We include and as , against which to compare the other three network . In the case of , each a single user , and all operate without , at maximum power over the same frequency band . In of the model , this toto be a diagonal matrix , or any row or column permutation of it . Since the user signal can be chosen freely , there is no loss in generality in assuming for this case that I , where I is the identity matrix . With this , the sum rate achievable with an system is readily found to be 
 In this expression , , is the power of the signal as received by its intended user . On the other hand , 
 user , produced by the serving all the other . , the interference affecting the th 
 In this case , each at full power a fraction of the available spectrum . We will assume that this spectrum is partitioned into adjacent non equal width . Therefore , assuming the same total as for , the maximum achievable rate of is trivially given by 
 The idea in is to eliminate , in the , all interference produced by the . To do this , the is chosen as . This all to have and each of them to process the intended to all . 
 In this case , the highest sum rate per second per achievable under a is readily found to be 
 and where the entry on the th row and th column of . The optimization problem defined by and been shown to be convex in . Therefore , a global maximum for the right hand side of can be numerically standard convex optimization . Notice that the solution to subject to will , in general , yield antenna transmit for one or more , ,...,. is known to perform poorly at low but it is easy to improve its achievable rate under such by a inverse of its true inverse . We denote this variant of as zero forcing . 
 This technique was first in and then further studied in . The idea behind is to utilize the linear assure that the th user no interference from the , for all , and then use to avoid the effects of the interference . More precisely , is chosen so as to obtain 
 where , are the of some lower triangular is the th element of the vector 
 the unitary matrix in a decomposition of , i . e ., , an upper triangular matrix and the conjugate transpose operator . Choosing 
 which the interference avoidance outcome by with . invertible as is the case in all the channel matrices considered in this work , then all this factorization differ only by a sign inversion in any subset of their . Therefore , in our case , the absolute value of each element of and thus well is fixed known . 
 In , the effect of the interference by the sum at the right end of is by dirty paper . The latter , at the end , perfect as well as full knowledge of all user . After and the result through the channel , each is as if there had been no interference at all . Therefore , the achievable sum rate of is given by 
 where , are the in the diagonal where the maximization is over all row power that satisfy the for each permutation . Notice that in this case the , in are the of in for the row matrix . 
 It is easy to show that the optimization problem defined by and is also convex for each permutation . To see this , it to notice that the change of , and , ,, the optimization problem defined by , equivalent to , . 
 The performance of been channel data in , under a sum power constraint , and in , under a . In both , was shown to be superior to and various non . 
 Although the practical implementation of still specific code , we use this technique as an ultimate upper bound for the achievable data in each scenario . 
 In , no a restriction is on the matrix . Here , the signal intended for user , , is by possibly all as if there were no other being , i . e ., . In contrast , the data at user is into , the fact that all have full , which together with knowledge of , its interfering effect . The resulting signal is then sent possibly all as well . Notice that , by doing this , is added to the signal received by user as interference . A similar process is employed to successively encode the data for the , previously as known interference . In this setting , the signal with power all through the th column of , thus at the corresponding user with power as before , , the entry on the th row , th column of . Therefore , the maximum rate achievable with this technique is 
 Fig . . of rate per user , when , and achieve their maximum sum , under the power constraint , from empirical channel taken in scenario aisle . 
 where the maximum is taken over all of the over all matrices and 
 satisfying for each permutation . An efficient method to numerically solve the optimization problem by been in . This is the method we used to evaluate for the measured and channel data . 
 In the next section we use the above to compare the performance of the various in realistic indoor . We use both actual measured channel matrices as well as channel matrices by the model from our . 
 In this section we evaluate the maximum sum of , and as respectively by , , , and , subject to the , for the measured channel matrices in all . We then repeat the sum rate evaluation channel matrices by the model , the appropriate as derived from our . For the transmission , we calculate the per user at specific , i . e ., the maximum per user rate that is to be met or with some given probability . 
 We further assume that the in this system is perfectly unbiased in that , on average , all are an equal amount of time . At the same time , our procedure of to user is designed to avoid that will yield high interference when better exist . To achieve both we proceed , on each time slot , as : 
 Assign each user the access point providing the signal and group the with the same . 
 It is assumed that the number of in each list is comparable . Over a long period of time this fairness as all have an equal chance of being , while the possibility of choosing in the same time slot more than one user the same preferred . More precisely , the channel matrix , in every time slot is such that 
 This that interference prone such as are not unfairly in our comparison by particularly poor . In addition , and are ordered so that the th is the path loss for the th user . This a channel matrix in which the magnitude element in each row on the main diagonal of the matrix . 
 The fact that the above procedure in equal likelihood of service for all was confirmed in our by that when the algorithm was repeated many times , any specific choice of position received service the same number of times . While it may be possible to find that further benefit the performance of the scheme , it should first be that these gains are not at the expense of fairness . 
 As is to be , unobstructed hall provide less natural isolation between user than aisle to office . This in the fact that the hall were by a significantly smaller path loss exponent , see Table I . As a consequence , for any given distribution of between and , the channel matrices associated with hall will tend to be less diagonally dominant , i . e ., the off diagonal be on average . The effect of this on the rate achievable by each transmission technique will be later in this section . 
 In all the , the noise variance in the receiver , for all full band i . e ., excluding , was chosen to be , which roughly to the thermal noise in a receiver with a noise figure of , operating at room temperature , over a of . 
 For each aisle to office scenario , by channel matrices satisfying were selected by randomly choosing from the set of measured channel data for the corresponding site . 
 Fig . . User at different for aisle to office first column and hall second column . The of the per user , are chosen so as to maximize the sum rate in each scheme , is shown in Fig . for scenario aisle to office , under the b with . The in the other aisle to office were quite similar . It can be seen directly from the graph that at this transmit power limit , the three network outperform and for all . We also observe that the performance of and are very close , which is consistent with before for i . i .. . It should also be noted that the optimization used only assure that the sum capacity of is the best of all . While this was in all our , it does not necessarily imply that at all availability , the per user rate of will exceed that of the other . In fact we found that for some , the would exceed those of , as can be from the at low . At high however , invariably proved to be best . As the transmit power limit is reduced , e .. to below , we found that an increasing number of have a non zero probability of being assigned zero rate when , or . This is a consequence of the fact that we are considering the per user when each of the network is for maximum sum rate , not fairness . As a consequence , the best strategy may include not serving some at all in some channel . This behavior will be in more detail below . 
 The corresponding per user of each scheme in office scenario , as a function of , are shown in the first column of Fig . , for and . On the upper edge of each of these we have included an additional horizontal axis to provide an algorithm independent measure of the received signal to noise ratio for each site . This to the average received in the setting excluding interference i . e ., supposing interfering are turned off , that is 
 where , is the th diagonal entry of the th . This would also be the true for all the full band considered here if the off diagonal of the channel matrix were zero . Since the latter condition never , this notion of is not the actual per user signal to noise ratio , which is algorithm dependent . Instead , it is the average ratio of the power received by a user from its , to the receiver noise . This ratio only on the environment , specifically on the average path loss between user , as can be seen from . 
 Figure that , in scenario aisle to office and for a power limit , and attain a data rate gain in excess of three times , for both and , when with or . Similar gains are at higher power . In relation to this , we note that , loosely speaking , for large of , a gain over of at most four times would be , since for large of and assuming perfect interference cancellation , the improvement in capacity will be dominated by the ratio of transmission used , which is in our case . 
 variance , which that the performance of is , in are small to the noise 
 noise limited . In the specific case of , the power of some will be reduced below to meet the while the channel by choosing . The cost of such interference avoidance effort is than the benefit stemming from zero interference , yielding a sum rate smaller than that with at full power without . The poor performance in low is a well known shortcoming of pure , which can be upon by zero forcing , as in . We illustrate this in Fig . which in greater detail the behavior of all at low power for the case of availability . Under such the performance improvement of over becomes evident . As previously , it can also be seen in this figure that at low power , the objective of sum rate may be by 
 Fig . . of rate per user , when , and achieve their maximum sum , under the power constraint , from empirical channel taken in scenario hall . 
 not serving some at all , and as a consequence the rate that can be to of may drop to zero . 
 A procedure similar to the one before was to generate by channel matrices in the two hall where were taken . The of the peruser with each network technique when for sum rate under power constraint are shown in Fig . , for scenario hall . In this figure we see again that the three network considered here provide higher per user data than and . 
 The corresponding per user for several as a function of for this scenario type are shown in the right column of Fig . , where scenario hall been chosen . The for the other hall scenario are very similar and are for the sake of brevity . 
 In this scenario , in the high range , and attain roughly the same gain , in per user over , as that in the aisle to office . On the other hand , in this more interference prone environment , the maximum are significantly reduced when to those of the aisle to office , by higher natural isolation between user . 
 The gain of the over is also when to the aisle to office . Loosely speaking , this can be to the fact that the aisle to office channel matrices are more diagonally dominant than the hall channel matrices . As a consequence , and in view of the disc theorem , the determinant , at times , be smaller in hall . Since for the matrix the inverse of , it will be possible have in the hall . In view of the power constraint , this smaller signal and thus smaller sum for see . In contrast , is less sensitive to the smaller determinant matrices that arise in the hall . The reason for this behaviour can be found in the fact that the determinant equal to the product of all the diagonal of . Thus , a given decrease in the determinant , in general , entail a smaller reduction of rate for see . 
 As can be from Fig . , high interference do not always result in per user . While this certainly for a non system as clearly seen from the and to a lesser degree may yield an performance in the hall in comparison to that in the better isolated aisle to office . The improvement is particularly significant a factor of around at low power , as seen at the left extreme of the . We note that the performance improvement is still evident when the comparison is carried out at equal , i . e ., when the in average path between user for the been . This behaviour can be by that , at low power , noise rather than interference is the relevant factor in limiting transmission . Since the channel matrices of the office are more strongly diagonal dominant , less power a user from access other than the one that is dominant . In contrast , in an environment with less isolation such as the unobstructed , the can take advantage of the fact that significant power from all bases will reach each user , while to turn most of this power into signal , not interference . Indeed , it can be seen from that for sufficiently large noise power , increasing the magnitude of the off diagonal produce a relatively increase in the numerator of the fraction within the log than on its denominator . Thus the signal plus interference ratio is . As transmit power however , interference becomes dominant , and the hall suffer from the need to compensate for this impairment . 
 In order to assess the accuracy of the log normal plus Rice model at the maximum achievable by the in Section , we those from channel matrices by the general model for each scenario type aisle to office or hall , that is , the in the of Table I . The data by the model were , in general , in good agreement with those from empirical data , for each scenario . Figure of per user rate with respect 
 Fig . . availability empirical and per user for scenario aisle to office . were from the model the scenario type in row All aisle to office of Table I . 
 to maximum transmit power constraint for availability for the scenario aisle to office . It can be seen that , for this scenario , the from are smaller than those from empirical data . This may be to the fact that the scenario specific parameter for aisle in the third row of Table I is smaller than that of the corresponding general model used for the scenario type in the fourth row of Table I . Thus , in this case the actual received will be on average than those by the model . 
 In this paper we have the data achievable in an indoor wireless by system by network such as zero forcing , zero forcing dirty paper and dirty paper . We these with those achievable with frequency diversity and no , measured and indoor channel matrices at . . Our show that a single slope log normal plus fading model with properly chosen is good at the performance statistics of these . By numerical evaluation based on empirical data we have shown that in the tested , at high , the network are able to achieve about a three fold increase in per user data over or , when considering and . It was also found that dirty paper can attain higher per user in hall , where interference is greater than in aisle to office . This difference was greater for high availability data and at small . Our also revealed that while in general higher data than and , this advantage is lost at low , particularly when high availability is considered . In contrast , based provide significant under virtually all practical . 
 The would like to thank Silva , and for their valuable help . 
  
 ﻿ This work demonstrates a formal connection between density estimation with a data-rate constraint and the joint objective of fixed-rate universal lossy source coding and model identification introduced by Raginsky in 2008 (IEEE TIT, 2008, 54, 3059–3077). Using an equivalent learning formulation, 
 we derive a necessary and sufficient condition over the class of densities for the achievability of the joint objective. The learning framework used here is the skeleton estimator, a rate-constrained learning scheme that offers achievable results for the joint coding and modeling problem by optimally adapting its learning parameters to the specific conditions of the problem. The results obtained with the skeleton estimator significantly extend the context where universal lossy source coding and model identification can be achieved, allowing for applications that move from the known case of parametric collection of densities with some smoothness and learnability conditions to the rich family of non-parametric L1-totally bounded densities. In addition, in the parametric case we are able to remove one of the assumptions that constrain the applicability of the original result obtaining similar performances in terms of the distortion redundancy and per-letter rate overhead. 
 Keywords: fixed-rate lossy source coding; joint coding and modeling; universal source coding; learning with rate constraints; the skeleton estimator; L1-totally bounded classes 
 Universal source coding (USC) has a long history in information theory and statistics [1–5]. Davisson’s seminal work [4] formalized the variable-length lossless coding problem and introduced important information quantities for performance analysis [1,2]. In this lossless setting, it is well-understood that the Shannon entropy provides the minimum achievable rate (in bits per sample) [2] to code a stationary and memoryless source when the probability (model) of the source is available. When the probability of the source is not known but belongs to a family of distributions F (the so called universal source coding problem), the focus of the problem is to characterize the penalty (or redundancy in bits per sample) that an encoder and decoder pair will experience due to the lack of knowledge about the samples’ probability [1]. In the lossless case, a seminal result states that the least worst-case redundancy over F (or the minimax solution of the USC problem for F) is determined by the information radius of F [1]. 
 Building on this connection between least worse-case redundancy and information radius of F, there are numerous important results developed for lossless USC [1,6–9]. In particular, it is known that the information radius grows sub-linearly (with the block-length) for the family of finite alphabet stationary and memoryless sources [1], which implies the existence of a universal source code that achieves Shannon entropy as the block length goes to a large value for every distribution in F. 
 However universality is not possible for the family of alphabet stationary and memoryless sources because the information radius of this family is unbounded [3,5,7]. More recent results on lossless USC over countable infinite alphabets have looked at restricting the analysis to specific collections of distributions (with some tail bounded conditions) to achieve minimax universality [7–9] and also looked at weak variations of the lossless source coding setting [10–12]. 
 In the fixed-rate lossy source coding problem, assuming first that the probability µ of a memoryless source is known, the performance limit of the coding problem is given by the Shannon distortion-rate function Dµ(R) [2,13]. Consequently, the universal lossy source coding problem reduces to compare the distortion of a coding scheme (satisfying a fixed-rate constraint) with the Shannon distortion-rate function assuming that the designer only knows that µ ? F. The literature on this problem is rich [3,5,14–18] with a first result dating back to Ziv [17] who showed the existence of weakly minimax fixed-rate universal lossy source code for the class of stationary sources under certain assumptions about the source, the alphabet, and the distortion measure. More refined results were presented in [5,16] one of which established necessary and sufficient conditions to achieve weakly minimax universality for the class of stationary and ergodic sources. To provide a more specific analysis of universal lossy source coding, Linder et al. [14] presented a lossy USC scheme with a distortion redundancy that goes to zero as O  for the case of independent and identically distributed (i.i.d.) bound sources. Later Linder et al. [15] improved previous results showing a fixed-rate lossy 
 construction with a distortion redundancy that vanishes as O(n-1log n) and O(pn-1 log n) with n for finite alphabet i.i.d. sources and bounded infinite alphabet i.i.d. sources, respectively. Similar convergence results were obtained using a nearest-neighbor vector quantization approach in [19]. 
 It is also understood that universal variable length lossless-source coding is connected with the problem of distribution estimation [3,6,20] as there is a one-to-one correspondence between prefix-free codes and finite-entropy discrete distributions in the finite and countable alphabet case [1,2,21]. Building on this one-to-one correspondence in the lossless case, Györfi et al. ([3], Theorem 1) showed that the redundancy (in bits per sample) of a given code upper bounds the expected divergence between the true distribution of the source µ and the estimated distribution derived from the code. Therefore, the existence of a universal (lossless) source code for F implies the existence of a universal (distribution-free in F) estimator of the distribution in expected (direct) information divergence [22]. This means that achieving lossless USC not only provides a lossless representation of the data, but it offers a consistent (error-free) estimator of the distribution at the receiver. 
 The connection between coding and distribution estimation that is evident in the lossless case is not, however, present in the (fixed-rate) lossy source coding problem. As argued in [18], a fixed-rate lossy source code does not offer a direct map with a probability distribution (model) for the source. In light of this gap between lossy codes and distributions (models) and motivated by some problems in adaptive control, where it is relevant to both compress data in a lossy way and identify the distribution of the source at the receiver [18,23], Raginsky explored the joint objective of fixed-rate universal lossy source coding and model (i.e., distribution) identification in [18]. 
 Inspired by Rissanen’s achievability construction in [6,20], Raginsky [18] proposed a new setting for the problem of fixed-rate universal lossy compression of continuous memoryless sources based on the idea of a two-stage joint coding and model or distribution identification framework. In this context, he proposed a two-stage scheme to consider two objectives: fixed-rate universal lossy source coding and source distribution (model) identification. The first objective of the scheme is to transmit the data (optimally) in the classical distortion-rate sense [24], while the second objective is to learn and transmit a description (quantized version) of the source distribution (model) [25,26]. Taking ideas from statistical learning, Raginsky proposed [18] splitting the data into training and testing samples. The training data is used in the first-stage of the encoding process to construct a quantized estimation of the source distribution and encode it (the first stage bits). Then in a second stage of the encoding process, the first-stage bits are used to pick a matched (with the estimated distribution) fixed-rate lossy source code to encode the test data (the second stage bits). In this joint coding and modeling setting, the existence of a zero-rate consistent estimator of the density (in expected total variation) is sufficient to show the existence of a weakly minimax universal fixed-rate source coding scheme [18] (Theorem 3.2), achieving the Shannon distortion-rate function [2,24,27,28], for any given rate. This result is obtained for a wide class of single-letter bounded distortion functions and for a family of source densities F = {µ? : ? ? T} indexed over a bounded finite dimensional space  L, L] ? Rk (i.e., a parametric collection) with some needed smoothness and learnability conditions [18] (Theorem 3.2). 
 It is important to highlight that the joint coding and modeling achievability results in [18] did not degrade the performance of the source coding objective. In fact by restricting the analysis to the source coding objective alone, the joint coding and modeling framework in [18] showed the same state-of-the-art performance results as conventional two-stage universal source coding schemes (or universal vector quantizers) [14,15,19] in terms of distortion redundancy and per-letter rate 
 overhead (O(plog(n)/n) and O(log(n)/n), respectively) as the block length n tends to a large number. Importantly, the first-stage bits of this joint coding and modeling scheme are used to achieve model identification at the receiver with arbitrary precision in total variation (with a rate of convergence 
 of O(plog(n)/n) as n goes to infinity), with no extra cost in bits per-letter compared with conventional fixed-rate lossy source coding methods. 
 This work formally studies the interplay between density estimation under a data-rate constraint and the joint fixed-rate universal lossy source coding and modeling problem with training data or memory introduced in [18]. The first main result (Theorem 1) establishes a connection between zero-rate density-estimation and a universal joint coding and modeling scheme that achieves optimal lossy source coding (in a distortion-rate sense) and lossless model identification. This result is obtained for the general family of bounded single-letter distortions [13]. Remarkably, this connection implies that the construction of a joint coding and modeling scheme reduces to the construction of a zero-rate density estimator. From this result, the second main result (Theorem 2) stipulates a necessary and sufficient condition for the existence of a weakly minimax universal joint coding and modeling scheme. For the achievability part of this result, we used the skeleton estimator as our learning framework [29]. Using this learning framework we extend the parametric context explored in [18] to the rich non-parametric scenario of L1-totally bounded densities [30]. 
 Furthermore, revisiting the parametric case studied in [18], by using the skeleton estimator we are able to remove some of the assumptions that limit the applicability of the original result. We show that the skeleton estimator matches the best performance reported in [18] in terms of the distortion redundancy and (per-letter) rate overhead, in particular obtaining rates of convergence to 
 zero of O(plog(n)/n) and O(log(n)/n), respectively, as the block-length tends to infinity. To obtain this, our result relaxes the finite Vapnik and Chervonenkis (VC) dimension assumption considered in [18]. On the other hand, when the finite VC dimension assumption is added in the analysis, 
 the skeleton learning scheme offers a convergence rate of O(1/vn) for the distortion redundancy as the sample-length goes to infinity. Finally, the skeleton framework is implementable in the parametric case as its minimum-distance decision is carried out on a finite number of candidates and the oracle e-skeleton (or the e-covering in total variation of F) [30] (Chapter 7) can be replaced by a practical uniform covering of the compact index set T ? Rk (Theorem 4). Finally, it is worth noting that a preliminary version of this work (in the context of density estimation under a data-rate constraint) was presented in [31]. 
 The rest of the paper is organized as follows: Section 2 introduces the setting of the joint coding and modeling with training data. Section 3 elaborates the connections with zero-rate density estimation. Section 4 presents the main joint coding and modeling result (Theorem 2) and introduces the skeleton estimator. Finally, Section 5 revisits a special case where the distributions are indexed by finite dimensional bounded space (the parametric context). A summary of the results is presented in Sections 6 and 7. Finally, the proofs are presented in Section 8. 
 The fixed-rate coding and modeling problem introduced in [18] is presented in this section. This joint coding and modeling problem will be the main focus of this work. In addition, notations and definitions used in the rest of the paper will be presented. 
 Let X ? B(Rd) be a separable and complete subset of Rd where B(Rd) is the Borel sigma field. Let P(X) be the collection of probability measures on (X, B(X)), with B(X) denoting the Borel sigma field restricted to X, and let AC(X) ? P(X) denote the set of probability measures absolutely continuous with respect to the Lebesgue measure ? [32]. For any µ  denotes its probability density function. The total variational distance [30] of v and µ in P(X) is given by (to avoid any confusion, if S is a set then |S| denotes its cardinality). 
 For µ and v belonging to AC(X), if we define the Scheffé set for the pair (µ, v) by 
 Let {Xn : n = 1} be an i.i.d. stochastic process (or stationary and memoryless source), where Xi takes values in X ? Rd and has a distribution µ in F = {µ? : ? ? T} ? AC(X). T is in general an index set for F. The problem of lossy source-coding of a finite block of the process Xn = (X1, ..., Xn) reduces to find a mapping (or code) Cn(·) from Xn to Sn, where Sn is a finite set. Given a cardinality constraint on Sn, the design objetive is to make Cn(Xn) as close as possible to Xn (in average) using for that a distortion function. The standard coding problem assumes the knowledge of µ for finding the optimal code (for any finite block n) [1,2,13], as well as for characterizing the fundamental performance limits of this task as n goes to infinity [2,24,28,34–36]. 
 A more realistic scenario is the universal source coding (USC) problem [2], where the source distribution µ ? F is unknown and a coding scheme needs to be designed optimally for the family F. Here we focus on a specific learning variation of this task introduced by Raginsky in [18], where in addition to the data that needs to be compressed and recovered (with respect to a fidelity criterion), 
 we have a finite number of i.i.d samples following the same distribution µ and that can be used to estimate µ in the encoding process (more details of this approach in Section 2.3). This additional data can be interpreted as memory, training data, or side information about µ available at the encoder because it is data that is not required to be compressed and recovered. The existence of this memory departs from the standard zero-memory setting considered in universal source coding [1]. However, this information can be seen as a realistic assumption in the context of a sequential block by block coding of an infinite sequence, where the data is partitioned into blocks of the same finite length and compressed sequentially block by block. Then in a given stage of this sequential process, the data from previous blocks are available at the encoder (lossless) for the process compressing the current block [18]. 
 More specifically following the fixed-rate block coding and modeling setting introduced by Raginsky in [18], we consider an n-block coding scheme with finite memory m, where there is a distinction between the data Zm = (Z1, ..., Zm) that is available (as side information) to estimate the source distribution (training data) and the data Xn that needs to be encoded and recovered (source or test data), under the important assumption that both data sets are i.i.d. samples of the same unknown probability µ ? F. A systematic exposition of this coding setting and its connection with the classical setting of zero-memory block coding is presented in [18] (Section II). Formally, let us define an (m, n)-block code by the pair 
 Then given a set of training samples zm ? Xm and a finite block of the source xn ? Xn, Cm,n is the composition of: a encoding function f(zm, xn) that maps xn to an element in a finite set Sn conditioned on the training data (or memory) represented by zm, and a decoding function f(·) that maps a 
 symbolof Cm,n. In this context,s ? Sn into the reproduction pointsXˆ denotes the reproduction space. As a short-hand, we denote byGCm,n = {f(s) : s ? Sn} that we called the codebookxˆn = 
 Cm,n(xn) = f(f(zm, xn)) the reconstruction of xn obtained by Cm,n and its memory zm (for simplicity, the dependency of xˆn or Cm,n(xn) on the memory zm will be implicit in the rest of the exposition.). 
 The rate of Cm,n in bits-per-letter is given by R . In general, it is not possible to recover xn from xˆn given the cardinality constraint on Sn, and thus a single-letter distortion measure ? : X × Xˆ ? R+ is used to quantify the n-block discrepancy by [24] 
 Finally considering Xn ~ µn and Zm ~ µm, the average distortion per-letter of Cm,n given Zm is 
 which is a function of Zm and hence the average distortion per-letter of Cm,n is 
 In universal source coding the performance of a code Dµ(Cm,n) is evaluated over a collection of distributions µ ? F and is compared (point-wise) with the best code that can be obtained assuming that µ is known. For this analysis, we need the following definitions: 
 Definition 1 ([18]). For a finite block length n and distribution µ ? F, the n-order operational distortion-rate function of µ at rate R is 
 In this context, the operational distortion-rate function (DRF) [2,28] is given by 
 The celebrated Shannon lossy source-coding theorem [27] provides a single letter theoretical characterization for Dµ(R) in (8) (also known as the Shannon DRF). A nice exposition of this celebrated result can be found in [2,24,28]. 
 It is worth noting that the operational distortion-rate function in (7) is equivalent to the classical zeromemory n-order operational distortion-rate function given by inf  : such that R 
 (Lemma 2.1). Then, allowing a nonzero memory (side information at the encoder) does not help in the minimization of the distortion when µ is known. 
 For the rest of the exposition, we will concentrate on the simple case studied in [18] where n = m (i.e., the block-length is equal to the memory of the code). To be precise about the meaning of universality in this context, we resort to some standard definitions: 
 Definition 2 ([16]). A coding scheme {Cn,n : n = 1} is weakly minimax universal for the class F at rate R, 
 the first term  is the n-order distortion redundancy, which is the discrepancy that can be attributed exclusively to the goodness of the coding scheme. The second term in (11), i.e., 
 , has to do with how fast Dµn(R) converges to the Shannon DRF as the block length 
 tends to infinity (see further details in [14] (Section III) and references therein). From this observation, 
 Definition 3. A coding scheme {Cn,n : n = 1} is strongly finite-block universal for the class F at rate R if 
 Note that if {Cn,n : n = 1} is strongly minimax universal then it is strongly finite-block universal, but the converse result is not true in general. The missing condition to make these two criteria equivalent is the uniform convergence of Dµn(R) to Dµ(R) in the class F. More discussion about this point in Section 6. 
 Motivated by the work of Rissanen [6], Raginsky [18] proposed a two-stage block code with finite memory (training data), with the objective of doing both fixed-rate lossy source coding, and identification of the source distribution at the receiver. More precisely, given Zn ~ µn? and Xn ~ µn? (the training and the source-data samples, respectively), an (n, n)-joint coding and modeling rule is given by 
 where Sn and S˜n are finite-set functions of n. Cn,n processes (Zn, Xn) in two stages. In the first stage, the pair (fn, fn) in (13) uses Zn to do density estimation and finite-rate encoding (quantization) by fn(Zn), and fn(·) decodes an estimated density in . At the end, the first stage 
 Using the index s˜ = fn(Zn) ? S˜n, the second stage of Cn,n, represented by  ), encodes and decodes the source data Xn by 
 In summary, the outcome of the whole encoding process is the concatenation of the bits that represent fn(Zn) (first-stage bits), and the bits that represent fn,fn(Zn)(Xn) (second-stage bits). The decoding process, on the other hand, reads the first-stage bits to recover ?ˆn(Zn) and then reads the second-stage bits to recover  . (see Figure 1 in which this process is illustrated). The rate (in bits per letter) of Cn,n is 
 Figure 1. Illustration of Raginsky’s two-stage joint source coding and modeling scheme. Top figure illustrates the coding process and the bottom figure shows the respective decoding process. 
 Based on this two-stage scheme, we could simultaneously achieve source coding and density estimation (modeling) at the decoder. This new joint coding and modeling objective motivates the introduction of the following definition: 
 Definition 4. A joint coding and modeling scheme {Cn,n : n = 1} in (13) is strongly minimax universal for a class of distribution F = {µ? : ? ? T} ? AC(X) at the rate R > 0, if 
 Consequently, if {Cn,n : n = 1} is strongly minimax universal for F, it follows that as n tends to infinity, density estimation is achieved at the decoder (in expected total variations) and, from the source coding perspective, {Cn,n : n = 1} is strongly finite-block universal for F in the sense of Definition 3. For the rest of the paper, the strongly minimax universality of Definition 4 will be the main coding and modeling objective. 
 This section formalizes a connection between the objective of joint coding and modeling (declared in Definition 4) and a problem of zero-rate density estimation. 
 3.1. Density Estimation with a Rate Constraint Let us first introduce the problem of rate constrained density estimation. Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities as introduced in Section 2.2. 
 Definition 5. An (n, 2nR) learning rule of length n and rate R for F is a pair of functions (f, f), with f : 
 The composition of these two functions p = f ? f : Xn ? T defines the rate-constrained learning rule for F taking values in the codebook {f(s) : s ? S} ? T, where R(p) = log2(|S|)/n denotes its description complexity in bits per training sample. 
 Definition 6. The rate R = 0 is achievable for F, if a learning scheme ? = {(fn, fn) : n = 1} exists such that 
 where Z1, Z2 . . . in the left hand side (LHS) of (18) corresponds to i.i.d. realizations driven by µ ? F. In this case, we say that ? is an R-rate uniformly consistent scheme (or estimator) for the class F. 
 Proposition 1. If for a given R > 0, {Cn,n : n = 1} is strongly minimax universal for the class F at the rate R (Definition 4), then its induced finite-description learning scheme obtained from the first stage in (13), i.e., ? = {(fn, fn) : n = 1}, is a zero-rate uniformly consistent estimator for F (Definition 6). 
 Interestingly, the existence of a zero-rate uniformly consistent scheme for F is also sufficient to achieve the joint coding and modeling objetive (Definition 4) if some mild conditions are adopted from the work in [18]. This is stated in the following result: 
 ? : X×Xˆ ? R+ can be expressed by ?(x, xˆ) = d(x, xˆ)p where d(, ) is a bounded metric in X?Xˆ ×X?Xˆ with p > 0 and 
 for all µ ? F, for all n = 1, and for all R > 0, there exists a (0, n)-block code, say Cµ*n, that achieves the n-order operational DRF Dµn(R) in (7). 
 Then the existence of a learning scheme ? = {(fn, fn) : n = 1} that is zero-rate uniformly consistent for F implies that ?R > 0 there exists a joint coding and modeling scheme {Cn,n : n = 1} that is strongly minimax universal for F at rate R (Definition 4). 
 Remark 1. The construction proposed for {Cn,n : n = 1} at any rate R > 0 (in Section 8.2) using the zero-rate density estimation scheme ? = {pn = fn ? fn : n = 1} satisfies that: 
 ?n = 1, where C > 0 is a constant. It is worth noting that these two inequalities summarize the result in Theorem 1 and, importantly, these two bounds are independent of R. 
 Remark 2. An important consequence of the bounds in (19) and (20) is the fact that constructing a learning scheme ? = {pn : n = 1} with specific rates of convergence for supµ?F E(V(µpn(Zn), µ)) and R(pn) (as n goes to infinity) produces a joint coding and modeling scheme that achieves a uniform rate of convergence to zero (over F) of the overhead in distortion by (19) and a uniform rate of convergence to zero of the overhead in rate by (20). This observation will be used in all the achievable results presented in Sections 4 and 5, where, consequently, the problem reduces to determine ? and expressions for supµ?F E(V(µpn(Zn), µ)) and R(pn). 
 From the connection with zero-rate density estimation in Section 3, here we present a set of new results for the joint coding and modeling problem of Section 2.3. In these results, the general conditions (i) and (ii) stated in Theorem 1 are assumed. 
 Definition 7. Let F ? AC(X) be a class of densities. We say that F is L1-totally bounded if for every e> 0, there is a finite set of elements {µi : i = 1, ..., N} in F such that, 
 Definition 8. For F L1-totally bounded, let Ne denote the smallest positive integer that achieves the condition in (21). Ne is called the e-covering number of F and K(e) = log2(Ne) is called the Kolmogorov’s e-entropy of F [30]. 
 Definition 9. An e-covering Ge of F such that |Ge| = Ne is called an e-skeleton of F [29]. 
 Theorem 2. There is a strongly minimax universal joint coding and modeling scheme for F at rate R for any rate R > 0 if, and only if, F is L1-totally bounded. 
 The achievability part of the proof of Theorem 2 relies on the adoption of the skeleton estimator [29] (with its minimum distance learning principle in (42)), which is a zero-rate uniformly consistent density estimator for F (Definition 6). Furthermore, Theorem 2 can be complemented saying that the proposed construction {Cn,n : n = 1} derived from the skeleton estimator satisfies that (Pµ is a short-hand for the process distribution of (Zn)n=1 characterized by µ ? F under the i.i.d. assumption.) 
 Knowing specific expressions for K(e) = log2 Ne < 8, the skeleton estimator can be optimized selecting its design parameter appropriately. In particular, the sequence (en)n=1 (see details in Section 8.3) is selected as the solution of the optimal balance between estimation and approximation errors (see (45) 
 in Section 8.3), which is given by e  ] (Chapter 7.2). The details of this analysis are presented in Section 8.3 and [30] (Chapter 7). By doing so, an optimized zero-rate skeleton scheme  , with concrete rate of convergence for supµ?F EZn~µn(V(µpen*(Zn),µ)) and 
 R(pen*), can be obtained. From Remarks 1 and 2, these results imply specific performance results for the induced joint coding and modeling scheme. To illustrate, we present three interesting examples below. 
 Let F = {µ? : ? ? T} with  be the class of measures which are a convex combination of {µ1, ..., µd} ? AC(X), i.e., k · µk(A). F is 
 L1-totally bounded with K(e) being O(d log(1/e)) [30] (Chapter 7.4). From (45) the optimal sequence 
 is O(vd/n) [30], which implies the following finite-rate performance bound [30] (Chapter 7.4): 
 with C a universal non-negative constant. The rate in bits per-sample R /n is O(log n/n). 
 Let F be the collection of densities with support on [0, 1]d, monotonically decreasing per coordinate and bounded by a constant L > 0. This class is known to be L1-totally bounded, and furthermore K(e) = CLd [30] (Lemma 7.1), with the constant C depending only on d. From (  being O(Ld/d+2/n1/d+2) ed 
 is optimal (please see details in [26,30]) with the following performance bound, 
 Let F be the class of densities defined on the bounded support [0, 1], with r absolutely continuous derivatives (with r an integer greater than zero) and satisfying that:  C for a constant C > 0. This class is L1-totally bounded with K(e) being O(1/er+1) [30] (Chapter 7.6). From (45), the optimal sequence  , where supµ?F EnV(µpen*(Zn), µ)o is O(1/n1/3+r) and the rate in bits per sample R /n is O(1/n2/3+r). 
 Notably, the last two examples are fully non-parametric, where K(e) is a polynomial function of 
 1/e. Richer non-parametric examples of L1-totally bounded clases of densities, where K(e) is even exponentially in 1/e, are presented in [30] (Chapters 7.6 and 7.8) and its references. 
 Looking at the distortion redundancy bound in (19), when F is totally bounded the fastest rate ofv 
 convergence that could be achieved with the skeleton estimator proposed in Theorem 2 is O( 1/n) (see Section 8.3 and the estimation error bound in (45)). In this section, more specific density collections 
 are studied to achieve this best rate O(v1/n) for density estimation and distortion redundancy from (19). We follow the path proposed by Yatracos in [38], who explored families of distributions with a finite Vapnik and Chervonenkis (VC) dimension the so-called VC classes [39,40]. Let us first introduce some definitions: 
 Definition 10 ([38]). Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities. The Yatracos class for such a collection is given by 
 the Yatracos class AT has a finite VC dimension (DefinitionF	A1 in Appendixv	B), and 
 the Kolmogorov’s entropy of	associated with the sequence en = 1/	n grows strictly sub-linearly, i.e., log2(N1/v n) is o(n), then there is a zero-rate density estimator scheme ? = {(fn, fn) : n = 1} for F such that 
 where pn(Zn) = fn(fn(Zn)) is the skeleton estimator in (42) with en = 1/vn. Furthermore, ? is also a zero-rate strongly consistent density estimator where ?µ ? F 
 From Definition 7, log2(Ne) is inversely proportional to e. In fact, depending of how riche F is, log2(Ne) can go from being O(log 1/e), passing from being polynomial in 1/e, to being O(e1/ ) (see a number of examples in [30] (Chapter 7) and its references). Then the role of (iii) in the statement of Theorem 3 is to bound how fast Ne should tend to infinity as e goes to zero, to guarantee a zero-rate in the skeleton learning scheme. It is simple to show that Ne being O(e(1/e)q ) with q ? [0, 2) is sufficient to achieve that log2(N1/v n) is o(n). This is a condition satisfied by a rich collection of L1-totally bounded classes in AC(X). Concrete examples are presented in [30] (Chapter 7). 
 The results presented so far are of theoretical interest because they rely on the skeleton estimator that is constructed from the skeleton covering of F (see Definition 9), which is unknown in practice. Moving towards making the zero-rate skeleton learning scheme of practical interest, we revisit the important parametric scenario in which T, the index set of F, is a compact set contained in a finite-dimensional Euclidean space Rk. Interestingly, in this context we can consider a practical covering of F induced by the uniform partition of the parameter space T, as used in [18]. Unlike [18], 
 where a minimum-distance estimate is first found and then quantized, here we first quantize the space T and then find the minimum-distance estimate among a finite collection of candidates (i.e., over a finte number of prototypes in T). Some assumptions will be needed. 
 Definition 11 ([18]). Let F = {µ? : ? ? T} with T ? Rk. Let IF : T ? F be the index function of F that maps ? to µ?. IF is said to be locally uniformly Lipschitz, if there exists r > 0 and m > 0, such ?? ? T, 
 where Br(?) ? T denotes the ball of radius r (with respect to the Euclidean norm in Rk) centered at ?. 
 The following lemma shows that F is L1-totally bounded under some parametric assumptions. 
 Lemma 1. Let F = {µ? : ? ? T} ? P(X) with T ? Rk. If T is bounded (?L > 0 such that T ? 
 bounded. Furthermore, NL, L]) and the mappinge is O(I1F/: kT) for this family.? F is locally uniformly Lipschitz (Definition 11), then F is L1-totally 
 It is important to note that the e-covering of F used in the proof of Lemma 1 to derive an upper bound for Ne is practical (see Appendix C). This offers the possibility of implementing a practical skeleton estimator, which is the focus of the following result. 
 Under the assumptions of Lemma 1, let (f˜n,e, f˜n,e) denote the learning rule of length n associated with the minimum-distance principle in (42) with parameter e (see details in Section 8.3), where instead of using the e-skeleton Ge of F (in Definition 9), the implementable (see Appendix C) e-covering of T presented in the proof of Lemma 1 is used. This practical e-covering is denoted by G˜e (by definition, Ne   N˜e ~ O(1/ek), this last part from Lemma 1.). With this, let  (f˜n,en,   denote our practical learning scheme indexed by the precision numbers (en)n=1 ? (R+)N. We are in a position to integrate Theorem 3 and Lemma 1 to state the following: 
 Theorem 4. Under the assumptions of Lemma 1, the practical skeleton estimator ?˜ ((en)n=1) with en* = 1/vn satisfies that 
 When X ? Rd, Raginsky [18] showed that the finite VC dimension assumption of Theorem 4 is satisfied by the class of mixture families presented in Section 4.2.1 and a rich collection of exponential families of the form F = {µ? : ? ? T} ? P(X) with  dµ  , where f(x) is a reference density, {hi(·) : i = 1, ..., k} is a set of arbitrary real-valued functions, g(?) is a normalization constant (g dx see details in [18] (Section V)), and T is a compact subset of Rk (see details in [18] (Section V)). 
 We summarize the results of the proposed zero-rate density estimation approach adopted for the problem of joint fixed-rate lossy source coding and modeling of continuous memoryless sources. 
 Proposition 1 and Theorem 1 formalize the interplay between the two-stage joint fixed-rate coding and modeling objective and the problem of zero-rate uniformly consistent (in expected total variation) density estimation. 
 Theorem 2 establishes a necessary and sufficient condition on a family of densities for the existence of a strongly minimax joint coding and modeling scheme achieving both source coding and model identification objectives (Definition 4). The result is obtained for the rich non-parametric collection of L1-totally bounded densities. 
 For the modeling stage, we propose using the skeleton estimator, which first quantizes the data and then finds the minimum-distance decision on this finite set of density candidates (42). This is a practical solution in the sense that the inference (minimization) is carried out over a finite set. 
 By introducing combinatorial regularity conditions on the family of distributionsv	F = {µ? : ? ? T}, 
 the skeleton scheme achieves O(1/ n) rate of convergence in the n-order distortion redundancy, and the same rate in the expected total variational distance for the modeling part (Theorem 3). • Finally, for a relevant parametric setting, a practical skeleton-based joint coding and modelingO v scheme is proposed that achieves a rate of (1/ n) for the n-order distortion redundancy (Theorem 4). This rate is slightly better than the O(plog n/n) achieved in [18] under the same rate overhead of O(log(n)/n). Furthermore, Theorem 4 removes the finite-VC-dimension assumption over the Yatracos class AT considered in [18] (Theorem 3.2), while achieving the same performance 
 rates in terms of n-order distortion redundancy O(plog n/n), uniform expected risk to learn the 
 Concerning the last parametric result, we note that the result in [18] can be improved by the adoption of Dudley’s entropy bound [41], which would yield the same asymptotic rate reported in this work for the n-order distortion redundancy. 
 A final remark is that under the bounded distortion metric assumption of Theorem 1 condition (i), Linder et al. [14] (Theorem 2) showed that ?? ? T, and for every R > 0 such that Dµ?(R) > 0, there is a constant K?(R) > 0 such that 
 where (rn) is a sequence that converges to zero (o(1)) uniformly in T. This result offers a rate of convergence of the n-order operational distortion-rate function to the Shannon DRF as the block length tends to infinity. In view of (11), we can adopt this result in Theorems 3 and 4, to say that the average distortion of the respective joint coding and modeling schemes at rate R, i.e., Dµ(Cn,n), convergences to the Shannon DRF Dµ(R) as O  point-wise ?µv? F. Therefore in the process of comparing 
 This work revisits the problem of fixed-rate universal lossy source coding and model identification with training data proposed in [18] from a learning perspective. Remarkably, we found that the problem is equivalent to the problem of density estimation of the source distribution with some concrete but non-conventional operational data-rate constraints in bits per sample. This learning problem can be seen as the task of estimating and encoding the distribution of samples with a zero-rate in bits per sample, while achieving a consistent estimation in expected total variations of the distribution after the decoding process. From our perspective, the rate-constraint density estimation problem is interesting in itself and can have relevant applications in other contexts such as distributed learning scenarios and sensor network problems. 
 Importantly for the joint coding and modeling problem, the connection with density estimation provides a context for the use of the skeleton estimator proposed by Yatracos in [29]. We highlight two important implications from its use. First, we extend results about minimax universality from the parametric context explored in [30] to the rich non-parametric family of L1-totally bounded densities [26,30]. This result significantly expands the contexts where the joint model and coding objective can be achieved. We illustrated this with some examples in Section 4.2 and many more can be found in the literature of density estimation [26,30]. 
 Second, in the parametric case studied in [18], we were able to remove some of the assumptions and obtain not only the same performance result in terms of rate of convergence of the n-order distortion redundancy but also slightly better convergence results. Therefore, the Skeleton estimator, though essentially a non-parametric learning scheme, is shown to be instrumental in enriching the applicability of the joint coding and modeling framework. 
 Proof. The fact that ? is uniformly consistent for F is directly from Definition 4. On the other hand, the rate of pn = fn ? fn is R . From the definition of Dµn(R), it is simple to show from the strict monotonicity of Dµ(R) that in order for limn?8 supµ?F hDµ(Cn,n) - Dµn(R)i = 0, it is required that  e for any e > 0. Then, from (16), and since log |S˜n|/n = R(pn), lim supn?8 R(Cn,n) = R implies that limn?8 R(pn) = 0. 
 Proof. The proof builds upon the ideas elaborated in [18] (Theorem 3.2, p. 3065). Let us consider an arbitrary R > 0 and let ? = {(fn, fn) : n = 1} be the zero-rate learning scheme of the assumption. Using ?, let us construct the joint coding and modeling rule of length n by: 
 Concerning the first stage of {Cn,n : n = 1}, it is induced directly from the coding-decoding rules of ?. 
 For the second stage, ?n = 1, ?s˜ ? S˜n the pair (fn,s˜, fn,s˜) is picked such that 	?n s	fn,s˜, which 
 is the optimal n-block code that achieves Dµn?n s (R) (from the hypothesis in (ii)), with ?n,s˜ = fn(fn(s˜)) 
 short-hand for the reproduction codeword induced from the first stage-pair (fn, fn), and Sn satisfying the R-rate constraint, i.e., |Sn| = 2nR. From construction and the fact that ? has zero-rate, 
 then {Cn,n : n = 1} satisfies the rate condition. On the other hand, based on the assumption that ? is zero-rate uniformly consistent, it follows that 
 where ?ˆn(Zn) = fn(fn(Zn)). Then {Cn,n : n = 1} achieves the modeling objective. Concerning the coding objective, we use the following key result: 
 Lemma 2 ([18] (Lemma C.1)). Let P and Q be two probability measures in (X, B(X)). Let Cn = (f, f) be a zero-memory n-block coder with the nearest neighbor property (i.e., Cn is nearest neighbor if, n,   arg minxˆ  with GCn the reproduction codebook of Cn.). If we denote the performance 
 where Pn denotes the product measure with marginal P in (Xn, B(Xn)), and ? satisfies the condition i) of Theorem 1 and is bounded by dmax, then 
 Furthermore, the inequality can be extended for the n-order operational distortions in (7), i.e., 
 For the first equality we use (5). The inequality in (35) is from the definition in (31) and (33), and the equality in (36) is from the construction of ?nˆn(Zn) which is n-operational optimal for the distribution µ?ˆn(Zn) at rate R. Finally, (37) is from (32). 
 Concluding, Dµ(Cn,n|Zn) - Dµn(R) is random (a measurable function of Zn) and dominated by 
 V(µ?ˆn(Zn)µ). Hence taking the expected value (with respect to Zn) on both sides of this inequality 
 Proof. Let us first assume that F is L1-totally bounded and prove the direct part of the statement. 
 We adopt the skeleton estimate proposed by Yatracos [29] and extended by Devroye et al. [42,43] 
 (a complete presentation can be found in [30] (Chapter 7)). For any arbitrary e> 0, let us consider the 
 o	dµ?e e-skeleton Ge	1, ..., Ne	of F. We use g?ie(x) = d?i (x) as short-hand for the i-th pdf in 
 to represent the index set of Ge. Let us consider the Yatracos class of Ge given by [30] 
 Hence, given i.i.d. realizations X1, ..., Xn with Xi ~ µ? (µ? ? F), let us propose the encoder-decoder pair (fn,e, fn,e) associated with Ae by, 
 is the well-known skeleton estimate [29]. ?ˆe(X1n) is the minimum-distance approximation of µˆn with elements of Ge [29,30], adopting the measure in the right-hand-side of (42) that is reminiscent of the total variational distance in (1). In order to choose a sequence (en)n=1, we consider the following performance bound. 
 Equation (43) is valid for any e > 0 and, consequently, it provides a trade-off between an approximation error term and an estimation error term. The approximation error is minv?Ge V(v, µ), 
 which is bounded by the definition of Ge. For the estimation error, on the other hand, Yatracos proposed the use of Hoeffding’s inequality [44] to obtain that ?µ ? P(X) [30] (Theorem 7.1), 
 Using (44) in (43), it follows that, supµ . This last expression is distribution-free and it is valid if the approximation fidelity e is a chosen function of n [30]. Consequently, for any sequence (en)n=1, 
 (Chapter 7.2), which is well-defined and converges to zero as n tends to infinity. Consequently from (45), limn?8 supµ  0. Then the learning scheme 
 particular R   is O(1/vn) by construction. To conclude the argument of this part (i.e., presenting the construction of the second stage of a joint coding & modeling scheme), we adopt the result and the construction presented in the proof of Theorem 1 (see Remark 1 for details). This result implies that ?R > 0 there is a strongly minimax universal joint coding and modeling scheme for F at rate R. 
 For the other implication (the converse part of the statement), let us fix R > 0 and assume that we have a joint coding & modeling scheme that is strongly minimax universal (Definition 4) for F at rate R. Then from Proposition 1, we have a learning scheme ? = {(fn, fn) : n = 1} such that limn?8 R(pn = fn ? fn) = 0 and 
 For the learning rule of length n, we have its reproduction codebook that we denote by Tn =  . Let us define the minimum-distance oracle solution in Tn by 
 From (46), we have that limn?8 supµ?F V(µ?˜n(µ), µ) = 0. In other words, ?e > 0, there exists 
 N(e) <8, such that for all n = N(e), V(µ?˜n(µ), µ) <e uniformly for every elementS µ ? F. This means that ?e> 0 there exists N(e) <8, such that for any arbitrary n¯ > N(e), F ? ? Tn¯ Be(µ?), where by 
 | n¯ | <8. Then F is totally bounded, which concludes the proof. ? construction T 
 with Aen the Yatracos class of the skeleton Gen. It is clear that ?e> 0, Ae ? AT. Then by monotonicity 
 µ ? P(X). Here is where we use the assumption that AT has finite VC dimension J, which implies from [30] (Theorem 3.1) that 
 for some constant c > 0. Substituting this result in (48), the argument concludes by replacing (en) = (1/vn), a solution which achieves the intended rate of convergence for supµ? EnV(µ?ˆ vn(Xn), µ?)o. Finally, the rate of the learning rule is dlog2(Nn1/vn)e, which tends to 
 For the almost-sure convergence part if e 	vn, it is sufficient to show that the second term in 
 the right hand side (RHS) of (48) is O(plog n/n) Pµ-almost surely. From the fact that AT has finite VC dimension (Definition A1), and from the classical VC inequality [30] (Corollary 4.1 and Theorem 3.1) and [45] (Chapter 12.4), it follows that 
 the proof. As (an) is o(1), this result implies the almost-sure convergences to zero of V(µ?ˆe*(Xn), µ?) as 
 Finally, using similar arguments, it is possible to show that V(µ?ˆe*(Xn), µ?) is o(1/nt) Pµ-almost 
 Proof. First note that T is contained in a compact set k, consequently, T inherits the finite covering property of a compact set, i.e., ?e> 0, there exists a finite covering  such that, K(e) 
 On the other hand, from the locally uniformly Lipschitz assumption on IF : T ? F, there exists r > 0 and m > 0 such that V(µ?, µf) = m ||? - f||, ?? ? T, ?f ? Br(?). Then, by considering eo < r, it follows by construction of Teo that 
 where BdV(µ) = {v ? P(X) : V(v, µ) <d} is the ball centered at µ ? P(X), induced from the total variational distance, and the last inequality stems from the Lipschitz condition. Hence, from (51), ?e> 0 
 e o	SM there exists M(e) = K(min {e/m,r},..., µM(e)	? P(X), such that F ?	i, which proves the result. 
 For the final part, let (m, r) be the uniform parameters that characterize the Lipschitz condition of 
 from (IF(·) 51(Definition) Ne is upper bounded by11). Without loss of generality, let us assume the critical regime whereK(e/m), which is the covering number of	me <L, Lr, hence] ?	k, 
 we will work with a uniform partition of   to find a bound for K(e/m). Let e¯ = me , then inducing a product-type partition, where in each coordinate we have   uniform length cells, we have the required e¯-covering. The number of prototypes is O , which is O(1/ek) as a function of e (e = e¯ · m). 
 To clarify the constructive nature of the e-covering used to prove this result, an algorithm with the basic steps of the construction of this practical covering is sketched in Appendix C. 
 Proof. Let G˜e ? F be the e-covering induced from the uniform partition of T presented in Lemma 1. 
 From this we can construct the minimum-distance estimate in (42) adopting the Yatracos class of G˜e (with index set T˜ e), i.e., A˜e, which, from (39), yields 
 The latter upper bound is asymptotically dominated by (plog n/n) from the fact that  is O(k log(n)) (Lemma 1), which proves the assertions made in (26). 
 Concerning part (ii), using the arguments presented in the proof of Theorem 3, we can obtain that ?e> 0, 
 From this point, the proof follows from the arguments of Theorem 3 and the fact that  is O(k/2 · log2 n). 
 Author Contributions: Conceptualization, J.F. Silva and M.S. Derpich; Methodology, J.F. Silva and M.S. Derpich; 
 Formal Analysis, J.F. Silva and M.S. Derpich;	Investigation and Results, J.F. Silva and M.S. Derpich; 
 Writing—Original Draft Preparation, J.F. Silva and M.S. Derpich; Writing—J.F. Silva & Editing, M.S. Derpich; Project Administration, J.F. Silva; Funding Acquisition, J.F. Silva. 
 Funding: The work is supported by funding from FONDECYT Grants 1170854 and 1171059, CONICYT-Chile and the Advanced Center for Electrical and Electronic Engineering (AC3E), Basal Project FB0008. In addition, J.F. Silva acknowledges support from Project Anillos ACTI 1405, CONICYT-Chile. 
 Acknowledgments: We want to thank the anonymous reviewers for their constructive comments that were instrumental to improve the technical content and organization of this work. We thank Diane Greenstein for editing and proofreading all this material and Sebastian Espinosa for preparing Figure 1. 
 First, we show that the zero-rate skeleton estimate ?((en)) = {(fn,en, fn,en ) : n = 1} proposed in (40) and (41) is also strongly consistent. 
 Proof. Let us consider the skeleton estimate µ?ˆe*(Xn), where the sequence was chosen by the rule 
 V(µ . As by construction  , we just need to concentrate on the estimation error term. Applying Hoeffding’s inequality [44] ?d> 0, 
 where from the Borel-Cantelli lemma [46,47], the estimation error convergences to zero almost-surely. 
 Finally considering the inequality in (37), we have that Dµ  V(µ?ˆn(Zn), µ), ?µ ? F, which concludes the argument. 
 Let C ? B(X) be a collection of measurable events, and xn = (x1, ..., xn) be a sequence of n points in Xn. Then we define by S(C, xn) the number of different sets in 
 The shatter coefficient is an indicator of the richness of C to dichotomize a finite sequence of points in the space, where by definition Sn(C) = 2n. 
 Definition A1. The first time (in the index n) where Sn(C) is strictly less than 2n is called the Vapnik and 
 Chervonenkis (VC) dimension of C [45]. If C has a finite VC dimension then it is called a VC class; otherwise if Sn(C) = 2n ?n = 1, then the class is said to have an infinite VC-dimension. 
 Appendix C. Pseudo Algorithm to Implement the Practical e-Covering Presented in Lemma 1 
 Under the parametric assumptions of Lemma 1, we recognize four structural parameters that characterize F: k the dimension of the Euclidean space that contains T, L > 0 associated with the assumption that  L, L], and (r, m) the parameters associated with the locally Lipschitz assumption of IF. Given these four parameters (k, L, m, r) and e> 0, there is a constructive e-covering presented in the proof of Lemma 1 that can be implemented in the following steps: 
 In each of the k dimensions ofv	T, the interval [-L, L] is partitioned uniformly with sub-intervals-	d v	e 
 of length 2e/(m	k). This produces a scalar quantization of [	L, L] with	m	kL/e prototypes per coordinate. 
 A product partition of L, L] is made with the scalar quantizations of the previous step. 
 From the proof of Lemma 1, this is a e/m-covering of T with K = dmvkL/eek prototypes. Let us denote this set by {?i, i = 1, ..., K} ? T. 
 From the proof of Lemma 1, the covering of T constructed in the previous step induces an e-covering of F by applying the indexing function IF, i.e., by {IF(?i) : i = 1, ..., K} . 
 ﻿We derive closed form expressions for the secondorder statistics of the spectral power gain of wide-band microwave indoor channels. We obtain our results within a framework general enough to be compatible with several popular channel models, such as those proposed by the  task group, as well as the Saleh-Valenzuela channel model. As all these models, our channel description is based upon clusters and rays with Poisson arrivals and random amplitudes. Our results consist of closed form expressions for the secondorder statistics of the channel power frequency response, where statistical averages involve expectations over ray amplitudes and arrival times. We first express the auto-covariance of this frequency response in closed-form. We then use this result to obtain an analytical expression for the variance and secondorder moment of the channel power within any given interval of frequencies. This allows us to express the channel spectral diversity as a function of model parameters and bandwidth. From this function, we determine the range within which diversity scales approximately linearly with bandwidth and its upper limit. 
 Stochastic wireless channel models allow one to predict the statistics of the radio propagation conditions over an ensemble of scenarios with similar characteristics. This is specially useful in complex, heterogeneous and time varying environments, such as office, residential and industrial indoor scenarios. The the IEEE 802.15.3a task group accepted a channel model [1] for ultra wide-band indoor communications, based on the Saleh-Valenzuela (S-V) channel model [2], and similar models have been discussed in drafts that led to the IEEE 802.15.3c standard. Like the S-V model, the IEEE 802.15.3a channel model is a discrete-time description of the impulse response of a wireless channel, in which multi-path components are grouped into clusters. The arrival times of clusters and rays within them follow Poisson distributions. One exponential ray gain decay profile is defined among clusters and another one for the rays in each cluster. Unlike the S-V model, the fading statistics of multi-path components in the 802.15.3a channel model are log-normal, not Rayleigh. 
 Some of the useful temporal statistics of channel models like this are the power delay profile, the average delay and the RMS delay, which allow one to determine spectral statistics such as coherence bandwidth and average power gain under suitable assumptions, has been extensively discussed in the literature [3], [4]. In contrast, the second-order statistics of the channel power frequency response have received less 
 attention. In [5], it has been shown that the number of significant eigenvalues of the covariance matrix of the random channel impulse response, and hence the diversity order of the channel, scales approximately linearly with bandwidth. Such an increase in diversity, which relates to the reduction of the relative channel power variance as the bandwidth increases, has been reported to reach a saturation point [6], [7]. In a recent paper [8], analytical expressions for the autocorrelation of the channel frequency response squared magnitude, here denoted by |H(j?)|2, as well as the variance of the power over any frequency band, have been derived for the IEEE 802.15.3a channel model, conditioned to fixed and given ray and cluster arrival times. The corresponding statistics over random arrival times were obtained via simulations. To the best of the authors’ knowledge, no closed form expressions are available in the literature for second-order un-conditional statistics of the spectral power gain of wireless channel models such as the IEEE 802.15.3a and the S-V. 
 In this paper, we derive closed form expressions for the second-order statistics of |H(j?)|2 for frequencies greater than some minimum value ?min rad/s, whose precise definition will be given later. Statistical averaging considers randomness of ray amplitudes an arrival times, for a generalized version of the ultra-wide-band microwave indoor channel model accepted by the IEEE 802.15.3a task group [1]. We obtain these results within a framework general enough to be compatible with other related channel models, such the S-V [2]. The flexibility of our formulation allows it to include some of the features in the IEEE 802.15.4a standard channel model for high frequencies [9] as well. As in all these models, our channel description is based upon clusters and rays with Poisson arrivals, with different average power decay profiles among and within clusters. We first derive a closed form expression for the autocovariance of |H(j?)|2. We then use this result to obtain an analytical expression for the variance and raw second-order moment of the channel power gain PB within any given interval of frequencies B above ?min. This allows us to estimate fade depth as a function of channel bandwidth using explicit formulas. Our derivations also predict a positive lower bound to the ratio between the variance of PB and its squared mean. This bound only vanishes as the arrival rate of clusters in the model tends to infinity. 
 In this section we formulate the wireless channel model framework which will be utilized throughout this work. As in the IEEE 802.15.3a channel model, we describe the radio propagation properties of indoor scenarios by a random baseband impulse response of the form [1] 
 is the random arrival time of the m-th path in the i-th cluster, Ti = 0 is the random arrival time of the i-th cluster, and ti,k = 0 is the random delay of the m-th path in the i-th cluster relative to Ti. Each (real valued) random coefficient ai,m represents the amplitude of the m-th path in cluster i. These random numbers are formed as [1] 
 where Ai ? R is a random variable representing the amplitude of the i-th cluster, {pi,m} are un-biased binary random variables taking values from {-1,1}, and each real valued random variable {ai,m} denotes the amplitude (or gain) of the m-th path (or ray) within the i-th cluster relative to that cluster’s amplitude. For simplicity, we do not consider here a large-scale fading factor before the sum in (1), but its effects can be applied to our results with ease.  As in [1], [2], [9], all the random variables in the super-set {Ti}?{ti,m}?{Ai}?{pi,m}?{ai,m} are independent, except Ai from Ti, and ai,m from ti,m, for all i,m. For future use, we describe the dependency of the second- and fourthorder moments of Ai and ai,m on Ti and ti,m, respectively, by means of the following functions: 
 In our formulation, the number of delays Ti and the number of ti,m that take values within any unit-length time interval follow Poisson distributions with mean rates ? and ?, respectively. This is also part of the channel models of [1] and [2]. 
 By choosing the |ai|’s as Rayleigh distributed, for fixed and given delays, and deterministic Ai’s with the form Ai = b0 exp(-Ti/G), g(t) = exp(-t/?), for some ß0,G,? > 0, the above description yields the widely used Saleh-Valenzuela model [2]. A similar choice with log-normally distributed amplitudes |Ai| and |ai| yields the model proposed by the IEEE 802.15.3a task group [1] (excluding its large-scale fading term). The decay profiles of the amplitudes (represented by the functions b and g of (3)) can also be chosen to match those that the IEEE 802-15.4a standard recommends for high-frequencies in some scenario types (such as office and industrial) [9]. Mixed Poisson arrivals, another model feature present in [9], can be handled within the current framework with little additional effort. 
 In (1), the number of clusters Nc and the number of paths within each cluster, Nr, are finite. Such choice responds to the fact that, in practice, choosing Nc and Nr large enough will be sufficient to include all multi-path component with significant power. This is equivalent to saying that for every e > 0, there exist finite Nc and Nr such that 
 with probability close to 1. Clearly, this equivalence requires that the amplitudes decay fast enough with increasing delay. We will make this requirement precise by assuming that the conditional moments of the amplitudes satisfy 
 We will also assume that the conditional moment functions b,b2,g and g2 decay slow enough so that, for some minimum angular frequency ?min rad/s, we can approximate 
 where the non-negative integer exponents {ni} satisfy n1 + 2n2 = 2, n3 + 2n4 = 2. Thus, our results will be valid for situations and channels where the band of interest lies above ?min. In general, the smallest ? for which (6) is a good approximation will be determined by whichever of the functions within the integral of (6) decays fastest. Therefore, ?min will depend mostly on the decay time of the clusters, the reciprocal of which should be several times smaller than ?min. 
 Since amplitudes and arrival times are random, so is |H(j?)|2. In the following sections we will derive analytical expressions for the second-order statistics of |H(j?)|2 as well of the channel power over a given frequency interval. 
 Here we will obtain closed-form expressions for the mean, autocovariance, and correlation coefficient of |H(j?)|2 over any interval of angular frequencies [?l,?r] ? [?min,8]. To derive these results, we will make use of the following technical lemma. 
 Lemma 1: Suppose the sequence of non-negative random numbers   forms a Poisson process with finite arrival rate  be any two functions such that   exist and are bounded. Then 
 where E[·] denotes expectation with respect to all the random variables that determine the expression within the square brackets.	N 
 Proof: For each  of i.i.d. random variables uniformly distributed over [0,N/R]. We have that 
 Applying (2) and the independence relationships between amplitudes stated in the previous section, we obtain 
 Thus, the expected value of |H(j?)|2 at any ? = ?min is the product of the average energy of the cluster amplitudes {Ai} and the ray relative amplitudes {ai,m}. Notice also that ?g(t) = ?E[ai2|ti = t] represents the average power profile of rays in a cluster with amplitude Ai = 1. For this reason, it will be referred to as the the intra-cluster PDP. Similarly, ?b(T) = ?E[A2i|Ti = T] represents the average power density of cluster amplitudes. Accordingly, we will call ?b(T) the inter-cluster power delay profile (PDP). 
 For notational simplicity, we will temporarily adopt a single indexing nomenclature for the path arrival times ti,m. More precisely, and with a slight abuse of notation, we define the infinite random set   , . For our purposes, it will not be necessary to define a mapping between the indexes i,m and the index l. Indeed, it will be sufficient to note that the double-index matching condition 
 the single index conditionti,m = tj,n	??	(i =tjl =ANDtk m??= ln=) is equivalent tok. 
 Using single index notation, the first term on the right hand side of (11) is obtained from (5), which yields 
 ray arrival times ti,m, which imply that the terms in the sum of (12) are zero for the combinations (l = k = l =? = kr)),. (l = k,l ?= l,r =? l), (k =? l,l = r), and (k =? l,l = l,r 
 Subtracting this from (14), using  ( ), and exploiting the independence relationships between {Ai}, {ai,m}, {Ti} and {ti,m} described in the previous section, we obtain 
 It can already be seen from (15) that the autocovariance ofis the sum of a frequency-independent term and a term which depends only on the frequency difference ?1 . From (6), the latter term vanishes as this frequency difference tends to infinity, so because of the first term, the autocovariance between two infinitely distant frequency values is greater than zero. At first sight this may seem counterintuitive. Nevertheless, such behaviour has also been found in [8], and is consistent with the saturation of the channel diversity order as bandwidth increases reported in [6], [7]. As will be discussed in Remark 1 below, such behaviour is due to the fact that the arrival rates for clusters and its multi-path components are finite. Indeed, as the multi-path environment becomes “richer”, i.e., the number of multi-path component increases, the constant term tends to zero. 
 Before performing the last step in our derivation of a simpler closed form expression for c(?1,?2), we define 
 With these definitions, and applying Lemma 1, we obtain that the autocovariance of |H(j?)|2 is given by 
 ?b(T) and ?g(t), respectively. Notice in this expression that only the constants K1 and K2 depend on the conditional fourth-order moments of the amplitudes. Therefore, for given inter- and intra-cluster PDPs, changing the PDF of ray and cluster fades will modify the frequency independent terms K1 and K2. Nevertheless, such change would have no effect on the frequency dependency of the spectral power autocovariance. 
 Remark 1: (Dense impulse responses) If we keep the average energy of the the cluster and rays impulse responses constant while increasing the cluster arrival rate ? to infinity, then the frequency-independent terms in (15) and (16) vanish. To see this, notice that the expected cluster impulse response energy is given by 
 where Lemma 1 has been used. Thus, if we keep this average energy constant and increase the density ? by a factor ? > 1, then the function b would have to be divided by ?. By uniformly scaling the Ai’s to yield such reduction, the resulting function b2 is divided by ?2. Therefore, the integral of b2 in (17) takes the form   = (1/?)?08 ?b2(T)dT, which clearly tends to zero as . 
 From this analysis it follows that the autocovariance of |H(j?)|2 when the impulse response becomes infinitely dense, say c¯c(?), takes the simple form 
 That is, in this case c¯c(?) is the product of the squared magnitude Fourier transforms of the inter-cluster PDP and the intra-cluster PDP evaluated at ?. 
 In this section we will derive closed-form expressions for the mean and variance of the channel power over any band 
 1) Expected Value of the Channel Power Over a Frequency Band: Denote the channel power over the frequency band B by  = . Given the fact that E[|H(j?)|2] is constant for all ? ?min (see (9)), it follows from (10) that 
 The variance of the channel power over a band B , [?l,?r] ? [?min,8) can be obtained directly from the autocovariance of |H(j?)|2 as follows: 
 The fluctuation of the received power over a wireless1 2 c? c? where K , K , b and g are as defined in (17). 
 channel defines its fade statistics. The fact that fade depth decreases with channel bandwidth has been reported in the literature, both from empirical data (see, e.g., [7]) as well as using simulations [8]. However, a closed form formula relating fade depth and the classical channel model parameters used here has, to the best of our knowledge, not been reported to date. The results derived above allow us to easily deal with this issue, as seen in the example of the next section. 
 In this section we will illustrate the application of the previous results, adapting our framework to the “classical” Saleh-Valenzuela model [2]. In this model, the amplitudes |ai,m| are Rayleigh distributed and the Ai’s are deterministic exponentially decaying. The decay profile of the ray amplitudes is given by 
 where ß0 is the average power gain of a ray at the beginning[ ] of a cluster located at Ti = 0. Since the fourth moment4 2 2 E[x4] of a Rayleigh distributed random variable x is related to its second-order moment as E[x ] = 2E[x ] , we have that 
 In order to find the autocovariance of |H(j?)|2 for this example, we substitute (27) into (16), obtaining 
 The variance of |H(j?)|2? = 0is directly obtained by evaluating, which yields the previous expression for 
 Thus, again it can be verified that, as discussed in Remark 1, the lower asymptote of the spectral power correlation vanishes only2 when the cluster arrival rate ? ? 8 as the mean channel power is kept constant. 
 B. Second-Order Statistics Over a Band The variance of the channel power over a band B ? 
 (?min,8) rad/s is obtained by substituting (27) into (25), and then evaluating the integrals in (25). This yields 
 On the other hand, from (10), E[PB] = ß0??G?B. Thus, the normalized channel power variance for this example is 
 Equation (31) reveals several interesting aspects about the normalized channel power variance, which are discussed below: 
 The normalized channel power variance is the sum of three terms. The first one corresponds to K1, which depends on all the parameters except the bandwidth B. It is inversely proportional to the product G?, which is in turn proportional to the average number of clusters with significant amplitudes in the channel impulse response. This term is the lower asymptote of var(PB)/E[PB]2 when B ? 8, and the latter limit corresponds to total channel power variance divided by its squared mean. Again, in agreement with what was shown in Remark 1, this term vanishes only when the impulse response becomes infinitely dense with clusters. From the viewpoint of (31), this is explained by the fact that, with a finite number of clusters with random amplitudes, the law of large numbers does not apply to the total impulse response energy, and hence its variance-to-squared-mean ratio is not zero. 
 The second term on the RHS of (31) depends on B and all the other parameters, except ?, which means it does 
 2Although letting G ?8 also makes the right hand side of (29) vanish, letting G grow unbounded would violate the conditions under which the above results are valid (see (26) and (4)). 
 Figure 1. Autocorrelation coefficient for the S-V model with parameters from the IEEE802.15.3a [1] CM3. Simulated values are averages over 60.000 channel realizations. ?1/(2p) = 1 GHz. The analytical plot was obtained using (28) and (21). 
 not change with the arrival rate of rays within clusters. This term is monotonically decreasing in ?B and nonnegative, approaching 1/2(G?) when ?B ? 0, and 0 when ?B ? 8. It also vanishes when the cluster arrival rate tends to infinity. 
 The third term depends only on B and the intra- and inter-cluster power decay exponents. In particular, it is independent of cluster and ray arrival rates. It is monotonically decreasing with the bandwidth B, tending to zero when the bandwidth B is increased to infinity, and to 1 when B ? 0. 
 It can be seen that the normalized channel power variance over a band grows unbounded as G??? ? 0. This is due to the fact that, when G??? ? 0, the channel power is close to zero with increasing probability. 
 To see how this tendency can make the ratio var(P )/E[P ]2 go to infinity, consider a simpler exam- 
 ple. Let the discrete random variable X take the values X = 0 with probability p and X = 1 with probability 1 - p. Then var(X)/E[X]2 = p/(1 - p), which clearly goes to infinity as p ? 1. 
 The narrow-band normalized channel power variance, var(P0)/E[P0]2 is obtained from (31) by letting ? ? 0. The ratio between the normalized variances of power of a narrowband and a wideband system, i.e., [var(P0)/E[P0]2]/[var(PB)/E[PB]2] can be regarded as a measure of frequency diversity order. From (31), such diversity order would range from 1 (when B ? 0) to 1+(??+2?G??)/(1+??), when B ? 8. For ?? » 1, the latter limit is approximately 2+2?G, which is roughly twice the average number of clusters that fall within G seconds. It can also be seen from (31) that before reaching this limit, and for B » 1/?, the diversity order grows approximately linearly with B, consistent with emprical results [6], [7]. 
 The autocorrelation coefficient for the S-V model with the parameters of the IEEE802.15.3a CM3 [1] is plotted in Figure 1, both from simulations and from our equations. The parameters of the CM3 are: ? = 0.0667 ns-1, ? = 2.1 ns-1, G = 14 ns, and ? = 7.9 ns. It can be seen from this figure that values obtained from channel simulations agree well with (21). In particular, ?(w1,?1 +?) crosses the 0.5 threshold for ?/(2p) between 1/(2pG) ? 10 MHz and 1/(2p?) ? 20 MHz, as predicted by (28), and gets close to its lower asymptotic value when ?/(2p) ? 100 MHz. 
 We derived general expressions that characterize the secondorder statistics of the frequency response power gain in microwave indoor channels. Our results are applicable to several well established channel models discussed in the literature. In particular, the closed-form formula obtained here for the autocovariance of the squared frequency response magnitude of the channel allows one to predict the variance of the channel power over any frequency band. This provides an approximation to the diversity order of wide-band over narrowband systems in such channels. Our results also allow one to obtain an upper bound for this measure of diversity, and show how (and why) this limit arises from having a finite number of clusters in the channel impulse response. 
 ﻿The objective of this note is to report some potentially useful mutual information inequalities.
 Preliminaries
 Throughout this section, and unless otherwise stated, x, xi, i ? N0, y, z and n are continuous random variables taking values in appropriate subsets of Rn. We assume that they all have well defined probability density functions (PDFs), which we denote by fx, fxi, fy, fz and fn, respectively, and well defined joint PDFs denoted by fxy, fxz, etc.  We also use the notation fx|y to refer to the conditional PDF of x, given y. All definitions and results in this section are standard and can be found in [1].
 Definition 1 (Differential entropy) The differential entropy of x is defined via 
 Z
 	h(x) , -	fx(u)lnfx(u)du.
 The conditional differential entropy of x, given y, is defined via	(1)
 Z h(x|y) , -	fxy(u,v)lnfx|y(u,v)dudv.	(2)
 ¤¤
 The differential entropy has the following properties:
 Fact 1 (Properties of h)
 h(x|y) = h(x) with equality if and only if x and y are independent.
 h(x + y|y) = h(x|y).
 If a ? R \ {0}, then h(ax) = h(x) + ln|a|.
   (this property is called chain rule for differential
 entropy).
 If x and y are independent, then e2h(x+y) = e2h(x) + e2h(y) (e2h(x) is called entropy power of x. This property is called entropy power inequality.)
 ¤¤¤
 Definition 2 (Mutual information) The mutual information between x and y is defined via
 	 	(3)
 The conditional mutual information between x and y, given z, is defined via
 	 	(4)
 Mutual information has the following properties:
 Fact 2 (Properties of I)
 I(x;y) = h(x) - h(x|y) = h(y) - h(y|x) = I(y;x).
 I(x;y|z) = h(x|z) - h(x|y,z) = h(y|z) - h(y|x,z) = I(y;x|z).
 I(x;y) = 0 with equality if and only if x and y are independent.
 I(x,y;z) = I(x;z) + I(y;z|x) (chain rule of mutual information).	¤¤
 ¤¤¤
 Definition 3 (Markov chain) The random variables x,y and z are said to form a Markov chain (in that order) if and only if f(x,z|y) = f(x|y)f(z|y), i.e., if and only if x and z are conditionally independent given y. If that is the case, we write
 	x ? y ? z.	(5)
 ¤¤
 Theorem 1 (Data processing inequality) If x ? y ? z, then I(x;y) = I(x;z). Equality holds if and only if, in addition, x ? z ? y.	¤¤¤ Definition 4 (Divergence between PDFs) The divergence of the distribution of x with respect to the distribution of y (in short, the divergence between x and y) is defined by 
 	 	(6)
 ¤¤
 Relevant properties of D(·||·) are summarized below:
 Fact 3 (Properties of D)
 D(x||y) = 0 with equality if and only if fx = fy almost everywhere  (a.e.).
 If xG is a second order Gaussian random variable and x is any other random variable with the same mean and covariance matrix, then
 D(x||xG) = h(xG) - h(x) = D(ax||axG),
 where a ? R \ {0} is any real number.	(7)
 ¤¤¤
 Remark 1 (Conditional divergence) It will prove useful to consider an extension of the definition of divergence. Given two joint distributions fxy and fwz, we define the conditional divergence between them5 via
 	 	(8)
 It is possible to show that the following holds:
 D(x|y||w|z) = 0.
 If xG and yG are jointly Gaussian random variables having joint PDF fxGyG, and x and y are arbitrary random variables having a joint PDF fxy with the same first and second order moments as fxGyG, then
 	D(x|y||xG|yG) = h(xG|yG) - h(x|y).	(9)
 ¤¤
 We end this section with an extension of the notion of differential entropy to random processes.
 Definition 5 (Differential entropy rate) Consider an asymptotically stationary process x. The differential entropy rate of x is defined by 
 	 .	(10)
 ¤¤
 If x is stationary, then it is clear that h¯(x) = h(x(k)), with equality if and only if x is a sequence of independent random variables (recall Fact 1).
 Theorem 2 (Differential entropy rate (see, e.g., [2,3])) If a stationary process xˆ is filtered by a stable filter having frequency response H(ej?), then the filter output x has an entropy rate given by
 	 	(11)
 ¤¤¤
 Results
 This section presents the main results of this note.
 Lemma 1 Consider the situation depicted in Figure 1, where x and n are m-dimensional random variables that have arbitrary distributions. If x and n are independent, and xG and nG denote independent m-dimensional Gaussian random variables having the same mean and covariance matrix as x and n, respectively, then
 I(x;y) = I(xG;yG) + D(n||nG),
 with equality if and only if x and n are jointly Gaussian.	(12)
 Proof: Using Facts 2 and 1, the independence of x,n and xG,nG, and the definition of D(·||·), it is easy to see that
 I(x;y) - I(xG;yG) = h(y) - h(y|x) - h(yG) + h(yG|xG)
 = h(x + n) - h(x + n|x) - h(xG + nG) + h(xG + nG|xG)
 = h(nG) - h(n) - h(xG + nG) + h(x + n) (=a) D(n||nG) - D(x + n||xG + nG)
 = D(n||nG),	(13)
 where the last inequality follows from Fact 3. The result is now immediate.	¤¤¤
  
 Figure 1: Additive channel.
 Lemma 2 Consider the situation depicted in Figure 1, where x and n are m-dimensional random variables, x is Gaussian and n has an arbitrary distribution. If nG denotes an m-dimensional Gaussian random variable, jointly Gaussian with x, having the same mean and covariance matrix as n, and such that the cross-covariance between n and x equals the cross-covariance between nG and x, then
 	I(x;x + nG) = I(x;x + n),	(14)
 with equality if the covariance matrix of x+n is non-singular, and n is Gaussian and jointly Gaussian with x.
 Proof: Using Fact 2 it is possible to write
 	I(x;x + n) - I(x;x + nG) = h(x|x + nG) - h(x|x + n).	(15)
 Use of the facts in Remark 1 the first part of the result follows. Clearly, if n is Gaussian, then equality holds in (14). The proof of the converse can be found in [4]. ¤¤¤
 Lemma 3 Consider the situation depicted in Figure 1, where x and n are independent scalar random variables with arbitrary distributions. If xG and nG denote independent scalar Gaussian random variables having the same mean and covariance matrix as x and n, and D(x||xG) = D(n||nG), then
 	D(x + n||xG + nG) = D(n||nG)	and	I(xG;xG + nG) = I(x;x + n),	(16)
 with equality if and only if x and n are jointly Gaussian.
 Proof: We will use the proof of Lemma 1. If the right hand side in equality (a) in (13) were positive, then the result would be true. Thus, we will start examining the difference D(n||nG)-D(x+
  	(17)
 where we have used Fact 3, the independence of xG,nG and Gaussianity. On the other hand, the entropy power inequality allows one to conclude that, since x,n are independent,
 	 	(18)
  
 Figure 2: Feedback system considered in Lemma 4.
 Use of (18) in (17) yields
 	 	(19)
 and, since the variance of the Gaussian and non-Gaussian random variables is the same, we have from
 (19) that
  
 where (a) follows from Fact 1 and (b) from Facts 3 and 1, and the fact that the variance of the Gaussian and non-Gaussian random variables is the same. The result follows using (20) and (19) in equality (a) in (13).	¤¤¤
 Definition 6 Consider two random processes v and w. We define (if the defining limits exist) the mutual information rate between v and w as
 	 ,	(21)
 and the average mutual information between v and w as
 	 .	(22)
 ¤¤
 Lemma 4 Consider the feedback system in Figure 2, where 1-F(z) is stable and strictly proper (i.e., limz?8 F(z) = 1), d is a random process, and q is an i.i.d. sequence that is independent of d and of the initial state of F(z). Then,
 	 ,	(23)
 where  denotes the set of non minimum phase zeros of F(z).	¤¤¤
 Proof: By definition of mutual information rate and the chain rule of mutual information we have that
 	 .	(24)
 Since w depends causally on d, it follows that I(w(i);dk-1|wi-1) = I(w(i);di|wi-1). Thus,
 	I¯8(d;w) = I8(d ? w).	(25)
 Define n , w - d and note that
 	n = F(z)q.	(26)
 We first note that
 I(w(i);di|wi-1) - I(w(i);vi|wi-1) (=a) h(w(i)|wi-1;vi) - h(w(i)|wi-1,di)
 (=b) h(w(i)|wi-1;vi) - h(n(i)|wi-1,di)
 (=c) h(w(i)|wi-1;vi) - h(n(i)|ni-1,di)
 	(=c) h(w(i)|wi-1;vi) - h(n(i)|ni-1),	(27)
 where (a) follows from Fact 2, (b) follows from the definition of n and Fact 1, (c) follows from the fact that, by definition of n, M ? (wi-1,di) ? (ni-1,di) for every random variable M, and (d) follows from the fact that, since d is independent of q and of the initial state of F(z), d is independent of n and, thus, n(i) ? ni-1 ? di holds. We also have that
 h(w(i)|wi-1,vi) (=a) h(v(i) + q(i)|wi-1,vi)
 (=b) h(q(i)|qi-1,vi)
 	(=c) h(q(i)|qi-1),	(28)
 where (a) follows from the definition of variables in Figure 2, (b) follows from Fact 1 and the fact that, by definition, M ? (wi-1,vi) ? (qi-1,di) for every random variable M, and (c) follows from the fact that both the initial state of F(z) and d being independent of q, q being i.i.d., and F(z) being strictly proper guarantees that q(i) ? qi-1 ? vi.
 From (25), (27), (28) and Fact 1 it follows that
 I¯8(d;w) - I8(v ? w) = h¯(q) - h¯(n).	(29)
 Use of Theorem 2, (26) and the Bode integral theorem (see, e.g., [5]) yields the result.	¤¤¤
 ﻿ There are two major which underly the design of an to digital converter ; namely when to sample and how to represent the amplitude of each sample . In the majority of past work , these two have typically been separately . Recently we have a novel algorithm which moving horizon optimization to determine both when and how to sample . Our work gave a heuristic description of the algorithm and , via , that a performance gain was in both the bit rate and the distortion level . This seemingly paradoxical result is due to the interpolative inherent in . The goal of the current paper is to give theoretical support to the algorithm . In particular , we provide on the probability that beneficial interpolation for the particular case of horizon length with flat , unity gain error weighting filter . 
 The two fundamental that arise when an to digital converter are when to sample and how to represent the amplitude of each sample . Several deal with the first question quantization effects , see , e .., , and rise to irregular and or periodic non uniform sampling . Other deal with each value . In particular , conversion been in a wide range of , see , e .., , . Recently , conversion been extended to incorporate stemming from finite set constrained predictive control , see . 
 work by the present , in , moving horizon optimization to address the dual question of when to sample and how to quantize . The scheme , which we in , to be marked as mute whenever it is desirable to replace them by a linearly value derived from their . Notice that this procedure implicitly to signal dependent adaptive non uniform sampling . 
 Our work , gave an introduction to the algorithm and , via , that distortion in the conversion process can be reduced , when to , such as straight quantization , conversion and its step . Additionally , we that , in general , the number of for the digital representation of the signal can also be reduced . This seemingly paradoxical result from the interpolation inherent in the algorithm . 
 In the current paper , we will give theoretical support to the algorithm by examining the under which beneficial interpolation . In particular , a bound on the probability that interpolation is derived for the special case of horizon optimization . 
 The goal behind is to perform quantization so as to minimize both bit rate and distortion . It is based on the idea of linear interpolation between . can mark a sample as mute . Later , mute , are reconstructed by linearly between . This concept is in Fig . . 
 Fig . a and show the reconstruction of an signal from a uniformly discrete time signal via straight quantization . The dashed line in Fig . a to reconstruction zero order hold . The benefit of 
 Fig . . Reconstruction of an signal from via : a straight quantization ; linear interpolation 
 linear interpolation is clear in Fig . , where the for the at times , , were by linear interpolation of of the . It can be that the reconstructed signal in Fig . b better the original signal . The algorithm on this observation and distortion by virtual quantization between the original every time a sample is . Additionally , data compression due to the fact that mute do not require to represent their value thereby reducing the average bit rate . 
 Consider a scalar discrete time signal with underlying sampling rate . The purpose of to digital conversion is to obtain a representation of ak , i . e ., a discrete time signal . Each value is restricted to belong to a given finite set of , s , s ,..., . 
 In the algorithm , is to be assigned as mute . This condition is by the symbol . Thus , the on can be expressed as : 
 In order to incorporate mute into the conversion problem , we will make use of an interpolator whose output is defined as : 
 that for to a vacant sample non mute sample before non mute sample after 
 We can see that is a discrete time signal formed of all of and the linearly mute of . Notice that is not constrained to belong to quantization process : 
 can be designed by a frequency weighted measure of the error , i . e .: 
 the total number of of ak . In , is the error , defined as : 
 Finite Horizon Formulation : To ensure that the are feasible , it is necessary to restrict the number of decision as well as the number of future of ak considered in the optimization . Thus , at sampling will replace the infinite horizon cost function by the following quadratic cost function defined over a shorter optimization horizon of length : 
 The finite horizon cost in is determined by only a finite constrained . These decision are grouped into the vector 
 Note that the last element of , that is , , must not take the value mute sample , because , if it did , then the calculation of the corresponding value of the sample would require the value of the successive value , namely , which outside and ahead of the horizon . 
 Moving Horizon Approach : Minimization of the step cost in rise to the : 
 Given , a feasible sequence over the optimization horizon . If one was to use the standard moving horizon concept as , e .., in , then only the first element would be . At the next optimization step the whole procedure would be repeated again with the horizon by . While this is in principle possible , the inclusion of mute to additional that are in . In general , the optimization method may give rise to several consecutive mute . In that case , if the horizon one step , the decision for the next element would have to consider the backwards propagation effect , which is not taken into account in the finite horizon cost . This would lead to performance degradation . To overcome this problem , the following modification to the standard moving horizon approach : 
 such that , i . e ., is non mute . The horizon will then be by . The above procedure that , at every optimization step , the precursor is non mute . 
 Remark : Notice that the use of mute rise to a non uniform sampling pattern , and that the average sampling rate will be lower than . 
 Remark : The non interpolative such as straight quantization , conversion and the Converter of in a more general setting . Since the conversion process is carried in an optimal manner , see , the will , in general , give lower distortion than these other conversion . In the sequel , we will analyze the produced by the algorithm . 
 In it was shown by simulation that lower quantization noise than traditional to digital conversion , and at the same time it less . Clearly , these gains depend on a high proportion of mute , where : 
 here , I the number of been converted . 
 Notice that in the case of horizon length , at most one out of every two can be marked as mute , limiting the value . . We also define the probability of a sample as : 
 In it was also shown that the Bit Ratio , defined as the total number of by the output sequence from , namely R , divided by the total number of by traditional quantization , is given by : 
 As a consequence , the data compression of on what proportion of are marked as mute and on the size of the constrained set nU , see . 
 Towards the goal of for Pint see , in this section we will elucidate the under which the interpolation rather than quantization . The following are used : 
 Q , a standard nearest scalar , with output set s ,..., . 
 In the sequel , we will restrict our analysis to the case , in which , , and where si si 
 We will also assume that input signal and output set are scaled such that no overload , i . e . 
 or . However , restricted the horizon length to , the of the algorithm a 
 imply that see . , if a given sample ak is a candidate for interpolation , then the previous converted sample result is non mute . Similarly , if the sample ak was to be marked as mute , then the conversion result for the sample just after it , namely , would be non mute . As a consequence , when considering the possible interpolation of ak , the expression for becomes 
 Probabilistic analysis of is non trivial since its left hand side a sum of correlated continuous random . For our purpose , it is convenient to first assign to each quantization level an integer number in ascending order see Fig . and then define the Amplitude Band Number .. Region for a sample ak as 
 This integer quantity the number of the quantization level for sample ak . As an illustration , for the example in Fig . , , and . Given the above , the ak can be decomposed into : 
 is an integer number that could be as a coarse second difference of ak . 
 Since ¨ is integer valued and , by assumption we only need to distinguish that fulfill , namely : 
 It can be seen from that the for ¨ depend on the of ak , ak and ak . For the purpose of probabilistic analysis , it is more convenient to express it in of : 
 These can be through careful examination of the given in , and Fig . . For sake of simplicity , are . 
 could be numerically . Alternatively , it is possible to devise a graphical method to represent and solve , as in Fig . . In this figure , the range of of which satisfy are shadowed . The horizontal axis is , the value for the first difference see , as by the vertical marked . The vertical axis is a . The horizontal horizontal of mark yielding the same result for see . Thus , a given value for the of a that in yield either , or in order to satisfy the for interpolation of sample ak . 
 The diagonal line ... in Fig . entirely inside the solution area , and the condition . If , this line to the case a a , that is , a ¨ . Since , by assumption we can conclude , in accordance to intuition , that in order to satisfy the condition for interpolation , a ¨ must be as close to zero as possible . In general , a given value of a ¨ will satisfy the condition of interpolation depending on the value of . It can also be seen from figure that the of the 
 Fig . . Graphical solution to equation . Shaded represent the of satisfying it . 
 Additionally , if , the difference between each of these periodic and the diagonal line the for a ¨. The for a ¨ then depend upon . Fig . the effect of quantization error in the range for a ¨ that the condition for interpolation , for three different of a . In particular , it can be from and that , if a ¨ , then 
 We can utilize the framework established in analyze Pint see . Towards this goal , we interpret , a and a ¨ as random which we denote by , a and ¨ a respectively . Assuming that is uniformly distributed on 
 the interval and statistically independent from a and a ¨ , it is possible to obtain see Fig . , for a given value of a and a ¨, the set of to which must belong in order to satisfy . Since is uniformly distributed , the total length of these , which we will denote here as a ,¨ a , is proportional to the probability that the condition for interpolation is satisfied . Thus , the conditional probability of interpolation given that a and ¨ a is given by : 
 Note that the function , a random variable a ,¨ a . If the joint probability density function for a and ¨ a is fa ,¨ a ,, then Pint can be calculated via : 
 Note that the and in the first integral arise from the fact that , , as previously noted . Function , can be numerically or graphically . Fig . , y for three different of . 
 Based on the analysis on section , the probability of interpolation for a given signal depend on the statistical of the first and second of the signal . Clearly , if fa ,¨ a , is known , the probability of interpolation can be precisely from . However , in practice it is desirable to estimate the probability of interpolation a , based on coarse information about the signal . We present next a lower bound for Pint for the case where only the second moment of ¨ a , that is , E ¨ a , is known . 
 THEOREM : For each y , , and provided E ¨ a , a lower bound for the probability of interpolation can be by 
 where ¨ a is the probability density function of ¨ a . Equation then to 
 REMARK : Clearly , the above theorem on the choice of y . Since , by construction , y is monotonically decreasing , the derivative of the right hand side of is zero only at the global maximum . Thus , the value for the bound can be found numerically by y . 
 were by the algorithm to two quality music each of length , . We grouped the into , for of a and of a ¨, and calculated the statistical of interpolation within each category . According to the law of large , the greater the number of in each category , the closer the statistical frequency will approach the actual value of the probability of interpolation for each category . More specifically , for a category defined as ak such that a and a ¨ , it can be that for highly the statistical frequency will approach the conditional probability of interpolation , as , . 
 show the for quantization . In these , the dashed line the function , for the corresponding value of see Fig . . 
 It can be seen that statistical frequency ,. The in the of the from the dashed line are from the fact that in some the number of was rather small . This is particularly true for of a or a ¨ . Despite that , it is important to 
 verify that the for which no interpolation is indeed no at all except an isolated bar on the extreme right in Fig . , from a category of one sample whose existence is by the fact that that category with a not strictly equal to zero . Similarly , for the where , probability of interpolation equal to , all were in deed . 
 To illustrate the derived in section , we converted different audio , each one with and quantization . For each signal , the value for E ¨ a was from the entire signal . Fig . the empirical of interpolation . Furthermore , it the lower from by choosing which . and y which . . It can be from this figure that the lower , although weak , are confirmed by . In particular , as by the , the empirical probability of interpolation as E ¨ a . 
 This paper a recently algorithm which simultaneously reduced bit and lower distortion . The algorithm moving horizon optimization together with interpolation . In this paper , we give a lower bound on the probability that interpolation can be beneficially used for the particular case of horizon length and flat error weighting filter . Simulation have also been which the validity of the lower bound in practical . 
  
 ﻿This paper studies networked control systems closed over noiseless digital channels. We focus on noisy linear timeinvariant (LTI) plants with stationary Gaussian disturbances, Gaussian initial state, scalar-valued control inputs and sensor outputs. For this set-up, we show that the absolute minimal directed information rate that allows one to achieve a prescribed level of performance (not necessarily stationary), over all combinations of encoder-controller-decoder is achieved when the decoder output is jointly Gaussian with the other signals in the system. This directed information rate lower bounds the achievable operational data rates. When restricting our attention to encoder-controller-decoders which make the random processes in the loop (strongly) asymptotically wide-sense stationary, this bound can be expressed in terms of their asymptotic power spectral densities. Then we show that the directed information rate and stationary performance of any such scheme can be achieved when the concatenated encoder, channel, controller and decoder behave as an AWGN channel with LTI filters. We also present a simple coding scheme that allows one to achieve (operational) average data rates that are at most (approximately) 1.254 bits away from the derived lower bound, while satisfying the performance constraint. A numerical example is presented to illustrate our findings. 
 Index Terms—Networked control systems; optimal control; average data rate; information theory; signal-to-noise ratio. 
 This paper studies networked control problems for linear time-invariant (LTI) plants where communication takes place over a digital communication channel. Such problems have received much attention in the recent literature [1], [2]. This interest is motivated by the theoretical challenges inherent to control problems subject to data-rate constraints, and by the many practical implications that the understanding of fundamental limitations in such a set-up may have (some applications are illustrated in, e.g. [3]–[6]). 
 The literature on networked control systems subject to datarate constraints can be broadly classified into two groups. A first group, which includes [7]–[13], uses approaches that are rooted in non-linear control theory. An alternative approach that uses information-theoretic arguments has been adopted in, e.g., [14]–[20]. A key question addressed by the works in the latter group is how to extend, or adapt if necessary, standard information-theoretic notions to reveal fundamental limitations in data-rate-limited feedback loops. Related results have been published in [21]–[23], where the interplay between information constraints and disturbance attenuation is explored. 
 The most basic question in a data-rate limited feedback control framework is whether closed-loop stabilization is possible or not. Indeed, stabilization is possible only if the channel data rate is sufficiently large [11], [24]. These early observations spawned several works that study minimal data rate requirements for stabilization and observability (see, e.g., [16], [25]–[27]). A fundamental result was presented in [15]. For noisy LTI plants controlled over a noiseless digital channels, it is shown in [15] that it is possible to find causal coders, decoders and controllers such that the resulting closed-loop system is mean-square stable, if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. A thorough discussion of this and related work can be found in the survey paper [1]. Recent extensions, including stabilization over time-varying channels, are presented in [28]–[32]. 
 It is fair to state that, for LTI plants, stabilization problems subject to data rate constraints are well-understood. However, the question of what is the best closed-loop performance that is achievable with a given data rate is largely open. Such problems are related to causal (and zero-delay) rate-distortion problems (see, e.g., [33]–[37]). In the latter context, the best results are, to our knowledge, algorithmic in nature, derived for open-loop systems and, at times, rely on arbitrarily long delays [33], [34]. It thus follows that the results in the above references are not immediately applicable to feedback control systems. 
 In the rate-constrained control literature, lower bounds on the mean-square norm of the plant state have been derived which show that, when disturbances are present, closed-loop performance becomes arbitrarily poor when the feedback data rate approaches the minimum for stability [1], [15]. This result holds no matter how the coder, decoder and controller are designed. Unfortunately, the bounds in [1], [15] do not seem to be tight in general. In contrast, for fully observable noiseless LTI plants with bounded initial state, [18] shows that one can (essentially) recover the best non-networked LQR performance with data rates arbitrarily close to the minimum average data rate for stabilization. Other results valid in the noiseless or bounded-support noise cases can be found in, e.g., [38]–[40]. 
 Relevant work on optimal control subject to rate-constraints, and dealing with unbounded support noise sources, include [1], [17]. Those works establish conditions for separation and certainty equivalence in the context of quadratic stochastic problems for fully observed plants, when data rate constraints are present in the feedback path. It is shown in [1] that, provided the encoder has a recursive structure, certainty equivalence and a partial separation principle hold. The latter result is relevant. However, [1] does not give a practical characterization of optimal encoding policies. The results reported in [17] share a similar drawback. Indeed, performance-related results in [17] are described in terms of the sequential ratedistortion function, which is difficult to compute in general. Moreover, even for the cases where an expression for such function is available, it is not clear whether the sequential rate-distortion function is operationally tight [17, Section IVC]. Partial separation in optimal quantized control problems has been recently revisited in [41]. 
 Additional results related to the performance of control systems subject to data-rate constraints are reported in [38], [42] and [43]. In [38], noiseless state estimation problems subject to data rate constraints are studied. The case most relevant to this work uses an asymptotic (in time) quadratic criterion to measure the state reconstruction error. For such a measure, it is shown in [38] that the bound established in [15] is sufficient to achieve any prescribed asymptotic distortion level. This is achieved, however, at the expense of arbitrarily large estimation errors for any given finite time. On the other hand, [43] considers non-linear stochastic control problems over noisy channels, and a functional (i.e., not explicit) characterization of the optimal control policies is presented. In turn, [42] presents a computationally-intensive iterative method for encoder and controller design for LTI plants controlled over noisy discrete memoryless channels. Conditions for separation and certainty equivalence are also discussed in [42] for some specific set-ups. 
 In this paper, we focus on the feedback control of noisy LTI plants, with one-dimensional control inputs and sensor outputs, that are controlled over an error-free (and delay-free) digital channel. The plant initial state xo and the disturbance process d are assumed to be jointly Gaussian. By considering causal but otherwise unconstrained coding schemes, we study the minimal (operational) average data rate, say R(D), that guarantees that the steady-state variance of an error signal is below a pre-specified level D > 0. Our approach is mainly information-theoretic and thus some of our findings complement previous results in [21], [44]. 
 We have previously addressed this problem in [45] and [46]. In [45] we have shown that R(D) is lower bounded by the directed information rate I8(y ? u), where y and u are the input to the encoder and output from the decoder, respectively. Then, for a restricted class of encoder-decoder pairs, called “independent coding schemes” (characterized by behaving as a unit transfer function form y to u plus coloured additive wide-sense-stationary noise independent of (xo,d)) we showed that I8(y ? u) is minimized only if y and u are jointly Gaussian. For the latter case, it was shown that this information rate can be written as  where Su is the asymptotic power spectral density of u and sn2 is the asymptotic variance of the error in estimating u(i) from u(1),··· ,u(i - 1) and y(1),··· ,y(i - 1). It was also shown that an operational average data rate exceeding I8(y ? u) by less than (approximately) 1.25 bits/sample was achievable by using an entropy-coded dithered uniform quantizer (ECUDQ). In [46], and also for the class of independent coding schemes, we then derived the minimum feedback directed information rate I8(y ? u) for an asymptotic quadratic performance measure. 
 An important departure of this paper from our previous work is that here the encoder-decoder is not constrained to exhibit a unit transfer function from y to u (i.e., it is not restricted to be an independent coding scheme). As a result, the encoderdecoder pair also performs as a controller for the plant, and thus reaching R(D) means designing the jointly optimal encoder-controller-decoder system. For this broader context, the main contributions of this paper can be summarized as follows: 
 1)	We show that, for any given performance D, I8(u ? y) is minimized if and only if u and y are jointly Gaussian (in Section III). 
 2)	For the jointly Gaussian case, we also show that I8(y ? for the class of coding 
 schemes yielding strongly asymptotic wide-sense stationary (SAWSS)  processes u and y. This class (namely, SAWSS schemes) is much larger than (and includes) the family of independent coding schemes. For instance, it allows for encoder-controller-decoders built using any combination of linear and non-linear causal mappings, provided the SAWSS behaviour is reached. This result is presented in Section III. 
 3)	In Section IV we show that the asymptotic rateperformance of any Gaussian SAWSS scheme can be achieved also by a set of LTI filters and an exogenous additive white Gaussian noise. 
 4)	In Section V we use the latter result to show how to minimize I8(y ? u) within the class of SAWSS schemes by solving a related stochastic control problem subject to an SNR constraint in the feedback channel (Section V). This same SNR-constrained problem has been addressed in [47] and in [48] (for different but related networked control problems subject to SNR constraints, see, e.g., [49]–[51]). 
 5)	Exploiting the fact that the latter problem was shown to be convex in [35], [47] and the convenient procedure to solve it presented in [48], we explain how to design a simple encoder-controller-decoding scheme which performs within (approximately) 1.25 bits/sample of R(D) within the class of SAWSS schemes, for any prescribed performance D > 0. This result is presented in Section VI. 
 To illustrate the above results, section VII presents a numerical example. Concluding remarks are presented in Section VIII. 
 Notation: R denotes the set of real numbers, R+ denotes the set of strictly positive real numbers,  , 
 N0 ? {0,1,···}. In this paper, log stands for natural logarithm, and |x| for the magnitude (absolute value) of x. We work in discrete time and use k for the time index. An LTI filter X is said to be proper (i.e., causal) if its transfer function X(z) remains finite when z ? 8, and it is said biproper if it is proper and limz?8 X(z) = 0? . We define the set U8 as the set of all proper and stable filters with inverses that are also stable and proper. 
 In this paper, all random processes are defined for k ? N0. All random variables and processes are assumed to be vectorvalued, unless stated otherwise. Given a process x, we denote its kth sample by x(k) and use xk as shorthand for x(0),...,x(k). We say that a random process is a secondorder one if it has first- and second-order moments that are bounded for every k and that also remain bounded as k ? 8. 
 Gaussian processes are, by definition, second-order ones [52]. We use E[·] to denote the expectation operator. A process x is said to be asymptotically wide-sense stationary (AWSS) if and only if there exist µx and a function Rx(t), both independent of the statistics of x(0), such that limk?8 E[x(k)] = µTx and limk?8 E[(x(k + t?) - E[x(k + t)])(x(k) - E[x(k)]) ] = 
 Rx(t) for every t N0. The steady-state spectral density of an AWSS process is denoted by Sx (and defined as the Fourier transform of Rx(t) extended for t < 0 according to Rx(t) = Rx(-t)T). The corresponding steady-state covariance matrix is denoted by Px, and sx  ? trace{Px}. Jointly second-order and jointly AWSS processes are defined in the obvious way. The covariance matrix of a random sequence   is denoted by  ···	E[u1n])T], where u1n ? [u(1) u(n)]T. For a matrix A, the notation [A]k,l refers to the element of A on the k-th row, l-th column, while ?m(A) and ?M(A) denote the eigenvalues of A with the smallest and largest magnitude, respectively. Two sequences of matrices , with An,Bn ? Rn×n, are said to be asymptotically equivalent (see [53]) if the following two conditions are met: 
 for some M < 8. Appendix B recalls some useful notation and results from Information Theory [54]. 
 This paper focuses on the networked control system (NCS) of Fig. 1. In that figure, P is an LTI plant,e u ? R is the control input, y ? R is a sensor output, e ? Rnd is a signal related to closed-loop performance, and d ? Rn is a disturbance. The feedback path in Fig. 1 comprises a digital channel and thus quantization becomes mandatory. This task is carried out by an encoder whose output corresponds to a sequence of binary words. These words are then transmitted over the channel, and mapped back into real numbers by a decoder. The encoder and decoder also embody a controller for the plant. 
 where Pij are proper transfer functions of suitable dimensions. We will make use of the following assumptions. 
 Assumption 2.1: P is a proper LTI plant, free of unstable hidden modes, such that the open-loop transfer function from u to y (i.e., P22 in (3)) is single-input single-output and strictly proper. The initial state of the plant, say xo, and the disturbance d are jointly Gaussian, d is zero-mean white noise with unit variance Pd = I, and xo has finite differential entropy (i.e., the variance of xo is positive definite). ? 
 We focus on error-free zero-delay digital channels and denote the channel input alphabet by C, a countable set of prefix-free binary words [54]. Whenever the channel input symbol yc(k) belongs to C, the corresponding channel output is given by uc(k) = yc(k). The expected length of yc(k) is denoted by R(k), and the average data rate across the channel is thus defined as2 
 We assume the encoder to be an arbitrary (hence possibly non-linear and time-varying) causal system such that the channel input yc satisfies 
 where ak is shorthand for a(0),...,a(k), SE(k) denotes side information that becomes available at the encoder at time instant k, and Ek is a (possibly non-linear and time-varying) deterministic mapping whose range is a subset of C. Similarly, we assume that the decoder is such that the plant input u is given by 
 where SD(k) denotes side information that becomes available at the decoder at time instant k, and Dk is a (possibly nonlinear and time-varying) deterministic mapping. 
 Assumption 2.2: The systems E and D in Fig. 1 are causal, possibly time-varying or non-linear, described by (5)–(6). The side information sequences SE and SD are jointly independent of (xo,d), and the decoder is invertible upon knowledge of ui and , i.e., ?i ? N0, there exists a deterministic mapping g?i such that . 
 The assumption on the side information sequences is motivated by the requirement that (causal) encoders and decoders use only past and present input values, and additional information not related to the message being sent, to construct their current outputs (see also page 5 in [55]). On the other hand, if, for some encoder E and decoder D, the decoder is not invertible, then one can always define an alternative encoder and decoder pair, where the decoder is invertible, yielding the same input-output relationship as E and D, but incurring a lower average data rate [45, Lemma 4.1]. Accordingly, one can focus, without loss of generality, on encoder-decoder pairs where the decoder is invertible. 
 Definition 2.1: We say that the NCS of Fig. 1 is asymptotically wide-sense stationary (AWSS) if and only if the state of the plant x, the output y, the control input u, and the disturbance d, are jointly second-order AWSS processes. ? 
 Remark 2.1: The notion of stability introduced above is stronger than the usual notion of mean-square stability (MSS) where only supk?N0 E{x(k)x(k)T} < 8 is required to hold 
 In what follows we will be interested on expressing information rates in terms of the asymptotic spectral densities of the processes in the system. To do so, it will be necessary to use a stronger notion of asymptotic wide-sense stationarity: 
 Definition 2.2 (Strongly AWSS Process): An AWSS scalar random process u1 which converges to a WSS process u¯ is said to be strongly asymptotically wide-sense stationary (SAWSS) if the sequences  are asymptotically equivalent. Likewise, a NCS is said to be SAWSS if the covariance and cross-covariance matrices of all the processes in it are asymptotically equivalent to their stationary counterparts. ? Clearly an SAWSS NCS is also AWSS, but the converse may not be true, in general. The same holds for processes. 
 The goal of this paper is to characterize, for the NCS of Fig. 1, the minimal average data rate R that guarantees a given performance level as measured by the steady-state variance of the output e. We denote by Dinf the infimal steady-state variance of e that can be achieved by setting u(k) = Kk(yk), with Kk being a (possibly non-linear and time-varying) deterministic mapping, under the constraint that the resulting feedback loop is SAWSS. With this definition, we formally state the problem of interest in this paper as follows: Find, for any D ? (Dinf,8) and whenever Assumption 2.1 holds, 
 where se2 ? trace{Pe}, Pe is the steady-state covariance matrix of e, and the optimization is carried out with respect to all encoders E and decoders D that satisfy Assumption 2.2 and render the resulting NCS SAWSS. 
 It can be shown that the problem in (7) is feasible for every D ? (Dinf,8) (see Appendix A). If D < Dinf, then the problem is clearly unfeasible. On the other hand, achieving D = Dinf incurs an infinite average data rate, except for very special cases. We will thus focus on D ? (Dinf,8) without loss of generality. 
 The remainder of this paper characterizes R(D) within a gap smaller than (approximately) 1.254 bits per sample. Such characterization is given in terms of the solution to a constrained quadratic optimal control problem. We also propose encoders and decoders which achieve an average data rate within the above gap, while satisfying the performance constraint on the steady-state variance of e. 
 This section shows that a lower bound on R(D) can be obtained by minimizing the directed information rate across an auxiliary coding scheme comprised of LTI systems and an additive white Gaussian noise channel with feedback. The starting point of our presentation is a result in [45], which does not require any stationarity assumptions. 
 The quantity I8(y ? u) corresponds to the directed information rate [55] across the source coding scheme of Fig. 1 (i.e., between the input y and the output u of the source coding scheme). Note that I8(y ? u) is a function of the joint statistics of y and u only. 
 We will now derive a lower bound on the directed information rate across the considered coding scheme, in terms of the directed information rate that would appear if all the involved signals were Gaussian. 
 Lemma 3.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If, in addition, (xo,d,y,u) are jointly second-order, then I8(y ? u) = I8(yG ? uG), where yG and uG are such that (xo,d,yG,uG) are jointly Gaussian with the same first- and second-order (cross-) moments as (xo,d,y,u). 
 Proof: Our claim is due to the following chain of equalities and inequalities: k-1 I(u(i);yi|ui-1) (=a) I(xo,dk-1;uk-1) i=0 
 (x1,o,d1) = (xo,d), y¯ = y, u¯ = u and (x2,o,d2) = (SD,SE), and (b) follows from Lemma B.1 in Appendix B. In turn, equality (c) in (9) holds since 
 where (a), (c) and (e) hold from the chain-rule of mutual information, (b) stems from (52b) in Theorem B.3, (d) is a consequence of   being a deterministic function of  , and (f) follows from (52a) in Theorem B.3. This completes the proof. 
 It follows from Theorem 3.1 and Lemma 3.1 that, in order to bound R(D) from below, it suffices to minimize the directed information rate that would appear if the source coding scheme of Figure 1 (including the channel) were replaced by a block which yields signals y and u which are jointly Gaussian. Again, this holds irrespective of the stationarity of the signals involved. 
 Now we can relate the directed information rate from yG to uG to their associated spectral densities. For that purpose (and from here on), we shall focus on encoder-decoder pairs which render the NCS SAWSS. 
 Lemma 3.2: Assume that u and y are jointly Gaussian AWSS processes and that u is SAWSS and such that  , for some d > 0. Then, 
 where Su¯ is the steady-state power spectral density of u, and sn2 is the steady-state variance of the Gaussian AWSS sequence of independent random variables n, defined via n(k) ? u(k) - uˆ(k), uˆ(k) ? E[u(k)|yk,uk-1]. (11) 
 Proof: We start by noting that, since (u,y) are jointly Gaussian AWSS processes, a simple modification of the proof of Theorem 2.4 in [58, p. 20] yields the conclusion that n is also Gaussian and AWSS. To proceed, we note that 
 where (a) follows from Property 1 in Appendix(b) follows from the definition of uˆ, (c) follows from Property 2 in and the fact that, by construction, uˆ(i) is a deterministic function of (yi,ui-1), and (d) follows from Property 3 in Appendix n(i) is independent of (yi,ui-1). Now, (12) and the definition of directed information rate yields 
 where (a) follows from Properties 3 and 4 in Appendix and the fact that, by construction, n(k) is independent of nk-1, and (b) follows from Lemma B.5 in the Appendix and the fact that n is Gaussian AWSS and that u is SAWSS with 
 Lemma 3.2 characterizes the directed information rate between Gaussian SAWSS processes in terms of the spectrum of the process towards which the mutual information is directed. Lemma 3.2 generalizes Theorem 4.6 in [59], where the author calculates directed information rates between Gaussian processes that are linked by an additive white Gaussian noise channel with unit transfer function. Thus, Lemma 3.2 generalizes the latter theorem by considering a channel with coloured Gaussian noise and with an input-output transfer function different from unity. 
 Remark 3.1: It is worth mentioning that simply requiring u to be AWSS (instead of SAWSS with  for all n) may yield a left-hand side of (56) smaller than the term on its right. Indeed, to the best of our knowledge, the only result related to (56) available for AWSS processes is [60, Lemma 4.3], which yields an inequality (=) in (56) instead of equality. To see why AWSS is not enough for equality, consider the following example: 
 Let G be an unstable LTI system with zero initial state that can be stabilized by unit feedback. Let q be a white Gaussian process with zero mean and variance sq2. Construct u as 
 gain closed loop with feedback AWGN noise q. In this case, u can be written as will be AWSS with asymptotic power spectral density (PSD) Su¯(ej?) = sq2/|1 + G|2. Then, the entropy rate of the asymptotic process u¯ is 
 where the last equality follows from Jensen’s formula (Bode Integral Theorem) and pi are the poles of G. 
 However, it turns out that, for all , for some lower triangular matrix Fn with ones along its main diagonal. Therefore, 
 Fig. 2. Auxiliary LTI system that arises when the encoder and decoder of Fig. 1 are replaced by proper LTI filters F and L and an additive white noise channel with (one-step delayed) feedback. 
 In this section we show that, for any given performance D, the directed information rate achieved by any SAWSS scheme can also be achieved by an encoder-decoder pair built with only LTI filters and an exogenous source of white Gaussian noise. To that end, we begin by noting that Theorem 3.1 and Lemma 3.1 readily imply that for any encoder and decoder satisfying Assumption 2.2, and rendering the resulting NCS SAWSS, R = I8(yG ? uG), where (yG,uG) are such that (xo,d,uG,yG) are jointly Gaussian with the same first- and second-order (cross-) moments as (xo,d,u,y). We also note that the adopted performance measure is quadratic. The above observations imply that one can always match (or improve) the rate-performance trade-off of a given encoder-decoder pair by choosing, instead, an encoder and a decoder which, besides rendering the NCS SAWSS, yields (u,y) jointly Gaussian with (xo,d). Since the plant initial state and the disturbance are Gaussian, and the plant is LTI, one possible way of achieving such pair of signals (u,y) is by using the LTI feedback architecture of Fig. 2. We formalize these observations below. 
 Define the auxiliary LTI feedback scheme of Fig. 2, where everything is as in Fig. 1 except for the fact that we have replaced the link between the plant output y and the plant input u by a set of proper LTI filters, F and L, and an additive noise channel with (one-step delayed) feedback and noise q' such that 
 where z-1 stands for the unit delay. In Fig. 2, we assume that the plant P, the disturbance d and the plant initial state xo satisfy Assumption 2.1, that the initial states of F,L and of the delay are deterministic, and that q' is zero mean Gaussian white noise, independent of (xo,d), and having constant variance sq2'. 
 In Fig. 2, we have added apostrophes (as in e') to all symbols that refer to signals that have a counterpart in the scheme of Fig. 1 with possibly different statistics. To streamline our presentation, we adopt the convention that, whenever we refer to the auxiliary feedback system of Figure 2, it is to be understood that we are implicitly working under the assumptions stated in the above paragraph. 
 Theorem 4.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If D ? (Dinf,8), then 
 where the optimization defining ?'u(D) is performed with respect to all proper LTI filters L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of Fig. 2 with 
 F = 1 internally stable and well-posed, and Su' and se2' denote the steady-state power spectral density of u' and the steady state variance e' in Fig. 2, respectively. 
 Proof: Denote by CD the set of all encoders E and decoders D such that, when placed at each end of the discrete channel of Fig. 1 satisfy Assumption 2.2, render the NCS of Figure 1 SAWSS and guarantee that se2 = D. Also, define CD,G to be the set of encoder-channel-decoder triplets which consider a unit-gain (i.e., continuous alphabet) noisy channel with (possibly time-varying) linear filters as encoders/decoders which render u and y jointly Gaussian and yield se2 = D. (Since D > Dinf, both CD and CD,G are non empty; see Appendix A.) The fact that D > Dinf guarantees the existence of at least one encoder-decoder pair in CD, say (E˜,D˜), producing processes y˜ and u˜, such that its average data rate 
 where the last equality is a consequence of Lemma 3.2. Since y˜G and u˜G are jointly SAWSS, their statistics (and corresponding variance of e˜G) are yielded by an encoderchannel-decoder triplet in CD,G, consisting of a sequence of linear mappings Lk, k ? N0, such that 
 . Since Lk is linear and causal, it induces a relationship between  and  which can be expressed 
 where {Bk} and {Gk} are sequences of lower triangular matrices such that, ?k ? N, Bk-1 and Gk-1 are the top left corners of Bk and Gk, respectively. Now, since (y,˜ u˜) are jointly SAWSS, the covariance matrices  , and associated cross-covariance matrices are asymptotically equivalent to Toeplitz matrices as well. Using the transitivity of asymptotic equivalence for products and sums of matrices in [53], it is possible to show that {Bk} and {Gk} must be asymptotically equivalent to a sequence of lower triangular Toeplitz matrices as well (see, e.g. [53, Section 6.3]). Also, the associated limiting Lk renders the resulting NCS internally stable and well-posed (otherwise the underlying encoder and decoder would not be in CD,G), and defines the steady-state spectrum Su˜ of u˜ and the steady-state variances  of both n˜ and e˜. (Here, we use the fact that n˜ is also SAWSS; see the proof of Lemma 3.2.) 
 Now, consider the auxiliary LTI feedback system of Figure 2 described before. Assume that F = 1, that L reproduces the steady-state behaviour of Lk in (18) (i.e., L is comprised of two LTI filters, having as impulse responses the last rows of 
 variance equal tolimk?8 Bk and lims2 k(?8see previous paragraph). With the aboveGk, respectively), and that q' has a 
 n choices for F, L and q', and since the matrix representations of L for each k are asymptotically equivalent to the sequence 
 , it follows that the feedback system of Figure 2 is internally stable and well-posed. In particular, it yields the an SAWSS set of processes in the closed-loop such that the plant input u' admits a steady-state power spectral density Su' that, by construction, equals Su˜ in the previous paragraph. Similarly, the error signal e' in Figure 2 admits a steady-state variance se2' that equals . It thus follows from Lemma 3.2 
 We thus conclude that, for any encoder and decoder (E˜,D˜) in CD producing processes y˜ and u˜, there exist a proper LTI filter L and a Gaussian white noise source q' such that, when F = 1, the mutual information rate I8(y' ? u') in Fig. 2 equals I8(y˜ ? u˜) while achieving . Recalling that R(D) is the infimum of R over CD, it follows from (17) and (20) that R(D) is lower bounded by ?'u(D) as in (16), completing the proof. 
 Theorem 4.1 states that a lower bound on the minimal average data rate that guarantees a given performance level, can be obtained by solving an optimization problem which is stated for the auxiliary LTI feedback system of Fig. 2, where communication takes place over an additive white Gaussian noise channel with feedback. 
 We finish this section by deriving a simpler lower bound on R(D). To that end, we will first state an auxiliary result. 
 Lemma 4.1: Consider the LTI feedback system of Fig. 2. Fix sq2' ? R+ and define (whenever the involved quantities exist) 
 where Sw' is the steady-state power spectral density of w'. If the pair (F,L) = (F(0),L(0)) renders the feedback system of Fig. 2 internally stable and well-posed, then there exist a second pair of filters, namely (F,L) = (F(1),L(1)), with F(1) biproper, that also defines an internally stable and well-posed feedback loop, leaves the steady-state power spectral density of e' unaltered, and is such that for any (arbitrarily small)  . 
 Proof: Consider Fig. 2 and the partition for P in (3). Introduce proper transfer functions Ly and Lw such that L = [Lw Ly ] (see (15)). A standard argument [61] shows that the feedback system of Fig. 2 is internally stable and well-posed if and only if the transfer function T between [q' d n1 n2]T and [e' y' w' u']T in Fig. 3 is stable and proper. It is straightforward to see that T has the structure shown in the unnumbered equation at the top of the next page, where 
 We will write T(i) to refer to the matrix T that arises when (F,L) = (F(i),L(i)), i ? {0,1}. Similarly, Ly(i) and L(wi) refer to the components of L, when L = L(i). Set 
 where1. Given (24) and the fact thatn0 is the relative degree ofXF? U(0),8X, it follows that? U8 and X(8F) =(1) 
 (F(1),L(1)) renders the feedback system of Figure 2 internally stable and well-posed if and only if (F(0),L(0)) does so. It also immediately follows that (F(1),L(1)) defines the same stationary spectral density for e' than (F(0),L(0)). 
 To complete the proof, we now propose specific choice for X. Denote by w'(i) the signal w' that arises when (F,L) = (F(i),L(i)). Write Sw'(0) = |?w'(0)|2, where ?w'(0) is stable, biproper, and has all its zeros in {z ? C : |z| = 1}. Denote by c1,...,cnc the zeros of ?w'(0) that lie on the unit circle. Define, for ? ? (0,1), 
 By construction, X?(8) = 1 and X? ? U8 for every ? ? (0,1). It now follows, by proceeding as in the proof of Theorem 5.2 in [45], that there exists ? ? (0,1) such that setting X = X? in (24) guarantees that (F(1),L(1)) is such that (22) holds for any ? > 0. 
 Corollary 4.1: Consider the NCS of Fig. 1 and suppose that Assumptions 2.1 and 2.2 hold. If D ? (Dinf,8), then 
 where the optimization defining ?'(D) is performed with respect to all proper LTI filters F and L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of 
 Fig. 2 internally stable and well-posed, and sv2' and se2' denote the steady-state variances of v' and e' in Fig. 2, respectively. 
 Proof: Consider the LTI feedback system of Figure 2 and recall the definition of both ?'u(D) and ?'w in (16) and (21). Since D > Dinf, the problem of finding ?'u(D) is feasible (see Appendix A). Thus, for any ? > 0, there exist a proper LTI filter L?, and s?2' ? R+, such that se2' = D and ?'u(D) + ? = ?'w(1,L?,s?2'), (26) 
 where we have used the fact that u' = w' whenever F = 1. On the other hand, Lemma 4.1 guarantees that there exists a pair of proper filters (F¯?,L¯?), with F¯? biproper, such that the auxiliary feedback system of Figure 2 is internally stable and well-posed, 
 where the inequality follows from the definition of ?'(D). Since (28) holds for any ?,? > 0, our claim is now immediate from Theorem 4.1. 
 Corollary 4.1 shows that a lower bound on R(D) can be obtained by first characterizing ?'(D), i.e., by first characterizing, for the auxiliary LTI feedback system of Fig. 2, the minimal steady-state SNR ?' = sv2'/sq2' that guarantees that the steady-state variance of the error signal e' is upper bounded by D. Section VI-A discusses how to obtain a numerical approximation to ?'(D). 
 This section shows that it is indeed possible to achieve any distortion level D ? (Dinf,8) while incurring an average data rate that exceeds the lower bound on R(D) in Corollary 4.1 by less than (approximately) 1.254 bits per sample. 
 Definition 5.1: The source coding scheme described by (5) and (6) is said to be linear if and only if, when used around an error-free zero-delay digital channel, is such that its input y and output u are related via 
 where v and w are scalar-valued auxiliary signals, q is an independent second-order zero-mean i.i.d. sequence, and both F and L are the transfer functions of proper LTI systems that, together with the unit delay z-1, have deterministic initial states. ¦ 
 Remark 5.1: In Definition 5.1, the requirement of q being independent (without reference to other random variables or processes) is to be understood as requiring q to be independent of all exogenous processes and initial states in the (feedback) system in which the source coding scheme is embedded. In particular, when an independent source coding scheme is used in the NCS of Fig. 1, q is to be assumed independent of (xo,d). ¦ 
 The class of linear source coding schemes is motivated by the results of Section III and generalizes the class of independent source coding schemes introduced in [45].  We note that independent source coding schemes do not necessarily satisfy Assumption 2.2. 
 Linear source coding schemes are defined in terms of their input-output relationship with no regard as to how the channel input yc is related to the source coding scheme input y. A simple way of making that relationship explicit is by using an entropy-coded dithered quantizer (ECDQ; [45], [62]). When using such a device, v and w in (29), and the channel input yc and output uc, are related via 
 where dh is a dither signal available at both the encoder and decoder sides, Q : R ? {i?;i ? Z} denotes a uniform quantizer with step size ? ? R+, Hk is a mapping describing an entropy-coder (i.e., a loss-less encoder [54, Ch.5]) whose output symbol is chosen according to the conditional distribution of s(k), given dh(k), and Hk-1 is a mapping describing the entropy-decoder that is complementary to the entropy-coder at the encoder side. 
 Lemma 5.1 (Theorem 5.3 in [45]): Consider the set-up of Figure 4, where the ECDQ is as in (30) and has a finite quantization step ?. Assume that P¯ is a proper real rational transfer function, that the open-loop transfer function from w to v is single-input single-output and strictly proper, and that the signal d¯is a white noise sequence jointly second order with the initial state x¯o of P¯. If the dither dh is i.i.d., independent of (¯xo,d¯) and uniformly distributed on (-?/2,?/2), then 
 w--v is i.i.d., independent of¦ (¯xo,d¯) and uniformly distributed in ( ?/2,?/2). 
 It follows that any coding scheme described by (29) and (30), with dither as in Lemma 5.1, is a linear source coding scheme. Any such coding scheme will be referred to as an ECDQ-based linear source coding scheme. Figure 5 depicts an ECDQ-based linear source coding scheme where we have made explicit the fact that, since the channel is error-free and has zero delay, sˆ = s and, thus, w can be obtained at the encoder side without making use of any additional feedback channel. 
 The next lemma gives an upper bound on the (operational) average data rate in an ECDQ-based linear source coding scheme. 
 Lemma 5.2: Consider the NCS of Fig. 1 and suppose that that Assumption 2.1 holds. Then, there exists an ECDQ-based linear source coding scheme such that the resulting NCS is SAWSS. For any such coding scheme, 
 where sv2 is the steady-state variance of the auxiliary signal v, and sq2 = ?2/12 is the linear source coding scheme noise variance (see (29)). 
 Proof: Consider the NCS of Fig. 1 and assume that the source coding scheme is linear. Since Assumption 2.1 holds, there exist proper LTI filters L and F such that the resulting NCS is internally stable and well-posed (one possibility is to choose L such that v = y and to pick any F which internally stabilizes P). For any such choice of filters, the open loop system linking w with v is stabilizable with unity feedback. Our claim now follows immediately upon using Corollary 5.3 in [45] and the description for the coding noise in Lemma 
 Theorem 5.1: Consider the NCS of Fig. 1 and suppose that Assumption 2.1 holds. If D ? (Dinf,8), then there exists an ECDQ-based linear source coding scheme satisfying Assumption 2.2 such that the resulting NCS is SAWSS, se2 = D, and 
 Proof: Since D > Dinf, the problem in (25) is feasible (see Appendix A). Thus, there exist proper LTI filters L and 
 F rendering the feedback system of Fig. 2 internally stable and well-posed, and sq2' ? R+, such that, in the scheme of Fig. 2, and for any ? > 0, se2' = D and 
 Denote the above choices for L, F and sq2' by L?,F? and s?2', respectively. Given Lemma 4.1 and Jensen’s inequality, F? can be assumed to be biproper without loss of generality. 
 Consider the NCS of Fig. 1 and assume that the link between y and u is given by an ECDQ-based linear source coding scheme with parameters (L,F,?) = (L?,F?,(12s?2')1/2), and set the initial states of L?, F? and of the channel feedback delay to zero. The definition of L?, F? and s?2', together with Lemma 5.1, guarantee by construction that the NCS that results from the above choice of coding scheme is SAWSS and that, in addition, the plant output e and the auxiliary signal v in (29) have steady-state variances satisfying 
 By Lemma 5.2 we also conclude that, for the above described ECDQ-based linear source coding scheme, the average expected length of the channel input yc satisfies, for some suitable d > 0, 
 where we have used (34). Thus, inequality (32) follows upon choosing a sufficiently small ? > 0. 
 To complete the proof, we now show that the proposed source coding scheme satisfies Assumption 2.2. Except for the invertibility of the decoder, the properties of dh guarantee that Assumption 2.2 holds (note that, in our case, SE = SD = dh). Since F? is biproper and its initial state is deterministic, knowledge of uk is equivalent to knowledge of wk. If one now proceeds as in the proof of Corollary 5.1 in [45], it follows that one can recover ukc from wk upon knowledge of . Thus, upon knowledge of , one can recover ukc from uk and the decoder is invertible as required. 
 Remark 5.2: The proof of Theorem 5.1 is constructive. Indeed, it suggests a way to build a source coding scheme that renders the resulting NCS of Fig. 1 SAWSS, and achieves se2 = D while incurring an average data rate that is upper¦ bounded by the right-hand side of (32). 
 Fig. 5. Proposed source coding scheme. If the channel is noiseless and delay free, then sˆ = s and wˆ = w. 
 Theorem 5.1 shows that the lower bound on R(D) derived in Corollary 4.1 is tight up to   nats per sample (i.e., tight up to approximately 1.254 bits per sample). Whilst the lower bound in Corollary 4.1 was derived by using an information-theoretic argument, the upper bound in (32) hinges on a specific source coding scheme that uses suitably chosen LTI filters in conjunction with an ECDQ. It follows from the discussion in Section V-B in [45] that the gap between the derived upper and lower bounds on R(D) arises from two facts: First, ECDQs introduce a coding noise which is uniform and not Gaussian (this amounts to the additional  nats per sample). Second, the proposed coding scheme works on a sample-by-sample basis and practical entropy-coders are not perfectly efficient [54, Chapter 5] (this amounts to an additional log2 nats per sample). We emphasize, however, that the above gap corresponds to a worst case gap and it can be significantly smaller in practice (see Section VII). 
 A key aspect of our results is that they are stated in terms of the solution to the constrained SNR minimization problem in (25). As such, they highlight the role played by SNR constraints in networked control systems, and thus complement, e.g., [63] where the connection between SNR constraints and other communication constraints has been explored. As already mentioned before, a way of obtaining a solution to the problem in (25) will be discussed in Section VI below. 
 Remark 5.3: It is well-known [15] that, when causal source coding schemes of arbitrary complexity are employed, it is possible to mean-square stabilize an LTI plant if and only if the corresponding average data rate R is larger than , where pi denotes the ith unstable plant pole. On the other hand, it is straightforward use Theorem 17 in [50], in conjunction with the proof of Theorem 5.1, to show that any plant satisfying Assumption 2.1 can be stabilized in the sense of Definition 2.1 by incurring an average data rate that satisfies 
 The above observation shows, for plants satisfying Assumption 2.1, that it suffices to use an ECDQ-based linear source coding scheme to achieve stability at rates which are at most   nats per sample away from the absolute minimal average data rate compatible with stability (see also 
 The bounds on R(D) presented in Corollary 4.1 and Theorem 5.1 are functions of the minimal SNR ?'(D) in (25). In this section, we show that the problem of finding ?'(D) is equivalent to an SNR constrained optimal control problem previously addressed in [35], [47], [48]. 
 To proceed, we first note that a straightforward manipulation based on Fig. 2 yields, for any sq2' ? R+ and any proper LTI filters F and L that render the LTI feedback system of Figure 
 Ly are such that L = [Lw Ly ] (see (15)), and we have used that fact that, since F and L are internally stabilizing, Lw is proper and P22 is assumed to be strictly proper, S is stable, 
 We now define, for the feedback system of Figure 2, the auxiliary problem of finding 
 where the minimization is performed with respect to all proper LTI filters F and L, and auxiliary noise variances sq2' ? R+, that render the LTI feedback system of Fig. 2 internally stable and well-posed. Given our assumptions, if the plant P is unstable, then the problem in (39) is feasible if and only if G > Ginf, where Ginf denotes the infimal SNR ?' that is compatible with mean-square stability in the feedback system of Fig. 2 (see [50], [64]). If P is stable, then the problem in (39) is feasible if and only if G = Ginf = 0. 
 Lemma 6.1: Consider the problems of finding both ?'(D) and J'(G) in (25) and (39), respectively. Assume, in addition, that the plant P is such that P21 = 0? and P12 = 0? . 
 , then ?'(D) is a strictly decreasing function of D and the inequality constraint in (25) is active at the optimum. 
 2)	If G > Ginf, then J'(G) is a strictly decreasing function of G and the inequality constraint in (39) can be assumed to be active at the optimum without loss of generality. 
 optimum. (Since D > Dinf, D-? is always non negative for any feasible set of parameters F,L and sq2' > 0.) Indeed, assume on the contrary that D - ? = 0 at the optimum. Given (38), this would imply that sq2' = 0 or P12FS = 0 at the optimum. Given our assumptions and the definition of ?'(D), only F = 0 is possible. However, F = 0 is not compatible with internal stability when the plant is unstable. In the stable plant case, F = 0 implies 
 (note that F = 0 =? K = 0). The latter equality is however unfeasible since, by assumption, 
 Given the above, if D > Dinf, and the plant is unstable or is stable with  , then D - ? > 0 at the optimum. Hence, the performance constraint se2' = D in the definition of ?'(D) is equivalent to 
 at the optimum (see (25) and (38)). Since ?' is a nondecreasing function of sq-'2, it follows that the optimal choice for sq2' is such that the inequality constraint is active at the optimum. 
 We now show that ?'(D) is strictly decreasing in D. Our assumptions guarantee that K = 0? and hence FLy =? 0. By using (40) in (37), and the fact that the optimal choice for sq2' achieves equality in (40), the result follows immediately. 
 2) Consider the definition of J'(G) in (39). By using an argument similar the one used in Part 1 above, it follows that our assumptions imply that one can assume, without loss of generality, that G + 1 - ||S|| > 0 at the optimum (see also [48, pages 103–104]). Our claims now follow by proceeding as in Part 1) above. 
 Lemma 6.1 shows, for almost all cases of interest,7 that the inequality constraints in the optimization problems defining both ?'(D) and J'(G) can be assumed to be active at the optimums, without loss of generality. This fact is exploited below to relate the solutions to these problems. 
 Theorem 6.1: Consider the optimization problems defining both ?'(D) and J'(G) in (25) and (39), respectively. Assume that G > Ginf, D > Dinf, that the plant P is such that P12 = 0? and P21 = 0? , and that, if P is stable, then   holds. 
 Proof: We will only prove that G = ?'(J'(G)). Our remaining claim follows by using a similar argument. Since G > Ginf, the problem of finding J'(G) is feasible. Thus, for any ? > 0, there exist proper LTI filters F? and L?, and s?2' ? R+, that render the system of Figure 2 internally stable and well-posed, and guarantee that 
 where we have used Lemma 6.1 to write an equality in the SNR constraint. Since the inequality in (42) is valid for any ? > 0, it follows that there exist a feasible point for the problem of finding ?'(J'(G)) and, in addition, that ?'(J'(G)) = G. The proof of our second claim would follow if we show that ?'(J'(G)) < G is impossible. Assume that ?'(J'(G)) < G is indeed true. Then, there exist decision variables such that ?' = ? <? G and se2' = J'(G). (Again, we 
 7If G = Ginf, then either the problem of finding J'(G) is unfeasible (unstable plant case) or G = 0 (stable plant case). In the latter case, no information can be conveyed through the channel. On the other hand, if the plant is stable and  and it is optimal to leave the plant in open loop. The above cases are clearly uninteresting and have thus been omitted from the discussion in Lemma 6.1. 
 use Lemma 6.1 to write an equality in the constraint defining ?'(J'(G)).) Thus, we conclude that 
 where the first equality follows from the fact that the constraint is active at the optimum when calculating J'(??). The above inequality contradicts the fact that, given our assumptions, Lemma 6.1 guarantees that J'(G) is a strictly decreasing function of G. The proof is thus completed. 
 Theorem 6.1 shows, for almost all cases of interest, that the problem of finding ?'(D) in (25) is equivalent to that of finding J'(G) in (39). The latter problem was shown to be equivalent to a convex problem in [47]. For doing so, [47] showed that the problem of finding J'(G) is equivalent to the open-loop causal rate-distortion problem which was shown to be convex in [35]. Shortly thereafter, the convexity of the SNR constrained optimal control problem in (39) was re-derived independently in [48], where a formulation more amenable for numerical computations is presented. We will thus not delve into the details on how to numerically find ?'(D) here, and refer the interested reader to Section 3.3 in [48] for details. 
 The previous subsection explained how a numerical characterization of ?'(D) can be obtained. Here, we will briefly comment on the implementation of a source-coding scheme which achieves the desired level of performance D, while incurring an average data rate R satisfying (32). In principle, such coding scheme can be designed as follows (see proof of Theorem 5.1): 
 •	Use the procedure in [48] and Theorem 6.1 to find thefilters L and F, and the auxiliary noise variance sq2', 
 •	Use these filters in the ECDQ-based linear source coding scheme of Fig. 5 and set all initial states to zero. Choose the ECDQ quantization step as ? = (12-sq2')1/2, an i.i.d. dither signal uniformly distributed on ( ?/2,?/2) and independent of (xo,d), and appropriate entropy coder and decoder mappings Hk and Hk-1 (using, for instance, the Huffman algorithm [54]). 
 Implementing an ECDQ requires the availability of the dither at both the encoder and the decoder sides. Additionally, the entropy coder Hk needs to generate a binary word for each input value according to the conditional probability of that input, given the current dither value. The above requirements are impossible to meet exactly in practice. Indeed, the first one is tantamount to requiring an additional perfect channel for being able to communicate the dither from the encoder to the decoder. The second one would require an uncountable number of dictionaries [54], one for each dither value. 
 Leaving finite range and precision issues aside, the behavior of an ECDQ can be approximated in practice by using synchronized uniformly distributed pseudo-random dither sequences, generated at both the encoder and the decoder from the same seed, and using entropy-coders and decoders which work conditioned upon a uniformly quantized version of the dither. By using such an approach, all signals in the NCS of Figure 1, except for the channel input and output, will have the same statistics as if an ideal ECDQ was employed. For each possible quantized dither value, one can build the corresponding conditional dictionary in Hk by using, for example, the Huffman coding algorithm [54]. The conditional statistics of the quantizer outputs needed for this purpose, can be approximated by the corresponding stationary statistics which can be estimated empirically by simulation. 
 that using more than 11 quantized dither values brings only negligible benefits in terms of rate reduction.) The curve “Measured entropy of quantizer output (11 dither values)” corresponds to an empirical estimate of the conditional entropy of the quantizer output s, given the quantized dither values. 
 Our results show that our upper bound is loose. This is consistent with the fact that our upper bound was derived by using worst case considerations. The gap between the measured rate (with conditioning) and our lower bound is about 0.45 bits per sample, which is smaller than the worst case gap   nats per sample (about 1.254 bits per sample). On the other hand the estimated conditional entropy of the quantizer output, given the dither values, is about 0.25 bits per sample above the lower bound. This implies that, in our simulations, the 0.46 bit per sample gap is composed by about 0.21 bits per sample due to the inefficiency of the considered entropy coders, and by about 0.25 bits per sample due to the fact that the ECDQ generates uniform and not Gaussian noise. 
 1.4588rates, converge rapidly as D ? 8. Thus, whilst achieving an average data-rate arbitrarily close to the minimal rate 
 Our results show that, as expected, achieving a closed loop performance arbitrarily close to the best non networked performance Dinf requires arbitrarily high data rates. Interestingly, however, for this example, it suffices to use less than 3 bits per sample to achieve a performance that is essentially identical to the best non networked performance. It is also interesting to observe that our bounds, and the measured average data- 
 and that (xo,d) is Gaussian with d being unit-variance white noise. By using the results of Sections III and V we computed upper and lower bounds on R(D) for several values of D > Dinf = 0.2091. We also simulated an actual ECDQbased linear source coding scheme for each considered value for D. To that end, we followed the suggestions at the end of Section VI-B and simulated ECDQs where the dither is uniform and perfectly known at both ends of the channels, and where the entropy-coders work conditioned upon a quantized version of the dither. The results are presented in Figure 6. In that figure we plot our upper and lower bounds, and several other curves which report simulation results. All simulation results (referred to as “Measured” in Fig. 6) are averages over twenty 104-samples-long realizations. In particular, “Measured rate (no conditioning)” corresponds to the average data rate in a case where an empirically-tuned entropy-coder is employed which does not make use of the knowledge of the dither values. Even in this case our upper bound proves to be rather loose. The curve “Measured rate (cond. using 11 dither values)” corresponds to the rate achieved when using and entropy coder that works conditioned upon 11 uniformly-quantized dither values. As expected our results show that conditioning reduces the incurred average data-rate. (Simulations suggested results suggest that the performance loss incurred when forcing the average data rate to be low might be modest in some cases. 
 This paper has studied networked control systems subject to average data rate constraints. In particular, we have obtained a characterization of the minimal feedback directed information rate I8(y ? u) average data rate (to within 1.25 bits/sample) that guarantees a prescribed level of quadratic performance, over all encoder-controller-decoder combinations. Our results have been derived for LTI plants that have one scalar control input, one scalar sensor output, and that are subject to Gaussian stationary disturbances and Gaussian initial states. The digital feedback channel considered was assumed to be free of errors and delays. Where no constraints besides causality are imposed on the considered coding schemes, the directed information rate between the encoder input y and decoder output u (a known lower bound to the mean operational data rate) was shown to be minimized only when y and u are jointly 
 Gaussian. This, together with the introduction of the notion of strong asymptotic wide-sense stationarity (SAWSS), allowed us to define a wide class of coding schemes (namely, SAWSS schemes) for which a data-rate lower bound in terms of the asymptotic power spectral densities of y and u was derived. We then showed that, for a given stationary performance, the infimum of this lower bound coincides with that of the directed information rate obtained when encoder, channel and decoder are replaced by an AWGN channel surrounded by LTI filters, 
 taking the infimum over all causal filters which satisfy the same performance constraint. Such insight was then used as motivation for building a source coding scheme capable of achieving rates which are less than   nats per sample away from our derived lower bound, while satisfying the desired performance level constraint. Such coding schemes are based upon entropy dithered quantizers and constitute conceptually simple coding schemes. A numerical example has been include to illustrate our proposal. 
 Future work should focus on multiple input and multiple output plant models, multichannel architectures, and on ways of reducing the gap between the derived upper and lower bounds on the minimal average data-rate that guarantees a given performance level. 
 We would like to thank the AE and the anonymous referees for their careful reviews as well as for their helpful comments, which contributed to improve the technical presentation of this paper. 
 In this appendix we show that D > Dinf is sufficient for the optimization problems in (7), (16) and (25) to be feasible. We will make extensive use of the definition of ?'(D) in (25), the related equations in (37) and (38), the conventions regarding the feedback scheme of Fig. 2 made on the paragraph preceding Theorem 4.1, and the definitions of ECDQs and ECDQ-based linear source coding schemes made in Section V. 
 Consider the feedback system of Fig. 7, where P, d and xo satisfy Assumption 2.1, and the controller K is such that u(k) = Kk(yk) for some arbitrary mappings Kk. Since P is LTI and all the involved random variables are Gaussian, it follows from well-known results [65] that 
 where S is the set of all proper LTI filters which render the the feedback system of Figure 7 internally stable and well-posed. Our assumptions on P guarantee that the above problem is feasible. 
 The fact that D > Dinf and that the problem of finding Dinf is feasible, implies that for every ? ? (0,D - Dinf), there exists K0 ? S such that, in Figure 7,  = 
 F = 1 and L = L0, where L0 is such that v = K0 y. Since K0 ? S, the above choice renders the feedback system of 
 Fig. 2 internally stable and well-posed for any additive noise variance sq2' ? R+. This means that the resulting variance of v', say sv20, will be finite. It also means that if q' in Fig. 2 is zero-mean AWGN with variance sq', then the variances of e' and of v' will increase to  , and sv20 + 
 , respectively, for some finite factors  which depend only upon K0. As a consequence, for every D > Dinf, there exists K0 ? S such that in Fig. 2 and for the above choice of filters,, q' 
 by picking ? = (D-Dinf)/3 and q' as zero mean AWGN with variance  . It immediately follows that the above choice of parameters is also such that, in Figure 2, 
 The latter inequality shows that the problem of finding ?'(D) in (25) is feasible for any D > Dinf (indeed, feasible while yielding all signals in the system jointly Gaussian). Now, from Jensen’s inequality and the concavity of log, it also follows from (47) that the problem of finding  in (16) is feasible for any D > Dinf. 
 We end this section by showing that the problem of finding R(D) in (7) is also feasible when D > Dinf. To that end, it suffices to consider an ECDQ-based linear source coding scheme to link y and u in Fig. 1, with parameters 
 the properties of the latter choice of parameters (see preceding paragraph), our claim follows by proceeding as in the proof of Theorem 5.1 to show that the above defined ECDQ-based linear source coding scheme satisfies Assumption 2.2, renders the NCS of Figure 1 SAWSS, and achieves se2 < D at a finite average data rate R. 
 The following definitions and facts are standard and, unless otherwise stated, can be found in [54]. We assume all random variables to have well defined (joint) probability density functions (PDFs). The PDF of x (x,y) is denoted f(x) (f(x,y)). f(x|y) refers to the conditional PDF of x, given y. Ex {·} denotes mean with respect to the distribution of x. 
 The differential entropy of x is defined via h(x) ? -Ex {logf(x)}. The conditional differential entropy of x, given y, is defined via h(x|y) ? -Ex,y {logf(x|y)}. The mutual information between two random variables x and y is defined via I(x;y) ? -Ex,y {log(f(x)f(y)/f(x,y))}. The conditional mutual information between x and y, given z, is defined via I(x;y|z) ? I(x,z;y)-I(z;y). The following are properties of the above quantities: 
 (Property 4)  , where x-1 can be taken to be a deterministic constant, in which case h(x0|x-1) = h(x0). 
 Lemma B.1: Assume that (x,z) are jointly second-order random variables, x is Gaussian, and z is arbitrarily distributed. 
 Then, I(x;z) = (x;zG), where zG is such that (x,zG) are jointly Gaussian and have the same first- and second-order (cross-) moments as (x,z). 
 Lemma B.2: Let x,y,z be arbitrarily-distributed random vectors satisfying the Markov chain 
 Let xG,yG,zG be jointly Gaussian random variables with the same first- and second-order statistics as (x,y,z). In addition, suppose that E[x|y] = Ly, for some matrix L of appropriate dimensions. Then the Markov chain 
 Proof: The shared statistics between (xG,yG,zG) and (x,y,z) imply that E[yGzG] = E[yz]. The conidtion E[x|y] = Ly implies that E[x|y = y] = E[xG|yG = y] = Ly. With this we obtain that 
 where the last equality is due to (48). But for jointly Gaussian variables (xG,yG,zG), E[(xG - E[x|yG])zG] = 0 if and only if (49) holds, completing the proof. 
 Lemma B.3: Let the random variables (uk,yk,xo,dk) be such that, for all k ? N0 and i = k, 
 Proof: The sequences uk, yk are recursively generated as y(0) = L0(xo,d(0)) u(0) = g0(s(0),y(0)) = g0(s(0),L0(xo,d(0))) y(1) = L1(xo,d1) + M1u(0) 
 This reveals that (ui,yi) = g˜i(si,Li(xo,di)) for some deterministic functions {g˜i}, from where (51) follows (since sk ? (xo,dk)). 9 
 8Lemma B.2 is a stronger version of Lemma 3.2 in [67], since the statement of the latter requires the existence of a linear operator L such that the estimation error x - Ly is independent of both z and y, while Lemma B.2 only requires E[x for all y. 
 L0(xo,d(0)),L1(xo,d1),...,Li-1(xo,d -1) are (nested) sub sequences of the longest sequence Li(xo,di). 
 Theorem B.3: Let the random variables (uk,yk,xo,dk) correspond to those in the feedback system shown in Fig. 1. 
 Let   be jointly Gaussian with the same firstand second-order moments as (uk,yk,xo,dk). Then, for evey k ? N0, 
 uG(i) ?? uiG-1,yGi ?? xo,dk,	?i = k uG(i) ?? uiG-1,xo,di-1 ?? dki ,	?i = k	(52a) 
 Proof: In the scheme of Fig. 1, the LTI plant causally maps its inputs (ui-1,xo,di) to its output yi. This means that that yi = Li(xo,di) + Miui-1 for some linear mappings 
 Li(xo,di) and (xo,dk) are jointly Gaussian, and thus E[xo,dk|Li(xo,di)] = FiLi(xo,di) for some sequence of linear mappings  . Hence, we can directly apply Lemma B.2 and obtain from (53) that 
 , which is equivalent to (52a). The inequality (a) is due to the chain rule and the nonnegativity of mutual information (which also implies (c)), and (b) holds since  . Starting 
 inequality follows from the chain rule and the non-negativity of mutual information. This completes the proof. 
 Lemma B.4: Consider the generic feedback system of Figure 8, where S1 is an arbitrary (hence possibly non-linear and time-varying) causal dynamic system with initial state x1,o and disturbance d1, such that  for some (possibly non-linear and time-varying) deterministic mapping S1,k, and S2 is an arbitrary causal dynamic system with disturbance d2 and initial state x2,o, such that   for some (possibly non-linear and 
 time-varying) deterministic mapping S2,k. If (x2,o,d2) are jointly independent of (x1,o,d1), then 
 Lemma B.4 corresponds to a stronger version of Theorem 5.1 in [44]. Indeed, the latter result makes use of additional assumptions on system S2, does not take side information into account, and only shows that the left hand side of (55) is lower 
 Proof: The definition of an SAWSS process implies that   and   are sequences of asymptotically equivalent Hermitian matrices. This, together with the fact that d > 0, and that log(x) 
 is continuous for all x = d > 0, allows one to apply [53, Corollary 2.4] to obtain 
 Since the matrices  are positive-definite Hermitian Toeplitz with each row being absolutely summable for all n, it follows from the fundamental eigenvalue distribution theorem of Szego (see, e.g., [53, Corollary 2.4 or Theorem 5.4]) that¨ 
 ﻿This paper presents an empirical study of the achievable data rates of network multiple-input multiple-output (MIMO) techniques including zero-forcing (ZF), zero-forcing dirty paper coding (ZF-DPC) and dirty paper coding (DPC) using actual 4-by-4 indoor wireless channel measurements at 3.5 GHz. Their performances are contrasted with those of conventional techniques, in which either the base stations are not coordinated (NC), or their interference is avoided using frequency division (FD) multiplexing. The measurements were taken in aisle-to-office and large unobstructed hall scenarios. The study of these results reveals that, at high signal-to-noise ratios (SNRs), DPC and ZF-DPC can yield more than a three-fold increase in attainable data rates when compared to NC and FD. The gains obtained using ZF are smaller, but still significant. At low SNRs the system is noise-(rather than interference-) limited, and only DPC exhibits gains. The evaluations in this paper also show that collaborative systems such as DPC can benefit from interference-prone environments to yield increased transmission capacity. With regard to the propagation channel, the classical log-normal plus Rayleigh/Ricean fading model, with parameters fitted to the scenario type, was found to be good at predicting the statistics of the achievable data rates of all the strategies considered. 
 Index Terms—Wireless communication, MIMO, channel characterization and modeling, performance analysis. 
 HERE is an ever increasing demand for high data rates in homes and offices with all the mobility provided by wireless technology. Since growing numbers of users are to be served within confined spaces, the performance of practical indoor wireless communications systems is expected to be limited, increasingly often, by interference [1]–[3]. Therefore, in order to improve (or even maintain) high data rates, it will become necessary to make use of communication techniques capable of exploiting the coupling between the various propagation links within a given service scenario so as to reduce interference and increase received signal power. 
 This management of the interference produced by access points (APs) and users can be achieved by means of coordination. Having all APs operating in a coordinated fashion turns the set of users and APs into a network multiple-input multiple-output (MIMO) system, and the downlink communication medium into a MIMO broadcast channel (MIMO-BC). Network-MIMO systems hold the potential of eliminating interference in a MIMO-BC, greatly increasing bandwidth efficiency in multi-AP wireless systems [3], [4]. 
 Several strategies have been proposed in the literature to take advantage of the high interconnectivity between several APs and a group of users that is often encountered in MIMOBC scenarios. The technique called dirty paper coding (DPC) [5], [6] has been shown to be capacity-achieving in this setting, and thus it is optimal [7], [8]. DPC is a nonlinear precoding technique that requires full knowledge of channel state information (CSI) at the transmitter. 
 The high implementation complexity of DPC suggests the need to consider other simpler, near-optimal MIMO-BC transmission schemes. One such technique, commonly referred to as zero-forcing-DPC (ZF-DPC) [9], [10], achieves sum rates close to those of DPC with smaller computational complexity. In ZF-DPC, part of the interference is removed by linearly combining the signals intended for all users so as to effectively obtain, from end to end, a lower triangular channel matrix. The triangularization of the channel matrix, which requires perfect CSI at the transmitter side, makes it possible to avoid part of the interference. The effect of all remaining interference is eliminated by applying DPC in a sequential fashion. 
 Another suboptimal technique, simpler than DPC and ZFDPC, is zero-forcing (ZF) beamforming as described in [11]– [13]. Zero-forcing is an entirely linear preprocessing strategy. In this case, APs have perfect CSI and cooperate to eliminate interference for all users, yielding an effectively diagonal channel matrix between transmitted data flows and users. 
 Simpler strategies to attain downlink communication in a MIMO-BC are obtained if each AP serves a single user and its CSI is limited to the radio link to that user only. In our work, we consider two possible approaches. The first one is frequency division (FD), that is, splitting the available spectrum into disjoint frequency bands, each one allocated to a single AP-user pair, with each AP transmitting at full power. The second approach, which will be referred to as the noncoordinated (NC) strategy, is to let each AP transmit at full power using all the spectrum available, thus accepting that interference from other users will limit data rates as transmission power increases. The simpler FD and NC schemes in general have inferior performances than those of DPC and ZF-DPC. On the other hand, the former strategies impose the minimum possible backhaul load, since each AP needs to know only the data intended for (and the CSI related to) the user it is serving. There exist other strategies for downlink communication in a MIMO-BC that do not require coordination among APs and whose performance is under certain conditions better than that of FD and NC, such as fractional frequency reuse (FFR) [14]. Although it is known that in some specific cases FFR outperforms FD (see, e.g., [14], [15]) our objective here is only to provide a few simple baseline systems as a reference for comparison. It is understood that whichever is the choice, different propagation conditions may favor different baseline strategies. 
 The performance evaluation of diverse network-MIMO techniques for simulated channels has been treated in various works [11], [16]–[19]. This has included the assumption of log-normal plus Rayleigh fading, and the use of the Wyner model [20]–[25]. The effect of limited backhaul capacity between APs in overall performance is assessed in [26]–[28]. 
 Regarding the channel model, it may be reasonable to assume that small-scale fades are independent for each channel when antennas are several wavelengths apart, but this may not hold for shadow fades. Several articles report on the correlation of fades in different locations of transmit and receive antennas [29]–[34]. To what extent such correlation may affect the network-MIMO channel, based on widely separated base station antennas, is not self-evident. To the best of our knowledge, no empirically based results in support of the assumption that a network-MIMO channel is equivalent to a collection of independent SISO channels have been published. 
 The fact that in network MIMO all APs can act in a coordinated fashion taking advantage of CSI, generates two related sources of capacity improvement. Interference between AP-user pairs can be reduced and, at the same time, the total received signal power at each user terminal can be increased. Since these effects depend on the degree of connectivity (or, conversely, isolation), this raises the question of how much performance improvement can be expected for a given type of deployment scenario. 
 In this paper, we evaluate and compare the maximum data rates achievable by the DPC, ZF-DPC, ZF, FD and NC strategies, for measured as well as simulated 4-by-4 MIMO indoor channels, under a per antenna power constraint (PAPC). These rates were all calculated under the assumption of perfect CSI at the transmitter. Although this would be hard to attain with currently available technology, the results serve as a comparison basis, being the best performance achievable under equally idealized conditions for all schemes. Channel measurements were obtained for two representative scenarios: aisles with offices alongside them, and large halls. A single slope path-loss model with log-normal shadow fading and Rice/Rayleigh small scale fades [35], was found to be adequate in all the scenarios for the distances at which measurements were taken. With regard to achievable data rates, we found that at high SNRs (in the range of 30 dB), DPC and ZFDPC achieve about a three-fold gain when compared to NC or FD. The gains of ZF are somewhat smaller, depending on the scenario, but still quite significant. A particularly interesting finding is that both DPC schemes not only are able to effectively mitigate the effect of interference but also in some cases can exploit low isolation between AP-user pairs, to yield higher data rates than in scenarios with greater natural isolation. As would be expected, at low SNR, where interference is not the dominant limitation on capacity, the gain over the non-coordinated systems decreases. Our study also reveals that the log-normal plus Rice/Rayleigh fading model is accurate at predicting the statistics of the maximum rates achievable by each of the network-MIMO strategies considered, when the model parameters are selected according to the type of scenario. Thus, an indoor MIMO channel described as a collection of independent SISO channels, would yield similar conclusions to those of our empirically based study, provided that the proper model parameters are used. However, rather than justifying what could have been deemed as somewhat arbitrary choices of parameters to illustrate the achievable benefits of a coordinated system, we decided to use actual measured channel data from typical indoor scenarios to validate our comparison. In the following sections we describe our measurement procedures and the empirical results obtained from them. 
 In all measurements, the custom-built channel sounder used consisted of a single channel transmitter and a 4-channel receiver operating at 3.5 GHz for narrowband measurements. The single transmit antenna was placed at locations that would in practice correspond to possible positions of a user. Four receive antennas, one per AP, were placed at the positions typical of AP locations. The antennas used for transmission and reception were omnidirectional coaxial dipoles, vertically polarized. The transmit antenna was mounted on a 50-cm long rotating arm and moved stepwise under computer control in 6-degree increments. 
 In all scenarios, measurements were taken at night in the absence of pedestrian movement. For each of the 5 scenarios (3 aisle-to-office and 2 halls), more than a 1000 complex 1by-4 channel vectors were collected, each vector containing the complex gains from a user location to the four APs. 
 The single conversion receiver generates 4 channel outputs at 10 kHz. The sampled receiver output sequences were converted into a vector of 4 complex channel gains using the Fast Fourier Transform (FFT). Phase coherence was achieved by locking the transmitter and receiver oscillators to GPS disciplined sources. Exact synchronization of sampling intervals produces data sequences corresponding to an integer number of periods of the 4 sinewave outputs, which allowed FFT processing without the need for windowing. For each position of the transmit antenna, the 4 channel outputs were simultaneously sampled for 1 second, and the total interval was partitioned into 100 non-overlapping subintervals of 10 ms each. This allowed us to verify the consistency of our measurements which should only differ as a consequence of receiver noise. Since the MIMO capacity depends on the 
 relative rather than the absolute phases of the channels, we calculated phase differences with respect to one of the channels, arbitrarily chosen as a reference. In all our measurements, the ratio between the average gain (magnitude) and its rms fluctuation, calculated over the sequence of 100 measurements, exceeded 20 dB. Thus by averaging our measurements, we further reduced the effect of the measurement noise. 
 We performed measurements in two types of scenarios which we considered relevant and where, as our data subsequently confirmed, different propagation behaviours are to be expected. These scenario types are as follows: 
 1)	Aisle-to-office Scenarios: Channel measurements were carried out in three different aisle-to-office scenarios. In all cases the building was a steel reinforced concrete structure with interior divisions made of wood and particleboard. For these scenarios, the four receive antennas were wall-mounted at a 2.5 m-height along straight aisles, with a 10-cm separation from the particleboard interior walls. Adjacent AP antennas were separated by approximately 8 m. The transmit antenna was placed at desktop height inside various offices and laboratories adjacent to the aisle. These are non-line-of-sight (NLOS) scenarios. The distances between the transmit antenna and the 4 receive locations ranged from 3 to 25 m. The locations of transmit and receive antennas for scenario aisle-to-office 1 is indicated schematically in Fig. 1. In this figure, the rectangles and circles represent AP and user locations, respectively. The other two scenarios of this type had similar topologies. In each office, measurements were obtained placing the rotating arm on three or four different user locations, more than 1-m apart from one another, as space would allow (for simplicity, only one of these user locations per office is shown in Fig. 1). 
 2)	Large Hall Scenarios: We obtained measurements from two large halls. The first one, hall-1, was a 53-m × 14-m, glass-roofed central hall, about 7-m high, with concrete floor and several lateral aisles leading to offices and classrooms on two floors. This hall is built with steel beams with concrete walls separating the hall from the adjacent spaces. The four receive antennas were located at the corners of an imaginary rectangle, with each antenna at approximately 10 cm from a wall on the hall, at heights of about 2.5 m. The transmitter rotating arm was placed in 38 different positions over a grid of locations within the rectangle formed by the receive antennas. This is a line-of-sight (LOS) scenario, in which link lengths ranged from 4 to 26 m. 
 The second hall scenario, hall-2, was a 17-m × 23-m gymnasium with wooden floor and concrete walls. The four receive antennas were placed as in hall-1, while the rotating arm was placed in 43 different positions over a grid inside the gymnasium. Transmitter-receiver distances varied from 5 m to nearly 24 m. 
 Before using the obtained channel measurements to calculate the data rates achievable with network-MIMO techniques, we describe the observed statistical properties of the fades in each scenario. 
 The measured channel gains proved to be consistent with what has been reported in the literature for similar scenarios [35]–[42]. We found that log-normal shadow fading combined with Rayleigh or Ricean small-scale fades resulted in an excellent model fit to our narrowband measurements, provided the model parameters are adjusted to the measured data. The statistics of each type of fading are described below. 
 For each wireless link, small-scale fading statistics were analyzed by studying the 60 channel gain measurements corresponding to a single turn of the rotating arm. As expected, in all aisle-to-office scenarios, these channel gains showed fading statistics that were well described by a Rayleigh distribution. This is consistent with a NLOS situation [35]. 
 In the scenarios hall-1 and hall-2, these gains fit a Rice distribution with K-factor between 0 and 3. It is worth noting that despite the fact that these scenarios are LOS, they are, as the previous ones, characterized by strong multipath propagation, although to a lesser extent. In all scenarios, the channel phases associated with each angular position of the rotating arm were uniformly distributed and uncorrelated. 
 The large-scale fading statistics were obtained by calculating, at different locations, the averages of the channel gains over an arm rotation. The statistics of these average gains were well described by a log-normal distribution. More precisely, for each link distance ??, the path-loss, ??(??) in dB, corresponding to each rotation-averaged channel gain followed the behaviour of a random variable of the form 
 ??(??) = ???????? + 10??log10(??/????????) + ????, (1) where ?? ? R+ is a path-loss exponent, ???? is a zero-mean Gaussian random variable with variance ??2 and ???????? is the path loss in dB at ?? = ????????, which we chose as 1 m. For each measured scenario, the model described by (1) was fitted to 80 measured large-scale path losses, finding ???????? and ?? by linear regression, and then setting ??2 as the empirical variance of ??(??) - ???????? - 10??log10(??/????????). The values of ?? and ?? so obtained are listed in Table I. For each scenario type, a 
 row in boldface contains the model parameters resulting from combining all data of its corresponding scenarios. 
 It is worth mentioning that no evidence of a “break point” in the path-loss exponent was found from the measurements. More precisely, no multi-slope models [43] were found to provide a better match (in a least-squares sense) to our empirical data. This is consistent with what has been reported before for short-range indoor scenarios [44], [45]. 
 It was also found that the correlation between the shadow fades in the links between a transmitter antenna and any two receiving antennas was, in all cases, below 0.26 (in modulus). This was true even when the two receiving antennas (in all cases several meters apart) were seen by the transmitter at an angle narrower than 10 degrees. 
 In this section we present a brief review of the transmission techniques to be evaluated in Section V and their relation to the channel model. 
 A general MIMO wireless downlink narrowband channel between ?? single-antenna APs and ?? single-antenna users can be represented by a ?? × ?? complex valued matrix H. Denoting the vector of signals transmitted by the ?? access points by x ? C??, the vector of received signals y ? C?? can be written as 
 where the noise vector n ? C has i.i.d. circularly symmetric complex Gaussian elements with variance ??. Notice that h??,??, the element in the ??-th row and ??-th column of H, is the narrow-band channel gain between the ??-th AP and the ??-th user. Notice also that no joint processing of the output of the MIMO channel is allowed, since it is assumed that cooperation between users is not practical. 
 For the three network-MIMO strategies considered in this work, x can be constructed as 
 where the ??-th element in the vector u ? C?? is the information-bearing signal intended for the ??-th user, W ? C??×?? is a linear preprocessing matrix, and T : C?? ? C?? is either a non-linear transformation, in the case of DPC schemes, or the identity matrix, in the case of ZF. We note that an element of x can be a function of two or more user signals only if the APs are able to operate cooperatively. Following standard practice, we will assume that the elements of u are independent zero-mean complex Gaussian random variables with variances 
 where ??x2?? is the variance of the ??-th element of x, for some maximum power ?? = 0. All channel matrices considered in the sequel are 4-by-4, that is, ?? = ?? = 4, restricting our analysis to four users served simultaneously by four APs. 
 We include NC and FD as baseline strategies, against which to compare the other three (network-MIMO) schemes. In the case of NC, each AP serves a single user, and all APs operate without coordination, transmitting at maximum power over the same frequency band. In terms of the model (4), this amounts to restricting W to be a diagonal matrix, or any row or column permutation of it. Since the user signal variances {????} can be chosen freely, there is no loss in generality in assuming for this case that W = I, where I is the identity matrix. With this, the sum rate achievable with an NC system is readily found to be 
 (6) In this expression, ??|h??,??|2 is the power of the signal ???? as received by its intended user. On the other hand, 
 ??user, produced by the APs serving all the other users.???/=?? |h??,??|2 represents the interference affecting the ??-th 
 In this case, each AP transmits at full power using a fraction of the available spectrum. We will assume that this spectrum is partitioned into ?? adjacent non-overlapping equal-width bands. Therefore, assuming the same total bandwidth as for NC, the maximum achievable rate of FD is trivially given by 
 The idea in ZF is to eliminate, in the downlink, all interference produced by the APs. To do this, the matrix W in (4) is chosen as  W = H-1. This requires all APs to have CSI and each of them to process the signals intended to all users. 
 In this case, the highest sum rate (bits per second per Hz) achievable under a PAPC ?? is readily found to be [46] 
 and where   denotes the entry on the ??-th row and ??th column of W. The optimization problem defined by (8) and (9) has been shown to be convex in [11]. Therefore, a global maximum for the right-hand side of (8) can be obtained numerically using standard convex optimization methods [47]. Notice that the solution to (8) subject to (9) will, in general, yield antenna transmit powers  for one or more ?? ? {1,2,...,??}. ZF is known to perform poorly at low SNR but it is easy to improve its achievable rate under such conditions by choosing W as a regularized inverse of H instead of its true inverse [48]. We denote this variant of ZF as regularized zero-forcing (RZF). 
 This MIMO-BC technique was first proposed in [9] and then further studied in [10]. The idea behind ZF-DPC is to utilize the linear preprocessing matrix W to assure that the ??-th user receives no interference from the signals ????, for all ?? > ??, and then use DPC to avoid the effects of the remaining interference. More precisely, W is chosen so as to obtain 
 where ????,?? are the elements of some lower-triangular matrix G and ???? is the ??-th element of the vector 
 where Q is the unitary matrix in a QR decomposition of H??, i.e., H = (QR)??, with R being an upper triangular matrix and (·)?? denoting the conjugate transpose operator. Choosing 
 which attains the interference-avoidance outcome described by (10) with G = R??. If H is invertible (as is the case in all the channel matrices considered in this work), then all matrices R satisfying this factorization differ only by a sign inversion in any subset of their rows.2 Therefore, in our case, the absolute value of each element of R (and thus of G as well) is fixed once H is known. 
 In ZF-DPC, the effect of the interference represented by the sum at the right end of (10) is avoided by employing dirty paper coding (DPC) [5]. The latter requires having, at the transmitting end, perfect CSI as well as full knowledge of all user signals. After applying DPC and transmitting the result through the channel (10), each ???? is decoded as if there had been no interference at all [5]. Therefore, the achievable sum-rate of ZF-DPC is given by 
 where ????,?? are the elements in the diagonal of R and where the maximization is over all row permutations of H and power allocations {????} that satisfy the PAPC (9) for each permutation. Notice that in this case the scalars ????,?? in (9) are the elements of preprocessing matrix W defined in (11) for the row-permuted matrix H. 
 It is easy to show that the optimization problem defined by (13) and (9) is also convex for each permutation. To see this, it suffices to notice that the change of variables ??¯?? ? ????/|????,??|2 and ??¯??,?? ? ????,??|????,??|2 yields the optimization problem defined by (13), (9) equivalent to (8), (9). 
 The performance of ZF-DPC has been evaluated using simulated channel data in [10], under a sum power constraint, and in [49], under a PAPC. In both cases, ZF-DPC was shown to be superior to ZF and various non-cooperative MIMO strategies. 
 Although the practical implementation of DPC still awaits specific code designs, we use this technique as an ultimate upper bound for the achievable data rates in each scenario. 
 In DPC, no a-priori restriction is imposed on the preprocessing matrix W. Here, the signal intended for user 1, ??1, is transmitted by (possibly) all APs as if there were no other signals being transmitted, i.e., ??1 = ??1. In contrast, the data aimed at user 2 is encoded using DPC into ??2, exploiting the fact that all APs have full CSI, which together with having knowledge of ??1, allows avoiding its interfering effect. The resulting signal is then sent using (possibly) all APs as well. Notice that, by doing this, ??2 is added to the signal received by user 1 as interference. A similar process is employed to successively encode the data for the remaining users, utilizing previously encoded signals as known interference. In this setting, the signal ???? with power ???? reaches all APs through the ??-th column of  , thus arriving at the corresponding user with power(as before, ??l,?? denotes the entry on the l  -th row, ??-th column of W). Therefore, the maximum rate achievable with this technique is [51] 
 Fig. 2. CDF plots of rate per user, obtained when NC, FD, ZF, ZF-DPC and DPC achieve their maximum sum-rates, under the power constraint ?? = 20 dBm, from empirical channel measurements taken in scenario aisle-tooffice 1. 
 where the maximum is taken over all permutations of the rows of H and over all matrices W ? C??×?? and powers 
 ???? satisfying (9) for each permutation. An efficient method to numerically solve the optimization problem posed by (14) has been proposed in [52]. This is the method we used to evaluate ??DPC(??) for the measured and simulated channel data. 
 In the next section we use the above expressions to compare the performance of the various schemes in realistic indoor environments. We use both actual measured channel matrices as well as channel matrices generated by the model obtained from our measurements. 
 In this section we evaluate the maximum sum rates of NC, FD, ZF, ZF-DPC and DPC as described respectively by (6), (7), (8), (13) and (14), subject to the PAPC (9), for the measured channel matrices obtained in all scenarios. We then repeat the sum-rate evaluation using channel matrices generated by the model (1), using the appropriate parameters as derived from our measurements. For the aforementioned transmission schemes, we calculate the per-user rates at specific availabilities, i.e., the maximum per-user rate that is guaranteed to be met or exceeded with some given probability. 
 We further assume that the scheduler in this system is perfectly unbiased in that, on average, all users are served an equal amount of time. At the same time, our procedure of AP-to-user associations is designed to avoid combinations that will yield high interference when better choices exist. To achieve both goals we proceed, on each time slot, as follows: 
 1)	Assign each user the access point providing the strongest signal (strongest AP) and group the terminals with the same strongest AP. 
 It is assumed that the number of users in each list is comparable. Over a long period of time this assures fairness as all users have an equal chance of being served, while precluding the possibility of choosing in the same time slot more than one user having the same “preferred” AP. More precisely, the channel matrix H = [h??,??] obtained in every time slot is such that 
 This assures that interference-prone schemes such as NC are not unfairly penalized in our comparison by particularly poor associations. In addition, users and APs are ordered so that the ??-th AP is the smallest path-loss AP for the ??-th user. This yields a channel matrix in which the largest magnitude element in each row lies on the main diagonal of the matrix. 
 The fact that the above described procedure resulted in equal likelihood of service for all users was confirmed in our simulations by verifying that when the scheduling algorithm was repeated many times, any specific choice of position received service the same number of times. While it may be possible to find scheduling algorithms that further benefit the performance of the NC scheme, it should first be verified that these gains are not achieved at the expense of fairness. 
 As is to be expected, unobstructed hall scenarios provide less natural isolation between AP-user pairs than aisle-to-office scenarios. This results in the fact that the hall scenarios were characterized by a significantly smaller path-loss exponent, see Table I. As a consequence, for any given distribution of distances between APs and users, the channel matrices associated with hall scenarios will tend to be less diagonally dominant, i.e., the off-diagonal terms of H will be larger on average. The effect of this on the rate achievable by each transmission technique will be discussed later in this section. 
 In all the evaluations, the noise variance in the receiver, for all full-band schemes (i.e., excluding FD), was chosen to be -90 dBm, which roughly corresponds to the thermal noise in a receiver with a noise figure of 10 dB, operating at room temperature, over a bandwidth of 20 MHz. 
 For each aisle-to-office scenario, 500 4-by-4 channel matrices satisfying (15) were selected by randomly choosing from the set of measured channel data for the corresponding site. 
 Fig. 3. User rates at different availabilities for scenarios aisle-to-office 1 (first column) and hall 2 (second column).The CDF of the per-user rates, attained when W and {????} are chosen so as to maximize the sum rate in each scheme, is shown in Fig. 2 for scenario aisle-to-office 1, under the PAPC (9b) with ?? = 20 dBm. The results obtained in the other aisle-to-office scenarios were quite similar. It can be seen directly from the graph that at this transmit power limit, the three network-MIMO techniques outperform NC and FD for all availabilities. We also observe that the performance of DPC and ZF-DPC are very close, which is consistent with results obtained before for Rayleigh i.i.d. channels [10]. It should also be noted that the optimization methods used only assure that the sum capacity of DPC is the best of all coordinated systems. While this was observed in all our calculations, it does not necessarily imply that at all availability levels, the per-user rate of DPC will exceed that of the other options. In fact we found that for some users, the ZF-DPC rates would exceed those of DPC, as can be observed from the CDFs at low availabilities. At high availabilities however, DPC invariably proved to be best. As the transmit power limit is reduced, e.g. to values below -20 dBm, we found that an increasing number of users have a non-zero probability of being assigned zero rate when using DPC, ZF-DPC or ZF. This is a consequence of the fact that we are considering the per-user rates when each of the network-MIMO systems is configured for maximum sum rate, not fairness. As a consequence, the best strategy may include not serving some users at all in some channel realizations. This behavior will be discussed in more detail below. 
 The corresponding per-user rates of each scheme in aisleto-office scenario 1, as a function of ??, are shown in the first column of Fig. 3, for availabilities 50% and 90%. On the upper edge of each of these graphs we have included an additional horizontal axis, labeled SNR, to provide an algorithm-independent measure of the received signal-to-noise ratio for each site. This SNR corresponds to the average received SNR in the NC setting excluding interference (i.e., supposing interfering APs are turned off), that is 
 where h(??,????) is the ??-th diagonal entry of the  -th channelmatrix instantiation. This would also be the true SNR for all the full-band schemes considered here if the off-diagonal terms of the channel matrix were zero. Since the latter condition never holds, this notion of SNR is not the actual per-user signal-to-noise ratio, which is algorithm dependent. Instead, it is the average ratio of the power received by a user from its “strongest AP,” to the receiver noise. This ratio depends only on the environment, specifically on the average path loss between AP-user pairs, as can be seen from (16). 
 Figure 3 reveals that, in scenario aisle-to-office 1 and for a power limit ?? = 5 dBm (SNR= 31 dB), DPC and ZFDPC attain a data-rate gain in excess of three times, for both 50% and 90% availabilities, when compared with NC or FD. Similar gains are observed at higher power limits. In relation to this, we note that, loosely speaking, for large values of ??, a gain over FD of at most four times would be expected, since for large values of SNR and assuming perfect interference cancellation, the improvement in capacity will be dominated by the ratio of transmission bandwidths used, which is 4 in our case. 
 ??variance, which implies that the performance of NC is???/=?? |h??,??|2 in (6) are small compared to the noise 
 noise-limited. In the specific case of ZF, the power of some APs will be reduced below ?? to meet the PAPC (9) while inverting the channel by choosing W = H-1. The cost of such interference avoidance effort is larger than the benefit stemming from having zero interference, yielding a sum-rate smaller than that obtained with APs at full power without coordination. The poor performance in low SNR regimes is a well-known shortcoming of pure ZF, which can be improved upon by using regularized zero-forcing (RZF), as discussed in [48]. We illustrate this in Fig. 4 which describes in greater detail the behavior of all techniques at low power for the case of 90% availability. Under such conditions the performance improvement of RZF over ZF becomes evident. As previously discussed, it can also be seen in this figure that at low power, the objective of maximizing sum rate may be achieved by 
 Fig. 5. CDF plots of rate per user, obtained when NC, FD, ZF, ZF-DPC and DPC achieve their maximum sum-rates, under the power constraint ?? = 20 dBm, from empirical channel measurements taken in scenario hall 2. 
 not serving some users at all, and as a consequence the rate that can be guaranteed to 90% of users may drop to zero. 
 A procedure similar to the one described before was followed to generate 4-by-4 channel matrices in the two hall scenarios where measurements were taken. The CDF of the peruser rates obtained with each network-MIMO technique when optimizing for sum rate under power constraint ?? = 20 dBm are shown in Fig. 5, for scenario hall 2. In this figure we see again that the three network-MIMO techniques considered here provide higher per-user data rates than NC and FD. 
 The corresponding per-user rates for several availabilities as a function of ?? for this scenario type are shown in the right column plots of Fig. 3, where scenario hall 2 has been chosen. The results for the other hall scenario are very similar and are omitted for the sake of brevity. 
 In this scenario, in the high SNR range, DPC and ZF-DPC attain roughly the same gain, in per-user rates over FD, as that observed in the aisle-to-office scenarios. On the other hand, in this more interference-prone environment, the NC maximum rates are significantly reduced when compared to those of the aisle-to-office scenarios, characterized by higher natural isolation between AP/user pairs. 
 The gain of the DPC schemes over ZF is also larger when compared to the aisle-to-office scenarios. Loosely speaking, this can be attributed to the fact that the aisle-to-office channel matrices are more diagonally dominant than the hall channel matrices. As a consequence, and in view of the Gersgorin-? disc theorem [50], the determinant of H will, at times, be smaller in hall scenarios. Since for ZF the preprocessing matrix equals the inverse of H, it will be possible for W to have larger elements in the hall scenarios. In view of the power constraint (9), this means smaller signal variances ???? and thus smaller sum-rates for ZF (see (8)). In contrast, ZFDPC is less sensitive to the smaller-determinant matrices that arise in the hall scenarios. The reason for this behaviour can be found in the fact that the determinant of H is equal to the product of all the diagonal elements of R. Thus, a given decrease in the determinant of H will, in general, entail a smaller reduction of rate for ZF-DPC (see (13)). 
 As can be observed from Fig. 3, high-interference scenarios do not always result in decreased per-user rates. While this certainly occurs for a non-coordinated system as clearly seen from the graphs, DPC, ZF-DPC and to a lesser degree ZF may yield an improved performance in the hall scenarios in comparison to that obtained in the better isolated aisle-to-office environments. The improvement is particularly significant (a factor of around 2) at low power, as seen at the left extreme of the graphs. We note that the performance improvement is still evident when the comparison is carried out at equal SNR, i.e., when the differences in average path losses between AP/user pairs for the scenarios has been compensated. This behaviour can be explained by recalling that, at low power, noise rather than interference is the relevant factor in limiting transmission rates. Since the channel matrices of the aisleto-office scenarios are more strongly diagonal-dominant, less power reaches a user from access points other than the one that is dominant. In contrast, in an environment with less isolation (such as the unobstructed halls), the DPC schemes can take advantage of the fact that significant power from all bases will reach each user, while managing to turn most of this power into signal, not interference. (Indeed, it can be seen from (14) that for sufficiently large noise power, increasing the magnitude of the off-diagonal elements of H may produce a relatively larger increase in the numerator of the fraction within the log(·) than on its denominator.) Thus the signal-tonoise-plus-interference ratio is improved. As transmit power increases however, interference becomes dominant, and the hall scenarios suffer from the increased need to compensate for this impairment. 
 In order to assess the accuracy of the log-normal plus Rayleigh/Rice model at predicting the maximum rates achievable by the techniques described in Section IV, we obtained those rates from channel matrices generated by the general model for each scenario type (aisle-to-office or hall), that is, using the parameters in the boldface rows of Table I. The data rates predicted by the model were, in general, in good agreement with those obtained from empirical data, for each scenario. Figure 6 shows plots of per-user rate with respect 
 Fig. 6. 90% availability empirical and simulated per-user rates for scenario aisle-to-office 3. Simulated rates were obtained from the model using the scenario-type parameters in row “All aisle-to-office scenarios” of Table I. 
 to maximum transmit power constraint ?? for 90% availability for the scenario aisle-to-office 3. It can be seen that, for this scenario, the rates from simulated channels are smaller than those obtained from empirical data. This may be attributed to the fact that the scenario-specific parameter ???????? for aisle-tooffice 3 (in the third row of Table I) is smaller than that of the corresponding general model used for the scenario type (contained in the fourth row of Table I). Thus, in this case the actual received powers will be larger (on average) than those predicted by the model. 
 In this paper we have evaluated the data rates achievable in an indoor wireless 4-by-4 system by network-MIMO techniques such as zero-forcing (ZF), zero-forcing dirty paper coding (ZF-DPC) and dirty paper coding (DPC). We compared these rates with those achievable with frequency diversity (FD) and no coordination (NC), using measured and simulated indoor channel matrices at 3.5 GHz. Our results show that a single-slope log-normal plus Rayleigh fading model with properly chosen parameters is good at predicting the sumrate performance statistics of these techniques. By numerical evaluation based on empirical data we have shown that in the scenarios tested, at high SNR, the network-MIMO techniques are able to achieve about a three-fold increase in per-user data rates over NC or FD, when considering 50% and 90% availabilities. It was also found that schemes employing dirty paper coding can attain higher per-user rates in hall scenarios, where interference is greater than in aisle-to-office scenarios. This difference was greater for high availability data rates and at small SNRs. Our evaluations also revealed that while ZF in general provides higher data rates than NC and FD, this advantage is lost at low SNR, particularly when high availability is considered. In contrast, DPC based schemes provide significant benefits under virtually all practical conditions. 
 The authors would like to thank Francisco Silva, Dragan Samardzija and Sivarama Venkatesan for their valuable help. 
 ﻿ Based on for a street canyon type cell we compare the spectral of various cell interference mitigation . This Zero Forcing , Zero Forcing and . As a reference for comparison we consider and . Out of cell interference , as additive noise , is included in two : a all transmit simultaneously and neighboring transmit in time . We find that , and can offer in spectral efficiency over and by of around under interference limited . This with gains of only around when a standard mode instead of our measured data . This difference is due to the greater inter cell isolation that our street canyon type test environment . We find that over of the spectral efficiency under virtually all operating . 
 Index outdoor to indoor , cell , inter cell interference , spectral efficiency 
 I . INTRODUCTION 
 F 
 are being to address the explosive demand for higher capacity in wireless , an increase in user density at where cannot cope with the traffic load . Spectral can be further some of the we discuss below . 
 Multiple Input Multiple Output transmission have received great attention . However , inter user interference in a low Signal to Interference plus and corresponding performance loss . This led to the proposal of MU , where transmission or cell interference . In cell , inter cell interference additional on performance . Many to reduce or avoid this interference have been . For example , almost blank can avoid inter cell interference when a small cell pico cell or cell is within a macro cell to address a local traffic . 
 Massive , a large number of at the Base Station to mitigate cell and interference , with a considerable increase in complexity and higher backbone overhead to carry inter cell Channel State Information . A far more elaborate scheme is the transmission , alternatively known as Network , where neighboring encode and decode for multiple simultaneous . Although , in theory , such a scheme can produce very large in throughput , only limited gains have been so far under . In it is shown , based on , that Network no gains with respect to an based scheme if the of the former are considered . 
 In this work we present an based evaluation of for single antenna , i . e ., Multiple Input Single Output , on the case where only cell is available . 
 We discuss below relevant . can be classified into nonlinear or linear . Among the nonlinear , Dirty Paper is a valuable capacity bound , , even though no practical have been . Simpler , suboptimal linear include Zero Forcing and Zero Forcing . In , cell interference is by choosing a matrix that the channel gain matrix and then the user via the water filling algorithm . It been shown that can asymptotically achieve the theoretical limit when considering the case of a large number of . poorly at low Signal to , when spectral are noise rather than interference limited . This effect becomes particularly significant for ill conditioned channel matrices . been to resolve this problem . In this scheme the matrix to be inverted noise dependent that , at low , compensate for the eigenvalue disparity of the channel matrix . 
 Simpler transmission that need little or no at the transmitter include and Time Division Multiple Access . In , cell interference is reduced by separating the coverage sector into smaller through the use of directive . The only feedback is that resulting from the association of a user terminal to a . Such a system , with no inter 
  
 a 
 Fig . . Example of measured scenario for a the cell campaign and the inter cell campaign . 
 user capability , will be limited by cell interference at high transmit power . 
 In , the transmission frame is divided into time , and each user is at full power during one of those time . The capacity gain of over for was in , but without inter cell interference . The in that work are used as reference for our . 
 The above are especially important when outdoor are intended to serve indoor , yet these have only been studied relatively recently . on based outdoor to indoor channel are in , but these were not in the context of MU . Only a limited number of include empirical data in the evaluation of MU . In spectral efficiency is for a distributed transmission system in an indoor environment , while a similar setup Network to compare the spectral efficiency of several , finding that and achieve about a three fold increase in per user data over non transmission and frequency division . In the rate distribution among is for an indoor MU scenario when time , frequency and spatial . In the aggregate capacity in a MU system , and is and with the sum of individual of various single user in outdoor and indoor . Our work is related to the above but inter cell interference and actual cell channel . 
 We evaluate the spectral efficiency of several MU in outdoor to indoor with along the streets , below the serving the of one city block . This from the often used assumption of hexagonal which is unrealistic in our deployment scenario . matrices are measured cell while out of cell interference is included as additive noise , as in , with power calculated a measurement based model . We treat out of cell interference mitigation through two simple . The basic time across . We also include a simplified version of the method in where inter cell interference is by . In particular , we the achievable spectral of and with . The is assumed to serve single antenna simultaneously . power is varied to cover noise and interference limited . Our measured outdoor to indoor scenario , in the next section , from those previously for MU : 
 indoor to indoor in and outdoor to outdoor . 
 In the following we describe the measurement setup and the statistical of our test environment . A summary of the various transmission is included in Section . The empirical and their comparison with those based on various channel are in Section , leading to the of our work . 
 . MEASUREMENT SETUP AND 
 Channel were done at ´ Mar ´ campus in ´ iso , Chile , and campus in , Chile . Two measurement were carried out : one at the channel matrices for the cell being by the and another at the interference by neighboring . 
 A . Cell Channel Matrix Campaign 
 To obtain the channel matrices within the cell , we used a linear array to emulate an outdoor . It of vertically coaxial spaced half a apart , with a reflective . With the , each antenna a maximum gain of and a half power . This array was along the street at a perpendicular distance from the building wall ranging from to . The antenna corresponding to the possible Subscriber Unit was a single gain vertically coaxial dipole antenna mounted on a long swivel arm . This antenna was rotated step wise in to obtain local statistics . A . , . Continuous Wave signal is through this antenna . For convenience , we measured the propagation transmission . 
  
 Fig . . Measurement system scheme . 
 were chosen inside in with outside facing sized . by . , from the outdoor array by between and . The array were at . above ground level , i . e . a height that would allow easy installation by a service provider aiming to illuminate or that are across the street . The indoor antenna was at height on a first floor , i . e , at roughly the same height as the array . The perpendicular distance from the exterior wall to the from to . The placement may be consistent with an access point in a street side room , which can also act as an indoor wireless router . 
 A schematic representation of one measured scenario is shown in Fig . a , where the inside are the of the and the bar the position of the array . the array at various along the direction of the street measuring to at angular of up to off array . The are made of concrete and the glass have steel and aluminum . The were carried out during the day in the absence of pedestrian traffic , to ensure static channel . A shadow antenna not used in is included at each end of the array to preserve the impedance due to mutual coupling in all the . A reference antenna in line with the , at a distance of about from its shadow antenna , measuring the relative phases of the array . A system of any one of three of four to a channel receiver , while the reference signal is permanently connected to the channel . The receiver the carrier frequency to and these are fed to a channel data acquisition system . A computer is to automatically control the of the swivel arm , the switching and the data acquisition . synchronize the transmitter and the channel receiver for phase coherence . Fig . a schematic of the measurement system . The are for at a rate of 
 , per channel , total . This interval is partitioned into contiguous sub of per position of the swivel arm . a Fast Transform algorithm , a value of magnitude and phase is per sub interval . Due to exact synchronization of sampling , the data correspond to an integer number of of the , the without . The existence of consecutive for any transmit receive combination of verification of proper equipment operation and the reduction of residual measurement noise through . We measured a total of of of the outdoor and indoor , i . e . by matrices . 
 B . Inter Cell Interference Campaign 
 As already , in our study we have not considered joint among . Instead we treat the out of cell interference as noise . To estimate this interference power we measured path over a wide range of since it is not a obvious which will cause significant interference . The sounding system was basically the same of the first campaign , except that at the end we used a single antenna , since only the scalar path loss is . We as . Over extending from to we measured the outdoor to outdoor path loss . This included same street as well as around the corner in the grid type street . For a subset of these we also determined the path loss from the to the indoor location that the outdoor position as shown in Fig . . This us to model the outdoor penetration loss . We describe this in further detail in the next section . We found that the outdoor to indoor statistics can be quite accurately by a penetration loss to the corresponding outdoor to outdoor path loss , as in . 
 The based channel data us to obtain the channel gain statistics and in the following section . From these we also assessed and the achievable spectral for the various . These are subsequently to those that would be with various . This is in detail in Section . 
 . CHANNEL STATISTICS AND MODELING 
 In this section we statistically characterize the propagation channel for the chosen environment . As will be seen , our are consistent with those in the literature for similar , which that we have chosen that are representative for urban . 
 Out door to In door Pen Loss 
 A . 
 To model this loss we used both cell and inter cell to compare received power in for by the exterior wall . The average power was from the of the swivel arm , and thus the small scale . The are shown in 
 Distance 
 a 
 Fig . . a Measured penetration loss and fit . model . Outdoor to indoor path loss at long . 
 TABLE I FITTED PENETRATION LOSS OF 
 Parameter Value 
 . 
 . 
 . 
 . 
 Fig . . Fig . a the difference in measured outdoor and indoor as a function of the incidence angle defined in Fig . . For comparison we show the mean penetration loss of the model . We a version of the model given by to our data , a Minimum Mean Square Error criterion to obtain the best fit for the average penetration loss . These are listed in Table I 
 L 
  
  
 ... cos H 
 L 
  
 where is a zero mean random variable , which we found to match a distribution . Its standard deviation was found to be angle dependent , with of . for links growing to . as . The latter correspond to those of inter cell interference links in our street canyon geometry . For cell links the average penetration loss is essentially constant at . , much less than by the model . In Fig . we show measured for outdoor to outdoor and outdoor to indoor links at relevant for inter cell interference . As will be in subsection , for up to the average outdoor path loss for a linear fit model a slope only slightly than that of free space links . The outdoor average path loss was found to have the same slope , but as shown , its are offset by an average penetration loss of . , consistent with . 
 B . cell Average Path loss and Shadow Fading 
 cell links were in general Line Of Sight through a window , with minor in the path . This is often to as Line Of Sight . The large scale path loss is calculated at each transmit receive placement , by again the received power of the of the swivel arm . these we fit a classical log distance model to the path loss in as : 
 log d 
 where is the path loss at the reference distance d , is the path loss exponent and is the shadow fading term , a zero mean random variable with standard deviation . It is assumed that . The value of is found through a regression fit of the empirical of path loss . The of , are shown in Table . We tested various other . This included the free space path loss model i . e . plus the average cell penetration loss from , a two slope model and an exponential model based on . j Type . The improvement of the fit in of is negligible . Fig . illustrate the comparison . In the same figure we also included for reference the line that to the free space path loss and that by the model for the outdoor to indoor case in urban micro with a grid layout . As will be later , we found that the smaller cell penetration loss with respect to the model had a significant impact on the achievable spectral . We a small proportion less than of path that are lower than those in . This is likely due to effects in our street canyon environment , particularly at shallow incidence . 
 C . Inter cell Average Path loss and Shadow Fading 
 Based on our inter cell measurement campaign , we an outdoor to indoor street canyon path loss model in order to evaluate the inter cell interference from neighboring . We firstly found that a two slope model accurately 
  
 Distance 
 Fig . . Single slope fit and exponential fit for the measured cell in function of the distance , in logarithmic scale . For comparison , free space and are shown . 
 the outdoor to outdoor street canyon path loss . This written as 
 d d 
  
 where the PLout is the outdoor to outdoor path loss , is the break point distance the change in slope and is the corresponding path loss . The outdoor to indoor inter cell path were found to be well by the sum of the average penetration and path from and , respectively , plus a shadow fade term with . standard deviation . We note that for the of incidence that will characterize most inter cell interference , the average penetration loss is . . This is consistent with what been in the literature . Table the of the model . 
 Through the same outdoor to outdoor street canyon we found that Non Line of Sight corner links show a path loss that is at least above that of links for the same distance , which is consistent with . Thus , the interference by around is negligible to that from neighboring same street bases . For this reason , the former are not considered in the performance analysis of Section . Our path loss model does not differ very significantly from previously and we therefore omit further . 
 D . Cell Small Scale Fading and Correlation 
 The small scale fading statistics for the cell links , from any array element to the , were from the complex channel gain corresponding to a rotation of the swivel arm , to their local spatial average . As for links rich in path , we found that a distribution a good fit for the channel gain envelope statistics . The corresponding factor was the two moment method . The average percentage of absolute value error in the fit of the empirical and model Cumulative Distribution 
 TABLE PATH LOSS FOR AND INTER CELL LINKS 
 Parameter cell Inter cell 
 . . 
 . . 
 . 
 s . . 
  
 . 
 . 
 was . . Higher were when fitting other commonly used small scale fade To account for possible due to the separation of the array , we the fade statistics per element and subsequently the individual . These were found to be similar , the being by the limited data set size . Following , , we a log normal distribution to the factor statistics , i . e . ,, with its shown in Table . 
 The correlation between the received by the different array of the serving was calculated all . To this effect we each by channel matrix by its norm , in order to combine the data corresponding to with different large scale path . We initially a single exponential model to the as done in , but found that a double exponential model provided a much better fit , which significantly the accuracy in the prediction of the spectral . For example , for the case , the estimation error of the capacity with the single exponential model is , which to when a . The general expression for both is given by 
  
 where is the correlation between by a spacing of , i . e . to adjoining and , the number of in the array . The corresponding model A ,, r and r were by a least fit , and are listed in Table . 
 We then the channel matrix as done in 
 h 
 with being the component of the matrix , , where is the distance between adjoining expressed in , , , ,..., is a vector and is the steering angle of the signal with respect to the of the array . In our simulation based this angle was as a random variable with a uniform distribution over to cover the range of possible user . is a vector of circularly symmetric zero mean complex random with unit variance , correlated following the procedure in . To this effect we used a 
 TABLE FITTED FACTOR DISTRIBUTION 
 Parameter Value 
 . 
 s . 
 TABLE FITTED CORRELATION 
 Parameter Single exponential Double exponential 
 A . 
 r . . 
 B . 
 r . 
 correlation matrix with calculated and Table . 
 E . Inter Cell Small Scale Fading 
 For the inter cell propagation model all links are , due to the shallow angle of incidence between the neighboring and the indoor . We found that the small scale in a neighborhood around the antenna are very close to distributed . This the in without a dominant component . The factor the two moment method was essentially zero . Empirical were found on average to differ from a distribution by no more than absolute value . However as will be in greater detail in Section , small scale in the interference calculation was found to have little effect on the resulting spectral . We confirmed this for several and subsequently small scale , which very considerably the . 
 F . Procedure for Model Application 
 When our measurement based with the above model we proceed as : 
 Step : Choose a a angle theta from the to the . The distance must be chosen in the range from to , and the in the range within with respect to the of the array . Also choose the number the . 
 Step : Calculate the shadow fading and path loss and the cell of Table . 
 Step : Generate a log normal distributed random variable with as in Table . 
 Step : Generate the correlation matrix and the of Table . 
 Step : Calculate by generating a by matrix of i . i .. circularly symmetric zero mean complex random with unit variance and correlate them the correlation matrix and the procedure detailed in . 
 Step : Generate the matrix the chosen angle and the appropriate value of . 
 Step : Calculate . Add the previously path loss according to generate the channel matrix to 
 h 
 After generating a channel matrix , the interference by the neighboring to that is by the following : 
 Step : Determine the distance between the neighboring and the street position directly across from the indoor . 
 Step : Choose the Sum Power Constraint and an azimuthal gain the neighboring in the direction of the street position of the previous step . 
 Step : Calculate the shadow fading and path loss to the indoor and with the inter cell from Table , the penetration loss from Table I and the distance calculated in Step . 
 Step : Calculate the interference power from the neighboring cell as 
  
 where neigh sin neigh neigh the gain of the antenna array , is a factor to adjust the width of the lobe and neigh is the direct path angle to the street position across from the indoor In order to the same as in our , the lobe width is set at , with . . is the total power and is the path loss between the cell and the user , which is dependent on the by . 
 . REVIEW OF , AND 
 SINGLE USER 
 We compute the per user spectral efficiency aggregate spectral efficiency number of for several a broadcast channel in the . We assume a transmission by the array with . There signal each with a single antenna . Thus , there an by channel , by the column vector , between the transmitter and the th user , with ,...,. The is formed by row into a channel dimension 
 K such that , where the of the matrix . 
 In this way , can be by 
 y 
 where the random vector is the signal , is the random noise at the receiver and is the total inter cell interference reaching the receiver . The signal is expressed as 
 x 
 where u is an independent zero mean complex random vector , is a linear matrix and : is a non linear transformation in the case of , and the identity matrix in the case of and . 
 In all the tested , a is on the power as done in . This is written as : 
 K 
 E WE wi 
 i 
 where is the trace of the matrix , wi is the i th column of , is the maximum power and pi is 
  
 with being the i th element of u and , the complex conjugate operator . The power to each user then is , for ,...,. 
 For our comparison we chose a receiver noise variance set at , which to a of and a receiver noise figure of . As already , the interference from neighboring is as additive 
 noise . Thus we write , where 
 is the sum of the interference from all neighboring . The procedure for the calculation of will be in the next Section . We note that as the transmit power , will do so in the same proportion , which will lead to the saturation of the achievable spectral . For our analysis we assume a antenna base and up to . The channel matrices will thus be by , i . e . . When , the knowledge of the equivalent each . If more were available at the , treating interference as noise would be pessimistic in that it the possibility of the interference signal structure in a detection MUD type receiver . We now briefly describe the various transmit considered . 
 A . 
 This scheme is included as against which to assess the of the below . Each user is during one of with power . We note that in this scheme we are only separating into time within a cell , cell being separately . The achievable spectral efficiency is therefore 
  
 In a lower bound for spectral efficiency is , which is tight for . This bound is when the best user is all the time with power P 
  
 where ,..., hi . 
 B . Dirty Paper 
 This is the capacity technique and it is to be considered as an upper bound of actual performance , given that its implementation is complex and to date there is no cost effective algorithm to implement it . 
 In , there is no a choice for the matrix , and full is at the transmitter . The algorithm this knowledge to avoid , for the th user , the known interference produced by the intended to the , ,..., namely , the previous . In doing so , the block by a transmit the sum random , as K 
 x i 
 i 
 where i , is the information bearing vector intended to the i th user . the covariance matrices Si , 
 , the maximization problem for the spectral efficiency is as stated in : 
  
 where the maximization is over all covariance matrices 
 satisfying 
 Si , i ,..., K 
 K 
 X 
 Si P 
 i 
 An efficient numerical method to find the solution to this problem is in , the duality between the and the multiple access channel . 
 A simple upper bound for the spectral efficiency been found in : 
  
 C . Zero Forcing 
 This scheme the weight cancel the interference in the . Thus , is , in general , the pseudo inverse of the channel matrix which is assumed full rank : 
 W 
 In particular , non singular , is . The spectral efficiency is therefore calculated as 
  
 subject to 
 K wi P 
 i 
 are to each user weighting the water filling solution . 
 Although the simplicity in the selection of the scheme attractive , poor performance if the channel matrix is ill conditioned . In these , usually one or a few of the matrix are very large to the rest . At low transmit , where spectral are limited by noise rather than by interference , the solution which sum rate may result in some being a zero data rate . 
 D . Zero Forcing 
 was to correct the problem by in presence of ill . A regularization factor is in the pseudo inverse of , which to normalize the of the matrix , allocate power to all and avoid noise enhancement . In this case , is chosen as 
 W aI 
 with a being the regularization factor . In it is shown that the optimum a for a large number 
  
 with . Despite not being the optimal value for a finite , for simplicity we will use this value of a . not exactly invert , cell interference will now affect . Thus , the spectral efficiency is calculated as 
  
  
 subject to 
 K wi P 
 i 
 In , the term i the total interference reaching the i th user . are by water filling , an iterative algorithm that the in each iteration considering the cell interference . 
 E . 
 In this scheme the sector is divided angular of the same width . In practice , this can be done a directive antenna per . To simulate the effect of various of directive in the we took advantage of our channel with the antenna array . This us to generate antenna based on between and dipole . We note that this steering with appropriate , which , however , remain fixed , i . e . which cannot be adaptively according to user . We define the antenna weight matrix as . The available power is distributed equally between and it is assumed that one user is associated with each of them . Then pi , i ,...,. The spectral efficiency of is thus calculated as 
  
  
 where di is the i th column of , i ,...,. 
 V . AND ANALYSIS 
 In this Section we present the main of our work . The general under which we the various are in sub section A . In present the achievable spectral based on the measured data . This , in . and the various cell interference avoidance in . . Finally , in evaluate the spectral channel matrices by various simulation . This , a i . i .. model for cell small scale and our own model in Section . These are with those in Subsection . . 
 A . General for the Comparison 
 our empirical data and we now evaluate the per user spectral efficiency aggregate spectral efficiency number of of , and for a cell system disposed along a street , as shown in Fig . . As a basis for comparison we considered with . In all we have considered perfect knowledge of cell at the serving and a sum power constraint with a range of realistic for the transmit power . In order to establish a for our comparison , we also calculate , as in , the upper bound for capacity , the spectral efficiency for with equal time allocation to and the achievable spectral efficiency when the sum rate is . To avoid , we will henceforth use the term cell when to the system where the time division multiple access only within the cell , in contrast to the case where the among each other , which we denominate time reuse . 
 Although our treatment on cell interference cancellation , for we have additionally included , based on , the case where extra allow mitigation of inter cell interference . We chose a very simple scheme in which the have and the extra of freedom are used to steer towards the being in the adjacent . Given our street canyon geometry this is likely to eliminate the of interference . We treat . Firstly we obtain the achievable performance when full is available across contiguous . We call this system . Alternatively , we consider the case where the information between is limited to channel correlation matrices . Although this is less effective , it will not require such frequent , less burden on the network . We denominate this as . 
 The cell system along a street , as shown in Fig . by . Each cell is assumed to be close to the building facade in the middle of the block and service to the across the street . This closely our measurement scenario . The spectral are for the in the central cell . We found that more interfering in the system did not impact our . 
  
 Fig . . Topology scheme of the system . Each red towards the city block across the street . 
 The evaluation of the achievable will consider two : no between reuse , and a scheme that time reuse , where adjacent transmit in alternate time to eliminate the source of inter cell interference . For the case of reuse , the achievable in Section are appropriately scaled . 
 At each scenario such as in Fig . we measured at least placement of the array and of the rotating dipole that to possible . Each such combination a by channel matrix , where each row to a possible user position . Since our work a antenna array , multiple of adjoining in the antenna array can be selected . Out of this ensemble we used a subset of for each placement to obtain the statistical description of the channel spectral . We found that further in the number of in negligible of the . Consistent with Release , Rev , we grouped , dividing the coverage region into . We further assume that each is illuminated by a pilot beam of a different frequency the . The corresponding phase are chosen so that in free space the would be in the direction of the center of the corresponding . A power distribution with level suppression was assumed . This choice was based on the fact that this level of suppression good for the case , as will be below . these and the measured channel gains corresponding to each user position , we associated the to the that them with the highest power , thus disjoint . One user is randomly chosen from each group , leading to one possible realization of the by channel matrix . There are however where no user with a , because their are such that all receive more power from one of the other . When this , the number limited to the number of . Given the extensive set of available , this was found to occur only rarely . In such we considered a by channel matrix . 
 The per user spectral efficiency of the is for each by matrix , for a given dividing the by the number of . The from all were then to generate of spectral efficiency for all . The total number of channel matrices used in our statistical was , per placement . 
 In the case of cell , the power is the same as the of the and the aggregate spectral efficiency from is divided by the number of , to obtain the per user spectral efficiency . 
 We found that the interference from neighboring , as additive noise , could be included only the scalar path loss model in Section , i . e . the effect of the small scale . This very considerably simplified and the . To validate this approach , we firstly calculated for a subset of , the spectral for , and with the small scale included . For each placement we considered randomly chosen channel . the for and is straightforward as it only and to take into account that the interference is dependent on the small scale fade affecting each user . For small scale is considerably more complex and to much longer . Following the reasoning in Chapter . , page , it is necessary to obtain the covariance matrices of the neighboring in order to calculate the inter cell interference they generate as in Section .. These covariance matrices are a method in . Then , the received power from one neighboring cell is calculated as 
 K 
  
 i 
 where is the channel vector between the neighboring cell and the user in the central cell and is the covariance matrix for the i th user in the neighboring cell . We found that for and , small scale on average in the spectral efficiency by less than . For the case of his difference was less than . Based on this finding we used the simplified approach that small scale and calculated the equivalent noise power due to interference as : 
  
 i 
 where is the interference power due to the i th neighboring cell , calculated as in , i . e . small scale . 
 B . Achievable Spectral for the Measured 
 Optimization of for the Single Cell Case : We evaluate as a reference the spectral of when considering only cell interference . As will be seen later , for this type of interference completely that from neighboring . Since this dominant cell interference on the shape of the radiation that cover the cell , our initial objective is to evaluate this aspect . We found that generating with only is far from optimum and thus we other that more flexibility in and 
  
  
 Lobe Width 
 Fig . . Per user spectral efficiency of . , for different number of in the array , at availability . 
 suppression . Both and suppression will affect the interference that the is exposed to . the beam pattern will eventually lead to the sector not being fully covered by the and at the same time the appearance of poorly suppressed , which will contribute to the interference in the neighboring . At the other extreme , very large suppression of in an which may invade the neighboring . For our measurement , we found that a suppression in excess of produced virtually no further with regard to the reduction of interference . The reason for this is that at a certain point the energy scattered from the of a into the adjoining region over that due to the . It from the above that for any antenna array there will be an optimum . To observe this effect , we between and antenna to synthesize radiation with suppression ranging from to . The power was set at , a power level that assured interference limited operation . 
 Fig . the . We have plotted the median per user spectral efficiency for various of directive . In this and in subsequent we will refer to the spectral efficiency available to of as the spectral efficiency at availability . We note that the independent variable chosen for Fig . is the antenna array , which is actually a result of the choice of suppression and number of antenna used in the directive array . As can be seen , more to synthesize the the achievement of higher spectral as it of freedom in the reduction of interference . In show for the median spectral efficiency that can be in our , the optimum choice of for various of directive . We include the relevant that characterize the antenna in free space and the attenuation of the at the sector border . As seen , under interference limited the width must be made much narrower than the sector width to achieve the best possible spectral efficiency . 
 Spectral Efficiency of Cell Interference Reduction : We now evaluate the per user spectral efficiency of the interference reduction in the non 
 MEDIAN PER USER SPECTRAL 
 Parameter Number of 
  
 . . . 
 attenuation at edge . . 
 suppression 
 Median spectral efficiency . . . 
 cell system , the power limit from to in of . From the of the spectral we then obtain the spectral available to and of . Fig . the . For reference we include to as and cell , both considering inter cell interference . For we used the best possible radiation for both and antenna as previously in Table . When the is limited to , the theoretical capacity upper all other at any power level and any availability . However , and can achieve an important proportion of spectral efficiency . In fact , and at for and availability respectively , while and . The improvement of over , particularly at availability , is attributable to dealing effectively with poorly conditioned channel matrices and high interference . As we will see later , these differ significantly from those through simulation a model with default . Our predict spectral much higher than those from that model . 
 The comparison with the case where the are assumed to have and is shown in Fig . . We here repeated the of and to contrast them with . For the latter we considered with and without inter cell interference mitigation . performance at the expense of added transmit is of course present in the region . However the more significant gains occur as the transmit power where the spectral efficiency of the without inter cell interference cancellation approach saturation around . Full exchange across will of course be best and in our case a relatively modest amount of information exchange . only the channel correlation matrices , which may require less frequent , more limited . 
 a very clear saturation for much lower than the interference . The reason for this is that with growing transmit power , for cell interference spectral efficiency before the effect of neighboring in . This is by the fact that the saturation for these match the best in Fig . , where no cell interference was included . As a result of this interference , will only achieve spectral comparable to the interference in the noise limited case , i . e ., for low transmit . In 
 Power Power 
 a 
 Fig . . Per user spectral efficiency . power constraint of various for a availability and availability . 
  
 Power 
 Fig . . Per user spectral efficiency . power constraint of inter cell interference mitigation for availability . with and and with are shown for comparison . 
 the saturation region it no better than cell , which is particularly simple to implement . In this region , around of spectral efficiency and of spectral efficiency for and availability . 
 As seen in Fig . , the bound in considerably its actual spectral efficiency , which is and of the bound at and availability , respectively . The spectral efficiency gain of over is in without taking into account inter cell interference . It is shown that this gain to one for low and is bounded by min , for high . This gain is shown for our data in Fig . for , and availability as a function of the . We have plotted the spectral efficiency gains for our cell system and , for comparison with the in , for the case when the neighboring are turned off , to as single cell . As seen , the spectral efficiency is always above that of cell , even for low . For high power , the gain asymptotically the bound in the single cell case , but well below this value for the cell case . 
 Since inter cell interference the asymptotic spectral efficiency , we the performance of the above reuse system with the reuse system , considering only the case where the have . Fig . this comparison 
  
 Fig . . efficiency gain of over cell for , and availability in the single cell and cell , in function of the . 
 for , and antenna . Under our , reuse worse than reuse for and availability . In the case , which is cell interference limited , reuse in a net loss of about half the spectral efficiency at all . It was found that for our data reuse only an advantage over reuse for availability and above in the case of . The reason for the advantage at high availability is that in such the spectral efficiency is limited by a small number of poorly , subject to particularly high inter cell interference . In contrast , for the rest of , the of reuse will not offset the disadvantage of only half the time . Similar have been found for hexagonal macro cellular with such as frequency reuse , fractional frequency reuse ., where it is also found that essentially those few at the between the interference . The proportion of such is even smaller in our linear layout . 
 We also the effect of the inter cell interference on the spectral efficiency of the most exposed to it . For this purpose we grouped the into edge in the external and central in the central . We compare their per user spectral by plotting the Complementary for at of . We this for reuse and reuse . The are shown in Fig . . As seen , in both 
  
 Power Power 
 a 
 Fig . . Comparison between reuse and reuse for the per user spectral efficiency . power constraint of , and antenna 
 , for a availability and availability . 
  
 Spectral Efficiency 
 Fig . . Comparison of of the per user spectral efficiency of for , for edge red and center black when reuse and reuse . 
 reuse is on average a better option than reuse . We illustrate this effect for , as due to the form in which spectral are for , it is not possible to obtain the individual per user separately . 
 In order to test the sensibility of the to the level of inter cell interference , we repeated the above assuming penetration of and . at of exceeding . . As , this in lower spectral and the interference limited zone starting at a slightly lower transmit power . We show this for and in Fig . . In particular , for a maximum penetration loss at availability , and come within of the saturation spectral efficiency at . and . lower than in the . penetration loss . The corresponding for . penetration loss are . and . . Not surprisingly , the benefit of reuse as the penetration loss . In fact we found that at availability , reuse will perform on par with reuse in the saturation region when the penetration loss is . At the same availability , when the penetration loss is . , the spectral of reuse at saturation are around higher than those of reuse for both and . However , reuse is still worse than reuse at availability . 
 C . Comparison of Empirical with Model Based 
 Finally we our based with those that are different simulation . This : the model based on our empirical data as in Section , a model based on our see . and combined with small scale i . i .. and two based on the . We compare the per user spectral efficiency at availability . power constraint for and under the same used to obtain the shown in Fig . . The are the empirical setup but the various channel to generate the gain matrices . for the array were , each with a distance between the and the center of the room determined by a uniform random distribution in the range from to . For each placement a set of one per is , all within a range of with respect to the previously chosen distance . are per array placement . The are shown in Fig . . 
 The model considered is in detail in . We by the model with the indoor urban default . We used these for both the cell being and for the interfering neighboring . This is as the default model in Fig . . As seen , this did not yield a good prediction , as it spectral efficiency by a large margin around of empirical spectral . We thus introduce a version that we refer to as , which better our measured . This from the default model in that for the central cell i . e . the cell being we use the outdoor to outdoor model to calculate the path loss , which we combine with outdoor small scale default . For the neighboring we use as before the outdoor to indoor urban scenario as in the model . This choice was by our empirical data . As in Section , the cell path loss for the outdoor to indoor scenario was close to that of a link , but at the same time the were quite small , typically below one . Thus 
  
  
 Power 
 Fig . . Effect of maximum penetration loss on the per user spectral efficiency of and for availability . 
 despite an excess path loss that is much smaller than that of the model , the scenario is still by very rich propagation . The resulting model a better prediction with an error of for . 
 The two based on our empirical data result as would be in the best match and provide added insight into the effect of the channel . The model that only based path loss combined with i . i .. small scale spectral efficiency , but the error is in general not very large . For , it is in the saturation region . Our model the small scale slightly the spectral . We found that the difference between both is essentially due to the inclusion of channel correlation . In fact , when this correlation in our model , the resulting spectral become very close to those of the i . i .. model . This is consistent with what was in this regard in . 
 . 
 We have the advantage of cell interference by their spectral with that of , which only interference fixed antenna . We found that for a typical scenario , by a street canyon type setting , the spectral efficiency gains over can reach between and , the exact value depending on the specific system and the percentage of being considered . is limited by cell interference and in fact does not perform better than cell . Among the cell interference , can achieve more than spectral efficiency . Interference from neighboring the performance , but we found that to improve this by alternating the transmission of contiguous in general in a net loss of throughput . Considerable gains are achievable if the interference of the neighboring is , but at the expense of added transmission and extra overhead . Our assumed service environment significantly higher spectral by a factor of or than by the standard model . This is because we that same cell links had significantly less path loss than 
  
 Fig . . Per user efficiency versus power constraint comparison empirical and data for availability . 
 by the model when the is at indoor free of significant blockage to the for example close to an outdoor facing window . The resulting advantage in spectral efficiency that choosing an adequate position for the in combination with indoor , will provide considerable gains over to directly serve indoor at all . 
  
  
 ﻿This paper presents closed-form expressions for the space-frequency (SF) second-order statistics for the power gain of indoor wideband channels. These expressions hold for channel models in which multi-path components are clustered, both in their arrival angles and their arrival times. Assuming that the arrival times are independent of the angles of arrival, we derive a closed-form expression for the covariance between the channel power gains at two receiving antenna positions d meters apart, at any pair of frequencies ?1 and ?2. This expression reveals that increasing d yields the same SF covariance asymptotic reduction attainable by increasing |?1 - ?2|, only if, and that the covariance is symmetric in its dependence with ?1 -?2 and with ?1+?2. Thus, the channel power gain at higher frequency bands has less variation than at lower frequencies. Finally, we show that the variable part of the SF covariance decays approximately as 1/d.
 INTRODUCTION
 The propagation of radio signals within indoor environments is a complex phenomenon, depending on the operating frequency, the type and location of the antennas and the presence of a usually large number of scatterers. Assuming linearity and that the channel conditions vary slowly, the channel behaviour is fully captured by its frequency response, H(j?), corresponding to the Fourier transform of its time-invariant channel impulse response h(t).
 Due to multiple reflections in the surrounding scatterers, h(t) is typically composed of a sequence of short impulses with random arrival times and amplitudes, which tend to decrease with arrival time. It has been observed that, in indoor scenarios, both the arrival times and arrival angles of these multi-path components (MPCs) distribute in clusters (see [1]–[3] and [4]–[7], respectively). As a result, given any two MPCs known to belong to the same cluster, one cannot assume their times to be independent. The same is true for their angles of arrival and for their corresponding amplitudes.
 The analysis of the space-frequency statistics of the indoor wireless channel has commonly been approached by studying the second-order statistics of the complex frequency response H(j?) or of its magnitude. The former case involves the complex correlation
 	?(1c ,2)(?1,?2) 	 E[H (j? )H (j? )*]	,
 This work was supported by the Chilean Research Council CONICYT under Projects 1095018, 1120468 and Basal project CCTVal (FB0821), and by the Research Direction of Universidad Tecnica Federico Santa Mar´ ´ia.
 978-1-4799-3083-8/14/$31.00 ©2014IEEE
 where H1(j?1) and H2(j?2) are evaluated at receiver (or transmitter) antenna positions x1 and x2, respectively, and where ()* denotes complex conjugation. This complex correlation, with ?1 = ?2, has been studied in [8], [9], empirically, and in [10], analytically. In both [8], [9] it is found that ?c decays with the separation d between x1 and x2 more rapidly when the operating frequency is increased. The analysis in [10] obtains closed-form expressions for ?1c,2(?,?) for non-isotropic three- and two-dimensional diffuse fields assuming independent scatterers, i.e., that the complex channel gain associated with MPCs from one direction are independent of those arriving from another direction. Under this assumption, it was shown in [10] that for the 2-dimensional case,
  
 where c is the speed of light,  is the complex Fourier series coefficient of the normalized distribution function of scattered power P(f), Jm(·) is the m-th order Bessel function of the first kind, and f12 is the angle of the vector x2 - x1 with respect to a reference azimuth direction.
 The second case (envelope correlation) is concerned with characterizing E[|H1(j?1)||H2(j?2)|] and the corresponding envelope correlation ?1e,2(?1,?2). The dependence of the latter expectation on the separation d has been studied in [9], [11], [12] from measured data and for ?1 = ?2. These studies reveal that, as expected, ?1e,2(?,?) decays as the separation d is increased, although it seems to approach a minimum, asymptotic value in the range 0.3–0.5, which is significantly larger than the lower asymptote of ?1c,2(?,?) [9].
 Our focus in this paper is on the channel power gain |H(j?)|2 and its second-order statistics, associated with E[|H1(j?1)|2 |H2(j?2)|2]. Considering the latter expectation (instead of that of the complex gains or their magnitudes) is relevant for two main reasons. First, the instantaneous channel power gain at a given frequency determines the narrowband associated capacity. Hence, characterizing in closedform the second-order SF statistics of |H(j?)|2 would allow for a better estimation of the performance of spatial diversity systems based upon, for example, maximum ratio combining with branches associated to different antenna positions and/or carrier frequencies. Second, in schemes based upon selection combining, as well as in multiple access channels subject to the power capture effect [13], the performance depends on the joint statistics of two or more power values, arising from different antennas and carrier frequencies.
 In this paper, we find closed-form expressions for the second-order statistics of the channel power gain |H(j?)|2, associated with the SF-autocovariance
  
 These expressions depend explicitly upon the frequencies ?1, ?2 (not necessarily equal) and the separation distance d. Our analysis is valid for an extended Saleh-Valenzuela model, with MPCs with clustered arrival times and angles, as in the models considered in [4], [5]. The obtained expressions reveal how c1,2(?1,?2) decays as d and |?1 - ?2| are increased, but predict a strictly positive lower asymptote, which is approached by letting |?1 - ?2| go to infinity, for any d, and by letting d ? 8, but only if .
 In other words, there exists a SF-dependent additive term in c1,2(?1,?2) and, in the limit, increasing frequency or spatial separation asymptotically reduces this term to zero, yielding the same asymptotic autocovariance reduction. A second finding is that c1,2(?1,?2) behaves in exactly the same way with respect to ?2 - ?1 as it does with ?1 + ?2. The latter implies, by considering the case ?1 = ?2, that the power gain of narrowband channels with higher central frequency have smaller variability. On the other hand, the obtained expressions show that, beyond separations of a few central-frequency wavelengths, the SF-dependent component in c1,2(?1,?2) decays with d approximately as 1/d.
 In the next section, we present the channel model and assumptions. The main results are derived in Section III, followed by conclusions. All proofs are presented in the Appendix.
 CHANNEL MODEL
 As in [4], [5], the wireless indoor channels considered here are characterized by their impulse responses. Each impulse response consists of a sequence of multi-path components (MPCs) with random amplitudes, arriving at random times and angles, and where arrival times and angles are clustered. At a given reference location x0 in space, such random impulse response h0(t) can be written as  
 	 	(2)
 where ti,m is the random arrival time of the m-th (MPC) in the i-th cluster and ai,m is the corresponding (real valued) MPC amplitude. Our assumptions associated with the arrival times, amplitudes and angles of MPCs are described next.
 A. MPC Arrival Times
 The arrival times can be decomposed as ti,m  Ti + ti,m, ?i,m ? {1,2,...}
 where Ti = 0 is the random arrival time of the i-th cluster, and ti,m = 0 is the random delay of the m-th MPC in the i-th cluster relative to Ti, where T1 = T2 = ··· and ti,1 = ti,2 = ···, for all i. By definition, the cluster begins with its first ray, and thus
 	ti,1 = 0,	?i ? {1,2,...}.	(3)
 In the forthcoming analysis, it will be useful to describe the overall distribution of arrival times by their corresponding arrival density functions. This requires one to introduce the following preliminary notions and notation. For an infinite sequence of random arrival times , with x1 possibly
 being deterministically equal to zero, let  
 denote the number of arrivals from   falling inside
 [x,x + ?). Define, for i ? {1,2}, the arrival densities
  
  ? 	i	i	, if 
 , if y = z
 We can now define the single and joint arrival density
 functions of  as
 if x1 is random
 , if x1 = 0 deterministically   is random
  , if   x1 = 0 deterministically
 B. MPC Amplitudes
 The random MPC amplitudes ai,m in (2) are assumed to be of the form
 	ai,m = pi,mAiai,m,	?i,m ? {1,2,...}.
 In this expression,  is a random variable representing the amplitude of the i-th cluster. The real-valued random variable ai,m = 0 is the amplitude of the m-th MPC within the i-th cluster relative to that cluster’s amplitude. The polarity factors {pi,m} take values from {-1,1} with equal probability [2].
 The cluster amplitudes   are independent given their arrival times  . As in [14], the dependency of the cluster amplitudes upon their arrival times can be characterized by the cluster mth-order moment delay profile functions
  
 if T1 = 0 deterministically, and bm(x) = E[Amk |Tk = x],k ? {1,2,...}
 if T1 is random. Likewise, the MPC relative amplitudes   are independent given , and we define
 their moment-delay profile functions as
 gm(x)  
 where m ? {1,2}, for every i ? N. As we shall see, here we will only need to consider the orders m ? {1,,...,4}.
 C. MPC Arrival Angles
 Let ?i,m be the angle of arrival, measured with respect to a reference vector d on the horizontal plane, of the MPC with arrival time ti,m, As in [4], [5], the angles of arrival of the MPCs within the k-th cluster can be decomposed as
  
 In this expression, ?˜k is the angle of arrival of the k-th cluster, while, for each   are the angles of arrival of the
 MPCs relative to the k-th cluster’s angle of arrival.
 If the transmitter and receiver positions and the scattering environment are chosen at random, then the cluster angles {?˜k} will be uniformly distributed, as observed in [6], [7]. Likewise, the absolute angles  will also be uniformly distributed. Moreover, the measurements in [6], [7] reveal that, in indoor scenarios, the distribution of the difference between cluster angles and the angle of the first cluster in each location is also uniform. This suggests that, conditioned to a given location (room and building), cluster arrival angles can be assumed independent. In contrast, we shall not assume that the angles   are independent, since the MPCs within each cluster have been found to arrive from a relatively small range of angles [6]–[8]. Instead, we will assume that, for all k, the relative angles   are independent and identically distributed.  More precisely, we will work based upon the following assumption:
 Assumption 1 (Statistics of arrival angles):
 The cluster arrival angles {?˜k} are independent and uniformly distributed over [0,2p).
 The MPC relative arrival angles {?¯i,m} are independent, identically distributed with probability density function f?¯ : [0,2p) ? [0,8).
 Cluster arrival angles and MPC relative arrival angles are independent, i.e., {?¯k,m} ? {?˜k}.
 All angles {?¯k,m} and {?˜k} are independent of  and of  .
 Let us momentarily write (2) in single-index notation as
 	 .	(2)
 In any given indoor scenario, the impulse response from transmitter to receiver will depend on their corresponding antenna positions. Suppose the impulse response (2) is obtained when the receiving antenna is located at a given point x0 on the horizontal plane. If we consider two additional receiver positions, x1  x0 + d/2, x2  x0 - d/2, then their MPC arrival times will increase or decrease, depending on their arrival angles. More specifically, if the separation   is small relative to the distance from the scatterers, then the arrival angles can be considered to be the same at x1 and x2. In such case, an arrival time tk at position x0 becomes tk - sk and tk + sk, at positions x1 and x2,
 respectively, where
 	 	(3)
 is the random delay increment, in seconds, for the k-th MPC produced by moving from x1 to x0 (or from x0 to x2) and c = 3 × 108 [m/s] is the speed of light. Thus, the channel frequency responses for receiver positions x1 and x2 are
  
 For the above model and with only the assumptions given within it, we will next derive closed-form expressions for the second-order statistics of |H(j?)|2, over both space (separation) and frequency.
 SECOND-ORDER STATISTICS OF |H(j?)|2
 In this section we will derive a closed-form general expression for the space-frequency covariance c1,2(?1,?2) defined in (1). This result is formally stated as follows:
 Theorem 1: Under the assumptions presented in Section II, let r and ? be two random variables distributed as
   Then,
    
 where,
 	 	(5d)
 
 ?¯¯t(x,y)g2(x)g2(y), ?(x,y)  ?¯¯T(x,y)b2(x)b2(y) and where  denote, respectively, the Fourier transform and the L1 norm of the function f. 
 Proof: We first note that
 E
 (6)
 where (a) holds from the fact that ak = pkAkak and that the polarities {pk} are zero mean, i.i.d, and independent of all the other variables. On the other hand,
 E 
  
 Since each amplitude ak = pkAkak and the polarities {pk} are i.i.d. with zero mean, it follows that all terms of (7) in which a given index is different from all the others will be zero. This leaves only the following index combinations:
  
 Thus,
 E 
 (8)
 Notice that letting   and   and returning to the double index notation (for clusters and MPCs within clusters), the function  can be written as,
  E 
  
 From Assumption 1 we have that the random variable   is identically distributed for every k and for every
  ,n. Therefore, E , for all k, ,n.
 Likewise, it follows from Assumption 1 that if 
 E[e , for all  ,n. Substituting these results in our previous expression
 for  we obtain
 The sums corresponding to T0, T1 and the functions T3(·) and T4(·) have been characterized in [14, proof of Theorem 1]. Proceeding as in the latter proof, it is easy to show that these functions are as in (5) (please see [14] for details). Substituting (6), (8) and (9) and into (1), we obtain (4), completing the proof.	 
 Theorem 1 reveals some interesting properties of the space-frequency covariance c1,2(?1,?2):
 The difference T1 - T02 > 0 is the variance of , i.e., the variance of the impulse-response energy. This is the only term which does not depend upon ?1,?2. As already discussed in [14, Remark 3], under mild conditions on the impulse response statistics, the functions T3(x), T4(x) on the right-hand side of (4) vanish as x ? 8. The latter means that, in our case, the space-frequency covariance does not go to zero as the frequency separation grows unbounded. Interestingly, the expression for c1,2(?1,?2) here obtained shows that this covariance asymptote (given by the term T1 - T02) is the same regardless of the separation d.
 The expectations of the complex exponentials involving d in (4) suggest that, as expected, increasing the separation between the two receiver antenna positions decreases the covariance of the channel power gain between any pair of frequencies such that  
 and  . Moreover, if the factors   E , is increased (which requires     ), then and E  should vanish as d
 the resulting asymptotic covariance reduction would be exactly the same that would be obtained by letting |?1 - ?2| ? 8.
 We now show that, as expected, the frequency-dependent terms in (4) do vanish as d ? 8, if O and T are non-zero. However, it turns out that the decay is not faster than 1/d.
 Proposition 1: Suppose Assumption 1 holds and let the random variables r and ? distribute as in Theorem 1. Then,
  [e	(10) E(11)
 where Jn(·) is Bessel’s function of the first kind and order n and
  
 are the complex Fourier series coefficients of f?¯(·).  It is well known that the Bessel functions {Jn(z)} are oscillating and that their envelopes decay as z-1/2. Hence, Proposition 1 reveals that the envelope of   E  decays as 1/d. The decay rate of E[ejOdr] with respect to d actually depends on the Fourier transform coefficients of f?¯. For instance, if ?¯ takes a deterministic value, then Cn(f?¯) = 1,
 ?n, and we obtain E[e   (the last equality follows from Parseval’s theorem and noticing that {jnJn(z)} are the coefficients of the Fourier series of ejz cos(f), see (14) in the proof of Proposition 1 below). On the contrary, if ?¯has a uniformly-bounded probability density function, then, when d ? 8, E[ejOdr] goes to zero as 1/d or 1/d2, but not faster than that. These results are formally stated and proved in the following proposition:
 Proposition 2: Suppose the conditions of Proposition 1 hold. Then, for every , there exists  such that
 	 	(12)
 If, in addition, , then for any , there exists   such that
  
 
 As a consequence of Proposition 2, we have that the spaceand-frequency dependent part of c1,2(?1,?2) (see (4)) decays with increasing d, asymptotically, as 1/d. Given the behaviour of the Bessel functions {Jn} and if the Fourier series coefficients of f?¯(·) decay sufficiently fast with n, then c1,2(?1,?2) will decay approximately as 1/d for d larger than a few wavelengths of the frequency difference O (the latter being given by 2pc/O), since |O| < T for ?1,?2 > 0 (see (4)). Likewise, for a fixed separation d, the two parts of this term decay as 1/O and 1/T, respectively, for relatively large values of O and T.
 CONCLUSIONS
 We have derived closed-form expressions for the spacefrequency (SF) second-order statistics of the power gain of indoor wideband channels. These expressions were obtained for the extended Saleh-Valenzuela channel model, wherein multi-path components arrival times and arrival angles are clustered. Our expressions reveal that the SF autocovariance c1,2(?1,?2), associated with two receiving antenna positions x1, x2 with channel power gains |H1(j?1)|2 and |H2(j?1)|2, respectively, is symmetric with respect to T = ?1 + ?2 and O = ?1 - ?2. Sine the autocovariance diminishes when O is increased, it follows that the variance of |H(j?)| also diminishes as ? grows. We have also shown that the SF-dependent part of c1,2(?1,?2) goes to zero as O and T go to infinity, which also happens if O and T are non-zero and we let d ? 8. In addition, we have demonstrated that the reduction of this SF-dependent term with d is approximately proportional to 1/d, for values of d larger than a few wavelengths of the frequency difference O.
 APPENDIX
 Proof: [Proof of Proposition 1] From the distribution of r and ? (see the statement of Theorem 1), we can construct r and ? as
  
 where ?˜ and ? are uniformly distributed over [0,2p), and ?¯1 and ?¯2 are independent and identically distributed according to the intra-cluster arrival-angle distribution with probability density function f?¯(·). From this,
 E ,
 which proves (10). In order to characterize E[ejrOd], we first recall the identity [15, p. 211]
 8
 e(14) n=-8
 from where it follows that, for any fixed z and f,
 E .
 Using this result we can express E  as follows:
 E 
 = E 
 which corresponds to (11). This completes the proof.	 
 Proof: [Proof of Proposition 2] For real-valued z, it holds that (see [16, section 9.2])
 Jn(z) =  pz cos(z - p2 n - p4 ) + O(z-1).
 Thus,
  
 Substituting (15) into (11), we obtain
 E[ejOdr]
 where the last equality holds because, by Parseval’s theorem,
 , and because   is bounded. Rearranging terms in (16) leads directly to (13), completing the proof.	 
 ﻿In this work we present an empirical study of the added propagation losses that may be associated with providing fixed wireless service from near-ground base-stations to homes in a suburban environment. We present results for various types of environments, classified according to the existence of obstructions in the propagation path and the choice of outdoor-outdoor or outdoor-indoor service. Our results indicate that while on average the additional path-losses associated with lowering the base antenna are relatively small, the variance of these losses will increase at near-ground level, particularly in obstructed links. This has as a result that the power margin required for high availability of a near-ground base antenna may be quite significant.
 Index Terms—Channel modeling, fading channels, fixed wireless service, near ground propagation, path-loss measurements.
 I. INTRODUCTION
 T
 HE explosive growth in wireless broadband demand creates an intense interest in diverse deployment options. New players, eager to participate in this very attractive market are considering various scenarios, including suburban residential areas where services (electricity, telephone, cable TV, etc.) are being supplied from underground, without the use of lampposts. This creates the need to also evaluate the effectiveness of providing wireless service from outdoor near-ground bases. We validate the applicability of well known path-loss and small-scale fading models, which will be useful when designing such systems. To the best of our knowledge, such studies have not been reported in the open literature for this type of environment. Most published results with near-ground bases (less than 1 m height) deal with urban environments or correspond to sensor networks in open or forested spaces, as will be discussed in detail below. Suburban areas have received less attention. Furthermore, as stated in [1], [2] there is a lack of results on the characterization of outdoor-indoor wireless links, which are important for voice/data transmission and wireless local area networks. The statistical description of the variation in the channel model parameters, associated with lowering the base-station (BS) will be useful for a system designer, considering that most currently available data corresponds to service provided by bases that are at least at lamp-post height, where well established propagation models apply. The scenario that is the objective of our study differs quite significantly from those that have been reported in the literature before. Therefore the characterization of this new propagation scenario cannot be based on currently known results.
 A thorough bibliographical search of published work related to the subject led us to classify the relevant papers into 4 categories. Each category deals with one specific aspect covered in our study: outdoor-indoor propagation, near-ground propagation, the effect of vegetation barriers, and path-loss models. We discuss this classification in what follows, stressing only those aspects that specifically relate them to our work.
 Outdoor-Indoor Propagation: Various papers such as [3][5] and references quoted therein have addressed this subject for settings that in some cases are similar to ours. However in contrast to our study, in all of them the BS was located at heights of at least 2.75 m, while in some ([3], [4]) the altitude of the subscriber unit (SU) was varied, to altitudes not lower than 1.5 m. We note that due to the considerable difference between the scattering elements surrounding an outdoor BS and those in proximity of an indoor SU, lowering the altitude of the latter cannot be expected to have the same effect as that of lowering the BS. However some reported observations are similar to those in our work, such as a wider range of fade values for the lower altitude antennas.
  2012 IEEE
 Near-Ground Propagation: This topic has been addressed in various papers, many of which are centered on modeling electromagnetic field propagation, such as [6]-[8]. Here the scope is sensor networks in indoor scenarios with both terminals at less than a 10 cm height and link lengths below 10 m. The analytical and empirical results address very specific propagation settings, which can be treated analytically from electromagnetic principles and do not lend themselves to the derivation of statistical models. Conversely, our study encompasses a wide range of scenarios of such complexity that they cannot be treated analytically and instead require statistical models such as those described in [9], [10]. Propagation losses for sensor networks in an office environment are treated in [11], again a setting too far removed from ours to extrapolate valid conclusions. Models for outdoor sensor network propagation losses with both terminals at low altitude over diverse types of open terrains have been presented in [12] and [13]. In [14] and [15] path-losses for urban environments using low-altitude bases are reported in the context of military applications. The effect of lowering the subscriber terminal from an altitude of 28 to 3 cm is quantized in [15], however this scenario is very different from our application. Nevertheless, among the reported results is the reduction of the Ricean K-factor with antenna altitude, a result to be compared with that observed in our work.
 Effect of Vegetation Barriers: Another element that influences the signal attenuation between base and user is vegetation. Theoretical models describing the effect of these barriers have been discussed in [16]-[21]. The effect of vegetation barriers on frequency dispersion was treated in [22] based on various simulation models. Only some of the available theoretical models for the attenuation caused by vegetation have been validated through empirical data [16]. None of them have been developed for the residential scenarios studied here, but some results relate to certain conditions present in our study. In [23] military applications are considered with bases at heights exceeding 0.75 m and links that can involve forested areas. In this work a decrease in antenna height is also reported to reduce the Ricean K-factor. Empirical studies reported in [3], [23]-[26], which involve various types of vegetation including rainforests, show excess path-losses between 10 dB and values exceeding 20 dB.
 Path-Loss Models: With regard to the statistical description of path-loss, single and multi-slope log-distance models that consider shadow and small-scale fading have typically been used. This is also the approach chosen for this work. In the case of outdoor propagation spaces, the two-ray model is often considered as a reference [14], [27]-[32]. The presence of a break point leading to the dual-slope path-loss model has been discussed extensively [27], [28], [32]-[34]. The use of these models and the effect of lowering the BS antenna in a microcellular setting is described in [27], however the altitudes considered range between 3.2 and 13.4 m and measurements were carried out on streets rather than in outdoor-indoor settings.
 The aforementioned papers provide a background with regard to methodology and previous theoretical and empirical results on modeling propagation links that may involve a terminal at low altitude. As discussed, they do not specifically address the problem that is the objective of our study, namely to draw conclusions regarding the effect of providing suburban outdoor-outdoor or outdoor-indoor residential wireless service from low-altitude bases, in contrast to doing so from a conventional height. We here consider both outdoor-outdoor and outdoor-indoor narrowband links, with and without the presence of vegetation. These results will be useful when planning fixed wireless service to suburban residences using narrowband channels, which may also be viewed as a few adjacent tones in an OFDM system. In such cases of course, the relation between coherence bandwidth for the specific environment and the transmission bandwidth will also need to be considered in order to take into account possible fade reduction through frequency diversity [35].
 We found that the additional propagation losses resulting from lowering the base antenna from 2.1 m to 0.3 m was not larger than 4 dB at the median level, but may grow to 17.5 dB when 90% availability is required. We also quantify penetration losses when serving indoor locations from the outdoor antenna, and power losses caused by vegetation barriers within the link. The latter are relevant when the lowering of the base antenna adds such an obstruction to the link. The measurement campaign was carried at 3.5 GHz using a narrowband transmitter and a power-measuring receiver. The choice of frequency is based on the fact that it has been used for fixed wireless service such as WIMAX [36]. A total of 23 different links were tested in 5 residential urban settings. For each link, the distance between base and subscriber units ranged from 5 to 40 m. Two large-scale propagation models were used as a comparison basis to our empirical model: Friis equation and the two-ray model [9].
 The remainder of this paper is organized as follows: Section II describes the measurement hardware and methodology. Section III presents the statistical models based on our empirical data. Finally, Section IV provides the conclusions.
 II. MEASUREMENT EQUIPMENT, SCENARIOS AND
 PROCEDURE
 The channel sounding system employed consisted of a synthesized continuous wave (CW) transmitter and a purpose built narrowband receiver coupled to a power meter. The receiver bandwidth is 200 kHz. This is wide enough to capture any frequency dispersion that affects the CW transmission, induced for example by movement of vegetation [37]. This receiver was connected to a rotating arm of 0.4 m length and moved stepwise under computer control in 6? increments (a displacement of approximately ?/2). At each of the 60 angular positions, 100 consecutive power measurements were made. This allowed verifying consistency and averaging to remove residual temporal fades. These were found to be very small (typically less than 0.5 dB). The resulting timeaverage was used as the power sample at that range and angular position. At each placement of the rotating arm, the 60 power measurements obtained in a rotation were used to calculate the spatially averaged path-loss at that range and to generate the statistics of small-scale fades with respect to that average. The size of the region for the spatial average is well within the shadow-fade correlation distance reported in previous work [38]-[40]. In fact the smallest values mentioned in the literature, corresponding to indoor scenarios, are in the range of 1 to 2 m [39], the values for outdoor suburban settings being much larger [38], [40].
 The operating frequency was 3.5 GHz. The BS and SU used dipole and patch antennas depending on the type of scenario tested. When the angular spread of arriving wavefronts in a multipath environment is comparable to the antenna beamwidth, the received power and thus the calculated pathlosses will be affected by the antenna gain pattern. For this reason, in selecting the antennas we chose elements that may be considered representative for the type of service at the specific scenario, as will be detailed later. In the case of the nominally omnidirectional antennas, we took into account in our calculations the minor gain variations (less than +/1 dB) over the 360? azimuth range through averaging of the measured antenna pattern. This is in correspondence with the fact that our measurements involve a 360? rotation. In the case where we used patch antennas, in our path-loss calculations we considered the measured gain of the antenna in the direction of the direct arrival path. All antennas were measured in an anechoic chamber, and automated measurement procedures were used to ensure repeatability.
 Before initiating measurements at each environment, two calibration procedures were performed. First, the transmitter and receiver system were connected back to back with a short, calibrated cable and a step attenuator, bypassing the antennas, to verify the expected received power. Next, the transmitter and receiver antennas were connected to the system and a free-space calibration was performed in an open area with the antennas extended 2.1 m above ground and separated by 2 m. The results were compared to the design specifications of the equipment to confirm its proper operation and to verify the dynamic range for reliable measurements. In all field measurements the received power was at least 20 dB above the noise floor. Measurements were carried out at 5 residential type settings, representative of a suburban environment where the base-station could be placed at the street curb, at a range of 5 m to 40 m from a one or two story house. This space was occupied by gardens with shrubs and trees no higher than 5 m. We tested suburban locations in Santiago and Vin˜a del Mar, Chile, and on the campus of Universidad Santa
 Mar´ia in Valpara´iso, which provides settings with garden-like vegetation outside of ground level laboratories. At each of these scenarios, multiple measurements were carried out for similar conditions (LOS: line-of-sight / NLOS: obstructedLOS; outdoor-outdoor / outdoor-indoor). The construction is of the brick and mortar type, with non-metalized windows of varying size as specified below.
 All measurements were made by first placing the antenna that simulates the wireless service base-station at 2.1 m and then at 0.3 m at exactly the same spot. This allows quantifying the effect of reducing the antenna altitude. The choice of 2.1 m as reference height was based on the fact that at such altitude, non-obstructed links exhibit practically freespace path-losses at the measured distances, thus providing a convenient reference value. Further increases in height would have added complexity without providing novel information.
 The measurement procedure at each type of setting was as follows:
 1)	LOS outdoor-outdoor: In this case, we emulate the BS terminal by placing the rotating arm (receiving the CW signal) outside the home at varying distances from the residence, with link lengths in the range 5-30 m. The SU in this case is designed to represent a customer premises terminal mounted on the wall of the residence, being serviced by the BS. The SU was placed in close proximity to an outside wall at an altitude of about 2.1 m. It was placed close to a window that would subsequently allow outdoor-indoor measurements through that same window without otherwise changing the setting. In this case, the SU used a patch antenna with a maximum gain of 8 dBi, typical for a wall-mounted element. This was placed at three different positions along the wall (for each link length tested) to measure at nominally similar
  
 Fig. 1.	Measurement configuration for the outdoor-outdoor case.
  
 Fig. 2.	Measurement configuration for the outdoor-indoor case.
 locations. The BS on the rotating arm, whose height was varied from 0.3 m to 2.1 m employed a 2 dBi dipole. The setup is illustrated in Fig. 1.
 2)	LOS outdoor-indoor: The setup is shown in Fig. 2. In this case, the SU terminal was emulated by placing the rotating arm inside the construction across a window with respect to the outside BS (transmitting the CW signal). Link lengths ranged from 8 to 33 m. The height of the SU antenna was 1.5 m, at locations that would in practice correspond to the possible positions of indoor pedestrians or of repositionable user terminals. The indoor floor level X shown in Fig. 2 was between 0.15 m and 0.4 m above ground. The BS antenna was placed outside of the residence at positions adequate for base-station locations, again with altitudes of 2.1 and 0.3 m. In this case, both antennas used were dipoles. Window sizes varied in width in the range of 1.2 m to 3.2 m and in height from 1.8 m to 2.5 m. The minimum window area was 2 m2.
 3)	NLOS outdoor-outdoor and outdoor-indoor: These measurements were performed under the same conditions as in the scenarios LOS outdoor-outdoor and LOS outdoor-indoor respectively, with the only difference that the direct path was obstructed by the foliage of one or two rows of shrubs and small trees. These were usually low, densely foliated hedges of the genus “ligustrum” (privet), while trees were typically “Aesculus hippocastanum” (horse chestnut). The thickness of the foliated obstruction was typically in the range of 0.5 m to 1 m. The link lengths tested ranged from 6 to 40 m.
 III. MEASUREMENT RESULTS AND ANALYSIS
 This section presents the results of the measurements performed. We describe models for the path-loss (PL) in each of the previously described settings and we include a thorough statistical description of the effect on the channel model that is a consequence of lowering the base-station antenna.
  
  
 (a)	BS at 2.1 m.
  
 (b)	BS at 0.3 m
 Fig. 3. PL Measurements, LOS outdoor-outdoor.
 A. Path-Loss, Contrast with Physical Model
 1)	LOS outdoor-outdoor: We here present the average pathloss (over one receive-antenna rotation) vs. distance for all measured links of this type, at the two base-station heights considered. This is shown in Fig. 3. In each case the plot includes as a reference the free-space path-loss (Friis equation) and the path-loss obtained using the two-ray model [9]. In the latter case, two conditions for permittivity () and conductivity (?), that cover the range of values mentioned in the literature [41]-[44] have been considered. Although these dielectric parameters are frequency dependent, their change -even over bands of 100 MHz or more- will not place them outside the range of values considered for our models [44]. As seen in Fig. 3 the theoretical two-ray path-loss model is not very sensitive to the actual dielectric parameter values and thus frequency dependent effects will be negligible. The fit of empirical data to the theoretical models (free-space and tworay) is remarkably good. The dispersion of the data points with respect to the free-space model is comparable to the difference between the free-space and the two-ray model losses. It should also be mentioned that the path-losses plotted here exclude small-scale fades, which as mentioned before were averaged out. However as will subsequently be shown, for these scenarios the small-scale spatial fades were typically quite shallow (high spatial Ricean K-factor). Consequently, the total path-loss at any specific antenna position within the rotation is quite close to the average plotted here. It is also interesting to note that the measurements follow the contour of the two-ray model particularly at the ground level, even
  
 (a)	BS at 2.1 m.
  
 (b)	BS at 0.3 mFig. 4. PL Measurements, LOS outdoor-indoor.
 though measurements were made in a variety of terrains, some of which included height irregularities of the order of one wavelength. In this setting, lowering the base antenna caused no significant change in path-loss, as will be discussed in greater detail in subsection C. It is worth noting that for both heights, the first Fresnel zone was not blocked.
 2)	LOS outdoor-indoor: As explained in Section II, for measurements in the LOS outdoor-indoor scenario, the movable arm antenna was located inside the residence at a height of 1.5 m, emulating user positions. Several indoor placements were used, all of them possessing LOS to the outside basestation antenna through a single window. The transmitting antenna, outside of the residence, was again located at 2.1 m and at 0.3 m height. The measured path-losses at both heights are shown in Fig. 4, where we also include free-space and two-ray model path-losses as a reference. As expected, outdoor-indoor path-losses are somewhat larger than those obtained for the outdoor-outdoor case and the dispersion of values is also larger, which may be attributable to the variety of window sizes considered. There is however no evidence that lowering the base antenna will alter the path-loss model in a significant way.
 3)	NLOS outdoor-outdoor and outdoor-indoor: Both NLOS cases considered here correspond to situations where the direct path was obstructed by one or two barriers of typical garden vegetation, basically bushes and small trees as previously mentioned. Fig. 5 shows the results for the NLOS outdoor-indoor scenario. The scarcity of data points between 11 m and 13 m is produced by the existence of a row of bushes
  
 (a)	BS at 2.1 m.
  
 (b)	BS at 0.3 mFig. 5. PL Measurements, NLOS outdoor-indoor.
 at this distance from the indoor antenna in some of the places where measurements were made. Additional vegetation existed at closer ranges from the window. Therefore, measurements at ranges in excess of 11 m were in many cases obstructed by two vegetation barriers. The results show added attenuation and larger dispersion of values than in the previous cases, a logical consequence of the greater variability of the environment. In this case, lowering the base antenna results in somewhat greater losses, as well as greater uncertainty in their values.
 B. Path-Loss, Statistical Modeling
 The extensively used log-normal model [10] assumes that the spatially averaged path-loss (in dB) may be described as:
  
 where d denotes distance between the base-station and subscriber unit, PLdB(d0) is the free-space path-loss at a distance d0 in dB units, n is the path-loss exponent and Xs is a zero-mean Gaussian distributed random variable (in dB) with standard deviation s. We applied this model to our data after averaging out small-scale fades over one rotation of the arm that holds the antenna. As conventionally done, we chose d0 = 1 m. Variations of the model (1), using multiple slopes or choosing the intercept point PLdB(d0) different from the free-space path-loss did not provide a measurably better fit for the set of link lengths tested, therefore we only used this very simple model.
 TABLE I
 SUMMARY OF PATH-LOSS EXPONENT n AND STANDARD DEVIATION s FOR VARIOUS TYPES OF ENVIRONMENTS AND ANTENNA HEIGHTS
 Type of Environment	n	s[dB]
 	2.1 m	0.3 m	2.1 m	0.3 m
 LOS	Outdoor-Outdoor	2.1	2.0	1.3	2.3
 	Outdoor-Indoor	2.3	2.4	4.3	3.7
 NLOS	Outdoor-Outdoor	2.6	2.8	4.1	4.3
 	Outdoor-Indoor	2.7	3.0	4.3	6.1
 We generated the empirical Cumulative Distribution Function (CDF) of the differences ?P between the best-fit linear regression (1) and the measured average path-losses. The CDF of ?P would be that of a zero mean Gaussian random variable (i. e. Xs), under the assumption that the log-normal model holds. We found that in fact the fit was very good, the corresponding graphs of the CDFs not being included here to meet space constraints. The results indicate that in general lowering the base antenna will increase the standard deviation of ?P, i.e., the model path-loss prediction becomes more uncertain. Having established that the log normal model is indeed accurate for our scenarios, we summarize in Table I the relevant parameters of the model (1) for all the studied cases.
 Our results so far have excluded small-scale spatial fades, which as stated were averaged out over the rotation of the arm with the receive antenna. Our analysis of the differences between the average path-loss and the path-losses measured at the various arm positions revealed that the corresponding histograms are very well matched by Rice/Rayleigh probability distributions. We thus use the Ricean K-factor as a metric to determine the small-scale statistics, estimated from the empirical data according to the method described in [45]. Fig. 6 shows the CDF of the observed Ricean K-factors for both heights and for the scenarios LOS outdoor-outdoor and NLOS outdoor-indoor, which provided respectively the conditions for the smallest and largest (small-scale) fade values among all scenarios tested. As can be seen, no significant differences in the small-scale fade statistics are associated with the changes in base-station altitude. As expected, small-scale spatial fades are much shallower for the LOS outdoor-outdoor case than for the far more complex NLOS outdoor-indoor environment.
 C.	Path-Loss Variation with Base Altitude
 We here present the statistics of the difference in path-loss associated with lowering the base antenna from the 2.1 m position to 0.3 m. We define this difference ?PL as
 	?PL[dB] = PL0.3- PL2.1,	(2)
 where PL0.3 and PL2.1 are respectively the path-losses in dB for base antenna altitudes of 0.3 and 2.1 m. These are obtained at exactly the same azimuth position of the rotating arm for both antenna heights, i. e., without any small-scale path-loss averaging.
 Fig. 7a shows the CDF of ?PL for the cases LOS outdoor-outdoor and NLOS outdoor-indoor. As before, these correspond to the two extreme conditions observed, the remaining curves falling between the two shown. The ensemble
  
 Fig. 6. CDF of Ricean K-factors at heights 0.3 m and 2.1 m for LOS outdoor-outdoor and NLOS outdoor-indoor scenarios.
 size for our calculated CDFs ranges between 2400 and 4300 points depending on the amount of measurements in each category. As can be seen, for the LOS outdoor-outdoor case, lowering the base antenna will result in a slight decrease in path-loss when considering the median ensemble value. This is consistent with what can be observed in Fig. 3b, which shows that at some ranges the path-loss is decreased with respect to the free-space value by the constructive interference of the ground ray. There is a clear increase in the median path-loss for the more complex NLOS outdoor-indoor scenario. For the latter case, as already discussed, there is a larger variability of path-losses with respect to the mean, particularly at low altitude. The result of this can also be observed in Fig. 7a. This figure reveals that a considerable fade margin may need to be added to the link budget if adequate availability is to be preserved when lowering the base antenna. This added margin grows from about 3 dB for the LOS outdoor-outdoor case to approximately 17.5 dB for the NLOS outdoor-indoor case at the 90% probability level.
 In many cases it may be more informative to have statistical knowledge on the increase in path-losses with respect to a well-known and deterministic reference, rather than with respect to losses at 2.1 m, which are random. Choosing as reference the free-space losses at the same distance (d), we thus obtained the statistics of the excess path-loss (EPL). This is defined as the difference in decibels between the path-loss PLH measured at height H at any given azimuth position of the receive antenna and the corresponding pathobtained from the Friis equation according to
 	Friis = 20log10 4?	.
 	EPL[dB] = PLH - PLFriis	(3)
 Fig. 7b shows the CDF of the EPL for the LOS outdoor-outdoor and NLOS outdoor-indoor scenarios, for the two antenna heights considered. Consistent with our previous results, for the LOS outdoor-outdoor case the values of the EPL are quite low for both antenna heights. At 0.3 m the EPL is actually somewhat lower than at 2.1 m. This is consistent with the previously discussed result on the stronger constructive interference from the near-ground antenna ob-
   
 (a)	CDF of the difference in path-loss ?PL for antenna heights of 0.3 m and 2.1 m.
  
 (b)	CDF of the Excess path-loss over Friis: EPL.
 Fig. 7.	CDFs of power attenuation for the scenarios tested.
 served in Fig. 3b at some ranges. In contrast, for the NLOS outdoor-indoor scenario, the EPL is always larger than at 2.1 m. As can be seen, to cover 90% of users, a fade margin of 19 dB needs to be added for a base placed at 2.1 m, while for a base at 0.3 m this margin grows to 26 dB, i. e. an extra 7 dB is required to achieve the same degree of coverage. This value may appear at first sight to be inconsistent with the 17.5 dB increase observed in Fig. 7a. However we note that the statistics of path-loss increase described in Fig. 7a are the result of the difference between two random variables. The increase may occur under conditions where the EPL is high or is low. When evaluating percentage of wireless coverage or link budgets, an increase in EPL due to the lowering of the antenna may be of little consequence when the initial value of the EPL is low. In contrast, Fig. 7b shows the difference in path-losses at both antenna altitudes with respect to an absolute reference. This allows us to draw conclusions on the fade margin with respect to free-space propagation at a specific coverage level for both antenna heights.
 IV. CONCLUSIONS
 We have obtained a large amount of statistical data for narrowband path-loss statistics in suburban environments, representative of fixed wireless services using near-ground base-station antennas. We observed that for unobstructed outdoor-outdoor locations, with base-antenna heights of 0.3 and 2.1 m, the propagation channel is well approximated by the two-ray or even the free-space models. Our results
  
 also indicate that the increase in excess path-loss (EPL) over free-space propagation, associated with lowering the base antenna is quite modest. Typical values are 3 to 4 dB at the median level and 7 dB for 90% availability, for all scenarios tested, provided all LOS (NLOS) links remain of the same type when lowering the base antenna. However, when we consider all links of all types, the EPL will vary over a considerable range. For 90% availability, 26 dB additional losses may have to be countered for an NLOS, near-ground, outdoor-indoor link, while only 4 dB are needed if the link is of the LOS outdoor-outdoor type. For a system designer this means evaluating the tradeoff between providing services through a wall mounted outdoor antenna, with additional indoor retransmission hardware, vs. considering the need for a significant fade margin to avoid such additional elements. Both solutions also involve taking into account the associated interference problems. Extra transmission power should be managed through link adaptation techniques to limit it to the site-specific requirements, while indoor retransmission may need to be coordinated to reduce interference between neighboring homes.
 
 ﻿We analyze the behavior of the mean squared error (MSE) achievable by oversampled, uniform scalar quantization using feedback, pre- and post-filters of unrestricted order, when encoding wide-sense stationary discrete-time random sources having (possibly) unbounded support. Our results are based upon the use of subtractively dithered uniform scalar quantizers. We consider the number of quantization levels,  , to be given and fixed, which lends itself to fixed-rate encoding, and focus on the cases in which   is insufficient to avoid overload. In order to guarantee the stability of the closed-loop, we consider the use of a clipper before the scalar quantizer. Our results are valid for zero-mean sources having independent innovations whose moments satisfy some mild requirements, which are met by infinite-support distributions such as Gaussian and Laplacian. We show that, for fixed , the MSE can be made to decay with the oversampling ratio as when   tends to infinity, where . We note that the latter bound is asymptotic in but not in , and that it includes clipping errors.
 	Index Terms—Oversampling, quantization,	converters.
 I. INTRODUCTION
 I
 T is well know that oversampling can reduce the magnitude of the reconstruction error that originates from quantizing the samples of an analog source, see., e.g., [1]–[3]. This reduction is exploited by analog-to-digital converters (ADCs) such as sigma-delta   modulators, which have been successfully utilized in audio and image quantization [1].
 	It was shown in [2] that the MSE of	modulation is
 , as , where is the oversampling ratio and denotes the order of the feedback filter (assumed fixed for all values of ). In their analysis, the authors of [2] utilized an additive noise model (ANM) [4], in which quantization errors are assumed to form a wide sense stationary (w.s.s.) random process, white and uncorrelated with the input samples. Also using the ANM, it was recently shown in [5] that by using different filters (of unrestricted order) for each value of  , the
 MSE can be made to decay as  , where   denotes the signal-to-noise ratio of the scalar quantizer. The analysis in [2] and [5] restrict to the cases where the effect of quantizer overload errors is negligible, which cannot be guaranteed if   when the source has unbounded support unless infinitely many quantization levels are available. Indeed, an important body of literature related to oversampled quantization avoids overload errors either by careful design of the converters
 
  
 Fig. 1. Scalar feedback quantization scheme with subtractive dither.
 or by simply assuming there exist enough quantization levels to avoid overload, see e.g., [3], [6] and the references therein.
 Families of 1-bit (two-level)   converters in which the quantizeris guaranteedtoneveroverloadhavebeenfoundin[7], [8], by following a deterministic approach. The converters in [7] yield a continuous-time reconstruction error that can be uniformlybounded bya term proportional to  , where  
 is independent of  . In turn, the continuous-time reconstruction error with the converters constructed in [8] can be uniformly bounded as when . This leads immediately to an MSE that behaves as , when  . To the best of the author’s knowledge, the latter is the fastest decay rate of the reconstruction error with   available in the literature.
   However, the results in [7] and [8] have not been extended to   modulators with more than two quantization levels, and rely upon the input samples being uniformly bounded. On the other hand, available results on the quantization of unbounded sources including the effects of overload errors do not consider oversampling, see, e.g., [9] and the references therein.
 In this letter, we study the behaviour of the MSE with increasing oversampling ratio when the source is a (possibly unbounded) wide sense stationary (w.s.s.) band-limited process. Our analysis is based upon the use of a subtractively dithered uniform scalar quantizer (SDUSQ) [10], preceeded by a clipper, together with feedback, pre- and post-filters of unrestricted order (see Fig. 1). We focus on the cases in which the number of quantization levels,  , is insufficient to avoid quantizer overload. We show that, for this architecture, the
 MSE can be made to decay with	 as	, where
  , provided the following holds.
 Assumption 1: The source process has independent innovations , with zero mean and symmetric probability density function (PDF). Moreover, there exists a constant such that the -th moments of each satisfy
 (1)
 This letter extends the work in [5] by taking account of clipping errors in the analysis.
  
 1070-9908/$25.00 © 2009 IEEE
 542	IEEE SIGNAL PROCESSING LETTERS, VOL. 16, NO. 6, JUNE 2009
  
 II. PRELIMINARIES AND PROBLEM STATEMENT
 Our results are related with the feedback quantization architecture shown in Fig. 1. In this scheme, the samples
 form a zero-mean w.s.s. process, obtained from sampling a w.s.s. band-limited analog signal. For each oversampling ratio
 	, the power spectral density (PSD) of	can be
 written as	, where
 (2)
 In (2), , and is the square root of the PSD of the input process when . It is assumed that the input process has finite power, i.e., that	. For simplicity, we shall further restrict the analysis to the cases in which . Notice also from (2) that the total power of (in units of variance per sample), remains constant for all .
 InFig.1, representsauniformscalarquantizer,withquantization interval and reconstruction levels.Thedither is a random process with i.i.d. samples independent of and uniformly distributed over the interval Adding dither to the input of the quantizer reduces the range for the input signal over which quantizer overload cannot occur. Since the dither is distributed over , this range is . It is well known that such a dither signal yields a quantization error process with i.i.d. samples which are also independent of the source [10], [11], provided
 	, i.e., as long as	does not
 overload. Quantization error samples appear in the output as the stationary process	. In order to keep fromoverloading,weconsidertheuseofaclipperbefore , as shown in Fig. 1. The clipper limits the value of the input signal so that , if , or
 	, if	,
 thus ensuring stability, see [5]. The key point here is that, unlike overload errors, clipping errors, given by  , are not injected into the feedback loop. Instead, clipping errors appear in the output after being filtered by , to yield the process  . Unless the source
 is a stationary process, one cannot guarantee that the samples of the clipping error will form a stationary, or even a w.s.s., random process. In order to quantify the contribution of clipping errors to the MSE for not-necessarily stationary sources, we define the average power of clipping errors in the output as
  where
 Two important parameters characterizing the conditions under which the combination of clipper and quantizer operate are the signal-to-noise ratio (SNR)
 	 	(5)
 and the loading factor
 	 	(6)
 It follows from (5) and (6) that, if and are kept fixed, then   can only be increased at the expense of reducing the SNR at
 which the clipper and	operate.
 For the scheme of Fig. 1, it was shown in [5] that the reconstruction MSE due to granular quantization errors only, which here corresponds to , can be upper bounded as
 (7)
 where and the scalar function yields the unique value of that satisfies
 (8)
 when	. Upon defining
 , it is also shown in [5] that, for (7) to hold, the frequency
 responses of	and	must satisfy
 (9)
 (10)
 (11) i.e., on , for every , where satisfies (8). In (10) and (11), can be any bounded, nonzero gain. With the optimal filters in (10)–(11), relates to and the variance via [5]
 (12)
 Notice the upper bound on the MSE due to granular quantization errors in (7) decays exponentially with . However, the behaviour of the average power of clipping errors with increasing is unknown. Therefore, in view of (4), the exponential decay of given by (7) does not necessarily hold for , for sufficiently large. In the next section we find an upper bound to the total average power of the reconstruction error  , including clipping errors.
 III. MAIN RESULT
 We start with the following technical lemma:
 . If there (13)
 where
 Proof: From one of Bernstein’s inequalities, given in [12, Sec. 5.5], we have that
 (14)
 	such that	. For
 every , the tightest bound for the first inequality in (14) is obtained with . Substituting this into
 (14) yields	. The latter, together with the fact that leads directly to (13), completing the proof.
 The following theorem provides an upper bound for applicable (but not restricted) to situations in which the source has unbounded support.
 Theorem 1: Suppose there exists a scalar   such that   [see (2)]. Suppose that Assumption
  
 DERPICH: A BOUND ON THE MSE OF OVERSAMPLED DITHERED QUANTIZATION WITH FEEDBACK	543
  
 544	IEEE SIGNAL PROCESSING LETTERS, VOL. 16, NO. 6, JUNE 2009
 Thus, we have obtained an upper bound on the MSE due to clipping errors that grows linearly with and decays exponentially with (provided the product does not tend to zero as , see (16)).
 then to granular quantization errors in (7) becomes
 (40)
  
 Upon substituting (35) and (15) in (39), we obtain the following upper bound:
 (41)
 The above upper bound for does not tend to zero with increasing unless one makes the loading factor grow with fast enough.Substituting and (6)into(5)weobtain
 . From the latter, we have that	, where	. Thus, the term due to clipping errors in
 (41) can be reduced only at the expense of having operate at a lower SNR. This, in turn, makes the term due to granular errors decay more slowly with increasing .
 	For example, if one makes the loading factor	grow with   as
 , where and are constants to be chosen, then the RHS of (41) becomes
 (42)
 The optimal decay rate when is achieved by choosing and so as to make granular and clipping error terms decay at the same asymptotic rate. This is achieved if and only if and are chosen so that
 (43)
 equals 1. Before evaluating the above limit, note that from (16) we obtain since , being a random variable uniformly distributed over , has standard deviation and satisfies (1) with . Applying l’Hôpital’s rule to (43) twice and substituting by , we obtain that
 , where
 IV. CONCLUSION
 We have studied the asymptotic behaviour of the reconstruction MSE of fixed rate dithered quantization with feedback as the oversampling ratio   tends to infinity, for w.s.s. sources having possibly unbounded support. It was shown that with the proper choice of filters and loading factor for each , the MSE can decrease with at least as fast as , where  does not depend on.
 ﻿ In order that can be , or it is necessary that they first be converted into digital form . This , in turn , the problem of how to digitize data so as to achieve the best trade off between data load and performance , i . e ., how to make the most out of a little . Two are involved in this problem , namely temporal quantization i . e ., sampling and spatial quantization . These two have traditionally been separately . Indeed , there substantial literature dealing with the temporal quantization problem , covering both band limited and non band limited . The usual underlying paradigm is that of an analysis filter , by a sampler , by a reconstruction filter . Various of this architecture can be once other have been . On the other hand , spatial quantization been studied extensively for a given sampling strategy , particularly in the framework of sigma delta conversion . Finally , it is also possible to formulate the joint design problem for sampling and spatial quantization . This typically to enhanced performance to that achievable by considering the two separately . 
 This paper will survey the general area of sampling and quantization and analyze for efficient data for signal and control . We will show how , on the one hand , contemporary control theory can contribute to the design of sampling and quantization and , on the other hand , how these impact on the performance of modern feedback control . 
 Key : Sampling , quantization model predictive control , constrained control , control 
 We live in a data rich world . Most technological operate by first , from the world into digital . This is a necessary precursor to allow to be , and without degradation other than that by the to digital conversion itself . 
 The above was indeed the motivation that led Alec to invent pulse code modulation seven ago . In his patent , the main of , namely : 
 These are remarkable for the time they were . Indeed , most of these have only become reality in recent times . Furthermore , the validity of the first two to be formally determined after they were , and is still subject of ongoing research . In the pursuit of better quality at lower bit and lower , increasingly parsimonious are continually so as to acquire , process and represent digitally . 
 This topic also important theoretical , from such as information theory , functional analysis , optimization , communication theory wavelet theory . As we will discuss in this paper , also control theory much to contribute to this circle of . Conversely , much of the theory and from digital signal are highly relevant to several of control , e .., control , where parsimonious signal representation is a key element , see , e .., . 
 In the present work we present some of the main of sampling , quantization and reconstruction of , continuous time . We will describe reconstruction quality and relate it to design such as filter complexity , data rate and sampling frequency . We also present some concerning the joint problem of sampling quantization , on one side , and reconstruction on the other . We limit our analysis to uniform sampling of scalar , sampling and reconstruction by single as opposed to filter , with scalar output and we will not discuss any related to further symbol . 
 The layout of the remainder of the paper is as : Section the of quantization and some of the that justify the introduction of a more general model for a reconstruction system . Section the sampling and reconstruction in a frame theoretic perspective . Section is a review of some recent generalized on the sampling and reconstruction problem . In Section we present some basic of scalar memory less quantization and . Section feedback . In particular , some of the basic of predictive and noise shaping to digital are . In Section we present noise shaping that generalize based on model predictive control . Section to analyze the joint problem of the quantization and sampling reconstruction design , some recent and . In Section we show how related to sampling and quantization can be in control . Section . Finally , an Appendix is included with some of the basic of frame theory necessary to understand several of the in the main body of the paper . 
 In this section we will first describe as a basic architecture used in AD conversion . Various of will then motivate us to introduce later a more general framework . 
 We consider the simple and system by the block diagram in Fig . . 
 The usual paradigm associated with this setup is that the input signal a ,, is taken to be band limited to some frequency , say , . Then , in accordance with the sampling theorem , the sampling step is chosen as . Since the input signal is directly , we have a , . The nearest scalar in Fig . to the non linear transfer function , defined by 
 where is the quantization step see Fig . and rounding to the integer value greater than a . 
 Thus , the output of in Fig . is the sequence of u , where 
 The synthesis Fig . is , in the case , an ideal continuous time low pass filter with cut off frequency t and impulse response , 
 R , where . The output the , continuous time signal a , given by the mixed convolution 
 If there were no quantization i . e ., if , then u would equal for all . In this situation , a in would equal exactly the input a for all , since , by virtue of the sampling theorem , if a is to , it can be reconstructed from by the interpolation formula 
 In the presence of quantization , it is generally no longer true that a a . Nevertheless , it is reasonable to expect that , if the quantization step is small , then the u will be close to the a for all , and the output of the simple system of Fig . will be close in some sense to the input a . Unfortunately , this and other in the above model are often far from realistic , as next . 
 Whilst the method above is certainly attractive , it from several that hinder its usefulness in many practical . In what , we will describe some of the main of this architecture . 
 Synthesis Filter The ideal low pass filter used in Fig . for synthesis cannot be in practice . Firstly , it is non causal . A very close approximation of the ideal low pass filter would still be non causal , which it out from any delay sensitive application . 
 Secondly , an ideal low pass filter an infinite impulse response length . For practical low pass , the closer they mimic the ideal filter , the longer the impulse response will be . One problem with a long , slow impulse response is that it the stability of the reconstruction , in the sense that bounded in the are able to produce unbounded point wise error in the reconstructed output . As an example , consider the ideal low pass reconstruction in . It is easy to show that any bounded periodical error in the a of the form , with , will yield an unbounded reconstruction error in the L norm . The second difficulty with a synthesis filter with long but finite impulse response is cost and complexity : In where synthesis is accomplished via discrete time FIR , longer impulse require higher computational complexity . 
 Another problem with the ideal low pass synthesis filter model is that , in many practical , the synthesis filter is not a design choice , but is by other . In such , the synthesis filter can have almost any frequency response . An important example of this situation is that of , where the plant itself can be thought of as part of the synthesis Fig . . We will return to this situation later in Section . 
 Not Necessarily Band Limited Input The assumption of band limitedness of the input signal a is also very restrictive . Most real have to deal with over a finite time interval strictly speaking , any non zero finite duration signal is not band limited . Even when a virtually infinite duration , perfectly band limited signal , only a finite number of can be used for the reconstruction . This truncation , i . e ., part of the inter sample the input signal is not by the . On the other hand , it is often the case that the sampling rate cannot be made high enough to completely avoid . Whilst this is commonly dealt with by a low pass anti filter before sampling , this paradigm may have significant whenever the signal relevant information in the high frequency part of its spectrum , or when the is not perfectly band limiting see , e .., , . In this case other of analysis should be considered . 
 Availability of the Input Signal Before being able to sample the value of any physical variable , it is necessary to convert it to an electrical signal by of a transducer , which in itself is a dynamical system . It is often the case that sampling is in the transducer itself . In this case , one does not have access to the underlying continuous time signal , but only to the taken . Depending on the situation , this can deprive further of knowledge of important inter sample behaviour of the physical variable . It is then necessary to make a wise design of the synthesis stage , so that the input signal can be well at the output see , e .., , , . 
 Quantization , Sampling Frequency and Data Rate In the simple system of Fig . , quantization is done element wise by a nearest , see . Thus , if one to obtain a small reconstruction error , one would naturally aim at reducing the quantization step . In practice , however , the reduction of is limited by cost and structural . Alternatively , if the statistics of the input signal are known , then the mean square reconstruction error can often be reduced by a in which the quantization step is not uniform along its dynamic range . 
 Moreover , even though the sampling theorem that when the are un an increase of the sampling frequency cannot improve reconstruction since it is already perfect , the situation with is different . More precisely , when quantization is , sampling rate can be to reduce quantization error see Sec . . . Thus , one often the chance to compensate the effects of coarse magnitude quantization by of a finer time quantization , i . e ., faster sampling rate . The reader may be well aware of this in bit used in some . 
 In practice , the the sampling rate and the number of quantization is often constrained by data rate . This is so because , although not explicitly shown in Fig . , the sequence of u , in binary form , to be or before reconstruction place at another location in time and space . This that the total number of , or similarly , the data rate , is limited . In principle , if the nU , then the data rate will be approximately given by 
 It is possible , however , to reduce the data rate by an efficient of the sequence of compression . When such is applied , the data rate limitation into an information rate limitation , precisely given by the entropy of the sequence of at the output of the . with entropy are also variable rate . In this paper , however , we will not consider such . Thus , we will only consider fixed rate , and the data rate will be given by . 
 In view of the of conversion above , a more general model for the analysis of sampling , quantization and reconstruction is in Fig . . 
 Weighting Filter Figure : A more general sampling , quantization and reconstruction system . 
 For the remainder of this work , we will restrict our analysis to input a which are as finite energy scalar of a single parameter i . e ., a L . For example , we could think time , for the case of time scalar . Thus , the analysis Fig . for all the continuous time linear of the input that before the sampling place . The sampling process is assumed uniform i . e ., regular sampling , with fixed sampling interval . 
 The synthesis Fig . the linear in continuous time possibly with some discrete time filtering applied to the u . The output as a . It a in some well defined sense . 
 The in Fig . is generalized because it is to have access to previous and future input during operation , and scalar , because it a sequence of , one at a time . We will only discuss of this type in the remainder of this paper , which a definition of the class of generalized scalar : 
 Definition Generalized Scalar . Any quantization strategy that can be within the following 
 • The no access to the continuous time signal a , but only to the . 
 • The a sequence of u at a constant rate , one element time . The total in the output sequence the number of input . 
 • Each of the in the output sequence of the can take only from a finite , given and fixed set of U , i . e , the output of the 
 Note that this definition for the uniform , nearest scalar in as a special case . The last condition in Definition that the generalized scalar in Fig . is , in principle , to determine the output u , for any , based upon knowledge of the entire sequence , i . e ., it is a dynamic system . Therefore scalar with memory such as the predictive and noise shaping to be in Section are special of the generalized scalar . 
 In Fig . an error frequency weighting added . Inclusion of this filter the fact that , depending on the application , the practical impact or cost of the reconstruction error is frequency dependent . Accordingly , the instantaneous error a a to produce a frequency weighted error signal eH . Based on the general setup in Fig . , throughout the remainder of this work the performance of the system will be assessed in of the squared L norm of the signal eH : 
 As above , a paradigm which many signal of a filtering or analysis stage , a sampling stage , a digital , discrete time stage and a post filtering also to as synthesis or reconstruction stage . It been shown that these are equivalent to a sequence of between see , for example , , , and . This viewpoint one to use the powerful of , and study and design sampling and reconstruction . It for elegant to otherwise complex design optimization , by projection . 
 To the best of our knowledge , the first author to apply theory to the sampling problem was . in . In he derived sampling for random stationary complex exponential . Further insight and for were provided by . in for other , see . Several with the space approach to the sampling problem in subsequent . Among , a paper by a unified approach to generalized sampling . It the idea of regarding the approximation of in a , more general , finite dimensional reconstruction space , instead of to perfect reconstruction by . Interestingly , in , a finite number of of a signal was used , as opposed to an infinite number of raw . By the early , the recently wavelet theory , to stimulate a strong revival of sampling theory see , for example , , , , by the mathematics of basis and in . This framework for the re formulation of the sampling and reconstruction problem in more general and practical inter , sampling and reconstruction from finite , , study of arbitrary input and reconstruction , , , sampling of non band limited , , , , non uniform sampling , , filter , , and and interpolation , . 
 of the sampling and reconstruction in a space frame theoretic context . For a more complete formal analysis , see , for example , , , , , , . 
 It will be shown next that the analysis and sampling , which map continuous time into discrete time , can be seen as the analysis operator of the sampling frame . This frame is made of of the time reversed impulse response of the analysis filter . Similarly , the reconstruction process , which discrete time into continuous time , can be seen as the synthesis operator of a reconstruction frame . It is made of of the impulse response of the reconstruction filter . 
 Filtering and Sampling Consider the input signal a in the block diagram of Fig . . Assume that a is known to belong to some space of , say A L . the output of the analysis filter , which impulse response L . Then , is given by the convolution : 
 If one now a taking the of at time , sampling process in fig . , one a 
 where . One can see that the last integral in to the conventional inner product in L , defined in , between a and . If we now define the shift operator by 
 Therefore , the input signal can be seen as the result of a sequence of inner . From and Definition see Appendix , this is indeed the process by the analysis operator associated with the frame . As a consequence : 
 Synthesis or Reconstruction Consider now the conversion from the discrete time sequence u to the continuous time signal a , see Fig . . If we denote the impulse response , then the band limited reconstruction scheme in can be generalized to : 
 It is clear from Definition Appendix that the reconstruction process can be by the synthesis operator associated with the frame : 
 In this new setting , becomes the generating function for the principal shift invariant reconstruction , which is , in general , different from the space of band limited . 
 which it a discrete time sequence u and a continuous time function , yielding a continuous time function a . If the impulse response is chosen such that is a sequence and therefore a frame for span , see , then is a bounded operator , and the output a u L for all 
 The Combined Sampling and Reconstruction Process It from the above that the sampling analysis and reconstruction synthesis process can be stated as a sequence of between : 
 Therefore , in the absence of quantization i . e ., if u , , the complete process can be expressed as 
 It is interesting to note that the above allow one to determine the ultimate and of a sampling and reconstruction system in of the related to sampling rate and . More precisely , the analysis and synthesis alone determine , respectively , the class of that can be sensed i . e ., the sampling space and the class of that can be i . e ., the reconstruction space . A rather remarkable implication is that in the intermediate one can only design the between these , but not expand the sampling and reconstruction themselves . 
 As a consequence , the design of an AD conversion scheme can be thought of as two , namely : 
 . Design of the between in the sampling space and in the reconstruction space i . e ., design of discrete time , quantization . 
 In what , we will describe of the separate design of the sampling reconstruction strategy and of the quantization method . Some of the joint design problem will be later in Section . 
 In this section we discuss the effect that analysis and synthesis have on the reconstruction quality . We will assume that the input and output are given and will neglect quantization effects . The implicit trade off here is between the quality of the reconstruction and the computational complexity and delay in the sampling and reconstruction . 
 As in Section , the ultimate sampling and reconstruction of a system are limited by the sampling and reconstruction . These , in turn , are entirely determined by the choice , as well as the sampling interval . This that , whenever possible , the design focus mostly on the sampling and reconstruction that one to obtain . Further refinement can be by careful design of discrete time which can be right after the analysis before the synthesis filter , see Fig . . Interestingly enough , it been shown that , in general , the optimal is by making the sampling and reconstruction of one another , . To achieve this for a given analysis frame , one can insert a discrete time correction filter before the synthesis filter to make the synthesis frame the dual of the analysis frame . Although , in general , the dual frame of some given frame is not unique , there only one shift invariant dual frame i . e ., a unique correction filter for each given shift invariant frame . In what , we will consider the following situation : 
 Depending on the relation between the input space A , sampling reconstruction space , we will consider three of reconstruction , namely : consistent , orthogonal and perfect reconstruction . 
 Consistent Reconstruction The first and most generally attainable reconstruction goal is that of consistent reconstruction , first in by and , see . A signal approximation is said to be consistent if it the same as the original signal when re into the system , i . e . a is a consistent approximation of a A if and only if 
 The idea of consistent reconstruction is in Fig . . a ; in this figure , a is , the null space of , see . 
 Figure : a Consistent reconstruction oblique projection ; reconstruction orthogonal projection . 
 The notion of consistent reconstruction was first for bases in , and then extended for in , , , , and . 
 Orthogonal Reconstruction The second type of reconstruction is orthogonal reconstruction , also reconstruction . It additional see next section . In this type of reconstruction , the system , for any input a A , the output a that 
 It is well known that this notion is equivalent to an orthogonal projection of the of A onto the output space see Appendix A . . . The intuitive notion of orthogonal projection is in Fig . .. Note that the a shown in this figure is , indeed , the point to a in the output space . 
 Perfect Reconstruction The third , and most demanding notion is that of perfect reconstruction , i . e ., 
 As will be shown below , depending on the A , and , perfect reconstruction can still be possible , even , for example , for non band limited , , . In the remainder of this section we will describe on the sampling and reconstruction method which ensure that each of these can be . 
 Under the assumption that the sampling and reconstruction satisfy the direct sum condition 
 necessary and sufficient have been found in order to make the sampling and reconstruction system achieve consistent reconstruction and , as particular , optimal and perfect reconstruction as well , , . For shift invariant and , the direct sum condition can be conveniently expressed in the frequency domain based on the 
 and the null of A and respectively , as A and , where 
 Proposition , Proposition . . Let , L , and assume that and are frame . Then the following are equivalent : 
 It is shown in that , if the direct sum condition is satisfied , then a is a consistent reconstruction of an input a if and only if a is the oblique projection of a , the null space of see Appendix A . . . Such a projector , by , is defined as 
 The following the concept of oblique dual frame and its relation with the oblique projector : 
 Lemma from , Lemma . . Assume that and are let span , span . Assume that . Then the following are equivalent : 
 Furthermore , if the above three equivalence are satisfied , then is an oblique dual frame of is an oblique dual frame of on 
 From Lemma one can see that becomes an oblique projector if and only if is an oblique dual frame of in . 
 Although , as with the conventional case considered in Definition see Appendix A . , the oblique dual frame within a given space is not unique , the shift invariant oblique dual frame of a shift invariant frame is unique . This that , once reconstruction and sampling are defined , there a unique analysis filter that the analysis frame the oblique dual of the reconstruction frame . Conversely , there a unique reconstruction filter that turns the reconstruction frame into the dual of the analysis frame . An expression in the domain for the oblique dual frame condition in of the frequency of the analysis and reconstruction is given in , which , by virtue of Proposition , can be as : 
 Theorem from , Theorem . . Let , L and assume that and are frame , the , respectively . If L , then the following : 
 • The result in one to obtain a shift invariant oblique dual frame for a a continuous or discrete time correction filter with transfer function 
 just before or just after the analysis filter . With such an arrangement , and provided i and in Theorem are satisfied , the system will yield perfect all a and consistent reconstruction for all a L , as . 
 • Conversely , from the reciprocity of oblique dual , also one to obtain the frame for a given . This can be by a continuous or discrete time correction filter with transfer function defined in . Notice that this correction filter does not alter the space associated to the stage in which it is inserted , i . e ., if the impulse response of is , then . 
 From the previous it that , if the direct sum and duality are met , then necessary and sufficient condition for each type of reconstruction can be stated as : 
 for Perfect Reconstruction Perfect reconstruction only for all a is possible , without any further requirement 
 for Reconstruction Orthogonal Projection If , additionally ,, then becomes an orthogonal projector onto , i . e ., , see Appendix A . . . This that the output signal a will be the best approximation the input signal a , i . e ., it will minimize . 
 for Consistent Reconstruction Oblique Projection Consistent reconstruction will be for all a L without further . 
 Quantization is the process of into which belong to a finite set . The representation of with infinite accuracy would require an infinite number of . Quantization one to achieve a approximate representation of infinite , which in turn can be with a finite number of . Hence , the main purpose of to digital conversion is to compress data , whilst aiming to obtain the best possible approximation of the signal . This is to be within data rate and according to some fidelity criterion , i . e ., making most out of a little . As already in Section . , the to be in this paper belong to the family of generalized scalar , see Definition . As such , generate an output sequence u whose are constrained to belong to a set of nU see , the quantization alphabet U , now formally defined as : 
 Traditionally , quantization been only in of discrete time performance , usually looking at the between input and . the input and the and u , the is given by : 
 We will next briefly discuss the realization of the generalized scalar in Fig . : the zero memory scalar . Its performance will be in of the as defined in . Other of the generalized scalar , such as quantization with memory by of feedback and quantization with memory and preview , will be in and , respectively . For a more comprehensive analysis of quantization see , e .., , , . 
 Scalar quantization is also to as zero memory quantization , since each sample is previous or future . Scalar partition the real line into a set of nU disjoint and consecutive I I ,..., , . A unique scalar from U is associated to each interval in I , usually satisfying , i ,..., nU . Depending on the choice of the partition , either a uniform or a non is . 
 Uniform The scalar is the nearest uniform in Section . , where the partition of the input space the real line is given by and the of U satisfy , i ,..., nU 
 the positive extreme output extreme input 
 the is said to be if the input . If the probability density function of the is smooth and the quantization step is small enough , then the quantization error can be approximately as a random variable with uniform distribution over , see for precise , and the mean squared error between the the output u of the is given by the distortion measure : 
 In of the number of to represent each sample , we first note that 
 Non Uniform For a given number of per sample , the be further reduced if the probability density function of the is known . This can be by a non uniform quantization step . Any form of non uniform quantization can be accomplished by complementary non linear before and after a nearest . The first block is a compressor , and its transfer function is a monotonically increasing function satisfying 
 The complementary block after the is expander , and a transfer function . an expression first derived in , one that , for a non uniform with a large number of quantization , compressor characteristic and without overload , the due to quantization is given by 
 where is the of the and . The no overload assumption and , and that , Notice that for i . e ., with a uniform quantization step , becomes . 
 Clearly , minimization of in a compressor the of the input signal . The optimal compressor characteristic is given by the solution to 
 where a is a constant such that . When the solution of is inserted into , the without overload and for found to be 
 where s and are the variance and the of an individual input sample , respectively . In relation to , it must also be pointed out that see must be made several times the assumption and to be valid . For more about the derivation and of this and other related to scalar quantization , see , e .., and the therein . 
 It is possible to further reduce the reconstruction , while keeping the quantization step constant , by increasing the sampling frequency above the frequency . This technique is . For ratio not too large , the mean square error is reduced as , i . e ., 
 where D is the when . Notice that this can also be seen as a particular case of the resilience of redundant frame in Appendix A . see also , e .., , . However , as the sampling frequency is , quantization noise becomes more and more correlated and the decrease rate of . Furthermore , asymptotically a lower , strictly positive limit . The bigger is , the higher this limit becomes . A quantization step also the decrease rate of to depart from sooner . 
 The reconstruction error can be further reduced , for a given ratio , by the use of feedback . Furthermore , feedback A yield a that steadily . Thus , one can obtain an arbitrarily low , for a given , by sampling fast enough . These are briefly in the next section . 
 Quantization that use feedback can be grouped into two main : predictive and noise shaping . of the first type are the delta modulator and differential pulse code modulator see , e .., . The popular sigma delta converter , see , e .., , to the latter type . 
 The following is a basic description of the main of both converter , based mainly on the approach in . In the sequel , the quantization process is as additive noise , corresponding to the quantization error of a scalar . 
 In this diagram , U and correspond , respectively , to the of the sequence and the output sequence u in Fig . . Thus , the in the dashed line rectangle in Fig . is a particular realization of the generalized scalar in Fig . . The filter included at the end of the chain in Fig . can be considered as part of the reconstruction stage in Fig . . The E and in Fig . correspond to the of the discrete time in each of the respective . is the transform of the error by the scalar , i . e ., U E . From Fig . , the expression for the output U is found to be 
 The key to the noise reducing of the predictive on the prediction filter . This filter is designed to minimize the variance of the prediction error 
 see Fig . . It is common to assume that the quantization noise is any of the in the loop . Thus , is chosen so as to reduce the contribution of 
 C to E in . By doing so , the variance energy per sample of the sequence that the is reduced . This in turn one to reduce the quantization step in the scalar , without increasing the number of quantization to avoid overload . Thus , by reducing a measure of the term in , one is also reducing the quantization noise contribution , and the is reduced accordingly . Of course , how much distortion reduction is will ultimately depend on how predictable the sequence is , i . e ., on the of . It will also depend on how well the prediction filter is able to capture this predictability 
 It been shown that the of the scheme in Fig . with the ratio not faster than , where is the order of the filter . If an additional ideal low pass filter with cut off frequency is after see Fig . , then the is reduced at most as A common choice of is of the form . 
 Note that the predictive in Fig . can reduce distortion even if are at frequency , as long as the input are correlated . If the input are uncorrelated white noise , then the predictive is unable to yield any reduction at all . It is the increase in the of the input produced by which for the behaviour in the reduction rate . 
 The second main category of feedback to the noise shaping such as A , first by and in . One possible form to represent a noise shaping is in Fig . . Again , and U correspond , respectively , to the of and u in Fig . . The noise shaping within the dashed line rectangle in Fig . is a particular realization of the generalized scalar in Fig . . 
 where the noise shaping filter a degree of freedom in the design process . Since is band limited , and because of , it is generally convenient to choose to be a high pass filter , see , e .., . With this choice , the quantization noise is within the signal band whilst outside of it see Fig . . This compensatory increase in the off band quantization noise is unavoidable , as determined by the Bode integral theorem Because of the frequency shaping of the quantization noise , most of its energy can be suppressed by low pass filtering U , leaving only the in band portion of the quantization noise . By doing so , it is in that the by increasing ratio at most as , where is the order of the noise shaping filter . Most common for have the form , where is an FIR filter . 
 As in control , one of the beneficial of feedback in to the robustness of the resultant system . Indeed , if properly designed , feedback allow one to achieve high accuracy quantization despite the use of inaccurate building such as the scalar itself , which can be to have a very coarse and uncertain quantization step . This feedback the preferred choice for many practical . 
 It should also be noted that the above decay rate of the with increasing ratio is not fast enough to be rate distortion efficient . Indeed , AD require , in general , a higher data rate than a system with finer quantization and no to achieve the same distortion . This can be seen by that , for feedback , the only with increasing the ratio , as O n , while the with increasing the per sample i . e ., reducing as O B , i . e ., exponentially . Nevertheless , recent show that the L norm of the reconstruction error in can be reduced as O , , by for each ratio an appropriate noise shaping filter from an infinite set of . Following a different approach , quantization based on threshold exhibit a reconstruction that exponentially with increasing ratio , , and are thus rate distortion efficient . 
 Interestingly , control theory can be used to design the generalized scalar in Fig . . More precisely , since the output of the is constrained to belong to a finite alphabet of , the situation can be as a control problem with input . This point of view us to apply Moving Horizon Optimization MHO to achieve a more effective noise shaping . This paradigm Model Predictive Control , and proved to be a powerful tool for dealing with constrained , , , , , . The quantization scheme so , Step Optimal Converter , typically , while the latter as a particular case . We will present next some of the fundamental behind the . The remainder of this section been basically from , 
 A more general formulation to analyze the discrete time performance of noise shaping quantization can be derived from the block diagram in Fig . . 
 In Fig . , and u represent , respectively , the input and the output sequence . The motivation for quantization noise shaping been 
 Figure : Scheme to generate the frequency weighted quantization error sequence . 
 by a frequency weighted reconstruction error sequence , by 
 In , is a stable , causal , linear , time invariant filter , which can be via : 
 where A , , R the state dimension , i . e . the order of the filter . This filter can , e .., represent the typical low pass filter in conversion , see e .. , in order to decimate the converter output . In audio it sense to choose as a acoustic model of the human hearing , compare also with work in , . The performance of the quantization process in Fig . will be by the measure 
 The distortion in the conversion process in a frequency . If the generalized scalar in Fig . is designed to minimize the , then its u will approximate the input , while the un quantization error , a u , will tend to have a spectrum similar to that of the inverse of the filter . Thus , the method will shape the quantization noise spectrum , just as the converter in Section does . 
 Unfortunately , minimization expression is not possible in practical , due to the complexity of the associated combinatorial optimization problem . Furthermore , in the general case , an optimal would need to view the entire signal . This is clearly unsuitable for on line . 
 In order to obtain a more practical method to minimize the cost in , it is convenient develop a recursive conversion method , which can be on line . For that purpose , we will first introduce a cost measure over a finite horizon , to deploy later the concept of moving horizon approximation , see . 
 Finite Horizon Formulation A practical conversion scheme , suitable for , must operate sequentially , a restricted number of decision and considering a moderate number of future of . For this purpose , it is convenient to characterize as the output in a state space representation of 
 This relation directly from . In , is the state vector . Note that , due to the structure of , at time the impact of the past u on future of is exactly by of the present state ,. 
 Given the above , we next replace the infinite horizon cost function by the finite horizon cost : 
 In , the prediction horizon a given positive semidefinite matrix . 
 is a measure of the distortion over the prediction horizon plus a measure of the final state , x . These are formed based upon the model . 
 The finite horizon cost in into account only a finite constrained . The value computational complexity for the minimization of . This should be with the infinite number of decision in the original cost . a finite the . a design parameter , it can be chosen so that the minimization can be carried out on line . 
 Moving Horizon Approach As noted above , the to , a feasible output sequence for time . Thus , in principle , one could think of an implementation in , where the minimization is carried out . Unfortunately , the last few of depend only on a small window of the distortion To improve performance , the step optimal converter only the first element of , say u . It becomes the th element of the converter output sequence , by setting : 
 At the next sampling instant , this new state value is used to minimize the cost , yielding u . This procedure is repeated ad . As in Fig . for the case , the prediction horizon of the criterion forward as . The past is forward in time via the state sequence , thus , yielding a recursive scheme . 
 The resultant architecture the . It an to digital converter architecture which the frequency weighted conversion distortion , based upon Model Predictive Control . 
 Interestingly , it been shown that the with and to the converter , see . However , it is easy to see that , in general , better performance , since more data is taken into account in the decision process of from U to the in the sequence u . In fact , one can expect that , chosen large enough relative to the time scale of 
 , then the effect of u for be negligible and the the will approach that , if the infinite horizon measure of were to be directly which , for the above , is impractical . This asymptotic behaviour been experimentally confirmed , see . 
 In summary , the prediction designer to trade off performance versus on line computational effort . Interestingly enough , excellent performance can often be with relatively small see , e .., , thus rendering the scheme quite easy to implement in practical . 
 Another advantage of the when to the converter in that the can be designed to ensure stability like of the , see . 
 Given that digital signal have to interact with the real , physical world , the design of a quantization scheme should take into account the sampling continuous to discrete time and reconstruction discrete to continuous time between which it is to be inserted there only partial understanding of how sampling reconstruction interact with a given quantization method in of the resulting , overall reconstruction error . Furthermore , most literature the performance of in of how close the input are by the output and not by the , continuous time underlying signal entering the system against the , continuous time reconstructed signal that comes out of the reconstruction stage of the system . Accordingly , performance is most often measured by the norm of the sample approximation error , see . Similarly , traditional works on sampling and reconstruction theory build their analysis based first upon ideal later the effect of quantization as the corruption of ideal by white additive noise . Although it been shown that this model of quantization is indeed accurate for small quantization and input whose certain rather weak , it is certainly not accurate , for example , when quantization are large , or when feedback are . As in and , it is often the case that quantization noise is deliberately made non white by the so as to minimize a frequency weighted measure of the reconstruction error . 
 Within the setup in Fig . , we aim to present in this section some and additional insight related to the joint problem of designing that make use of the filtering , sampling , quantization and reconstruction paradigm . 
 The model of the sampling and reconstruction process in Section to a somewhat trivial but nevertheless important result : it for a decomposition of the final reconstruction between the input a and the output a see Fig . of a sampling quantization reconstruction system into two . The first term to the error due to the mismatch , i . e ., the non coincidence of input and . The second comes deviation of the discrete time both linear and non linear from the optimal between input and output in the sampling and reconstruction , respectively . The following proposition this idea : 
 Proposition . Let L be the impulse response of the reconstruction filter , such that is frame , and the sampling interval . Then , the mean square reconstruction error between any input signal a L and an by the reconstruction stage can always be decomposed as 
 Since a a , and because , we have that a , see , and . 
 Corollary . From Proposition , it that for any a L , choice of quantization scheme and or discrete time , the continuous time reconstruction error is lower bounded by 
 We emphasize that the lower bound in to the minimum continuous time error attainable by any discrete time scheme , once the output space is given , even if and no quantization is applied to the . From Proposition , it is clear that the performance of discrete time e .., discrete time filtering and quantization should be in of the second term of the right hand side of , that is , the L norm of a a . In relation to the design of , this rise to the question of what information is by a generalized scalar to minimize a . We have this question in . A summary of the analysis and therein is below . 
 As noted above , the reduction of the continuous time by discrete time place by the second term on the right hand side of . For the general system under study see Fig . , the signal to be is actually a with L , the impulse response of : 
 as the impulse response of , the approximation of a by the system becomes 
 see , Fig . . The impulse response reconstruction frame , which the reconstruction space 
 As in Section . , the generation of the optimal output a can be accomplished by the frame operator associated with to the sequence of , i . e . 
 where is the analysis operator associated to , the canonical dual frame of see Definition in the Appendix . We will denote this optimal , un sequence of by 
 It is clear from the above that any quantization algorithm that to minimize the continuous time error needs to be able , in the first place , to obtain the target sequence u in . From the in Section . , this that the first necessary condition for the feasibility of optimal quantization is that sampling and reconstruction be for orthogonal reconstruction . 
 If we now suppose that the access to u , then the problem of optimal quantization is that of choosing the optimal sequence u , defined as 
 The solution to one to solve a continuous time optimization problem with discrete time , decision . It is shown in that this can be converted into an equivalent discrete time optimization problem . More precisely , 
 The operator is by the Gram matrix see , sec . . of the reconstruction frame , which is defined element wise as 
 where u and u are the vector of the u and u , respectively . 
 Figure : The sampling , quantization and reconstruction system from Fig . . Impulse and frame 
 The direct is that a can determine the optimal output sequence without full knowledge of the inter sample behaviour of the impulse of the reconstruction filter . Indeed , quantization performance can be measured by the weighted norm implicitly defined in . Note that the design of an optimal is not possible without knowledge of the matrix . are shown for each filter . 
 In general , minimization of would require one to evaluate it for every sequence u , u U , that can be by the . This optimization , however , becomes intractable for sufficiently long . Given the similarity of and , one can use the in Section and optimize over a short moving horizon of . can be found in , , where a step optimal converter is . Preliminary show that , interestingly , significant distortion reduction is even when converting non band limited . Indeed , since the focus is on the reduction of the total continuous time reconstruction error , if the sampling rate is lower than the rate , the resultant converter will attempt to reduce not only quantization noise , but also noise . Furthermore , as the horizon is made , the output of the converter the optimal feasible output sequence , defined in . 
 In previous of this work we have that the power of feedback can be used in the design of . In particular , we have shown in and that careful deployment of of Model Predictive Control may lead to high performance conversion . The purpose of the present section is to highlight the role by sampling and especially quantization in feedback control . Efficiency in data a central role in any control system where parsimony need to be taken into account . Thus , quantization and sampling are worth investigating , for example , in the following : 
 • when need to be over a digital network , i . e ., in Control , , ; 
 • when plant need to be e .., relay feedback , on off control , digital control , or also due to the presence of a human operator ; 
 • in large scale , such as those related to mining and supply chain management . 
 In the following , we will briefly describe how surrounding sampling and quantization translate into the design of these of control . 
 Sampling and Reconstruction In the design of a sampling reconstruction scheme for a control system , traditional reconstruction criteria should be with more appropriate . Indeed , reconstruction quality is only of secondary importance . The main objective is measured at the plant output . In particular , as shown in , for , open loop performance should be by closed loop . This can be through consideration of frequency weighted such as . 
 Quantization Interestingly , the noise shaping in can also be applied to control where are ; see , e .., . For example , when on the design of for with , a key point in realizing that the AD conversion scheme of Fig . is related to a control system with plant : The plant input u is to be chosen such that the plant output Ha the reference signal Ha . Thus , performance can be measured via the frequency weighted error signal eH , see and also . 
 on how to apply of Moving Horizon to can be found , for example , in . It is interesting to note that the framework can also be to incorporate dynamic into . The resultant methodology can be as sampling and quantization on demand and is , thus , highly efficient from a data representation perspective , see . 
 This paper basic and related to the process of sampling , quantization and reconstruction of scalar . With the introduction of a frame theoretic viewpoint , three of sampling and reconstruction have been . We have several generalized scalar quantization , and have how control theory to signal theory . Furthermore , we have given into the joint problem of sampling , quantization and reconstruction , and have outlined how these interact . Finally , we have the role by sampling and quantization in control . 
 Definition Space . a vector space with an inner the induced norm . If such a space is complete under its norm then it is a Space . 
 Definition Basis . A sequence of ,, in a a a basis and only if span and there exist two such that 
 • The in are orthogonal if and only if and only if . 
 • The lower bound in is equivalent to saying that is a set of linearly independent . 
 • The higher bound in that will be bounded for any choice of . 
 a space , then the best approximation in in the sense of the norm of given by the orthogonal projection , by , and defined as the operator 
 The orthogonal projection from a the null space of : 
 If , is an basis of , then the orthogonal projection operator can be explicitly written as 
 Orthogonal projection elegant to some otherwise complex optimization in functional analysis . This and a natural framework for the problem of efficient sampling and quantization . 
 A fundamental property of and is that they are able to define a precise form of equivalence between two different . It is isomorphism : two different are isomorphic if they have the same dimension . An isomorphism is indeed any linear invertible operator from one space onto the other . Of particular interest for our analysis are the between any separable space L function space of dimension , where , and space . Such an isomorphism can be stated by considering any basis of , namely , and the associated analysis operator 
 The analysis operator defined in is an unitary isomorphism . This that the respective in through of any group of their respective and relative , i . e . 
 This remarkable property of isomorphic one to study the relation between of a space by looking at their through in another , more convenient space . Actually , one can argue that all digital signal digital control is made possible the existence of signal and of . 
 Some of the basic of of and bases so far will be by the following simple example . 
 the space of all real valued satisfying the following : 
 • The of are constant over any of the open , , , , . 
 Figure : Example of a functional space , an base and a unitary analysis operator . a w , w and w belong to the space ; The , constitute an for ; Image of the w , w and w in R through the analysis operator . 
 W becomes a space . The inner product also a norm in , given by 
 It is easy to show a two dimensional space . This can be intuitively by that any completely determined by exactly two , such as , for example , the of the t . A basis for a space of dimension two two . Figure . a pair of , form an basis for . 
 Figure . the of w , w , w , and through the analysis operator see in R . As , the of the and are in R . How close is w to w in their norm . Since the analysis operator is a unitary isomorphism R , we have , from 
 Consider now the case of a function ,, that to a space L , such that Wand . 
 Figure : a onto . a Function to . Relative between , , an isomorphic space . 
 The and relative respect to an orthogonal basis as are shown in the dimensional representation of Fig . .. Here it can be seen a non zero orthogonal projection onto . This orthogonal projection is the vector , in accordance with , and is given by . Consequently , the best approximation in an L sense , expressed as a function of time 
 Despite the computational convenience of bases , one often needs to study by a set of linearly dependent over complete basis . The concept of , by and , one to analyze such . with over complete bases arise in practice not only by chance . It been shown that the redundancy of is beneficial , for it can reduce the effect of in the expansion , see . The formal definition of a frame is given next . 
 Definition Frame . A sequence of in a a frame there exist A , such that 
 The number A and satisfy are frame . Some important about are as : is a frame for a space , then k . 
 • A frame is said to be tight if one can choose A as frame . If A , it is a frame . 
 • If a frame to be a frame when an arbitrary element is removed , it is an exact frame . An exact frame is equivalent to a basis . 
 • If the of a frame are linearly independent then A see . 
 The redundancy of a frame with for a defined as the ratio 
 It is easy to show that , for a tight frame , A , where A is the lower frame bound in . Another important property of the of a frame is that they are also a sequence , i . e ., they 
 From remark and the above , orthogonal bases are a special type of basis , whilst bases are exact . Thus , by our analysis on , one is also orthogonal and bases as special . 
 Definition Synthesis Operator . The synthesis or operator for a frame is defined as 
 , the synthesis operator for a frame with frame A , is bounded , with operator norm , i . e ., is the minimum constant such that . 
 Definition Analysis Operator . The analysis operator for a frame is defined as 
 Definition Dual Frame . Let be a frame for a space . Another frame for , namely , that 
 As can be seen in , a dual frame an explicit method for any of , from be exactly through the synthesis frame . 
 Definition Frame Operator . The frame operator of a frame is defined as 
 Lemma from , Lemma . . . Let be a frame with frame frame A ,. Then the following : 
 The frame operator defined in is of particular importance for the problem of sampling and reconstruction , since it an explicit way to obtain a dual frame see . More precisely , defined in , if is a frame for , then the frame is a dual frame for in , i . e . 
 The frame is the canonical dual frame of in . This is a reciprocal relation , i . e ., 
 If the frame in were by additive noise e ,, then the reconstruction formula would yield a reconstruction error 
 Early the fact that the the frame the reconstruction error were provided in , whilst can be found in and . Due to the importance of this property of redundant , we present next an adaptation of the result in , which is also illustrative of the importance of the frame . 
 Proposition . Let be a frame of unit norm with frame A and let e , be a sequence of independent random with mean zero and variance s . Then the mean square value of we in 
 Proof . If e , is a sequence of independent random with mean zero and variance , we have 
 because is a frame . Combining with the result . 
  
 ﻿ In this work , we deal with zero delay source of an unstable vector Auto Regressive AR source under average mean square error fidelity criterion . To begin with , we consider zero delay of a vector AR source in state space form . It turns out to be convenient to use an asymptotically stationary time domain scheme with feedback , which was recently in in the context of filtering theory via the Information Nonanticipative Rate Distortion Function . In this scheme , the feedback path a filter , which an estimate of the source . Our idea is then to encode the vector due to filtering via lattice quantization with subtractive dither and memoryless entropy . We show that the resulting system is stable and furthermore provide new to the . We then generalize our to vector AR of any order . With this , we are able to establish a approach with feedback , which by use of a single filter very close to the zero delay for vector AR of any order . Interestingly , for infinite dimensional vector , the with the Optimal Performance Theoretically Attainable by zero delay . 
 INTRODUCTION 
 Zero delay of information in source is the reproduction of each source sample at the same time instant that the source sample is . Zero delay source is desirable in various , like for instance , signal and network control , . The class of zero delay is a subclass of the so causal source where the reproduction of the current source sample only on the present and past source but not on the future source . A preference for zero delay source over causal source from the fact that the latter does not exclude the possibility of long of , which for arbitrary end to end . Of course , every zero delay code needs to be causal , but the opposite is not true . 
 It is well known that zero delay and causal in with non causal cannot achieve the classical rate distortion function , i . e ., the optimal performance theoretically attainable by the class of . Indeed , an open problem in information theory is the gap between the by non causal , hereinafter by , and the by causal and , hereinafter by and , respectively . Notable where this gap is explicitly found are memoryless , stationary in high , and zero mean stationary scalar with average mean square error distortion . 
 Throughout the , the interest in information theoretic rate distortion that as tight as possible to the by causal or zero delay been very high . For example , the in the so entropy , hereinafter by , to demonstrate its utility in real time zero delay . Perhaps the most striking result inspired by , is the work of where the author the so sequential , hereinafter by , to investigate control related . A decade later , the in , the so information nonanticipative , by , in the context of filtering theory . The work of with the work of that under squared error distortion , for any AR source , the optimal reproduction distribution of the output , conditioned upon , X ,..., , Y ,..., , to i . e ., , , . the latter structural result , the in used an unstable AR source and a lower bound to . This bound can be by a feedback realization that was first in , where a filter a prediction of to the , which then via that act as the prediction error . Interestingly , it turns out that the filter in is from the noisy and thereby parallel a result of al . , on the of a stable , stationary , and scalar process by noisy prediction . Note that for stable stationary , the use of filter is not mandatory . In this paper , we show the following : 
 For the case of zero mean unstable vector fully AR with average fidelity , the feedback realization scheme of by the work in , that an upper bound to the can be by the due to a filter in a feedback loop , which the source based on its noisy . We provide a simple quantization strategy and assess the rate loss as to the achievable lower bound see Theorem . In addition , we show how to generalize our scheme to vector AR of any order see Theorem . 
 When the vector source is unstable , it is necessary to guarantee that the feedback realization is stable under quantization . We provide an upper bound to the due to finite dimensional quantization of the . We show that the rate loss is finite and directly linked to the space filling loss of the as well as the loss of the entropy coder due to zero delay . If the vector dimension of the source to infinity , it is possible to completely eliminate the rate loss and thereby show that in this limit , the with the see Section . 
 This paper is structured as . In Section we cast our problem . In and we derive upper and lower to the by zero delay and we discuss possible and related . In draw . 
 Notation : We let , , N , ,.... For N , we denote random and dimensional with , i . e ., and , respectively . 
 T 
 The transpose of a matrix or by . We denote by any term that is . For a square matrix with on the row and column , we denote by the matrix , i ,...,, on its diagonal and zero elsewhere . We denote the time index with and the dimension index with i . 
 PROBLEM STATEMENT 
 In this paper we consider the zero delay source setting in Fig . . In this setting , the dimensional vector source is by the following linear time invariant state space model 
 , N , 
 where A , and are known , x 
 N ; is the initial state , and the noise process is an i . i .. ; sequence , independent of x . 
 The system as . At every time step , ,..., the the source and a single binary from a alphabet set of at most a countable number of . Since the source is random , and its length are random . Upon , the an estimate of the source sample . We assume that both the and process information without delay and they are to have infinite memory of the past . 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable length binary . The countable set of all is time 
  
 Fig . : A zero delay source scenario binary . 
 to allow the binary representation to be an arbitrarily long sequence . There is no loss by to uniquely . The and are by of probability density as , : , ,... and , : 
 , ,..., respectively . 
 The design in Fig . is to yield an asymptotic average distortion 
 D , where is the distortion level , , yn The objective is to minimize the average length by 
 , over all zero delay 
 . These design are formally cast by the following optimization problem : 
  
 s .. 
 i . e ., the by zero delay . Note that in we 
  
 that A , is see , e .., to ensure that the limit in i . e ., . 
 LOWER ON THE BY ZERO DELAY 
 In this section , we present a lower bound on the by zero delay the , . In general , the expression of the by zero delay given by is very hard to find and often are see , for example , , . 
 First , recall that since the by zero delay is a subset to the by causal see Section I then 
 . 
 Moreover , it is also known that for general the following hold see , e .., , equation . 
 a 
 R . 
 Note that , inequality a is strict , in general , and becomes equality when the source is i . i .. or when the rate to infinity . In contrary , inequality is strict at high high resolution due to space filling loss and becomes equality at zero rate . Also , for unstable like the source model of , to be greater or equal to the sum of the unstable of matrix A , i . e ., whose are greater than one see e .., . 
 By combining and we obtain 
 R . 
 Since , the is a lower bound on the by zero delay to the classical , we can use this information measure to obtain on . 
 In nonanticipative rate distortion theory see e .., , , the mutual information can be written as 
  
 where E is the expectation with respect to the joint probability distribution 
 P , yn , yn , 
 with , , yn , while is the marginal induced by the joint probability distribution , yn . We now state the definition of as given in , 
 Definition . . 
 Definition . 
 The finite time horizon is defined by 
  
 s .. 
 where the in is taken with respect to the sequence of conditional probability density , : , ,...,. 
 The is defined by 
  
 s .. 
 If one by lim in , then an upper bound to is , defined as . 
  
 s .. 
 It is shown in that provided the limit in , and the source is stationary then na . The optimization problem of , in contrast to the one given in is convex see . In addition , due to the assumption of on the pair A , of . 
 A . Feedback Realization Scheme via Filtering 
 The in the causal and rate distortion optimal 
 test channel , : , ,... 
 AR such as the source model of with an 
 , channel , via an asymptotically stationary feedback realization scheme in Fig . to find the optimal causal and zero delay information based filter . A detailed analysis on this scheme is provided in , Section . Next , we briefly explain the methodology in Fig . . 
 at : Introduce the estimation error 
 : N of : N based on the previous y ,..., defined by 
  
 and its error covariance , defined by 
 . 
 The covariance matrix is by a unitary transformation E invertible matrix such that 
 T 
 E E ,.... 
 To facilitate the computation , we introduce the scaling process 
 , where N , 
 independent . 
 at : Analogously , we introduce the error process : N defined by , and the scaling process : N defined by with , , ;, and 
 F , being diagonal scaling matrices . 
 The fidelity criterion at not affected by the above of , : N , in the sense that the at both the and do not affect the form of the distortion function , that is , 
 . 
 basic of conditional entropy see , e .., . , it can be shown that 
  
 s .. 
 and 
  
 s .. 
 are equivalent . 
 UPPER ON THE BY ZERO DELAY ENTROPY 
  
 Fig . : Realization of the optimal distribution , see Section A . 
 In this section , we derive upper to the by zero delay a universal quantization scheme based on a subtractive dither with uniform scalar quantization on the feedback realization scheme in Fig . . Toward this end , we consider unstable vector AR source as in , and we quantize each time operating , with their being jointly entropy conditioned to the dither . Our approach is far from new in the literature . In fact , it is adopted in a variety of , e .., , , . to these works , our scheme with a purely information theoretic setup while at the same time it more general and . 
 Before we proceed , we state the definition of a scalar uniform with subtractive dither . A scalar function is defined as 
 Q i for 
 where is the quantization step which is freely designed by the designer . A scalar universal uniform with subtractive dither is defined as 
 r 
 the realization of a uniformly distributed random the interval . 
 The execution of a common randomness both at the and the . In practice , the a synchronized pseudo random noise generator that can be used at both and end . 
 A . Scalar Uniform quantization 
 Next , we use use the asymptotically stationary feedback realization scheme in Fig . to design an efficient 
 , pair . This procedure is next . 
 We select the step size so that the covariance of the resulting quantization error . The does not quantize the state directly . Instead , it the deviation of from the linear estimate of . This method is known in least estimation theory as approach . As a result , we name the . 
 We consider the zero delay source setup in Fig . with the additional change of the parallel channel operating . This change is in Fig . . Note that , all matrices and adopted in Fig . still hold when the change is applied . 
 For each time step , the input to the , is a scaled estimation error defined as 
 at , A FE . 
  
 p parallel . 
  
 Realization operating . 
 Fig . : scalar uniform quantization by a dimensional channel operating . 
 Moreover , at is an valued random process . The parallel dimensional channel is operating , hence we can design the covariance the corresponding to the parallel in such a way , that for each , each diagonal entry of , i ,...,, i . e .,, V ,..., to correspond to a quantization step size i , i ,...,, such that 
  
 This into a input output transmission of parallel and independent . We apply to each component of at , i . e ., 
  
 and we let be the valued random process of dither whose individual , ,... are mutually independent and uniformly distributed random independent of the corresponding 
 source input at , i , , i . The output of the is given by , i i at , i , i , i ,...,. 
 Note that , ,..., , can take a countable number of possible . In addition , by construction see Fig . , the at : , ,... and : , ,... are not any more since by the change in Fig . , at : , ,... and : , ,... contain of the uniformly distributed process : , ,.... As a result , the filter in Fig . is no longer the least mean square estimator since the may no longer be . 
 For completeness , we illustrate in Fig . the relation between a and a scalar uniform additive noise channel , a result that was first pointed out in . 
 Entropy : In what , we apply joint entropy across the vector memoryless across the time , that is , at each time output of the is conditioned to the dither to generate a . The by the signal from . 
 Specifically , at every time step , we require that a message is into a , designed , Chapter . . For a random variable , the based on scheme give an instantaneous prefix free code with code length that the following bound 
 H E . 
 a dimensional random vector then the version of 
 . 
  
 Fig . : An equivalent model to Fig . b based on scalar uniform additive noise channel . 
 In view of the assumption that the scalar uniform with subtractive dither memoryless entropy over time , the following theorem . 
 Theorem . Upper bound 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating in Fig . . If the vector process : , ,... of the output is jointly entropy conditioned to the dither signal in a memoryless fashion for each , then the operational zero delay rate , , 
  
 the dimension of the state space representation given in , while the average distortion the end to end average the system . 
 Proof : See Appendix A . 
 Evidently , by combining the lower and upper to the by zero delay , we obtain 
  
 In the next theorem we explain how the derived in 
 generalize to Gauss AR of any order . 
 Theorem . Generalization 
 The derived in based on the scheme of Fig . hold for vector of any order . 
 Proof : See Appendix . 
 In the next remark , we comment on and draw to in the literature . 
 Remark . For scalar AR , i . e ., , then to 
 . 
 This result was first in , Theorem but only for stable scalar AR . 
 Note that the zero delay rate distortion performance per dimension is given by the following expression 
  
 B . Vector Quantization 
 It is interesting to observe that if instead of scalar uniform quantization we quantize over a vector lattice by memoryless entropy conditioned to the dither , then the upper bound in becomes 
  
 where is the second moment of the lattice . If we take the average rate per dimension then becomes 
 . 
 Additionally , by assuming an infinite dimensional vector source , then by , Lemma and we obtain 
 . 
 As , vector quantization for infinite dimensional vector source , the term due to space filling loss and the loss due to entropy asymptotically goes to zero . 
 Hence , this result , and the lower bound of we obtain 
  
 p p 
 i . e ., is the by zero delay . 
  
 In this work , we considered zero delay source of an unstable vector AR source under average fidelity criterion . Based on a simple feedback realization scheme that the of a filter with a , we derived new to the by zero delay . We the performance of this scheme when lattice quantization . For infinite we that the is in fact the by zero delay . 
 APPENDIX A PROOF OF THEOREM 
 In the realization scheme in Fig . , with the change of channel with parallel independently operating , the operational rate for equal to the conditional entropy where , ,..., ,, , i i at , i , i , i ,...,, i . e ., the entropy of the output conditioned on the value of the dither signal . This to the following analysis . 
  
 where a from , Theorem ; from the fact that the quantization noise is at see Fig . ; from the fact that the relative entropy x x , see , e .., , Theorem . . ; from the fact that at , with equality if and only if : , ,... becomes a distribution ; e from the fact that the differential entropy of a random vector with covariance , V ,..., is 
  
 and the entropy of the uniformly distributed random vector is 
 . 
 Since we have that , the result . 
 Next , note that for , ,..., the following inequality in Fig . . 
 n 
 I ; yn a XI ; 
 t 
 n XI ; t 
 t n 
 X 
 I at ; 
 t 
 where a from the structural of specific problem resulting in the realization of Fig . see , e .., the analysis in Section and , Remark . ; from the analysis in , Equation ; from the fact that E ,, are invertible matrices and as a result the information from is the same as from at to information lossless operation . 
 Since we are assuming joint memoryless entropy operating scalar uniform with subtractive dither , then by , for , ,...,, we obtain 
  
 where a by and from . 
 Then , by first taking the per unit time limiting expression in and then the , we obtain 
  
 where a by and respectively , and na is the upper bound expression of for the unstable vector AR source model given by . 
 Finally , by assumption , i . e ., the source model given by is asymptotically stationary , hence as the source becomes stationary . this , we have na and the result . This the proof . 
 OF THEOREM 
 This can shown by the state of the source model given in as . Let denote a new state vector of the additional dynamics . Then the model of is as . 
 Ax 
 x Ax 
 where the matrices A , A are the coefficient matrices of augmented dynamics . At this point , , can be written as 
 . 
 But can be written as an augmented state space form as . 
 A 
 where 
 X , 
 The state space representation of with the same on the augmented coefficient matrices A , can be used precisely as the feedback realization scheme of Fig . via a filter where the structure of the filter is the one derived in , Section . This the proof . 
  
 ﻿ By on a class of source built around entropy , we develop a framework to deal with average data rate in a tractable manner that from both information and control . We focus on a situation where a noisy linear system been designed assuming transparent feedback and , due to implementation , a source scheme to be in the feedback path . We give a closed form expression for the minimal average data rate to achieve a given performance level , and also study the interplay between stability and average data for the considered architecture . 
 I . INTRODUCTION 
 Consider the control system of Figure , a given proper real rational transfer function , is an exogenous signal , e is a signal related to closed loop performance , can be measured , u is a manipulable input , and the channel is a noiseless digital channel . 
 , 
 where P is and strictly proper , and both P and P are non zero . The initial state a second order random variable a second order process with . 
 If no but causality are on the source and , then mild guarantee that the of Figure is mean square stable see definition in Section if and only if the average data the scheme i . e ., across the channel 
 R , 
 where pi is the unstable pole of . When performance are sought , then much less is known . A causal rate distortion inspired approach was in , but no achievable rate where established there . 
  
  
 Fig . . closed over a noiseless digital channel . 
  
 a 
 Fig . . a Independent source scheme and equivalent . 
 . THE SOURCE SCHEME 
 The average data rate of the source scheme in Figure is defined via 
 R , 
 where i is the length in of i . 
 Theorem : Consider a causal source scheme inside a feedback loop , as in Figure . Under suitable see , I u , where I u the directed mutual information rate defined by . 
 A . Independent source 
 We focus on a class of source : 
 Definition : The source scheme of Figure is said to be independent the of Theorem hold and the noise , a second order zero mean i . i .. sequence , is independent of and O is a stable and stably and causally invertible filter with deterministic initial state see Figure a . 
 Any independent source scheme can be written as shown in Figure , auxiliary , is as in Definition , and A auxiliary stable , and both A and are stably and causally invertible . Moreover : Theorem : If an independent is written as in Figure , then I u I . 
 The previous fact one to focus on I instead of I u . It is possible to establish upper on I that depend only on second order : 
 Theorem : If in the of Figure the source scheme is independent , then 
 , 
 where is the stationary of is the stationary variance of , and is the variance of . Equality in the first inequality is , whereas equality in the second inequality of is constant a . e . 
 An additional key fact of independent source is stated next see also : 
 Theorem : For any given independent source scheme , there another independent source scheme , with the same noise color O and the same directed mutual information rate across it , such that the gap between the left and right hand sides of the second inequality of Theorem can be made arbitrarily small . 
 B . Entropy 
 We consider as defined in with dither , output . 
 Theorem : Consider the system of Figure with A before . If an is used as the link , and the dither is i . i .., independent of ,, and uniformly distributed on , , then the system of Figure becomes an independent source scheme and the entropy coder inside the can be chosen so that 
 R 
 If the entropy coder inside the is memoryless , then it can be chosen so that 
 R . 
  
 . MEAN SQUARE STABILITY 
 Definition : A system , N , , , where is a second order random variable , and are constant matrices , a second order process is said to be mean square stable there exist finite and a finite and positive semi definite such that E , 
 , regardless of the initial state 
 The previous definition to the of Figure , when the source scheme is independent . 
 Theorem : Consider the setup and of Theorem . Then , irrespective of whether the entropy coder inside 
 D relative entropy . 
  
 the memory or not , the minimal average data rate compatible with , say , . 
 . 
  
 The bound on provided by Theorem is , by construction , achievable . 
 . PERFORMANCE 
  
 . 
 Theorem : Consider the setup and of Theorem . If , then , irrespective of whether the entropy coder inside the memory or not , the minimal average data rate that one to attain a performance level se , say , 
 , 
 where 
  
  
  
 , 
 p D 
 and is the unique positive real satisfying 
  
 If then does not exist , whereas 
 an infinite average data rate . 
 The bound on provided by Theorem is , by construction , achievable . To our knowledge , our result to the first closed form bound on the achievable average data rate to attain a given performance level . 
 We note that , if , is , then the bound on given in is tight up to sample . 
  
 ﻿ We consider a system subject to an average data rate constraint in the feedback path . We provide upper to the minimal source rate to achieve mean square stability and a desired level of performance . In the quadratic case , an almost complete rate distortion characterization is . 
 I . INTRODUCTION 
 This paper on the interplay between average in per sample and stationary performance for a control system a noisy plant and an average data rate constraint in the feedback path . In such a setup , the of guarantee that it is possible to find causal and such that the resulting closed loop system is mean square stable , if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant . This result been extended in several see , e .., , . However , when performance subject to average data rate are sought , there are relatively available . Indeed , to our knowledge , there are no computable of the optimal in control , , , , . 
 In this note , we present upper and lower on the minimal average data rate that one to attain a given performance level as measured by the stationary variance of the plant output . From a source perspective , we are aiming at the rate distortion function in closed loop . This beyond causal rate distortion theory due to being subject to a stability constraint . Our exploit a framework for control system design subject to average data in , . 
 . PROBLEM SETUP 
 Consider the of Figure , an plant with state and initial state , u is the control input , is a sensor output , e is a signal related to closed loop performance , and is a disturbance . We assume that , are jointly second order and with finite . The feedback path in Figure a noiseless digital channel , a causal whose output is a sequence of binary , and a causal . 
  
 Fig . . control system . average data rate across the channel is defined as 
 R , 
 where i to the length in of i . 
 We do not restrict the complexity of the or the a , and only assume them to be causal , and to have access to independent side information SE and . Our aim is 
 R , 
 where se trace , is the stationary variance matrix of e , is a desired level of performance , and the optimization is carried out with respect to all causal E render the resulting asymptotically mean square stable , i . e ., that render , u , jointly second order and asymptotically wide sense stationary . 
 . AN INFORMATION THEORETIC LOWER BOUND ON AVERAGE DATA 
 Theorem . : Consider the of Figure . Under suitable , 
 R I u I uG , where I a the mutual information rate between a and , and , uG are jointly with the same second order statistics as , u . 
 Thus , in order to bound from below , it to minimize the directed mutual information rate that would appear across the source scheme , when all in the loop are jointly . 
 Lemma . : Suppose that , in Fig . are second order and jointly random . Then can be from as u i Li , i , i ,..., where , for each i ,...,, i is a zero mean random variable such that i , , si , and 
  
 Fig . . that when , in Figure , the E a linear source scheme . 
 where Li : i is a linear operator such that Li , is the minimum mean square error estimator of u i given , . 
 We conclude from the above that , for a given performance level , the minimum of I uG over all causal and is achievable by an pair which as a linear system plus additive white noise such that i , , i . 
 . LOWER AND UPPER ON 
 We next define the class of linear source , which are capable of yielding a relationship u of the form given by . 
 Definition . : A source scheme is said to be linear if and only if , when used around a noiseless digital channel , is such that its output u are related via 
  
 auxiliary , is a second order i . i .. sequence , proper , independent of ,. 
 When a linear source scheme is used in the of Figure , the feedback system of Figure . 
 Lemma . : Consider the of Figure and assume that the E and a linear source scheme . Under suitable , I u I and 
 , 
 where is the stationary power spectral density is the variance of the auxiliary noise . 
 Linear source have sufficient of freedom to allow one to compromising . Thus , our lead to : 
 Theorem . : Consider the of Figure under suitable . Define , with reference to the feedback scheme of Figure , the signal to noise ratio function 
 , 
 where sa , a ,, e , is the stationary variance of a in Figure , and the optimization is carried out with respect to all and all render 
  
 the feedback system of Figure internally stable and . Then : 
 . 
 Moreover , there a linear source scheme such that 
 R 
  
 Theorem . the minimal average data rate that a given stationary performance level , in of , i . e ., in of the minimal that the desired performance level in a related architecture . Interestingly , the upper bound in is valid even if one the assumption of , being 
 To find , one can resort to the in . A case where an explicit solution is available is when , i . e ., when only stabilization is sought . In that case , it from Theorem . and that 
 , 
 where p ,..., are the unstable of . If one in and , then one , within a modest gap , the absolute minimal average data rate compatible with stability derived in . 
  
 ﻿ This work the problem of universal density estimation under an operational data rate constraint . We present a theorem that necessary and sufficient to learn and transmit a memoryless source distribution with arbitrary precision in total , under an asymptotic zero rate regime , in per sample . In the process , we propose a concrete scheme to achieve this learning objective , the Skeleton estimate by . , . 
 I . INTRODUCTION 
 This work the problem of universal density estimation under an operational data rate constraint . The basic setting of an agent the sensor observing i . i .. from an unknown distribution with the objective of jointly learning and a finite description of to a second agent the receiver , which that information to construct an estimate . This density estimation and problem taken the attention of the community because of its role in sensor , and because of its strong connection with 
 universal source , . 
 Making echo of the seminal work of , it is well understood that the problem of universal lossless source is connected with the problem of distribution estimation , as there a one to one correspondence between and finite entropy discrete , in the finite alphabet case . This interplay , however , is less obvious when we move to the source scenario . this issue , recently that connect the problem of fixed rate universal source , with the problem of the source distribution with arbitrary precision , from one point to another , under an asymptotically zero rate operational constraint . This connection was made under the two stage joint modeling and framework . Taking from statistical learning , the data was split in training and testing , where first , the training data is used to construct a finite description of the source distribution first stage , and the second stage the first to pick a with respect to the distribution source code to encode the test data . Remarkably , in this joint modeling framework , the existence of a zero rate consistent estimate of the distribution in total , is sufficient to show the existence of a universal fixed rate source scheme , the 
 . 
 distortion rate function , for any given rate , and for any distribution within a bounded parametric family with some regularity , Th . . . This the question of whether there are of non parametric for which this result is also valid . 
 In this work we study in the problem of universal density estimation under an asymptotically constraint . Our main result is a theorem that necessary and sufficient to guarantee that zero rate is achievable for this learning problem . Interestingly , there is a tight connection with the rich collection of L totally bounded . Furthermore , we propose a concrete scheme , the Skeleton estimate by , , , to achieve our objective , which is a concrete demonstration of its information theoretic , something that was by and . and which , to the best of our knowledge , not been before . In the parametric scenario considered in , the Skeleton scheme 
  
 an optimal learning rate of O p under the regime , where , furthermore , this rate is extended for general non parametric . 
 . DENSITY ESTIMATION UNDER A BIT RATE CONSTRAINT 
 Let be a separable and complete subset of i . e ., is a Polish subspace of . Let be the collection of probability in , and let denote the set of probability absolutely continuous with respect to the measure . For any , the Radon derivative of with respect to . 
 For the estimation problem the fidelity criterion adopted is the total variational distance . be two probability in . The total variation is given by 
 V , sup A A , 
 A 
 A absolutely continuous with respect to a measure , by , if for any event A such that A , then A . 
 Consequently is well defined , which is the Radon derivative or density , and furthermore , A , A RA . 
 which is a bounded metric in and been widely adopted in density estimation , . For the case when , to , the ‘ identity ´ a connection between total variation and the L norm of the involved , , more precisely , 
 . 
 Alternatively , if we define the set ´ for the pair 
 , with ,, respectively by , : , then , , , . 
 A . The Main Learning Problem 
 Let : be an indexed collection of of interest . the index set of , which can live , in general , in an infinite dimensional space a scenario . 
 Definition : A , learning rule of a pair of ,, with : and :, a finite set and 
  
 The composition of these two : the explicit learning rule taking in the set : , which is the reproduction of ,. For an arbitrary learning rule , the operator log , which is the description complexity of the range of , in per sample . 
 Definition : A finite description learning scheme with rate sequence is a collection of learning for all possible finite , i . e ., , : such that , for all . 
 Let X , X ... be independent and identically distributed i . i .. of a measure . In this context , the product measure of the block in , and the entire process distribution of X , X .... Hence , the problem is to study the existence of a universal learning scheme , : , such that its finite description density estimate induced by the data to , in the following sense , 
 . 
 In other , we are interested in a zero distortion estimate for the family , a scheme , and consequently , in the necessary and sufficient , if any , that this consistency assumption on the intrinsic complexity of . 
 Definition : Let : be an indexed collection of in ,. We say that the rate is asymptotically achievable for , if there a learning scheme , : , with such that 
 . 
 In this case we say that is an rate uniformly consistent estimate for the class . 
 Definition the density inwith zero distortion . We are interested in the zero distortion i . e ., lossless operational point , as it is the standard objective in density estimation , and because it is fundamentally related with the problem of universal source . The focus of the next section , the main of this paper , is to characterize the class of that a scheme for lossless density estimation . 
 . THE ZERO RATE LOSSLESS DENSITY THEOREM 
 Definition : Let be a class of . We say totally bounded if for every , there a finite covering of 
 that 
 , 
 where 
 is the L ball of radius centered at . the positive integer that . is the covering number is the 
 ‘ entropy of the class . Finally , an covering such that , is an Skeleton of F 
 . 
 We can now state the following theorem . 
 THEOREM : Let : be an indexed collection of in with index set that could be an infinite dimensional set in general . Then , there a zero rate uniformly consistent scheme for the class , if and only if , is totally bounded . 
 Hence , the rate zero is achievable with zero distortion for the class , if and only if , is totally bounded . This a concrete correspondence between zero rate density estimation and L totally bounded of . The proof two which are in detail next . 
 A . The Skeleton Estimate 
 For the argument , we present a concrete learning scheme that consistency in the sense of , under the desired zero asymptotic rate . The scheme is based on the Skeleton estimate , Chapter by . 
 For any arbitrary , let us consider one of the 
 . In what , we use as a short hand for the in , and we define to represent the index set of . The idea of was this collection of probability with a set of measurable in . More precisely , let us define the class of by 
 , 
 where is the 
 set of ´ with respect to , . Then the idea is to a the Skeleton index to adopt a minimum distance principle , . More precisely , given i . i .. X ,.., with Xi , the Skeleton estimator is given by , 
 min , 
 where is the standard empirical distribution . Note that is the minimum distance approximation of with of , the similarity measure in , that is reminiscent of the total variational distance in . The following key theorem a performance bound with respect to the minimum distance decision on the knowledge of the true distribution . THEOREM : For any , 
 . 
  
 This bound of an approximation error and an estimation error , the first and second term on the right hand side of , respectively The approximation error is bounded by the definition of , totally bounded . On the other hand for the estimation error , the use of the ‘ inequality . 
 THEOREM : al . , Th . . , 
 , 
 where this result in , we have that the skeleton 
 estimate that 
  
 Note that for any fixed , infinity one can make the estimation error component in arbitrarily small . The beauty of Theorem is that is in particular valid for any and , furthermore , given its finite length nature non asymptotic , it is valid even if the approximation fidelity to is chosen as a function of the amount of data . Consequently , for any sequence of non increasing positive , 
  
 for all . Here we can consider the sequence that a balance between the estimation and the approximation error , and by doing so , get a consistent estimate of in the sense of . A simpler idea was stated by and . If the classis totally bounded , we can consider the sequence , which is well defined and clearly to zero infinity . Consequently , it 
 . 
 To conclude , for any skeleton learning rule in , let us characterize its pair by , 
 , 
 for any sequence and for any , where is the empirical distribution induced from the argument . Then the learning scheme the uniform consistency re 
 in , and its rate is given by 
 v 
 , which by construction is O . Consequently the skeleton estimate a zero rate lossless learning scheme for the family , which the part . 
 B . Minimum Distance Oracle Solution Converse 
 Let us assume that there a learning scheme 
 , : such that limn 
 and 
 . 
 Associated with the learning rule of length , we have its reproduction that we denote by 
 . Let us define the minimum 
 distance oracle solution in by 
  
  
 From , we have that limn , . 
 This that for all there , such that for any arbitrary , where by construction . totally bounded , the proof of Theorem . 
 . CLASSES WITH FINITE DIMENSION 
 In this section we explore density with extra on top of the totally bounded assumption 
  
 to achieve the critical O p asymptotic rate of convergence in total variation . On this , we follow the by who , in the context of the minimum distance estimate , of with finite and dimension , also classes , . 
 The following focus exclusively on the skeleton based learning scheme in Section A for the part of Theorem , i . e ., indexed by a sequence of precision . 
 Note that from the structure of the estimation error bound in , the 
  
 rate of convergence that could be is order O p . 
 Definition : Let : be an indexed collection of . The class for such collection is given by , 
  
 with A , : the 
 set of ´ with respect to . 
 THEOREM : Let : A and let us assume that , 
 i is totally bounded , 
 the class AT a finite dimension see 
 Definition in Appendix A and the complexity with 
 the sequence strictly sub lineal , i . e ., 
 log is o . 
 v 
 Then the skeleton learning scheme zero rate and 
 , 
 where is the skeleton density estimate in . 
 Proof of Theorem : We consider the and in Section A . Let us first focus on the convergence rate of the skeleton estimate . First , from Theorem 
 , for any arbitrary sequence 
  
 with the class of the Skeleton . It is clear that . Then by 
  
 for all and for any distribution . Here is where we use the assumption that AT finite dimension , which see the in . and . in that 
 v 
 we can achieve the same rate O for the approximation error in if condition is satisfied . 
 Remark : From Definition , is inversely proportional to . In fact , depending of how rich is can go from being , passing from being polynomial in , to being see a number of in , 
 . and therein . In fact a bound on how tend to infinity as goes to zero , to guarantee zero rate in the learning scheme . It is simple to show with , is sufficient to get log N being o . This is a condition satisfied by a large collection of totally bounded classes in see . 
 V . THE PARAMETRIC SCENARIO 
 The so far are of theoretical interest , because they relay on the Skeleton partition of , which is typically unknown . Moving on the direction to make the Skeleton learning scheme of practical interest , we revisit the scenario studied in , in which : is indexed by , a compact set leaving in a finite dimension space , or what people in learning theory call parametric . Interestingly , in this context we can achieve optimal learning a practical of , induced by a uniform partition of . Let us first start with some . 
 Definition : Let : with . Let IF : be the index function of , that to . IF is said to be locally uniformly , if there and , such , , 
 V , , 
 , where the ball of radius with respect to the norm in centered at . 
 PROPOSITION : Let : with 
 T . bounded such that 
 The proof is in Appendix . 
 Under the of Proposition , 
 . The proof is given in Appendix . 
 for some constant . Substituting this result in , Under the of Proposition , let 
 denote the learning rule of to the minimum 
 . distance principle in , where instead of the oracle 
 Skeleton , it the uniform covering 
 The argument by , a a sub optimal covering of , solution which the intended rate of convergence by see of this construction in Appendix . in . Finally by construction , the rate of the learning rule Then by definition , 
 N e of length is , which to zero by . this last part from Corollary . With this , let 
 this result , by imposing extra our practical learning scheme regularity on and , we 
 indexed by the precision . We are 
 and the IF : is locally uniformly , totally bounded . 
  
 can achieve the optimal rate of convergence to estimate uniformly in . More precisely , by imposing that the 
 class is a class , we can achieve O rate of convergence for the estimation error in and , on the other hand , in a position to integrate the in Section 
 This parametric case is the setting considered by for the problem of fixed rate source and modeling . 
 Theorem , Proposition , Corollary and Remark to state the following implication . 
 THEOREM : Under the of Proposition , let us in addition assume that the collection AT 
 is a class . Then the learning 
 v 
 scheme that 
 , 
 , where 
 . 
 Proof : Let be the covering induced from the uniform partition Appendix . From this we can construct the minimum distance estimate in the class of with index set , i . e ., from : 
 min . 
 the same in the proof of Theorem , we can obtain an equivalent version of , i . e . 
 . 
 From this point , the proof from the of 
 Theorem and the fact that 
 from Corollary . To conclude note that the same result to the Skeleton estimate of Section A as . 
 v 
 The behavior of the distortion overhead of in , a faster asymptotic rate than its counterpart by O , under the same parametric setting and with the same per sample of O . In addition , an scheme , as its minimum distance decision is carried out on a finite number of . 
 . APPENDIX 
 A . Statistical Learning 
 Let be a collection of measurable , and be a sequence . 
 Then we define by the number of different in x , x ,.., n :, and the shatter coefficient of 
 C by is an indicator of the richness dichotomize a finite sequence of in the space , where by definition n . 
 Definition : The is strictly less than n is the and dimension of . If n for all , then the class is said to have infinite dimension . 
 B . Proof of Proposition 
 First note in a compact set , , then there a finite 
 ing such that . On the other hand , from the locally uniformly assumption on IF :, there and such that , , , . Then let us consider , then by construction 
 of 
  
 where :, is the ball induced from the total variational , and the last inequality is from the condition . Hence from , there 
 and 
 P , such that . 
 C . Proof of Corollary 
 Let , be the uniform that characterize the condition of IF Definition . Without loss of generality , let us assume the critical regime where , hence from is upper bounded by , which is the covering number of see Appendix . As 
 , we will work with a uniform partition of to find a bound for . Let . Then a product type partition , where in each we have uniform length , we have the covering . The number of is , which as a function of . 
 . 
 The work of . Silva is by from Grant , Chile .. The work of .. is by from post doc project . , and the project ACT . 
  
 ﻿ Abstract We prove of the recently quadratic rate distortion function subject to the constraint that the distortion is uncorrelated to the source . This result is based on shaped lattice quantization in the limit as the lattice dimension to infinity and for all positive . It turns out that this uncorrelated distortion can be causally . This feature , which in contrast to , is by causal transform . Moreover , we prove that by feedback noise shaping the uncorrelated distortion can be causally and with memoryless entropy . Whilst upon infinite dimensional , we prove that the rate loss in the finite dimensional case can be upper bounded by the space filling loss of the and , thus , is at most . bit dimension . 
 rate distortion function for a stationary zero mean memory and under the fidelity criterion can be written in a parametric form the reverse water filling solution 
 a b where the power spectral density the distortion is given , if c 
 , otherwise . The water level is chosen such that the distortion constraint b is satisfied . 
 It is well known that in order to achieve in the quadratic case , the distortion must be independent of the output . This clearly that the distortion must be correlated to the source . 
 Interestingly , many well known source actually lead , by construction , to source uncorrelated . In particular , this is the case when the source coder the following two : a The linear if any achieve perfect reconstruction in the absence of quantization ; the quantization error is uncorrelated to the source . The first condition is typically satisfied by , transform and feedback . The second condition is met when subtractive and often when non subtractive dither are employed . Thus , any scheme , for example , quantization , to source uncorrelated . 
 An important fundamental question , which was raised by the in a recent paper , is : What is the impact on rate distortion function , when we further impose the constraint that the end to end distortion must be uncorrelated to the input 
 In , we the notion of , which is the quadratic rate distortion function subject to the constraint that the distortion is uncorrelated to the input . For a source , we defined as 
 N , N where the notation the covariance matrix to the determinant . For zero mean stationary , we in that the above minimum in the limit when the following : 
 is the of the optimal distortion , which needs to be . Notice that here the parameter a akin to in does not represent a water level . Indeed , white , the of the optimal distortion for is not white , for all . 
 In the present paper we prove of by based on lattice quantization , which , in the limit as the dimension infinity , are able to achieve for any positive . We also show that can be causally , i . e ., that for all and for all positive one can build forward test that realize without non causal . This is contrary to the case of rate distortion function , where at least one of the 
 Other and between and are in . 
 of the forward test channel that needs to be non causal . To further illustrate the causality of , we present a causal transform architecture that it . We also show that the use of feedback noise shaping one to achieve with memoryless entropy . This a recent result by , and for . We conclude the paper by showing that , in all the , the rate loss with respect to when a finite dimensional can be upper bounded by the space filling loss of the . Thus , for any source with memory , by and scalar quantization , the scalar entropy conditioned to the dither of the output by at most . bit dimension . 
 A lattice is a lattice with subtractive dither , by entropy . The dither U V is uniformly distributed over a cell V of the lattice . Due to the dither , the quantization error is truly independent of the input . Furthermore , it was shown in that the rate of the , i . e . 
 can be written as the mutual information between the input and the output of an additive noise channel E , where E the channel additive noise and is distributed as . More precisely , N I ; N I ; E and the quadratic distortion per dimension is given by N N EkE k . 
 It furthermore been shown that when is white there a sequence of lattice where the quantization error and therefore also the dither to be approximately distributed in the divergence sense for large . Specifically , let E have a probability distribution fE , and let be distributed with the same mean and covariance as E . Then limN N fE e e 
 with a convergence rate of log if the sequence is chosen appropriately . 
 In the next section we will be interested in the case where the dither is not necessarily white . By shaping the of a lattice whose dither is white , we also shape , a colored dither . This situation was considered in detail in from where we obtain the following lemma which was proven in but not put into a lemma . 
 Lemma : Let E U V be white , i . e . E is uniformly distributed over the cell V of the lattice 
 the shaped cell some invertible linear transformation . Denote the covariance of E by . Similarly , let , KEG covariance matrix KEG and let 
 Proof : The divergence is invariant to invertible since E E log . 
 The forward channel that is shown in Fig . . According to , all that is for the mutual information per dimension equal is with equal to the right hand side of b . 
 In view of the asymptotic of lattice in Section , the of can be shown by the test channel of Fig . by an adequately shaped dimensional lattice and then . In order to establish this result , 
 Lemma : Let ,, and be mutually independent random . Let and be arbitrarily distributed , and the same mean and covariance as and , respectively . Then 
 Theorem : For a an infinite length random vector with zero mean , is achievable . 
 Proof : Let be the sub vector the . For a fixed distortion , the average mutual information per dimension N I ; is when and are jointly and 
 see . Let the dimensional shaped such that the dither is distributed as , with . It that the rate of the is given by 
 N I ; E . The rate loss due to to quantize is given by 
 where is the of the random vector , independent of E and , and the same first and second order statistics as E . In , inequality a directly from Lemma , since the use of subtractive dither the error E independent of . 
 To complete the proof , we invoke Lemma , which that the of as . 
 Remark : For zero mean stationary random , is by Theorem to be the complete input process . For this case , as shown in , the transform of the function of to the of b . 
 For vector , the of by Theorem from the concatenation of infinitely many consecutive . 
 Note that if one an infinite number of parallel scalar random , can be causally by Theorem from the th sample of each of the and entropy after . 
 The fact that can be causally is further in the following section . 
 We will next show that for a random vector with positive definite covariance matrix , can be by causal transform , . A typical transform architecture is shown in Fig . . In this figure , is , a vector , independent of , with covariance matrix I . The system clearly the perfect reconstruction condition W . The reconstruction error is the random vector ,, and the is N , 
 be lower triangular , the transform coder in Fig . becomes causal , in the sense that ,..,, the th of U and can be determined just the the th element of . 
 Since is lower triangular , is the decomposition of , which always . Thus , can be by causal transform . 
 In practice , transform are by the vector channel by a 
 or several by entropy . The latter process is simplified if the are independent . When with subtractive dither , this can be shown to be equivalent to 
 N I U ; in the transform coder when the channel . Notice that , is invertible , the mutual information per dimension is also equal to . By the chain rule of mutual information we have 
 with equality the of are mutually independent . If is , this is equivalent to being diagonal . Clearly , this cannot be with the architecture shown in Fig . causal matrices while at the same time satisfying . However , it can be by error feedback , as we show next . 
 Consider the scheme shown in Fig . , where A is lower triangular and is strictly lower triangular . Again , a sufficient and necessary condition to have 
 for some diagonal positive . If we substitute the factorization into , we obtain I I , and thus 
 Thus , there exist A and . Substitution of into A AT , and log log A log . From and the fact that I it that A , and therefore 
 We have seen that the use of error feedback one to make the average scalar mutual information between the input and output of each channel in the transform domain equal to . In the following section we show how this result can be extended to stationary . 
 In this section we show that , for any colored stationary stationary source and for any positive distortion , can be by noise shaping , and that is achievable memory less entropy . 
 The fact that can be by the additive colored noise test channel of Fig . that could also be by an additive white noise channel in a noise shaping feedback loop , see Fig . . In this figure , is a stationary process with . The A and are . The channel is situated , where white noise , independent of , is added . The reconstructed by passing through the filter A , yielding the reconstruction error . 
 Fig . : Test channel built by the channel in a noise feedback loop . 
 The following theorem that , for this scheme , the scalar mutual information across the channel can actually equal . 
 Theorem : Consider the scheme in Fig . . Let , be independent stationary random . Suppose that the differential entropy rate of is bounded , and that is white . Then , for every , there exist causal and stable A , A and such that 
 Proof : Consider all possible of the A and such that the sequence is white , i . e ., such that . From Fig . , this is 
 On the other hand , since is , a necessary and sufficient condition in order to achieve is that 
 are bounded and positive for all ,, and that a bounded differential entropy rate of that 
 . From the Wiener criterion see also , e .., , this that , A and A can be chosen to be stable and causal . Furthermore , recall that for any fixed , the corresponding value of a is unique see , and thus fixed . Since the variance is also fixed , it that each frequency response magnitude that a can be associated to a unique value of . Since is strictly causal and stable , the minimum value of the variance is when 
 i . e ., if no outside the unit circle equivalently , if is minimum phase , see , e .., . If we choose in a a filter that , and then we take the logarithm and integrate both sides of a , we obtain 
 where a from the of and , and from the fact that is independent of strictly causal . This the proof . Alternatively , 
 In a , equality is the right hand side of a , i . e ., optimal . Equality because , which from b . The fact that is stationary been used in , wherein equality is is minimum phase , i . e ., if . Equality in if an only if the of are independent , which , from the of , is equivalent to . Finally , e from the fact that is independent of . Notice that the key to the proof of Theorem on knowing a the of the end to end distortion to realize . Indeed , one could also use this fact to realize by the in a feedback loop , and then following a reasoning similar to that in . 
 In order to achieve by a instead of an channel , one would require the quantization to be . This cannot be with scalar . However , as we have seen in , lattice are able to yield quantization approximately as the lattice dimension to infinity . The sequential causal nature of the feedback architecture does not immediately allow for the possibility of vector . However , if several are to be simultaneously , we can overcome this difficulty by an idea in where the are in parallel by separate feedback . The feedback are operating independently of each other except that their scalar are by a single vector . If the number of parallel is large , then the vector that the marginal of the individual of the becomes approximately distributed . Thus , due to the within the vector , each feedback a sequence of i . i .. quantization . Furthermore , the effective rate per source is that of a high dimensional entropy constrained per dimension . 
  
 ,,,,,,, ,,, ,,,, , 
 ⊥, ,, , ,,, , , , , , , , , , , , , , 
 ,,,,,,,,, ,, ,,,,,,,,,, 
 ,,,,,,,,,,,,, ,,,, , 
 if i de para . and i . and i . : 
  
 ﻿There are two major issues which underly the design of an analog to digital converter; namely “when to sample?” and “how to represent the amplitude of each sample?”. In the majority of past work, these two aspects have typically been treated separately. Recently [1] we have introduced a novel algorithm called MSIC which uses moving horizon optimization ideas to determine both when and how to sample. Our earlier work [1] gave a heuristic description of the algorithm and showed, via simulations, that a performance gain was achieved in both the required bit rate and the achieved distortion level. This seemingly paradoxical result is due to the interpolative capabilities inherent in MSIC. The goal of the current paper is to give theoretical support to the MSIC algorithm. In particular, we provide bounds on the probability that beneficial interpolation occurs for the particular case of horizon length 2 with flat, unity gain error weighting filter.
 INTRODUCTION
 The two fundamental questions that arise when dimensioning an analog-to-digital converter are “when to sample?” and “how to represent the amplitude of each sample?”. Several studies deal with the first question (neglecting quantization effects), see, e.g., [2], [3] and gives rise to irregular and or periodic (non-uniform) sampling patterns. Other strategies deal with each sampled value. In particular, S?- conversion has been deployed in a wide range of applications, see, e.g., [4], [5]. Recently, S?- conversion has been extended to incorporate ideas stemming from finite set constrained predictive control, see [6]–[8].
 Earlier work by the present authors, reported in [1], uses moving horizon optimization ideas [9] to address the dual question of when to sample and how to quantize. The scheme, which we termed MSIC in [1], selects samples to be marked as “mute” whenever it is desirable to replace them by a linearly interpolated value derived from their neighbouring samples. Notice that this procedure implicitly leads to signal-dependent (adaptive) non-uniform sampling patterns.
 Our earlier work [1], gave an introduction to the algorithm and showed, via simulations, that distortion introduced in the conversion process can be reduced, when compared to existing schemes, such as straight quantization, S?- conversion and its multi-step extensions [6]–[8]. Additionally, we showed that, in general, the number of bits required for the digital representation of the signal can also be reduced. This seemingly paradoxical result arises from the interpolation inherent in the MSIC algorithm.
 In the current paper, we will give theoretical support to the MSIC algorithm by examining the conditions under which beneficial interpolation occurs. In particular, a bound on the probability that interpolation occurs is derived for the special case of horizon 2 optimization.
 REVIEW OF THE MSIC A/D CONVERTER
 The goal behind MSIC is to perform quantization so as to minimize both bit rate and distortion. It is based on the idea of linear interpolation between quantized values. MSIC can mark a sample as “mute”. Later, mute samples, are reconstructed by interpolating linearly between neighbouring quantized values. This concept is illustrated in Fig. 1.
 Fig. 1 a) and b) show the reconstruction of an analog signal from a (uniformly sampled) discrete-time signal obtained via straight quantization. The dashed line in Fig. 1-a) corresponds to reconstruction using zero-order hold. The benefit of using
  
 Fig. 1. Reconstruction of an analog signal from samples obtained via: a) straight quantization; b) linear interpolation
 linear interpolation is clear in Fig. 1-b), where the values for the samples at times k ? {3,6,9} were obtained by linear interpolation of quantized values of the neighbouring samples. It can be observed that the reconstructed signal in Fig. 1b) fits better the original analog signal. The MSIC algorithm builds on this observation and reduces distortion by creating virtual quantization levels between the original ones every time a sample is interpolated. Additionally, data compression occurs due to the fact that mute samples do not require bits to represent their value thereby reducing the average bit rate needed.
 A. Interpolation and Optimal Conversion
 Consider a scalar discrete-time signal  with underlying sampling rate fS. The purpose of analog-to-digital conversion is to obtain a quantized representation of ak, i.e., a discrete-time signal, uk, k ? N. Each value uk is restricted to belong to a given finite set of scalars, {s1,s2,...,snu-1}.
 In the MSIC algorithm, uk is allowed to be assigned as mute. This condition is denoted by the symbol “*”. Thus, the constraints on uk can be expressed as:
 where	uk ? U, ?k	(1)
 	U = {s1,s2,...,snu-1} ? {*},	(2)
 and nU denotes the cardinality of U.
 In order to incorporate mute samples into the conversion problem, we will make use of an interpolator whose output is defined as follows :
 	 	(3)
 where f and l are functions of k such that for every k corresponding to a vacant sample ul = closest non-mute sample before uk uf = closest non-mute sample after uk (4)
 We can see that  is a discrete-time signal formed of all nonmute values of uk and the linearly interpolated mute elements of uk. Notice that  is not constrained to belong to UThe quantization process:
 ak ? uk
 can be designed [1] by utilizing a frequency weighted measure of the error, i.e.:
 	 	(5)
 where L is the total number of samples of ak. In (5), ek is the filtered error, defined as:
 	 	(6)
 and W(z) is a linear time invariant filter  .
 B. Feasible Sub-Optimal Conversion: The MSIC Algorithm
 1) Finite Horizon Formulation: To ensure that the computations are feasible, it is necessary to restrict the number of decision variables as well as the number of future values of ak considered in the optimization. Thus, at sampling step k we will replace the infinite horizon cost function (5) by the following quadratic cost function defined over a shorter optimization horizon of length N:
 	 ,	(7)
 where N ? N is a fixed design parameter.
 The finite horizon cost proposed in (7) is determined by only a finite number N of constrained values uj. These decision variables are grouped into the vector
 	 	(8)
 Note that the last element of , that is, uk+N-1, must not take the value “*” (mute sample), because, if it did, then the calculation of the corresponding value of the interpolated sample, , would require the value of the successive value, namely uk+N, which falls outside and ahead of the horizon.
 2) Modified Moving Horizon Approach: Minimization of the multi-step cost introduced in (7) gives rise to the optimizer:
 	 	(9)
 Given (8),  contains a feasible sequence over the optimization horizon. If one was to use the standard moving horizon concept as deployed, e.g., in [9]–[11] , then only the first element would be implemented. At the next optimization step the whole procedure would be repeated again with the horizon shifted by 1. While this is in principle possible, the inclusion of mute samples leads to additional aspects that are addressed in [1]. In general terms, the optimization method may give rise to several consecutive mute samples. In that case, if the horizon moves one step, the decision for the next element would have to consider the backwards propagation effect, which is not taken into account in the finite horizon cost (7). This would lead to performance degradation. To overcome this problem, MSIC includes the following modification to the standard moving horizon approach :
 The converted output is assigned to the shortest sequence
 	{uk,uk+1,...,uk+M}, 0 = M = N - 1	(10)
 such that , i.e., uk+M is non-mute. The horizon will then be shifted by M+1. The above procedure ensures that, at every optimization step, the precursor is non-mute.
 Remark 1: Notice that the use of mute samples gives rise to a non-uniform sampling pattern, and that the average sampling rate will be lower than fS.
 Remark 2: The MSIC embeds non-interpolative converters such as straight quantization, S?- conversion and the MultiStep Optimized Converter of [8] in a more general setting. Since the conversion process is carried in an optimal manner, see (9), the MSIC will, in general, give lower distortion than these other conversion algorithms. In the sequel, we will analyze the quantized sequences produced by the MSIC algorithm.
 EFFECT OF THE PROBABILITY OF INTERPOLATION ON DATA COMPRESSION
 In [1] it was shown by simulation that MSIC yields lower quantization noise than traditional analog-to-digital conversion methods, and at the same time it utilizes less bits. Clearly, these gains depend on having a high proportion of mute samples p, where:
 	 	(11)
 here, I represents the number of interpolated samples after L samples have been converted.
 Notice that in the case of horizon length N = 2, at most one out of every two samples can be marked as mute, limiting the value of p to 0.5. We also define the probability of muting a sample as:
 	Pint = P{uk = ” * ”}	(12)
 The proportion of mute samples, p, is related to Pint by:
 	 	(13)
 In [1] it was also shown that the Normalized Bit Ratio, defined as the total number of bits required by the output sequence from MSIC, namely R2, divided by the total number of bits required by traditional quantization, RTrad is given by:
 	NBR  	(14)
 As a consequence, the data compression of MSIC depends on what proportion of samples are marked as mute and on the size of the constrained set nU, see (2).
 CONDITIONS FOR INTERPOLATION TO OCCUR
 Towards the goal of deriving expressions for Pint (see (12)), in this section we will elucidate the conditions under which the MSIC chooses interpolation rather than quantization. The following definitions are used:
 Definitions:
  , the first difference of ak  k-1	k+1 - 2ak, the second difference of ak.
 Q() , a standard nearest neighbour scalar quantizer [12], with output set {s1,...,snU-1}.
  , the quantization error.
 In the sequel, we will restrict our analysis to the simplest case, in which N = 2, W(z) = 1, and where si+1 = si +
 ?, ?i ? {1,...,nU-2} for a given and fixed quantization step ?. This leads to
  
 We will also assume that input signal and output set are scaled such that no overload occurs, i.e., .
 Given the above, a sufficient condition for interpolation is:
 	 	(15)
 where  corresponds to the interpolated value stemming from uk = ” * ”, i.e.:
  
 We note that, in general,   and   could be mute
 (interpolated) or quantized. However, having restricted the horizon length to N = 2, the rules of the MSIC algorithm  a
 	Fig. 2.	Amplitude Band Numbers
 imply that (see II-B.2), if a given sample ak is a candidate for interpolation, then the previous converted sample result, , is non-mute. Similarly, if the sample ak was to be marked as mute, then the conversion result for the sample just after it, namely , would be non-mute. As a consequence, when considering the possible interpolation of ak, the expression for   becomes
 	 	(16)
 Substituting (16) into (15), the condition for interpolation becomes:
  
 EMBELLISHMENT OF THE CONDITION FOR INTERPOLATION
 Probabilistic analysis of (17) is non trivial since its left hand side contains a sum of correlated continuous random variables. For our purpose, it is convenient to first assign to each quantization level an integer number in ascending order (see Fig. 2) and then define the Amplitude Band Number (c.f. Voronoi Region) for a sample ak as
 	 	(18)
 or, equivalently
 	Nk = i	??	Q(ak) = si
 This integer quantity represents the number of the closest quantization level for sample ak. As an illustration, for the example in Fig. 2, Nk-1 =1 , Nk = 2 and Nk+1 = 3. Given the above, the values ak can be decomposed into :
 	 	(19)
 Thus, (17) simplifies to:
 	 	(20)
 where
 	 	(21)
 is an integer number that could be interpreted as a “coarse” second difference of ak.
 Since N¨k is integer valued and, by assumption,  , we only need to distinguish 3 cases that fulfill (20), namely:
 	 	(22)
 It can be seen from (18) that the required values for N¨k (22) depend on the values of ak-1, ak and ak+1. For the purpose of probabilistic analysis, it is more convenient to express it in terms of:
  
 These expressions can be obtained through careful examination of the definitions given in IV, (18) and Fig. 2. For sake of simplicity, details are omitted.
 Substituting (23) and (24) into (21) yields that (22) can be written as:
  
 Noting that, equation
 (25) could be solved numerically. Alternatively, it is possible to devise a graphical method to represent (25) and solve (25), as depicted in Fig. 3. In this figure, the range of combinations of variables   which satisfy (25) are shadowed. The horizontal axis is , determining the value for the first difference Nk-Nk-1(see (23)), as denoted by the vertical marked bands. The vertical axis is a?k+1. The horizontal jigsaw bands (horizontal rows of diamonds) mark zones yielding the same result for Nk+1 -Nk (see (24)). Thus, a given value for   defines the ranges of a?k+1 that in (25) yield either -1, 0 or 1 in order to satisfy the conditions for interpolation of sample ak.
 The diagonal line (-.-.-.) in Fig. 3 lies entirely inside the solution area, and represents the condition . If  , this line corresponds to the case a?k+1 = a?k, that is, a¨k = 0. Since, by assumption,  , we can conclude, in accordance to intuition, that in order to satisfy the condition for interpolation, a¨k must be as close to zero as possible. In general, a given value of a¨k will satisfy the condition of interpolation depending on the value of . It can also be seen from figure 5 that the limits of the required
  
 Fig. 3. Graphical solution to equation (25). Shaded areas represent the combinations of  satisfying it.
  
 	a?k=0	 	a?k= ?2
 Fig. 4.	Range of a¨k required for interpolation as a function of  for
  .
 interval for a?k+1 are periodical functions along the mentioned diagonal line.
 Additionally, if  , the difference between each of these periodic functions and the diagonal line determines the required limits for a¨k. The required limits for a¨k then depend upon . Fig. 4 illustrates the effect of quantization error  in the range for a¨k that satisfies the condition for interpolation, for three different values of a?k. In particular, it can be inferred from figures 5 and 6 that, if |a¨k| > 2?, then
 (25) will not be met regardless of .
 PROBABILITY OF INTERPOLATION
 We can utilize the framework established in section V to analyze Pint (see (12)). Towards this goal, we interpret , a?k and a¨k as random variables which we denote by , a? and ¨a respectively. Assuming that  is uniformly distributed on
  
 Fig. 5.	Graph of conditional probability function  and .
 the interval   and statistically independent from a? and a¨ , it is possible to obtain (see Fig. 4), for a given value of a? and a¨, the set of intervals to which  must belong in order to satisfy (25). Since  is uniformly distributed, the total length of these intervals, which we will denote here as (a?,¨a), is proportional to the probability that the condition for interpolation is satisfied. Thus, the conditional probability of interpolation given that a? = x and ¨a = y is given by:
  
 Note that the function g(x,y) defines a random variable g(a?,¨a). If the joint probability density function for a? and ¨a is fa?,¨a(x,y), then Pint can be calculated via:
 	 	(27)
 -2? -8
 Note that the -2? and 2? limits in the first integral arise from the fact that g(x,y) = 0?y /? (- , as previously noted. Function g(x,y) can be numerically or graphically obtained. Fig. 5 shows g(x,y) v/s y for three different values of x.
 A LOWER BOUND FOR THE PROBABILITY OF INTERPOLATION
 Based on the analysis on section 6, the probability of interpolation for a given signal depend on the statistical properties of the first and second differences of the signal. Clearly, if fa?,¨a(x,y) is known, the probability of interpolation can be obtained precisely from (27). However, in practice it is desirable to estimate the probability of interpolation “a-priori”, based on coarse information about the signal. We present next a lower bound for Pint for the case where only the second moment of ¨a, that is, E[¨a], is known.
 THEOREM: For each y0 ? [0,2?], and provided E[¨a] < 4?, a lower bound for the probability of interpolation can be obtained by
 (28)
 where.
 PROOF: Since g(x,y) = 0, fa?,¨a(x,y) = 0, ?x, ?y, we can write:
  
  
 	Fig. 6.	Statistical frequency of interpolation for 
  
 where f¨a(y) is the probability density function of ¨a. Equation (29) then leads to
 	Pint = h?(y0) · P{|¨a| < y0}	(30)
 Application of Tchebychev’s inequality [13] to (30) then gives the result.
 REMARK: Clearly, the above theorem depends on the choice of y0. Since, by construction , h?(y0) is monotonically decreasing, the derivative of the right hand side of (28) is zero only at the global maximum. Thus, the tightest value for the bound can be found numerically by optimizing y0.
 SIMULATION STUDY
 Simulations were obtained by applying the MSIC algorithm to two CD quality music signals each of length 50,000. We grouped the samples into 192 categories, for 3 values of a? and 64 values of a¨, and calculated the statistical frequencies of interpolation within each category. According to the law of large numbers, the greater the number of samples in each category, the closer the statistical frequency will approach the actual value of the probability of interpolation for each category. More specifically, for a category defined as samples ak such that |a?k - x| < dx and |a¨k - y| < dy, it can be expected that for highly populated categories the statistical frequency will approach the conditional probability of interpolation g(x,y) as dx,dy ? 0.
 Figures 6 - 8 show the results for 8 quantization levels. In these figures, the dashed line represents the function g(x,y) for the corresponding value of x (see Fig. 5).
 It can be seen that statistical frequency approaches g(x,y). The differences in the heights of the columns from the dashed line are explained from the fact that in some categories the number of samples was rather small. This is particularly true for larger values of a? or a¨ . Despite that, it is important to
  
 	Fig. 7.	Statistical frequency of interpolation for   
  
 verify that the zones for which no interpolation is predicted shows indeed no interpolations at all (except an isolated bar on the extreme right in Fig. 6, arising from a category of one sample whose existence is explained by the fact that that category admits samples with a? not strictly equal to zero). Similarly, for the categories where g(x,y) predicts probability of interpolation equal to 1, all samples were in deed interpolated.
 To illustrate the bounds derived in section VII, we converted 22 different audio signals, each one with 4 and 8 quantization levels. For each signal, the value for E[¨a] was estimated from the entire signal. Fig. 8 illustrates the empirical frequencies of interpolation. Furthermore, it shows the lower bounds obtained from (28) by choosing  (which gives  =0.5) and y0 = ? (which gives  =0.25). It can be observed from this figure that the predicted lower bounds, although weak, are confirmed by simulations. In particular, as suggested by the bounds developed, the (empirical) probability of interpolation decreases as  E?[¨a] increases.
  
 Fig. 8.	Empirical probabilities of interpolation v/s   and lower bounds.
 CONCLUSIONS
 This paper has analyzed a recently described algorithm which simultaneously achieves reduced bit rates and lower distortion levels. The algorithm, designated MSIC, uses moving horizon optimization together with interpolation. In this paper, we give a lower bound on the probability that interpolation can be beneficially used for the particular case of horizon length and flat error weighting filter. Simulation results have also been presented which confirms the validity of the lower bound in practical circumstances.
 
 ﻿ We deal with zero delay source of a vector 
 AR source subject to an average mean squared error fidelity criterion . Toward this end , we consider the nonanticipative rate distortion function which is a lower bound to the causal and zero delay rate distortion function . We use the realization scheme with feedback in to model the corresponding optimal of the , when considering vector AR subject to an average distortion . We give on the vector AR source to ensure asymptotic of the realization scheme bounded performance . Then , we encode the vector due to filtering via lattice quantization with subtractive dither and memoryless entropy . This scheme a tight upper bound to the zero delay . We extend this result to vector AR of any finite order . Further , we show that for infinite dimensional vector AR of any finite order , the with the zero delay . Our theoretical framework is with a simulation example . 
 I . INTRODUCTION 
 Zero delay source is desirable in various real time , such as , in signal and control . Zero delay form a subclass of causal source see , namely , where the source on the source in a causal manner . However , zero delay source to causal source allow the reproduction of each source sample at the same time instant that the source sample is . Unfortunately , causal source does not exclude the possibility of long of , which may cost arbitrary end to end . 
 Zero delay and causal in to cannot achieve the classical rate distortion function . Indeed , an open problem in information theory is the gap between the optimal performance theoretically attainable by non causal , and the by causal and zero delay , hereinafter by and , respectively . Notable where this gap is explicitly found are memoryless , stationary in high , and zero mean stationary scalar with average mean squared error distortion . 
 Throughout the , the interest in zero delay is growing , thus , further research on the fundamental of the by zero delay . Unfortunately , it turns out that is very hard to compute and for this reason there been a turn in of classical that perform as tight as possible to 
 . 
 In this paper , we derive a tight upper bound to zero delay source for vector AR subject to an average distortion . We consider nonanticipative rate distortion function see , e .., , , , which a lower bound to to the classical see , e .., . Then , we employ the feedback realization scheme in , Fig , that to the optimal test channel of for vector AR and average distortion . Further , we give to asymptotically stabilize the performance of the specific scheme . By standard entropy , on the of the feedback realization scheme , we derive the tight upper bound . In addition , we show how to generalize our scheme to vector AR of any finite order . If the vector dimension of the Gauss AR source to infinity , we show that the with the . We demonstrate our with a numerical example . Notation : We let , , N , ,.... 
 We denote a sequence of its realization by , , ,...,. The distribution of by . The conditional distribution . For a square . The transpose of a with 
 on the row and column , we denote by the matrix , i ,...,, on its diagonal and zero elsewhere . 
 . PROBLEM STATEMENT 
 In this paper we consider the zero delay source setting in Fig . . In this setting , the dimensional vector source is by the following linear time invariant state space model 
 , N , where A , and are known , x 
 N ; is the initial state , and the noise process is an i . i .. ; sequence , independent of x . We allow A to have outside the unit circle which that can be unstable . 
 The system as . At every time step , the the source and a single binary from a set of of at most a countable number of . Since the source is random , and its length are . Upon , the an estimate of the source sample . We assume that both the and process information without delay and they are to have infinite memory of the past . 
 t 
 Fig . : A zero delay source scenario . 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable length binary . The countable set of all is to allow the binary representation to be an arbitrarily long sequence . The and are by of conditional probability as , : N and , : N , respectively . At , we assume , x x and , z z . 
 The design in Fig . is to yield an asymptotic average distortion 
 D , where is the distortion level , 
 . The objective is to min 
 the average length by 
 , over all 
 . These design are formally cast by the following optimization problem : 
 , 
 s .. 
 i . e ., the by zero delay . 
 . 
 In this section , we give the definition of of vector AR subject to an average distortion . 
 Define the source distribution by 
 , the reconstruction distribution by , and the joint distribution by . 
 The marginal on , , is induced by the joint distribution We assume that at , , x x . 
 Given the previous , we introduce the mutual information between and yn as 
 , 
 where E is the expectation with respect to the joint distribution 
 Definition . with average distortion The finite time is defined by 
  
 assuming the . 
 The per unit time asymptotic limit of is defined by 
 , 
 assuming the and the limit and it is finite . 
 The optimization problem of Definition , in contrast to the one given in is convex see e .., . In addition , for the source model and the average distortion , then , by , , the optimal test channel corresponding to is of the form 
 P , N , where at , , x x , and the corresponding joint process , : N is jointly . 
 . ASYMPTOTICALLY STATIONARY FEEDBACK REALIZATION SCHEME VIA FILTERING 
 The in , Theorem the optimal test channel of with the feedback realization scheme in , Fig that to a realization of the form : 
 where is a scaling matrix ; is an independent noise process with ; , independent of x ; the with 
 is 
 with ;, and . Moreover , are given by the following filter 
 : 
 Prediction : 
 , a 
 T T 
 A A , 
 Update : , b 
 G E x , c innovation , d x , e 
 Gain , 
 , 
 where , 
  
 Fig . : Asymptotically stationary feedback realization scheme corresponding to . 
 , and is an orthogonal matrix . It is easy to verify that the following hold : 
  
 where I the identity matrix . By substituting in , we can also observe that , 
 P 
 The realization scheme of becomes asymptotically stationary stable if one of the following two hold : A is stable , i . e ., its have magnitude less than one ; the limit of the covariance matrix , i . e ., with . This that with , and 
 . 
 Next , we briefly discuss the resulting asymptotically stationary realization scheme in Fig . . 
 at : Introduce the estimation error 
 : N , where 
 with error covariance , N . Under 
 or , we ensure that and it is unique . The error covariance matrix is by an orthogonal matrix E invertible matrix such 
 T 
 that E E . To facilitate the computation , we introduce the scaling process , where 
 , independent . 
 at : Analogously , we introduce the process : N defined by d and the scaling process : N defined by , with 
 , and , are the 
 asymptotic of and , respectively . The fidelity criterion at not affected by the above of , : N , in the sense that the at both the and do not affect the form of the squared error distortion function , that is , 
 . 
 T 
 The steady state of b is . The end to end distortion of the scheme in Fig . is 
 T 
 lim E 
 t 
 trace Hence , following , Theorem , the per unit time asymptotic limit of subject to the total distortion can be expressed as . 
 . 
  
 Clearly , the optimization problem in , is a minimization problem and can be , for instance , Tucker , Chapter 
 . . or semidefinite . A way of is in . However , to that work , our realization scheme is with feedback to take into account the effect of unstable in the dynamical system . 
 V . UPPER BOUND TO ZERO DELAY 
 In this section , we derive an upper bound to the zero delay a uniform scalar on the feedback realization scheme of Fig . . The scheme was in and since then it been used in several see , e .., , , under various realization . However , it never been for the realization scheme in this work . Here , we consider the vector AR source of , and we quantize each time operating , with their being jointly entropy conditioned to the dither . We extend our when vector quantization showing that at infinite dimensional , the space filling loss due to compression and the entropy , i . e ., and coincide . 
 A . Scalar quantization 
 Next , we use the asymptotically stationary feedback realization scheme in Fig . to design an efficient , pair . 
 We select the step size so that the covariance of the resulting quantization error . The does not quantize the state directly . Instead , it the deviation of from the linear estimate of . This method is known in least estimation theory as 
  
  
 Realization operating . 
 Fig . : Scalar quantization by a dimensional channel operating . 
 approach and , therefore , the is as an . We consider the zero delay source setup in Fig . with the additional change of the parallel operating . This is in Fig . . Note that , all matrices and adopted in Fig . still hold when the replacement is applied . 
 For each time step , the input to the , is a scaled estimation error defined as 
 at , A FE . 
 Moreover , at is an valued random process . The parallel dimensional channel is operating , hence we can design the covariance matrix of the corresponding to the parallel in such a way , that for each , each diagonal entry of , i ,...,, i . e ., , to correspond to a quantization step size i , i ,...,, such that 
  
 This a input output transmission of parallel and independent . We apply to each component of at , i . e ., 
  
 and we let be the valued random process of dither whose individual , ,... are mutually independent and uniformly distributed , i independent of the corresponding source input at , i , , i . The output of the is given by 
 , i i at , i , i , i ,...,. 
 Note that , ,..., , can take a countable number of possible . In addition , by construction see Fig . , the at : , ,... and : , ,... are not 
  
 any more since by the change in Fig . , at : , ,... and : , ,... contain of the uniformly distributed process : , ,.... As a result , the filter in Fig . is no longer the least mean square estimator . 
 Entropy : In what , we apply joint entropy across the vector memoryless across the time , that is , at each time output of the is conditioned to the dither to generate a . The by the signal from . Specifically , at every time step , we require that a message is into a , designed , Chapter . . For a , the based on scheme give an instantaneous prefix free code with code length that the E . 
 Since the memoryless entropy over time , the following theorem . 
 Theorem . Upper bound 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating in Fig . . If the vector process : , ,... of the output is jointly entropy conditioned to the dither signal in a memoryless fashion for each , then the operational zero delay rate 
  
 the dimension of the state space representation given in , while the average distortion the end to end average the system . 
 Proof . The derivation is shown in , Appendix A . 
 The previous main result combined with the lower bound on zero delay , to the following corollary . 
 Corollary . on zero delay 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating as in Fig . . Then , for vector stable or unstable AR the following hold 
  
 Proof . This is the fact that , and Theorem . 
 Theorem . Generalization 
 The derived in Corollary based on the realization scheme of Fig . hold for vector of any order . 
 Proof . This is shown by the state of the model of . For see , Appendix . 
 In the next remark , we draw to in the literature . 
 Remark . to 
 For stationary stable scalar valued AR , our upper bound in Theorem with the bound in , Theorem . However , the upper bound in is a realization scheme with four instead of only one that we use in our scheme . In addition , our result into account unstable too . 
 Compare to , we use based on a different realization setup that into different lower and upper . 
 Next , we employ Theorem to demonstrate a simulation example . 
 Example . We consider a two dimensional unstable AR source as : 
 x , A B 
 where R , the parameter matrix A is unstable because one of its , by i A , magnitude greater than one , the pair A , is and ; I . By we plot the theoretical attainable lower and upper to the . This is in Fig . . As from theory , source sample . 
  
 Fig . : on via the scheme of Fig . . 
 B . Vector Quantization 
 It is interesting to observe that if instead of uniform scalar quantization we quantize over a lattice vector by memoryless entropy conditioned to the dither , then the upper bound in becomes 
  
 where is the second moment of the lattice . If we take the average rate per dimension and assume an infinite dimensional vector source , then by , Lemma , , and the due to space filling loss and the loss due to entropy in asymptotically goes to zero . the latter , and the fact that , we obtain 
 p p 
  
 . AND FUTURE 
 We considered zero delay source of a vector AR source under distortion . Based on a feedback realization scheme that the of a filter with a , we derived an upper bound to the . We the performance of this scheme when lattice quantization . For infinite we that the coincide with the zero delay . An illustrative example is to support our . 
 As an ongoing research , we will apply the scheme based on to find the actual operational corresponding to the zero delay . Moreover , we will examine similar for fixed length rate . 
  
  
 ﻿We derive closed form expressions for the secondorder statistics of the spectral power gain of wide-band microwave indoor channels. We obtain our results within a framework general enough to be compatible with several popular channel models, such as those proposed by the  task group, as well as the Saleh-Valenzuela channel model. As all these models, our channel description is based upon clusters and rays with Poisson arrivals and random amplitudes. Our results consist of closed form expressions for the secondorder statistics of the channel power frequency response, where statistical averages involve expectations over ray amplitudes and arrival times. We first express the auto-covariance of this frequency response in closed-form. We then use this result to obtain an analytical expression for the variance and secondorder moment of the channel power within any given interval of frequencies. This allows us to express the channel spectral diversity as a function of model parameters and bandwidth. From this function, we determine the range within which diversity scales approximately linearly with bandwidth and its upper limit.
 I. INTRODUCTION
 Stochastic wireless channel models allow one to predict the statistics of the radio propagation conditions over an ensemble of scenarios with similar characteristics. This is specially useful in complex, heterogeneous and time varying environments, such as office, residential and industrial indoor scenarios. The the IEEE 802.15.3a task group accepted a channel model [1] for ultra wide-band indoor communications, based on the Saleh-Valenzuela (S-V) channel model [2], and similar models have been discussed in drafts that led to the IEEE 802.15.3c standard. Like the S-V model, the IEEE 802.15.3a channel model is a discrete-time description of the impulse response of a wireless channel, in which multi-path components are grouped into clusters. The arrival times of clusters and rays within them follow Poisson distributions. One exponential ray gain decay profile is defined among clusters and another one for the rays in each cluster. Unlike the S-V model, the fading statistics of multi-path components in the 802.15.3a channel model are log-normal, not Rayleigh.
 Some of the useful temporal statistics of channel models like this are the power delay profile, the average delay and the RMS delay, which allow one to determine spectral statistics such as coherence bandwidth and average power gain under suitable assumptions, has been extensively discussed in the literature [3], [4]. In contrast, the second-order statistics of the channel power frequency response have received less
 attention. In [5], it has been shown that the number of significant eigenvalues of the covariance matrix of the random channel impulse response, and hence the diversity order of the channel, scales approximately linearly with bandwidth. Such an increase in diversity, which relates to the reduction of the relative channel power variance as the bandwidth increases, has been reported to reach a saturation point [6], [7]. In a recent paper [8], analytical expressions for the autocorrelation of the channel frequency response squared magnitude, here denoted by |H(j?)|2, as well as the variance of the power over any frequency band, have been derived for the IEEE 802.15.3a channel model, conditioned to fixed and given ray and cluster arrival times. The corresponding statistics over random arrival times were obtained via simulations. To the best of the authors’ knowledge, no closed form expressions are available in the literature for second-order un-conditional statistics of the spectral power gain of wireless channel models such as the IEEE 802.15.3a and the S-V.
 In this paper, we derive closed form expressions for the second-order statistics of |H(j?)|2 for frequencies greater than some minimum value ?min rad/s, whose precise definition will be given later. Statistical averaging considers randomness of ray amplitudes an arrival times, for a generalized version of the ultra-wide-band microwave indoor channel model accepted by the IEEE 802.15.3a task group [1]. We obtain these results within a framework general enough to be compatible with other related channel models, such the S-V [2]. The flexibility of our formulation allows it to include some of the features in the IEEE 802.15.4a standard channel model for high frequencies [9] as well. As in all these models, our channel description is based upon clusters and rays with Poisson arrivals, with different average power decay profiles among and within clusters. We first derive a closed form expression for the autocovariance of |H(j?)|2. We then use this result to obtain an analytical expression for the variance and raw second-order moment of the channel power gain PB within any given interval of frequencies B above ?min. This allows us to estimate fade depth as a function of channel bandwidth using explicit formulas. Our derivations also predict a positive lower bound to the ratio between the variance of PB and its squared mean. This bound only vanishes as the arrival rate of clusters in the model tends to infinity.
 II. CHANNEL MODEL
 In this section we formulate the wireless channel model framework which will be utilized throughout this work. As in the IEEE 802.15.3a channel model, we describe the radio propagation properties of indoor scenarios by a random baseband impulse response of the form [1]
 	 	(1)
 where
 ti,m , Ti + ti,m, ?i ? {1,...,Nc};?m ? {1,...,Nr}
 is the random arrival time of the m-th path in the i-th cluster, Ti = 0 is the random arrival time of the i-th cluster, and ti,k = 0 is the random delay of the m-th path in the i-th cluster relative to Ti. Each (real valued) random coefficient ai,m represents the amplitude of the m-th path in cluster i. These random numbers are formed as [1]
 	ai,m = Aipi,mai,m,	(2)
 where Ai ? R is a random variable representing the amplitude of the i-th cluster, {pi,m} are un-biased binary random variables taking values from {-1,1}, and each real valued random variable {ai,m} denotes the amplitude (or gain) of the m-th path (or ray) within the i-th cluster relative to that cluster’s amplitude. For simplicity, we do not consider here a large-scale fading factor before the sum in (1), but its effects can be applied to our results with ease.  As in [1], [2], [9], all the random variables in the super-set {Ti}?{ti,m}?{Ai}?{pi,m}?{ai,m} are independent, except Ai from Ti, and ai,m from ti,m, for all i,m. For future use, we describe the dependency of the second- and fourthorder moments of Ai and ai,m on Ti and ti,m, respectively, by means of the following functions:
 	b(T) , E[A2i||Ti = T]],	b2(T) , E[[A4i||Ti = T]]	(3a)
 	g(t) , E[am2 tm = t ,	g2(t) , E am4 tm = t	(3b)
 In our formulation, the number of delays Ti and the number of ti,m that take values within any unit-length time interval follow Poisson distributions with mean rates ? and ?, respectively. This is also part of the channel models of [1] and [2].
 By choosing the |ai|’s as Rayleigh distributed, for fixed and given delays, and deterministic Ai’s with the form Ai = b0 exp(-Ti/G), g(t) = exp(-t/?), for some ß0,G,? > 0, the above description yields the widely used Saleh-Valenzuela model [2]. A similar choice with log-normally distributed amplitudes |Ai| and |ai| yields the model proposed by the IEEE 802.15.3a task group [1] (excluding its large-scale fading term). The decay profiles of the amplitudes (represented by the functions b and g of (3)) can also be chosen to match those that the IEEE 802-15.4a standard recommends for high-frequencies in some scenario types (such as office and industrial) [9]. Mixed Poisson arrivals, another model feature present in [9], can be handled within the current framework with little additional effort.
 In (1), the number of clusters Nc and the number of paths within each cluster, Nr, are finite. Such choice responds to the fact that, in practice, choosing Nc and Nr large enough will be sufficient to include all multi-path component with significant power. This is equivalent to saying that for every e > 0, there exist finite Nc and Nr such that
  
 with probability close to 1. Clearly, this equivalence requires that the amplitudes decay fast enough with increasing delay. We will make this requirement precise by assuming that the conditional moments of the amplitudes satisfy
 ,(4a)
 	,	(4b)
 and work in the sequel with the channel impulse response
 	 .	(5)
 We will also assume that the conditional moment functions b,b2,g and g2 decay slow enough so that, for some minimum angular frequency ?min rad/s, we can approximate
  8
 where the non-negative integer exponents {ni} satisfy n1 + 2n2 = 2, n3 + 2n4 = 2. Thus, our results will be valid for situations and channels where the band of interest lies above ?min. In general, the smallest ? for which (6) is a good approximation will be determined by whichever of the functions within the integral of (6) decays fastest. Therefore, ?min will depend mostly on the decay time of the clusters, the reciprocal of which should be several times smaller than ?min.
 From (5), the squared magnitude of the channel frequency response is
  
 Since amplitudes and arrival times are random, so is |H(j?)|2. In the following sections we will derive analytical expressions for the second-order statistics of |H(j?)|2 as well of the channel power over a given frequency interval.
 III. SECOND-ORDER STATISTICS OF |H(j?)|2
 Here we will obtain closed-form expressions for the mean, autocovariance, and correlation coefficient of |H(j?)|2 over any interval of angular frequencies [?l,?r] ? [?min,8]. To derive these results, we will make use of the following technical lemma.
 Lemma 1: Suppose the sequence of non-negative random numbers   forms a Poisson process with finite arrival rate  be any two functions such that   exist and are bounded. Then
 8
 	E[ß(xi)] =Rß(x)dx	(8a)
 0
 88
 	E[ß(xi)] = ?	Rß(x)dx,	?j = 1,2,...	(8b)
 	i=1,i=? j	0
 	?8 E[ß(xi)]E[?(xi)] = 0.	(8c)
 i=1
 where E[·] denotes expectation with respect to all the random variables that determine the expression within the square brackets.	N
 Proof: For each  of i.i.d. random variables uniformly distributed over [0,N/R]. We have that
 i=1
 When   becomes the Poisson process
   (see, e.g. [10]). Thus,
  E[ß(xi)] =  lim
 which proves (8a). The validity of (8b) and (8b) is shown in a similar manner.	 
 Expected value of |H(j?)|2 From (7) and (6), it is easy to verify that
 	E .	(9)
 Applying (2) and the independence relationships between amplitudes stated in the previous section, we obtain
 E 
 Thus, the expected value of |H(j?)|2 at any ? = ?min is the product of the average energy of the cluster amplitudes {Ai} and the ray relative amplitudes {ai,m}. Notice also that ?g(t) = ?E[ai2|ti = t] represents the average power profile of rays in a cluster with amplitude Ai = 1. For this reason, it will be referred to as the the intra-cluster PDP. Similarly, ?b(T) = ?E[A2i|Ti = T] represents the average power density of cluster amplitudes. Accordingly, we will call ?b(T) the inter-cluster power delay profile (PDP).
 Autocovariance of |H(j?)|2
 2 Denote the auto-covariance of |H(?)| by c(?1,?2) , E|H(?1)|2 |H(?2)|2]
 (11)
 	2	2
 -
 E[|H(?1)| ]E[|H(?2)| ].
 For notational simplicity, we will temporarily adopt a single indexing nomenclature for the path arrival times ti,m. More precisely, and with a slight abuse of notation, we define the infinite random set   , . For our purposes, it will not be necessary to define a mapping between the indexes i,m and the index l. Indeed, it will be sufficient to note that the double-index matching condition
 the single index conditionti,m = tj,n	??	(i =tjl =ANDtk m??= ln=) is equivalent tok.
 Using single index notation, the first term on the right hand side of (11) is obtained from (5), which yields
 E 
 =	?	E[alakalar e-j(?1[tl-tk]+?2[tl-tr])]	(12)
 l,k,l,r=1
 = ?l=1 [E[a4l] + ?l=1,l=l [a2la2l ]
 ?
 ) follows from Lemma 1, (6), the fact that
 , and the independence of the relative
 ray arrival times ti,m, which imply that the terms in the sum of (12) are zero for the combinations (l = k = l =? = kr)),. (l = k,l ?= l,r =? l), (k =? l,l = r), and (k =? l,l = l,r
 Rewriting (13) with double-indexing notation we obtain
 E 
 On the other hand, from (9),
 8
 E[|H(j?1)|2]E[|H(j?2)|2] = ?	E[ai,m2 ]E[aj,n2 ].
 Subtracting this from (14), using  ( ), and exploiting the independence relationships between {Ai}, {ai,m}, {Ti} and {ti,m} described in the previous section, we obtain
 	8 8	2
 c(?1,?2) = ?i=1 m?=1 (E[a4i,m ] - E[a2i,m] )
 	8	8	8
 + ?i=1 (E[A4i] - E[A2i]2) m?=1 E[ai,m2 ] n?=m E[ai,n2 ]
 ?
 	8	8
 	+ ?E[Ai4] ? E[ai,m2	e-j[?1-?2]ti,m]
 	i=1	m=1
 (15)
 It can already be seen from (15) that the autocovariance ofis the sum of a frequency-independent term and a term which depends only on the frequency difference ?1 . From (6), the latter term vanishes as this frequency difference tends to infinity, so because of the first term, the autocovariance between two infinitely distant frequency values is greater than zero. At first sight this may seem counterintuitive. Nevertheless, such behaviour has also been found in [8], and is consistent with the saturation of the channel diversity order as bandwidth increases reported in [6], [7]. As will be discussed in Remark 1 below, such behaviour is due to the fact that the arrival rates for clusters and its multi-path components are finite. Indeed, as the multi-path environment becomes “richer”, i.e., the number of multi-path component increases, the constant term tends to zero.
 Before performing the last step in our derivation of a simpler closed form expression for c(?1,?2), we define
  .
 With these definitions, and applying Lemma 1, we obtain that the autocovariance of |H(j?)|2 is given by
  
 where
 K1 , 	(17a)
  	(17b)
 and   and   denote the Fourier transforms of
 ?b(T) and ?g(t), respectively. Notice in this expression that only the constants K1 and K2 depend on the conditional fourth-order moments of the amplitudes. Therefore, for given inter- and intra-cluster PDPs, changing the PDF of ray and cluster fades will modify the frequency independent terms K1 and K2. Nevertheless, such change would have no effect on the frequency dependency of the spectral power autocovariance.
 Remark 1: (Dense impulse responses) If we keep the average energy of the the cluster and rays impulse responses constant while increasing the cluster arrival rate ? to infinity, then the frequency-independent terms in (15) and (16) vanish. To see this, notice that the expected cluster impulse response energy is given by
 E  
 where Lemma 1 has been used. Thus, if we keep this average energy constant and increase the density ? by a factor ? > 1, then the function b would have to be divided by ?. By uniformly scaling the Ai’s to yield such reduction, the resulting function b2 is divided by ?2. Therefore, the integral of b2 in (17) takes the form   = (1/?)?08 ?b2(T)dT, which clearly tends to zero as .
 From this analysis it follows that the autocovariance of |H(j?)|2 when the impulse response becomes infinitely dense, say c¯c(?), takes the simple form
 	 .	(19)
 That is, in this case c¯c(?) is the product of the squared magnitude Fourier transforms of the inter-cluster PDP and the intra-cluster PDP evaluated at ?.
 C. Correlation Coefficient of |H(j?)|2
 The correlation coefficient of |H(j?)|2, denoted by ?(?1,?2), is given by
 	 	(20)
 From (11) and (15), (20) becomes
 	 .	(21)
 IV. SECOND-ORDER STATISTICS OVER A FREQUENCY
 BAND
 In this section we will derive closed-form expressions for the mean and variance of the channel power over any band
 	B , [?l,?r] ? [?min,8),	(22)
 where ?l and ?r are in [rad/s].
 1) Expected Value of the Channel Power Over a Frequency Band: Denote the channel power over the frequency band B by  = . Given the fact that E[|H(j?)|2] is constant for all ? ?min (see (9)), it follows from (10) that
 E[ ,
 where
 B , ?r - ?l.
 A. Variance of the Channel Power Over a Frequency Band
 The variance of the channel power over a band B , [?l,?r] ? [?min,8) can be obtained directly from the autocovariance of |H(j?)|2 as follows:
  
 With the change of variables ? , ? - u and B = ?r - ?l, the latter becomes
 	 	(24)
 Substituting (16) into (24),
  
 The fluctuation of the received power over a wireless1 2 c? c? where K , K , b and g are as defined in (17).
 channel defines its fade statistics. The fact that fade depth decreases with channel bandwidth has been reported in the literature, both from empirical data (see, e.g., [7]) as well as using simulations [8]. However, a closed form formula relating fade depth and the classical channel model parameters used here has, to the best of our knowledge, not been reported to date. The results derived above allow us to easily deal with this issue, as seen in the example of the next section.
 V. EXAMPLE
 In this section we will illustrate the application of the previous results, adapting our framework to the “classical” Saleh-Valenzuela model [2]. In this model, the amplitudes |ai,m| are Rayleigh distributed and the Ai’s are deterministic exponentially decaying. The decay profile of the ray amplitudes is given by
 A2i|Ti=T = b(T) = ß0 e-T/G
 E ai,m2	|ti = t = g(t) = e-t/?	(26a)
 (26b)
 where ß0 is the average power gain of a ray at the beginning[ ] of a cluster located at Ti = 0. Since the fourth moment4 2 2 E[x4] of a Rayleigh distributed random variable x is related to its second-order moment as E[x ] = 2E[x ] , we have that
 	E 	(26c)
 	E[ai,m4	|ti,m = t] = g2(T) = 2e-2t/?	(26d)
 With these choices,
 	 	(27a)
 and the squared magnitudes of the Fourier transforms of ?b(T) and ?g(t) are
  	 	 
 In order to find the autocovariance of |H(j?)|2 for this example, we substitute (27) into (16), obtaining
  
 The variance of |H(j?)|2? = 0is directly obtained by evaluating, which yields the previous expression for
  
 From this, (28) and (21), the correlation coefficient satisfies
 	 .	(29)
 Thus, again it can be verified that, as discussed in Remark 1, the lower asymptote of the spectral power correlation vanishes only2 when the cluster arrival rate ? ? 8 as the mean channel power is kept constant.
 B. Second-Order Statistics Over a Band The variance of the channel power over a band B ?
 (?min,8) rad/s is obtained by substituting (27) into (25), and then evaluating the integrals in (25). This yields
  
 On the other hand, from (10), E[PB] = ß0??G?B. Thus, the normalized channel power variance for this example is
 var(P )	1 + ??	1
 B
 E[
 Equation (31) reveals several interesting aspects about the normalized channel power variance, which are discussed below:
 The normalized channel power variance is the sum of three terms. The first one corresponds to K1, which depends on all the parameters except the bandwidth B. It is inversely proportional to the product G?, which is in turn proportional to the average number of clusters with significant amplitudes in the channel impulse response. This term is the lower asymptote of var(PB)/E[PB]2 when B ? 8, and the latter limit corresponds to total channel power variance divided by its squared mean. Again, in agreement with what was shown in Remark 1, this term vanishes only when the impulse response becomes infinitely dense with clusters. From the viewpoint of (31), this is explained by the fact that, with a finite number of clusters with random amplitudes, the law of large numbers does not apply to the total impulse response energy, and hence its variance-to-squared-mean ratio is not zero.
 The second term on the RHS of (31) depends on B and all the other parameters, except ?, which means it does
 2Although letting G ?8 also makes the right hand side of (29) vanish, letting G grow unbounded would violate the conditions under which the above results are valid (see (26) and (4)).
  
 Figure 1. Autocorrelation coefficient for the S-V model with parameters from the IEEE802.15.3a [1] CM3. Simulated values are averages over 60.000 channel realizations. ?1/(2p) = 1 GHz. The analytical plot was obtained using (28) and (21).
 not change with the arrival rate of rays within clusters. This term is monotonically decreasing in ?B and nonnegative, approaching 1/2(G?) when ?B ? 0, and 0 when ?B ? 8. It also vanishes when the cluster arrival rate tends to infinity.
 The third term depends only on B and the intra- and inter-cluster power decay exponents. In particular, it is independent of cluster and ray arrival rates. It is monotonically decreasing with the bandwidth B, tending to zero when the bandwidth B is increased to infinity, and to 1 when B ? 0.
 It can be seen that the normalized channel power variance over a band grows unbounded as G??? ? 0. This is due to the fact that, when G??? ? 0, the channel power is close to zero with increasing probability.
 To see how this tendency can make the ratio var(P )/E[P ]2 go to infinity, consider a simpler exam-
 	B	B
 ple. Let the discrete random variable X take the values X = 0 with probability p and X = 1 with probability 1 - p. Then var(X)/E[X]2 = p/(1 - p), which clearly goes to infinity as p ? 1.
 The narrow-band normalized channel power variance, var(P0)/E[P0]2 is obtained from (31) by letting ? ? 0. The ratio between the normalized variances of power of a narrowband and a wideband system, i.e., [var(P0)/E[P0]2]/[var(PB)/E[PB]2] can be regarded as a measure of frequency diversity order. From (31), such diversity order would range from 1 (when B ? 0) to 1+(??+2?G??)/(1+??), when B ? 8. For ?? » 1, the latter limit is approximately 2+2?G, which is roughly twice the average number of clusters that fall within G seconds. It can also be seen from (31) that before reaching this limit, and for B » 1/?, the diversity order grows approximately linearly with B, consistent with emprical results [6], [7].
 The autocorrelation coefficient for the S-V model with the parameters of the IEEE802.15.3a CM3 [1] is plotted in Figure 1, both from simulations and from our equations. The parameters of the CM3 are: ? = 0.0667 ns-1, ? = 2.1 ns-1, G = 14 ns, and ? = 7.9 ns. It can be seen from this figure that values obtained from channel simulations agree well with (21). In particular, ?(w1,?1 +?) crosses the 0.5 threshold for ?/(2p) between 1/(2pG) ? 10 MHz and 1/(2p?) ? 20 MHz, as predicted by (28), and gets close to its lower asymptotic value when ?/(2p) ? 100 MHz.
 VI. CONCLUSIONS
 We derived general expressions that characterize the secondorder statistics of the frequency response power gain in microwave indoor channels. Our results are applicable to several well established channel models discussed in the literature. In particular, the closed-form formula obtained here for the autocovariance of the squared frequency response magnitude of the channel allows one to predict the variance of the channel power over any frequency band. This provides an approximation to the diversity order of wide-band over narrowband systems in such channels. Our results also allow one to obtain an upper bound for this measure of diversity, and show how (and why) this limit arises from having a finite number of clusters in the channel impulse response.
 ﻿In this paper we propose a new approach to robust optimal experiment design. The key departure from earlier work is that we specifically account for the fact that, prior to the experiment, we possess only partial knowledge of the system. We also give a detailed analysis of the solution for a simple case and propose a concave optimization algorithm that can be applied more generally.
 Keywords: Experiment Design, Optimal Input Design.
  
 1.	INTRODUCTION
 It is well known that the choice of experimental conditions has a strong influence on the accuracy of models obtained from system identification experiments. This has motivated substantial research on experiment design over almost a century. Early results appear in the statistics literature (Wald, 1943; Cox, 1958; Kempthorne, 1952; Kiefer and Wolfowitz, 1960; Karlin and Studden, 1966; Federov, 1971; Whittle, 1973; Wynn, 1972). This work was later adapted to the problem of identification of dynamic systems (Levadi, 1966; Gagliardi, 1967; Goodwin and Payne, 1973; G.C. Goodwin and Murdoch, 1973; G.C. Goodwin and Payne, 1973; Arimoto and Kimura, 1973; Mehra, 1974). Some of the later work is summarised in (Goodwin and Payne, 1977; Zarrop, 1979).
 Our focus here will be on experiment design for dynamic systems. Early work on this problem focused predominately on frequency domain designs. More recently, there has been substantial effort devoted to the inter-relationship between identification and control together with the associated issue of input signal design (Hilderbrand and Gevers, 2003; Hjalmarsson, 2005). However, a major drawback of all the above work is that, generically, the optimal test signal for dynamic system identification is a function of the unknown system (i.e. it depends on the very thing that the experiment is aimed at finding).
 Indeed, quoting (Hjalmarsson, 2005): “It should be noted that, as usual in experiment design, in order to compute the optimal design the true system has to be known. Methods that are robust with respect to uncertainty about the system is a wide open research
 field.”
 The goal of the current paper is to propose a methodology for addressing this problem. In particular, we formulate a robust optimal experiment design criterion. We also demonstrate the use of this criterion for a simple case.
 The layout of the remainder of the paper is as follows: In Section 2 we give a general formulation of the robust optimal experiment design problem. In Section 3, we focus on a simple (one parameter) problem so as to give insight into the problem. In Section 4 we convert the problem to an approximate matrix formulation and discuss asymptotic behavior when the prior knowledge is diffuse. In Section 5 we describe an algorithm for computing the robust optimal experiment and give a numerical example. In Section 6 we describe an extension to multi-parameter systems. Finally, in Section 7 we draw conclusions.
 2.	GENERAL FORMULATION
 Our focus in the current paper is on how to design the experimental conditions so that the information obtained from a particular experiment is maximized in some specific sense.
 To motivate our approach we consider a single input single output linear discrete time system of the form:
 yt = G1(q)ut + G2(q)wt (1) where G1(q), G2(q) are rational transfer functions in the forward shift operator q, G2(8) = 1 and where
 {wt} is Gaussian white noise of variance s. We let   where ? denotes the parameters
 in G1(q) and ? denotes the parameters in G2(q).
 We recall that the log likelihood function for data Y given parameters ß, is given by
  
 	where et = G2(q)-1 [yt - G1(q)ut].	(3)
 Fisher’s information matrix is obtained by taking the following expectation (Goodwin and Payne, 1977)
  
 and EY |ß denotes the expectation over the distribution of the data given ß.
 We assume an open loop experiment so that wt and ut are uncorrelated. We also assume that G1(q), G2(q) and s have no common parameters. Taking expectations, as in (4), we have that M can be partitioned as
 	 	(7)
 where M1 is the part of the information matrix related to ? and M2 is independent of the input. Then
 	 ,	(8)
 	where	 
 Notice that M1 depends on the full parameter vector ß. Assuming N is large, it is more convenient to work with the scaled average information matrix for the parameter ?, i.e.
 	 	(10)
 Utilizing Parseval’s Theorem, we finally have that
  
 where fu(ej?) is the discrete input spectral density.
 It is also possible to do a parallel development (Goodwin and Payne, 1977) for continuous time models. In the latter case, (11) is replaced by
  
 where G1 and G2 are continuous time transfer functions (assumed independently parameterized) and fu(?) is the continuous time input spectral density.
 Since M is a matrix, we will need a scalar measure of M for the purpose of experiment design. In the nominal case treated in the literature (i.e. when ß is
  
 assumed known), several measures of the “size” of M have been proposed. Examples are
 (i)	D - optimality (Goodwin and Payne, 1977)
  
 	Jd(ß,fu) = detM(ß,fu)	(13)
 (ii)	Experiment design for robust control
 (Hjalmarsson, 2005; Hilderbrand and Gevers,
  
 where f(ß,?) is a frequency dependent vector related to the ?-gap (Hilderbrand and Gevers, 2003).
 Thus nominal experiment design is aimed at choosing fu(·) to maximize a function of the type shown in (13), (14). Note, however, that the optimal input spectrum depends, inter-alia on the unknown parameter vector ß. To address this paradox, we propose an alternative robust optimal experiment design procedure.
 We assume that we do not have complete a-prior knowledge of the true parameter value ß. Instead, we assume that the parameters can take any value in a compact set T. We propose that fu*(·) be chosen as:
 	 	(15)
  
 where J is any suitable scalar measure of M. Note that we also allow J to depend explicitly on ß. This is standard in nominal experiment design, see for example (14). Also, we can use the explicit dependence on ß to
  
 associate different weighting to M(ß,fu) depending on the nature of the prior knowledge regarding ß.
 We also need to constrain the allowable set of input signals. A typical constraint used in experiment design is that the input energy is constrained i.e. we define
 S 
 We note that the results are independent of s as it appears only as a scaling factor in (8). For clarity in the following discussion we assume white noise and hence only refer to the parameter ? in the sequel.
 3.	A SIMPLE EXAMPLE
 To give insight into the robust optimal experiment design problem, we consider a simple continuous time problem where G2 (s) = 1 and
 	 	(17)
 For the model (17), it follows that
  
 3.1 Nominal Optimal Experiment Design.
 Nominal experiment design assumes that an initial estimate, is available. Based on this information, the function fu (·) is chosen so as to maximize some scalar function of  subject to a constraint on the output or the input power.
 One interesting observation is that equations (18) and (19) define a convex combination of the set of all single frequency matrices. This leads to several use-
  
 ful facts, e.g. any value of M (?,fu) for fu ? S can be generated by a finite number of frequencies. Also, in the scalar parameter case of equations (18) and (19) it can actually be shown that we need only use a single frequency input for optimal experiment design (Goodwin and Payne, 1977), namely, fu (?) = d (? - ?*). Moreover, by differentiation it is readily seen that the optimal input frequency is
 	?* = ?	(20)
 This is an intuitively pleasing result, i.e. one places the test signal at the (nominal) 3dB break point. However, equation (20) reinforces the fundamental difficulty in nominal experiment design, namely the optimal experiment depends on the very thing that the experiment is aimed at estimating.
  
  
 Fig. 1. ?2M (?,fu) as a function of ? for nominal input (dots), “1/f” noise (solid) .
 To gauge how important the dependence on ? is, we note thatin our example decays at the rate of 40dB per decade as a function of both ? and ?. Hence, given the prior estimate of the paramefrequency. Also, say that the true parameter lies in ter, ?, say we choose for the input signal
 the range  is ap-
 proximately 1/100th of the nominal value! This seems to suggest that nominal experiment design is limited to those cases where a good prior estimate is available. A
  
 plot of ?2M (?,fu) versus ? is given in Figure 1.
 The reason for multiplying by ?2 is that M-1 is a variance measure and thus   gives relative
 (mean square) errors.
 3.2 Robust Optimal Experiment Design
 We next turn to the robust experiment design described in Section 2. For the scalar parameter problem
  
 all measures of M are equivalent and we thus use
 	 	(21)
 Thus, our robust optimal experiment design can be
 stated as
  = arg max
 u
 where
 	 .	(23)
 In subsequent sections, we will give further insights into the above design problem.
 We first observe the following property of :
 Lemma 1. Consider the problem stated in equation (22), the optimal input has all its energy inside T. Namely,
 	 	(24)
 PROOF. Let   be the optimal solution of (22). Then,
  
 for all fu ? S
 Let us now define
  
 where XT (?) is the indicator function for the set T.
 Then we readily observe that	and
 ?
 = 0 then
  
 so that by (25) we must have which completes the proof.
 4.	APPROXIMATE ANALYSIS
 Here we develop an algorithm for the robust experiment design problem, as formulated in the last section, where we approximate the integral in equation (22) by a Riemann sum. Specifically, utilising Lemma 1, we
 choose a grid of N points  
 0 = m = N so that .
 Then
  (27)
 where
 (28)
 and
 	-	.	(29)
 Note that the matrix A = {Am,n} is symmetric and has positive entries.
 We can now state the following discrete alternative to the optimization problem in equation (22):
 	E* = arg max	 	(30)
 E?Sd 0=m<N
 where S ,
 E  is the mth column
 of the N dimensional identity matrix and 1 is an N dimensional vector of ones.
 We next show that if there exists a feasiblesuch that all entries ofare equal, then E is optimal.
 Lemma 2. Let E be defined by
 	 .	(31)
 Then, if 	we have*	
 	E = E.	(32)
 PROOF. Let E* be the optimal solution for (30) and E?Sd. Then we have
 	 	(33)
 and, since e  for all 0 = m < N,
  e
 	efor all 0 = m < N .	(34)
 On the other hand, using the symmetry of A we obtain
  
 negative we must conclude from (34) and (35) that ??? Since by definition, all the entries of E are nonE  which completes the proof.
 The discussion above holds for any choice of grid for
  . Let us now consider a special
 choice of a logarithmic grid:
 	?m = ?m = ??m	(36)
 where
 	  .	(37)
 With this specific choice we have the following result for our original problem (22),
 Lemma 3. The robust optimal test signal (for diffuse prior information) has spectrum approximately given by
 	 .	(38)
 PROOF. Note first that since  .
 Furthermore, clearly,  , so we have
  S . Let us now choose a large integer N
 and the corresponding grid as defined in (37) and (38).
 Then, by definition  and
 by (29)
 Clearly,  as
  ). On the other hand, we note
 that for the matrix A in (28), apart from the first and last few rows, the row sum is very nearly constant.
 Hence,   for some ? > 0.
 Namely, the conditions in Lemma 2 are approximately satisfied which completes the proof. ???
 Remark 4. The immediate consequence of Lemma 3 is that band limited “ ” noise is an approximation to the robust optimal input for our example (see Figure 1).
 5.	A NUMERICAL EXAMPLE
 We show that the optimization problem (30) can be converted into a standard linear programming (LP) problem. Let us denote
 	F 	(39)
 then we can readily show that (30) is equivalent to the following optimization problem:
 subject to:	maxF CF	(40)
 (41)
 where
 (42)
 and
 This problem is readily recognized as a LP problem.
 Numerical Example
  
 We consider the scalar parameter problem described in Section 3 where we assume  ,
 N = 100 and compare
  
 Fig. 2. Values of E for robust optimal input.
 (i)	A nominal input of frequency 1 radsec (Note that this is the optimal input if the initial estimate of the
 (ii)	Band limited white noise input, [0.1,10] rad/sec. parameter is ? = 1).
 (iii)	Band limited ‘f1’ noise input, [0.1,10] rad/sec. (iv) The robust optimal input generated by LP.
 Relative costs for the different experimental conditions are shown in Table 1.
 Table 1. Values of Cost for Different Input Signals
  
 We see from Table 1 that “1/f” noise is approximately an order of magnitude better than a white noise input in terms of the cost function (30). Furthermore, going to the true optimum gives a further 40% improvement. The optimal input energy is shown in Figure 2 and Fig-
  
 ure 3 shows the corresponding values of ?2M (?,fu) as a function of ?. It is interesting to note from Figure
  
 3 that ?2M (?,fu*) is an almost constant function of ?. This should be compared with the result in Lemma 2. The latter Lemma predicts that if the input that generates  independent of ? is feasible, then
 it is optimal. Here, the input which gives ?2M (?,fu) constant is not feasible but we see from Figure 3 that the optimal values of  are almost constant.
 6.	GENERALIZATION TO MULTI-PARAMETERPROBLEMS
 For the multi-parameter case we return to the general
  
 expression for M (ß,f) given in (11) and (12). We can again convert this problems into an approximate discrete form as was done in Section 4. We write
 	 	(43)
  
 Fig. 3. Variation of cost function with ? for optimal input.
 as an approximation to the integral in (12). The index k refers to the (discretized) element ?k of the parameter set T, the index m denotes the frequency and Em denotes the input energy at the mth frequency.
 In this case, we see that Qk is a matrix for each discrete parameter value. Hence, as discussed in Section 2, we need a measure of the “size” of Qk. Say we choose J (Qk,k), then the robust optimal design becomes
 E* = arg max minJ (Qk (E),k)	(44) E?Sd k
 Obviously, there are many choices for the scalar function J. One possible choice is
 	J (Qk (E),k) = ?min {Qk (E)}	(45)
 This cost function is motivated by the following observation: Defining the scalar function f (E) =
 min?min {Qk (E)} we note that, by (43) we have k
 f (µE1 + (1 - µ)E2)
 = min?min {Qk (µE1 + (1 - µ)E2)} k
 = min?min {µQk (E1) + (1 - µ)Qk (E2)} k
 min(µ?min {Qk (E1)} + (1 - µ)?min {Qk (E2)}) k
 µmin?min {Qk (E1)} + (1 - µ)min?min {Qk (E2)} k	k
 = µf (E1) + (1 - µ)f (E2)
 which shows that f (E) is a concave function of E. Hence, solving (44) becomes a standard concave maximization. This latter aspect is the subject of the journal version of this paper.
 7.	CONCLUSION
 This paper has proposed a robust optimal experiment design procedure. We have argued for a simple case, that a near optimal input is band-limited “1/f noise”. We have also proposed an algorithm to design robust experiments in more general cases and have presented results showing the gains obtained from the use of the algorithm compared with using either the “nominal optimal experiment”, band limited white noise or band-limited “1/f” noise.
 
 ﻿ In this work we obtain capacity for a class of pair bidirectional relay , where one relay can help between the corresponding user . For the , we apply the generalization of successive compute strategy for the linear of the of each user pair at the relay . The channel is considered as a broadcast network . It is shown that for all channel gains , the achievable rate boundary within of N and N sec below the cut set upper bound for restricted and nonrestricted , respectively . These tend to sec per user to infinity . We first derive a comprehensive formulation for the step asymmetric and use it to derive the capacity result for our problem . 
 I . INTRODUCTION 
 In , a compute and forward strategy been for relay with equal power and asymmetric channel gains based on lattice . The receiver can recover the associated linear which are closer to the channel fading . This strategy simultaneously protection against noise and the opportunity to exploit interference for gains . In , a scheme been for relay with unequal power and symmetric channel gains based on lattice where it an achievable cast rate within a constant gap below the capacity . In , a successive scheme been for the same network as defined in . In this method after a linear combination of , the receiver can combine it with its channel observation to obtain a new effective channel that is better for the next targeted linear combination . In and an asymmetric been for and relay , respectively , with unequal power and asymmetric channel gains based on lattice . 
 In and , a asymmetric strategy been . The method a set of scaling to decode integer linear of . The use of scaling in fact is equivalent to non integer linear of lattice . It different to have different . Thus , by appropriately those , different on the boundary of the rate region can be . Also , in , the idea of non integer linear of lattice been which can be considered as a special case of the of and . 
 In this paper we generalize the method of to . We also consider scaling as defined in . Then we use these to address Pair Relay , where one relay can help between the user of each of the relay . We present an achievable rate region for the based on the generalization of the strategy . It is shown that this method within log N and log N of the cut set upper bound for the restricted and non restricted , respectively . to infinity , these tend asymptotically to . Some of the previous works about these are as : 
 In , an achievable rate region was for two way relay based on the of lattice and . In that network , a relay node the communication between two of . They assume a complex channel model . It was shown that for all channel gains , to within sec per user of the cut set upper bound are achievable . In , we studied the same network model as in but with the assumption of real channel model , a achievable rate region for that network by lattice and the successive compute and forward approach of . It was shown that this method within and sec per user of the cut set upper bound for restricted and non restricted network , respectively . The paper is organized as : In Section the system model for the general relay network is defined . In Section we present our generalization of the approach . The system model , achievable rate and capacity for the pair two way relay are in Section . Some concluding are provided in Section . 
 . THE GENERAL RELAY NETWORK 
 In this section we consider a relay network . We use the same model as defined in , but with the assumption of unequal power and asymmetric channel gains . Each relay indexed by , a noisy linear combination of the through the channel , 
 L 
 ym `` , 
 ` 
 where are the channel gains and is i . i .. noise , , In . Let denote the vector of channel gains from relay . Each channel input ` is a length sequence subject to the average power constraint `, for all `, i . e . The channel gains are assumed to be known and constant . 
 Let , 
 ; denote the set of , and to the nearest integer value . Throughout the paper , are shown with . 
 . ASYMMETRIC SUCCESSIVE COMPUTE AND FORWARD 
 In this section we propose a straightforward and comprehensive upper bound to the rate of the step asymmetric successive compute and forward strategy . We consider the system model as defined in Section , which unequal power and asymmetric channel gains . We also consider real valued scaling for the non integer linear combination of lattice . 
 In step , at each step , the receiver can combine its estimation from the previous step with its channel observation to obtain a new effective channel that is better for the next targeted linear combination . This for . The general formulation step is expressed in the following theorem . 
 Theorem . Consider the relay network defined in Sec with unequal power P and equation coefficient ami , where ami ami , ami , and i ;. Let real . By step strategy , if the computation rate R is achievable , then 
 R ` 
 , i mi ,` N 
 for all ` ;, where is expressed as , 
 , 
 for all i , with 
 , 
 h , a 
 a 
 Proof : The proof can be done by combining the used in , , and . 
 Remark . For , and ` , for all `, to the in . 
 Remark . Equal power and asymmetric channel For ,` , and ` , for all `, to the in and . 
 Remark . Unequal power and symmetric channel For , , ` for all `, and a I , to the in . 
 As it is seen from the above , the formulation in is capable of both unequal power and asymmetric channel even if we omit the scaling from the . 
 . PAIR TWO WAY RELAY CHANNEL 
 In this section we present the system model for a class of pair two way relay channel of . We express the cut set upper bound , achievable rate based on the asymmetric strategy and calculate the capacity . We also show that the according to the channel gains the achievable rate region . 
 A . System Model 
 A shown in Fig . , where i and i denote the pair of which can communicate to each other via node a relay . There is no direct link between . The relay is assumed to be able to listen and transmit at the same time . and related to the pair are as and 
 , , respectively . 
 Let denote the node of the pair , where , . 
 For each ;, let ; denote the set of all the of , where i , and , such that if , nil then , i . e ., include two of a pair . For example for , , denote 
 the set of i , such that , for ;, number of the of . For , , let , denote the other node in a pair which with , i . e ., if then . Let ; denote the message that pair i to 
  
 Fig . : The pair two way relay channel . node of pair i . This message is into a an function , for ;, where , the number of channel , is a realization of a real random variable , satisfying the power constraint , and , the previously received at node , of pair i ;. This mode , non restricted , can introduce some dependency between the by different . In contrast , restricted the function . The are assumed to be independent . The rate of transmission is defined as . 
 The received at node , for all i ,, and at the relay node ,, at time instant , are given by 
 , a 
 N 
 , b 
 i 
 respectively , where the signal by the relay at instant . and are independent and of i . i .. real valued noise , . and denote the and channel gains , respectively , for all i ,. Node a function to decode as , where . 
 B . Cut Set Upper Bound 
 In this section the cut set upper bound for restricted and non restricted are given . These are of the cut set upper bound in and for two pair two way relay . 
 Theorem . The capacity region of the non restricted is upper bounded by , 
 , 
  
 b 
 where ;. 
 Proof : Consider the cut ;, the first and second in the right hand side of b denote the upper for the multiple access and the broadcast of the relay network , respectively . 
 Corollary . For the capacity region of the restricted , we have the outer bound a but b is as , 
  
 where ;. The proof can be considering the fact that the are mutually independent in the restricted network . 
 Lemma . The non restricted cut set upper bound is within bit sec of the restricted cut set upper bound . 
 Proof : The proof can be done by the same way as done for two pair two way relay network in . 
 C . The Achievable Rate Region 
 In this section we present an achievable rate region for the . In our scheme , we use a restricted function , so it will be an achievable rate region for both restricted and non restricted network . The idea is based on the fact that in the communication , can be considered as an network with N and one receiver , and so we can apply the asymmetric successive compute and forward method to decode the linear of the of each pair successively . 
 In the , is considered as a broadcast channel with one transmitter at the is done by successive interference cancellation . Note that we of N , because of the fact that each pair is a two way relay channel and the of each pair is not considered as a broadcast channel since each receiver about its own message and can subtract it from the its signal . 
 at the : N are such that N . For each ` : N , ` second moment ` , for some distinct i : and , , where P P . All are assumed to be simultaneously good . For each message of , the ` ` is , where ` the region of `, and ` : N . The transmitter i 
  
 where ` ` and ` is a random vector uniformly distributed in `. So , and to an ensemble 
 of lattice with sizes , with independent of ` and uniformly distributed in `. 
 at the Relay : at the Relay is done according to the successive compute and forward method in Theorem . Here we have one relay , so . Without loss of generality , we assume , 
 , where , for all i . 
 At the stage of successive compute and forward , the relay the linear of lattice code 
 , with the coefficient vector , 
 , 
 in which only the i th and i th are non zero . So , we have 
  
 Theorem . The coefficient ai can be successfully at the stage successive compute and forward , so long as , 
  
  
 for i ;. 
 Proof : The combination of the channel gains and node the following vector of incoming signal at the relay : is defined as 
 . 
 More similarity between and in a achievable rate region see Theorem . So we select and . Also in order to attain simpler and also a achievable rate , we consider power allocation of the as , 
 , 
 where , e .., if , we can set and 
 . 
 Substituting the above in and in . 
  
 at the relay : The ti to from with sizes for i ;, respectively , where 
 . The signal by the relay is as 
 N 
 q 
 where , 
 i 
 N 
 , 
 i 
 i 
 and is a with size d , for all i . 
 at : at is done by the successive interference cancellation method . In this way , the user with the channel gain first the related to the with worse channel gain . Then , it the message related to itself , i . e ., the message of the other user which to its pair . Thus , at each user with a specific channel gain , only the of other with higher channel gains can cause interference . For each , let ; i denote the such that for each , for some j , . By the strategy , it can be seen that the following message are : 
  
 For example , let , and assume that , so we have S , S , , S , , S , S , , S , , and at the is done as : 
 at the of the first pair , i . e ., , , with channel gains and can be done with an arbitrary small probability of error if the message associated to 
 the part part of satisfy the following : 
 a 
 . b 
 User of the first pair the highest channel gain , thus it first the other and then them from its received signal . Then it the message of user of the first pair . For user of the first pair so the 
 part and part of make interference in the of part . 
 By the same reasoning , the following message are for at the other , 
 , a 
 b 
 c 
 d 
 D . Capacity Gap Calculation 
 In this section we state the capacity gap for the and of the restricted in the following Lemma : 
 Lemma . By the strategy , for any rate i ;, , satisfying 
 a 
  
 X P log , b 
 , 
 d 
 where ;, there a choice of power such that of at the in the be done with arbitrary small error probability for all channel gain . 
 Proof . We must show that if , and a d are satisfied , all power scaling between zero and are . Next we prove the theorem for a and b , the proof for the can be done similarly . We need the following inequality : if then 
 x 
 From we have 
 . 
 By induction , for i , where ; , we have , 
  
 For , we have . 
  
 From the above Lemma , it is seen that the maximum gap between inner and outer is , and from Lemma for the non restricted , this gap is sec per user . 
 : 
 For , i . e ., the two way relay channel , the gap for the non restricted channel model is equal to which is compatible with the result of in the literature . For , i . e . the two pair two way relay , the 
 are equal to and 
 for restricted and non restricted channel , respectively which are the same as . 
 As , the for restricted and non restricted channel , i . e ., and asymptotically goes to . 
 V . CONCLUSION 
 In this work , we a comprehensive asymmetric step successive compute and forward scheme which the previously . Based on the approach , we the capacity for a class of pair two way relay , where one relay can . It was shown that for all channel gain , the compute and forward scheme to within constant log N and log N sec per user of the cut set upper bound for the restricted and the non restricted network , respectively . These tend asymptotically to sec per user to infinity . 
  
  
 ﻿ We present several novel and the mutual information and the directed information in with feedback . The internal within such are restricted only to be causal , but are to be non linear , stochastic and time . Moreover , the involved can be arbitrarily distributed . We bound the directed information between inside the feedback loop by the mutual information between inside and outside the feedback loop . This fundamental result an interesting interpretation as a law of conservation of information flow . Building upon it , we derive several novel and , which allow us to prove some 
 information under less restrictive . Finally , we establish new between directed inside a feedback loop . This a new and general data 
 The notion of directed information by in the amount of information that causally from a given random and ordered sequence to another . For this reason , it increasingly found use in diverse , from the capacity of with feedback , the rate distortion function under causality , some of the fundamental in 
 control , causal in neural , to portfolio theory and hypothesis testing , to name a few . 
 The directed information from a random sequence to a random sequence is defined as 
 where the notation xi the sequence , ,..., i . The causality inherent in this definition becomes evident when it with the mutual information between and , given by 
 sequence present in i , given the past . By contrast , in the conditional mutual in the sum of , only the past and current of are considered , that is , xi . Thus , I the amount of information causally from to . 
 There exist several the relationship between I and I ; . First , it is well known that I I ; , with equality if and only if is causally related to . A conservation law of mutual and directed information been found in , which that I I I ; , where the concatenation , ,.. . 
 Given its prominence in feedback , it is perhaps in these where the directed information becomes most important . For instance , the directed information been instrumental in the capacity of with feedback see , e .., , , and the therein , as well as the rate distortion function in feedback , , . 
 In this paper , our focus is on the and directed and mutual within feedback , as well as between directed different within the corresponding feedback loop . In order to discuss some of the related to this problem , it is convenient to consider the general feedback system shown in Fig . a . In this diagram , the S ,..., S represent possibly non linear and time causal such that the total delay of the loop is at least one sample . In the same figure ,,,, are exogenous random , or , which could represent , for example , any combination of random initial or side . We note that any of these exogenous , in combination with its corresponding deterministic Si , can also yield any desired stochastic causal . 
 For the simple case in which all the Si i are linear time invariant and stable , and assuming ,, , it was shown in that I does not depend on whether there is feedback from e to u or not . between mutual and directed in a less restricted setup , shown in Fig . , have been found in , . In that setting a system , is a strictly causal dynamic system vector state sequence , with x , being the random initial state in its state space representation . The external signal which could correspond to a disturbance is statistically independent of , the latter corresponding to , for example , side information or channel noise . Both are also statistically independent of x . The E , 
 Figure . a : The general system considered in this work . : A special case , corresponding to the closed loop system studied in . 
 D to an , a and a channel , respectively , all of which are causal . The to in a possibly time manner , i . e ., . Similarly , the concatenation of the , the channel and the , and to u as a possibly time dependent function u . Under these , the following fundamental result was shown in , Lemma . : 
 By further assuming in that Fig . is deterministic , the following chain naturally , 
 which is found in the proof of , Corollary . . The deterministic nature of crucial role in the proof of this result , since otherwise the chain does not hold , in general , due to the feedback from u to . 
 Notice that both and provide lower to the difference between two mutual , each of them a signal external to the loop such as x , to a signal internal to the loop such 
 which for the system in Fig . a and in , Theorem and later in , Lemma . . , the directed information between two internal and the mutual information between the second of these and an external sequence . A related bound , similar to but information and with the leftmost mutual information by the directed information from to which are two internal to the loop , been in , Lemma . : 
 . This result on three : a that the memory less and a conditional invertibility property , a finite memory condition , and a fading memory condition , these two related to the see Fig . . It is worth that , as defined in , these the use of side information by the and or the possibility affected by random noise or a random internal state which is non observable please see for a detailed description of these . 
 The inequality recently been extended in , Theorem , for the case of discrete valued random and assuming ,,, as the following identity written in of the and setup shown in Fig . a : 
 q in Fig . a and with the additional assumption that , , it was also shown in , Theorem that 
 for the in which u i i i i . e ., when the concatenation of S and S to a node . In , and play important in the capacity of with noisy feedback . 
 To the best of our knowledge , , , , and are the only available in the literature which lower bound the difference between an internal to internal directed information and an external mutual information . There exist even in relation to between two directed only internal to the loop . To the best of our knowledge , the only inequality of this type in the literature is the one found in the proof of Theorem . of . The latter the form of a quasi data inequality for directed in closed loop , and that 
 provided , and if S is such that is a function of i . e ., if S is conditionally invertible i . In , 
 to the causally conditioned directed information defined in . Inequality a crucial role , since it lower bounding the average data rate across a digital error free channel by a directed information . In , to a random dither signal in an entropy 
 In this paper , we derive a set of information and of internal or external to the loop in feedback . The first of these is an identity which , under an independence condition , can be as a law of conservation of information . The latter identity is the starting point for most of the which follow it . Among other , we extend and to the general setup in Fig . a , where none of the made in except causality needs to hold . Moreover , we will prove the validity of without assuming the conditional invertibility of S nor that ,. The latter result is one of four novel data derived in Section , each two directed valid for the system in Fig . a . The last of these is a complete closed loop counterpart of the traditional open loop inequality . 
 The remainder of this paper with a description of the under study and the extension of directed information to the case in which each of the in the loop may introduce an arbitrary , non negative delay i . e ., we do not allow for anticipation . The information and are in Section . For clarity of the exposition , all the are deferred to Section . A brief discussion of potential of our is in Section , which is by the in Section . 
 We begin by providing a formal description of the S ... S in Fig . a . Their are given by the possibly deterministic 
 e i S d i a i S d i , pi , b i S xi d i , si , c u i S d i d 
 where ,,, are exogenous random and the possibly time d , d , d , d , ,... are such that 
 That is , the concatenation of S ,..., S a delay of at least one sample . For every i ,...,, i i , i . e ., i is a real random vector whose dimension is given by some function : ,.. . 
 As stated in , the directed information as defined in is a more meaningful measure of 
 the flow of information between and than the conventional mutual information I ; 
 discrete valued , input and output , respectively , of a forward channel , and if there strictly causal , perfect feedback , so that i i a scenario in as part of an argument in favor of the directed information , then the mutual information becomes 
 I ; . 
 Thus , when strictly causal feedback is present , I ; to account for how much information about been to through the forward channel that between them . 
 It is important to note that , in as well as in many works concerned with , the forward channel is instantaneous , i . e ., it no delay . Therefore , if a feedback channel is , then this feedback channel must have a delay of at least one sample , as in the example above . However , when the system in Fig . a , we may need to evaluate the directed information between and which are , respectively , input and output of a strictly casual forward channel i . e ., with a delay of at least one sample , whose output is instantaneously fed back to its input . In such case , if one further perfect feedback and i i , then , in the same spirit as before , 
 As one can see , definition of directed information to be meaningful if instantaneous feedback is . 
 It is natural to solve this problem by that , in the latter example , the forward channel had a delay , say , greater than one sample . Therefore , if we are interested in measuring how much of the information in , not present in , was from xi through the forward channel , we should look at the mutual information I i ; xi , because only the input xi can have an influence on i . For this reason , we introduce the following , notion of directed information 
 Definition Directed Information with Forward Delay : In this paper , the directed information from 
 to through a forward channel with a non negative time delay of i is defined as 
 For a zero delay forward channel , the latter definition with . 
 Likewise , we adapt the definition of causally conditioned directed information to the definition 
 Before finishing this section , it is convenient to recall the following identity a particular case of the chain rule of conditional mutual information , which will be extensively in the of our : 
 We begin by a fundamental result , which the directed information between two within a feedback loop , , to the mutual information between an external set of and : 
 This fundamental result , which for the in which ,, can be understood as a law of conservation of information flow , is in Fig . . For such , the information causally information flow from ,, to . When ,, are not independent of , part of the mutual information between ,, and corresponding to the term can be thought of as being through , thus the forward link . This an intuitive interpretation for . 
 Figure . The flow of information between exogenous ,, and the internal directed information from to when ,,. 
 Remark : Theorem that I is only a part of or at most equal to the information flow between all the exogenous entering the loop outside the link namely ,,, and 
 y . In particular , if ,, were deterministic , then I , regardless of the S ,..., S 
 Remark : By ,. Then , Theorem , we recover , whenever ,,. Thus , , Theorem and , Lemma . . can be as 
 The following result an inequality I with the separate of information 
 Theorem that , provided ,, , I is lower bounded by the sum of the individual from all the in any given partition of , to , provided these are mutually independent . Indeed , both and can be generalized for any appropriate choice of external and internal . More precisely , the set of all external in a feedback system . Let a and be two internal in the loop . Define Ta , as the set of exogenous which are to the loop at every subsystem Si that in the path going from a to . Thus , for any Ta if Ta , Ta we have that and become 
 To finish this section , we present a , non asymptotic version of inequality : 
 Theorem : In the system shown in Fig . a , if ,,, are mutually independent , then 
 As , Theorem can be seen as an extension of to the more general setup shown in Fig . 
 a , where the made in , Lemma . do not need to hold . In particular , x in Fig . correspond to S and in Fig . a , respectively , we see that inequality even E have dependent initial , or if the internal state not observable . 
 Theorem also an interpretation in of information . This can be in the diagram shown in Fig . , which the individual full turn around the entire feedback loop stemming from , and . Theorem that the sum of these individual is a lower bound for the directed information , provided ,,, are independent . 
 Figure . A representation of the three first information on the right hand side of . 
 This section three closed loop of the data inequality two directed , both between of internal to the loop . As already in Section I , to the best of our knowledge , the first inequality of this type to appear in the literature is the one in Theorem . in see . Recall that the latter result stated that I I , S to be such that is a deterministic function of and that ,. The following result another inequality which also two directed , namely , I and I , but only that ,,. 
 Notice that Theorem does not be independent . This may seem counter intuitive upon loop between the link from e to . 
 The following theorem is an identity between two directed only internal . It can also be seen as a complement to Theorem , since it can be directly applied to establish the relationship between I and I . 
 Notice that , by additional independence upon the exogenous specifically ,, Theorem and , in particular , 
 which the inequality in , Theorem . stated above in . More precisely , does not require one of the directed and irrespective of the invertibility of the in the loop . 
 A closer counterpart of i . e ., of , Theorem . , I , is next . 
 Thus , provided , ,, that regardless of the invertibility of S , instead that , for all i ,...,, any statistical dependence between and si only in i . e ., that chain . 
 The derived so far relate directed either the same starting sequence or the same destination sequence . We finish this section with the following corollary , which directly by combining and and directed four different internal to the loop . 
 Corollary Full Closed Loop Directed Data Inequality : For the system shown in Fig . 
 Equality in a if , in addition , i . e ., if ,,, are mutually independent . N 
 To the best of our knowledge , Corollary is the first result available in the literature providing a lower bound to the gap between two directed , four different inside the feedback loop . This result can be seen as the first full extension of the open loop traditional inequality , to arbitrary closed loop . Notice that there is no need to consider with more than four , since all external entering the loop between a given pair of internal can be as exogenous to a single equivalent deterministic . 
 Proof of Theorem : It is clear from Fig . a and from that the relationship between ,,,, be by the diagram shown in Fig . . From this diagram and Lemma in the appendix it that independent of ,,, then the following chain : 
 Figure . Representation of the system of Fig . the dependency between ,,,, and . The dependency on i of the d i ,..., d i is for clarity . 
 In the above , a from the fact that , if is known , then xi d i is a deterministic function of i . The resulting on the right hand side of a correspond to I I , and thereby proving the first part of the theorem , i . e ., the equality in . In turn , from the non negativity of mutual , turning into equality if ,,, as a direct consequence of the chain in . Finally , equality in if ,,, upon 
 . This that equality in is if ,,, the proof . 
 Proof of Theorem : Apply the chain rule identity to the of to obtain 
 where the second equality since . The result then directly by combining with and . 
 where a is due to Theorem , from Theorem and the fact that , , and from the chain rule of mutual information . For the second term on the of the last equation , we have 
 where a since , , and e stem from the chain rule of mutual information , 
 and is a consequence of the chain e which is due to the fact that S d Finally , is due to the chain , which because ,, as a consequence of Lemma in the appendix see also Fig . a . Substitution of into , thereby the proof . 
 Proof of Theorem : Since ,, , we can apply where now , the role of , and obtain 
 where a from Theorem , which also that equality is if and only if ,, . In turn , is due to the fact that is a deterministic function of . Equality if and only if , . Finally , from Lemma in the appendix , turns into equality if ,,. Substitution of into , the proof . 
 Proof of Theorem : We begin with the second part of the theorem , proving the validity of the equality † in . We have the following : 
 where equality in a if and only if the chain si , pi for all i ,.. as a straightforward extension of Lemma . In our case , the latter chain since we are assuming , In turn , from the fact that , for all i ,.., xi d i is a function of , pi . To prove , we resort to and write 
 From the of the in , it can be seen that , given , the triad of random 
 is a deterministic function of at most . that and that see , it readily that , and thus each of the mutual 
 on the right hand side of is zero . To verify the validity of , we use and obtain 
 last term in this chain of was shown to be zero in the proof of . Equality in e if 
 and only if , a chain which is satisfied in our case from the fact that , , and from Lemma . 
 Finally , since , we have that the chain of from to , from which we conclude that 
 this result into and Theorem we arrive at equality † in . 
 To prove the first equality the , it to notice that I to the sum on the right hand side of , from where we proceed as with the first part . This the proof of the theorem . 
 Information and , in particular , the data inequality , have a fundamental role in Information Theory and its . It is perhaps the lack of a similar body of associated with the directed information and with non asymptotic , causal information transmission which limited the extension of many important information theoretic and to feedback or causality , . Two such , already in this paper , are the understanding of the fundamental in control over noiseless digital , and causal rate distortion . In those , causality is of paramount relevance an thus the directed information , naturally , as the appropriate measure of information flow see , for example , , , , , and . We believe that our might help gaining into the fundamental trade underpinning those , and might also allow for the solution of open such as , for instance , the minimal average data rate that a given performance level an version of the latter paper , which extensively the derived here , is currently under preparation by the . On a different vein , directed mutual information a role akin to that of standard mutual information when channel feedback capacity see , e .., , and the therein . Our may also play a role in expanding the understanding of communication over used with feedback , particularly when in the analysis additional exogenous such as a random channel state , interference and , in general , any form of side information . Thus , we hope that the and in Section may help in extending such as dirty paper , , distributed source , , , , terminal , , and data encryption , to causal feedback . 
 In this paper , we have derived fundamental between mutual and directed in general discrete time with feedback . The first of these is an inequality between the directed information between to inside the feedback loop and the mutual information a subset of all the exogenous incoming . The latter result can be as a law of conservation of information for closed loop . Crucial to these was the repeated use of chain for conditional mutual information as well as the development of new . The proof do not rely upon of or , and the hold in very general non linear , time and stochastic with arbitrarily distributed . Indeed , the only restriction is that all within the system must be causal , and that their combined delay must be at least one sample . A new generalized data inequality was also proved , which is valid for directed within the loop . A key insight to be from this inequality was that the further apart the are in the loop , the lower is the directed information between them . This closely the behavior of mutual information in open loop , where it is well known that any independent of the can only reduce their mutual information . 
 Lemma : In the system shown in Fig . , the exogenous , are mutually independent and 
 Figure . Two arbitrary causal S , S in a feedback loop . The exogenous , are mutually 
 for every possible pair of the : S , and 
  
 ﻿We show that the structural similarity (SSIM) index, which is used in image processing to assess the similarity between an image representation and an original reference image, can be formulated as a locally quadratic distortion measure. We, furthermore, show that recent results of Linder and Zamir on the rate-distortion function (RDF) under locally quadratic distortion measures are applicable to this SSIM distortion measure. We finally derive the high-resolution SSIM-RDF and provide a simple method to numerically compute an approximation of the SSIM-RDF of real images. 
 A vast majority of the work on source coding with a fidelity criterion (i.e., rate-distortion theory) concentrates on the mean-squared error (MSE) fidelity criterion. The MSE fidelity criterion is used mainly due to its mathematical tractability. However, in applications involving a human observer it has been noted that distortion measures which include some aspects of human perception generally perform better than the MSE [1]. A great number of perceptual distortion measures are nondifference distortion measures and, unfortunately, even for simple sources, their corresponding rate-distortion functions (RDFs), that is, the minimum bitrate required to attain a distortion equal to or smaller than some given value, are not known. However, in certain cases it is possible to derive their RDFs. For example, for a Gaussian process with a weighted squared error criterion, where the weights are restricted to be linear time-invariant operators, the complete RDF was first found in [2] and later rederived by several others [3, 4]. Other examples include the special case of locally quadratic distortion measures for fixed rate vector quantizers and under high-resolution assumptions [5], results which are extended to variable-rate vector quantizers in [6, 7], and applied to perceptual audio coding in [8, 9]. 
 In [10], Wang et al. proposed the structural similarity (SSIM) index as a perceptual measure of the similarity between an image representation and an original reference image. The SSIM index takes into account the crosscorelation between the image and its representation as well as the images first- and second-order moments. It has been shown that this index provides a more accurate estimate of the perceived quality than the MSE [1]. The SSIM index was used for image coding in [11] and was cast in the framework of 1-compression of images and image sequences in [12]. The relation between the coding rate of a fixed-rate uniform quantizer and the distortion measured by the SSIM index was first addressed in [13]. In particular, for several types of source distributions and under high-resolution assumptions, upper and lower bounds on the SSIM index were provided as a function of the operational coding rate of the quantizer [13]. 
 In this paper, we present the high-resolution RDF for sources with finite differential entropy and under an SSIM index distortion measure. The SSIM-RDF is particularly important for researchers and practitioners within the image coding area, since it provides a lower bound on the number of bits that any coder, for example, JPEG, and so forth, will use when encoding an image into a representation, which has an SSIM index not smaller than a prespecified level. Thus, it allows one to compare the performance of a coding architecture to the optimum performance theoretically attainable. The SSIM-RDF is nonconvex and does not appear to admit a simple closed-form expression. However, when the coding rate is high, that is, when each pixel of the image is represented by a high number of bits, say more than 0.5bpp, then we are able to find a simple expression, which is asymptotically (as the bit rate increases) exact. For finite and small bit rates, our results provides an approximation of the true SSIM-RDF. 
 In order to find the SSIM-RDF, we first show that the SSIM index can be formulated as a locally quadratic distortion measure. We then show that recent results of Linder and Zamir [7] on the RDF under locally quadratic distortion measures are applicable, and finally obtain a closed form expression for the high-resolution SSIM-RDF. We end the paper by showing how to numerically approximate the high-resolution SSIM-RDF of real images. 
 In this section, we present an important existing result on rate-distortion theory for locally quadratic distortion measures and also present the SSIM index. We will need these elements when proving our main results, that is, Theorems 2 and 3 in Section 3. 
 2.1. Rate-Distortion Theory for Locally Quadratic Distortion Measures. Let x ? Rn be a realization of a source vector process and let y ? Rn be the corresponding reproduction vector. A distortion measure d(x, y) is said to be locally quadratic if it admits a Taylor series (i.e., it possesses derivatives of all orders in a neighborhood around the points of interest) and furthermore, if the second-order terms of its Taylor series dominate the distortion asymptotically as y ? x (corresponding to the high-resolution regime). In other words, if d(x, y) is locally quadratic, then it can be written as d(x, y) = (x - y)TB(x)(x - y) + O(x - y3), where B(x) is an input-dependent positive-definite matrix and where for y close to x, the quadratic term (i.e., (x - y)TB(x)(x - y)) is dominating [7]. We use upper case X when referring to the stochastic process generating a realization x and use h(X) to denote the differential entropy of X, provided it exists. The determinant of a matrix B is denoted det(B) and E denotes the expectation operator. 
 The RDF for locally quadratic distortion measures and smooth sources was found by Linder and Zamir [7] and is given by the following theorem. 
 Theorem 1 (see [7]). Suppose d(x, y) and X satisfy some mild technical conditions (see conditions (a)–(g) in Section II.A in 
 where R(D) is the RDF of X (in bits per block) under distortion d(x, y), and h(X) denotes the differential entropy of X. (The distribution of image coefficients and transformed image coefficients of natural images can in general be approximated sufficiently well by smooth models [14, 15]. Thus, the regularity conditions of Theorem 1 are satisfied for many naturally ocurring images.) 
 2.2. The Structural Similarity Index. Let x, y ? Rn where n = 2. We define the following empirical quantities: the sample 
 mean1))(x µxµ) (1(x/n-)µ)ni==-01(xxi, the sample varianceTx/(n-1))-(nµ2x/(n-sx21)), and the(1/(n - 
 sample cross-variance sxy = syx  (1/(n - 1))(x - µx)T(y - µy) = (xT y/(n - 1)) - (nµxµy/(n - 1)). We define µy and sy2 similarly. 
 where Ci > 0, i = 1,2. The SSIM index ranges between -1 and 1, where positive values close to 1 indicate a small perceptual distortion. We can define a distortion “measure” as one minus the SSIM index, that is, 
 which ranges between 0 and 2 and where a value close to 0 indicates a small distortion. The SSIM index is locally applied to N × N blocks of the image. Then, all block indexes are averaged to yield the SSIM index of the entire image. We treat each block as an n-dimensional vector where n = N2. 
 In this section, we present the main theoretical contributions of this paper. We will first show that d(x, y) is locally quadratic and then use Theorem 1 to obtain the highresolution RDF for the SSIM index. 
 Theorem 3. The high-resolution RDF R(D) for the source X under the distortion measure d(x, y), defined in (3) and where h(X) < 8 and 0 < EX2 < 8, is given by 
 Proof. Recall from Theorem 2 that d(x, y) is locally quadratic. Moreover, the weighting matrix B(X) in (1), which is also known as a sensitivity matrix [5], is given by (A.8), see the appendix. In the appendix, it is also shown that B(x) is positive definite since a(x) > 0,a(x) + b(x)n > 0, for all x, where a(x) and b(x) are given by (5) and (6), respectively. From (A.9), it follows that 
 At this point, we note that the main technical conditions required for Theorem 1 to be applicable is boundedness in the following sense [7]: h(X) <  < , 
 E[log2(det(B(X)))] < 8, and E(trace{B-1(X)})3/2 < 8 and furthermore uniformly bounded third-order partial derivatives of d(X,Y). The first two conditions are satisfied by the assumptions of the Theorem. The next two conditions follow since all elements of B(x) are bounded for all x (see the proof of Theorem 2). Moreover, due to the positive stabilization constants C1 and C2, trace{B(x)}-1 is clearly bounded. Finally, it was established in the proof of Theorem 2 that the third-order derivatives of d(X,Y) are uniformly bounded. Thus, the proof now follows simply by using (7) in (1). 
 3.1. Evaluating the SSIM Rate-Distortion Function. In this section we propose a simple method for estimating the SSIMRDF in practice based on real images. Conveniently, we do not need to encode the images in order to find their corresponding high-resolution RDF. Thus, the results in this section (as well as the results in the previous sections) are independent of any specific coding architecture. 
 In practice, the source statistics are often not available and must therefore be found empirically from the image data. Towards that end, one may assume that the individual vectors {x(i)}Mi=1 (where x(i) denotes the ith N × N subblock of the image and M denotes the total number of subblocks in the image) of the image constitute approximately independent realizations of a vector process. In this case, we can approximate the expectation by the empirical arithmetic mean, that is, 
 where a(x(i)) and b(x(i)) indicates that the functions a and b defined in (5) and (6) are used on the ith subblock 
 Table 1: Estimated (1/2n)E[log2(det(B(X)))] + log2(N) values for some 512 × 512 8-bit grey images and block sizes n = N2,N = 4,8, and 16. 
 Table 2: Estimated (1/n)h(x) (in bits/dim or equivalently bits per pixel (bpp)) for different 512 × 512 8-bit grey images and block sizes n = N2, N = 4, 8 and 16. 
 x(i). Several estimates of (1/2n)E[log2(det(B(X)))]+log2(N) using (8) are shown in Table 1, for various images commonly considered in the image processing literature. 
 In order to obtain the high-resolution RDF of the image, according to Theorem 3, we also need the differential entropy h(X) of the image, which is usually not known a priori in practice. Thus, we need to numerically estimate h(X), for example, by using the average empirical differential entropy over all blocks of the image. In order to do this, we apply the two-dimensional KLT on each of the subblocks of the image in order to reduce the correlation within the subblocks(since the KLT is an orthogonal transform, this operation will not affect the differential entropy.) Then we use a nearestneighbor entropy-estimation approach to approximate the marginal differential entropies of the elements within a subblock [16]. Finally, we approximate h(X) by the sum of the marginal differential entropies, which yields the values presented in Table 2. 
 In this section, we use the JPEG codec on the images and measure the corresponding SSIM values of the reconstructed images. In particular, we use the baseline JPEG coder implementation available via the imwrite function in Matlab. Then, we compare these operational results to the information theoretic estimated high-resolution SSIM RDF obtained as described in the previous section. We are interested in the high-resolution region, which corresponds to small d(x, y) values (i.e., values close to zero) or equivalently large SSIM values (i.e., values close to one). Figure 1 shows the high-resolution SSIM-RDF for d(x, y) values below 0.27, corresponding to SSIM values above 0.73. Notice that the rate becomes negative at large distortions (i.e., small rates), which happens because the high-resolution assumption is clearly not satisfied and the approximations are therefore 
 Figure 1: High-resolution RDF under the similarity measure d(x, y) = 1 - SSIM(x, y) for different images and using an 8 × 8 block size. 
 not accurate. Thus, it does not make sense to evaluate the asymptotic SSIM-RDF of Theorem 3 at large distortions. 
 The information-theoretic high-resolution RDF characterized by Theorem 3 constitutes a lower bound on the operationally achievable minimum rate for a given SSIM distortion value. As discussed in [17], achieving the high-resolution RDF could require the use of optimal compounding, which may not be feasible in some cases. Thus, the questions of whether the RDF obtained in Theorem 3 is achievable and how to achieve it, remain open. Nevertheless, we can obtain a loose estimate of how close a practical coding scheme could get to the high-resolution SSIM-RDF by evaluating the operational performance of, for example, the baseline JPEG. Figure 2 shows the operational RDF for the JPEG coder used on the Lena image and using block sizes of 8 × 8. For comparison, we have also shown the SSIM-RDF. It may be noticed that the operational curve is up to 2bpp above the corresponding SSIM-RDF (a similar behavior is observed for the other four images in the test set). 
 The gap between the SSIM-RDF and the operational RDF based on JPEG encoding as can be observed in Figure 2 can be explained by the following observations. First, the JPEG coder aims at minimizing a frequency-weighted MSE rather than maximizing the SSIM index. Second, JPEG is a practical algorithm with reduced complexity and is therefore not ratedistortion optimal even for the weighted MSE. Third, the differential entropy as well as the expectation of the log of the determinant of the sensitivity matrix are empirically found—based on a finite amount of image data. Thus, they are only estimates of the true values. Finally, the SSIM-RDF becomes exact in the asymptotic limit where the coding rate 
 Figure 2: Operational RDF using the JPEG coder on the Lena image under the similarity measure d(x, y) = 1 - SSIM(x, y) for block size 8 × 8. For comparison we have also shown the high-resolution SSIM-RDF (thin line). 
 diverges towards infinity (i.e., for small distortions). At finite coding rates, it is an approximation. Nevertheless, within these limitations, the numerical evaluation of the SSIMRDF presented here suggests that significant compression gains could be obtained by an SSIM-optimal image coder, at least at high-rate regimes. To obtain further insight into this question, the corresponding RDF under MSE distortion (MSE-RDF) for the Lena image is shown in Figure 3. We can see that the excess rate of JPEG with respect to the MSE-RDF at high rates is not greater than 1.4bpp. This suggests that a JPEG-like algorithm aimed at minimizing SSIM distortion could reduce at least a fraction of the bit rate gap seen in Figure 2. 
 It is interesting to note that, in the MSE case, we have B(x) = I, which implies that log2(|det(B(x))|) = 0. 
 Thus, the difference between the SSIM-RDF and the MSERDF, under high-resolution assumptions, is constant (e.g., independent of the bit-rate). In fact, if the MSE is measured per dimension, then the rate difference is given by the values in Table 1, that is, (1/2n)E[log2(det(B(X)))] + log2(N). It follows that the SSIM-RDF is simply a shifted version of the MSE-RDF at high resolutions. Moreover, the gap between the curves illustrates the fact that, in general, a representation of an image which is MSE optimal is not necessarily also SSIM optimal. 
 We have shown that, under high-resolution assumptions, the RDF for a range of natural images under the commonly used SSIM index has a simple form. In fact, the RDF only depends upon the differential entropy of the source image as well as the expected value of a function of the sensitivity matrix of the image. Thus, it is independent of any specific 
 Figure 3: Operational RDF using the JPEG coder on the Lena image under the MSE distortion measure. For comparison we have also shown the high-resolution MSE-RDF (thin line). The horizontal axes on the top and the bottom show the PSNR and MSE, respectively. 
 coding architecture. Moreover, we also provided a simple method to estimate the SSIM-RDF in practice for a given image. Finally, we compared the operational performance of the baseline JPEG image coder to the SSIM-RDF and showed by approximate numerical evaluations that potentially significant perceptual rate-distortion improvements could be obtained by using SSIM-optimal encoding techniques. 
 We need to show that the second-order terms of the Taylor series of d(x, y) are dominating in the high-resolution limit where y ? x. In order to do this, we show that the Taylor series coefficients of the zero- and first-order terms vanish whereas the coefficients of the second- and third-order terms are nonzero. Then, we upper bound the remainder due to approximating d(x, y) by its second-order Taylor series. This upper bound is established via the third-order partial derivatives of d(x, y). We finally show that the second-order terms decay more slowly towards zero than the remainder as y tends to x. 
 Let us define f  ((2µxµy + C1)/(µ2x + µ2y + C1)) and g  ((2sxy + C2)/(sx2 + sy2 + C2)) and let h = f g. It follows that d(x, y) = 1 - h and we note that the second-order partial derivatives with respect to yi and yj for any i, j, are given by 
 Clearly f |y=x = g|y=x = 1, where (·)|y=x indicates that the expression (?µy/?yi = 1/n,?s·y2)/?yis evaluated at the pointi = (2/(n-1))(yi-µy), andy =?syxx. Since/?yi = 
 (1/(n - 1))(xi - µx), it is easy to show that ?f/?yi|y=x = ?g/?yi|y=x = 0, for all i. Thus, the coefficients of the zeroand first-order terms of the Taylor series of d(x, y) are zero. Moreover, it follows from (2 f/?yi?yj|y=x + ? A.1) that ?2h/?yi?yj|y=x = 
 ? 2g/?yi?yj|y=x, for all i, j. With this, and after some algebra, it can be shown that 
 We now let h(m) denote the mth partial derivative of h with respect to some m variables and note that from Leibniz 
 generalized product rule [(1) f (2) + 3(3)g(2) f (1) + gf(3)(3)18f|y. When evaluated at=]xit follows that+ g(3)|y=x sinceh(3)fy(1)==|yg f=xx, this(3)and+ 
 Let B be an n-dimensional ball of radius  centered at x, let ? = y - x, and let T2(?) be the second-order Taylor series of d(x,x + ?) centered at x (i.e., at ? = 0). It follows that 
 T2(?)  -12   i,j ?2?yh	i?yx, yj  y=x?i?j = ?TB(x)?,	(A.7) where B(x) is given by half the second-order partial derivatives of d(x, y), that is (see (A.2)), 
 Thus, B(x) has eigenvalues ?0 = a(x) + b(x)n and ?i = a(x), i = 1,...,n - 1. Since B(x) is symmetric, the quadratic form ?TB(x)? is lower bounded by 
 where ?min = min{?i}ni=-01 = min{a(x) + nb(x),a(x)} > 0, which implies that B(x) is positive definite. 
 On the other hand, it is known from Taylor’s theorem that for any y ? B, the remainder R2(?), where 
 that is, f is upper bounded by the supremum over the set of third-order coefficients of the Taylor series of h. Since for real images, the pixel values are finite, and since Ci > 0, i = 1,2, it follows from (A.3)–(A.6) that the third-order derivatives are uniformly bounded and f is therefore finite. Moreover, for all ? such that ?2 = e, it follows using (A.7), (A.12), and (A.14) that 
 where (A.16) follows since |?i?j?k| = maxi?{1,...,n}|?i|3, and the sum in (A.14) runs over all possible combinations of third-order partial derivatives of a vector of length n, that is, 
 i,j,k 1 = n3. Furthermore, (3 < ?A.173. Finally, () follows by use ofA.18) follows from(A.12) and the fact that |?i| 
 the fact that f is bounded by (A.15). Since the limit of (A.18) exists and is zero, we deduce that the second-order terms of the Taylor series of d(x, y) are asymptotically dominating as 
 The work of J. Østergaard is supported by the Danish Research Council for Technology and Production Sciences, Grant no. 274-07-0383. The work of M. Derpich is supported by the FONDECYT Project no. 3100109 and the CONICYT Project no. ACT-53. 
 ﻿ 
 This paper the performance of a feedback control loop closed via an error free digital communication channel with transmission delay . The system a discrete time noisy linear plant whose single measurement output is into its single control input by a causal , but otherwise arbitrary , and control scheme . We consider a single input multiple output channel between the controller and the controller which is lossless and random time delay . We derive a lower bound on the minimum average feedback data rate that a certain level of average quadratic performance over all possible of the random delay . For the special case of a constant channel delay , we obtain an upper bound by linear source that attain desired performance with that are at most . per sample greater than the lower bound . We give a numerical example that and operational are increasing of the constant delay . In other , to achieve a specific performance level , greater channel delay spending higher data rate . 
 I . INTRODUCTION 
 Taking communication into account for analysis and design proved to be an interesting topic within the area of control theory during recent . This interest is by of communication over point to point wiring and , on the other side , by the complexity that communication impose on classical control . Time delay , packet dropout and data rate quantization are among prominent . 
 an information theoretic approach , , report primary related to system performance . In these works , it is shown that the presence of a finite capacity communication channel in a strictly causal feedback loop a new performance limitation which from conventional Bode formula by a constant channel information rate . Moreover , the derive among entropy rate of internal inside the loop and external outside the loop , resulting in a general performance bound which is affected by finite feedback capacity . Inspired by , , lower and upper are derived on the minimum data rate that a level of quadratic performance in . These works consider noisy linear time invariant with , over an error free digital channel without delay . In particular , that over all causal which represent and control , the average data rate is bounded from below by the directed information rate by the that render the sensor input and control output jointly . Moreover , it is proved in that in an auxiliary structure , the minimum signal to noise ratio which stability and meeting a quadratic performance requirement the lower bound on the desired minimal data rate . For the upper bound analysis , entropy . Such a simple scheme is designed based on the constrained optimization giving the lower bound . Inspired by and , the of present a method based upon semidefinite to characterize the trade off between directed information rate and linear quadratic performance in rate constrained control with fully observable multiple input multiple output . In , the derive a lower bound on the zero delay rate distortion function associated with vector valued Gauss and mean square error distortion constraint . Based on the separation principle , this bound is in fact the lower bound on the minimum data rate for performance in control of fully observable . Then the optimal realization that to the class of vector valued to derive an upper bound on zero delay rate distortion function variable length entropy with lattice quantization . Similar are employed in for on minimum mutual , across a delay free channel , that guarantee specific linear quadratic regulator performance . Specifically , the lower bound based on lower bound and power entropy whereas the upper bound is 
 established via variable length and lattice based quantization . 
 subject to network induced are generally according to two : robustness and adaptation . The aim in the robustness framework is for certain stability or performance by that do not incorporate time stamp information as a variable . For instance , in , stabilization 
 and H performance for a singular cascade are . Fuzzy model based control is another approach in the robustness framework , where the are based on the size of , and the controller is to be robust over the delay range , . In the adaptation framework , one method is as stochastic switched . The recent on stability and H H performance of jump linear are in and , respectively . The second approach in this framework is predictive control ; a method which is currently quite popular in . According to this technique , the actuator among a sequence of control based on the transmission experienced by them . 
 In all the on system performance , either the effect of channel delay is , or the rate limitation is not taken into account . However , looking into the literature , one can find works investigating performance in with both rate and network induced see , e .., . Even so , a few the approach to treat with such . For example , on the minimum individual non asymptotic rate to guarantee meeting an individual performance requirement boundedness of the maximum ` norm of . 
 In this paper , we study the performance of a discrete time plant with initial state in a loop with exogenous and random or constant channel delay on the feedback path . For the setup with random delay in the channel , we seek the average data rate to achieve a performance level . We show that the average data rate over all possible of the delay is lower bounded by the average directed information rate . We prove for the random channel delay case that under certain , the average directed information rate can be stated in of average power spectral of the involved . We obtain a lower bound on the desired minimal average data rate which is stated as the average of a function of the power spectral of feedback path over all of the delay . To establish all these , we utilize the adopted in and . However , to and , the channel is not delay free in our setup . In other , we extend the information in to the case where there a random time delay between the sensor output and the control input . 
 For the setup with known constant delay in the channel , we show that the above lower bound on the average data rate for quadratic is equal to a function of of the channel over comprised of and with feedback and delay that meet the quadratic performance constraint . Our contribution in this case is showing how the presence of the channel delay the scheme yielding the lower bound . This an insight to the interplay between time delay , average data rate and performance in the considered . We also prove that even over a channel with a constant delay , any admissible performance level can be by an based linear scheme which an average data rate at most approximately . per sample away from the corresponding lower bound . We illustrate via a numerical example that lower and upper as well as empirical and are all increasing of channel delay . This in turn that with demand higher average data to allow for a certain system performance . 
 to our previous works in and , first , we here study the case of random channel delay and second , we employ a simpler proof than information and in and . In this work , we also show the effect of a delay at different in the loop on system . The last departure from our previous is that we incorporate some of into this paper . 
 The remainder of the paper is organized as . Section the notation . Section the main problem . Section the lower bound problem for the setup with random channel delay . lower bound on the desired minimal data rate in the case of constant channel delay . The analysis of upper bound problem in the constant delay case is in Section where the equivalence between with different delay is . A numerical example is given in Section . Finally , Section the 
 paper . 
 . NOTATION 
 By , we denote the set of real whose subset the set of strictly positive real . The set N is defined as N , set of natural . The time index of every considered signal , most , to N . E , log , and . k represent for expectation , natural logarithm , magnitude and H norm , respectively . Moreover , min and are respectively the and of the square which the element on the i th row and th column is by i ,. In addition , is shorthand for ,..., where the th sample of a discrete time signal . Furthermore , for the time dependent set a i , i N , ak is defined as ak , a a . However , if a is a fixed set , then ak , a a 
 times . 
 Random and are vector valued , unless otherwise stated . account as two random with known marginal and joint probability distribution . Their joint is by , while the marginal by and , respectively . The conditional by and . is the operator for the expectation with respect to the distribution of . We define the differential entropy the conditional differential entropy , and , , , respectively . The mutual information by by I ; and 
 defined as I ; , , log ,. Moreover , the definition of the conditional 
 mutual information between the random given by I ; , I ,; I ;. All the information theoretic in this 
  
 Fig . : Considered with a channel imposing random delay 
 paragraph are standard and follow . 
 We call the random process asymptotically wide sense stationary if E 
 and , and E E E hold , 
 where is a finite constant . Accordingly , the steady state covariance matrix and the steady 
 state variance of are defined as , trace , respectively . For the scalar random sequence 
 , we define the covariance matrix as . 
 Assume that , are square matrices . Then the and are 
 asymptotically equivalent if and only if they satisfy the following expression for finite : 
 . 
 . PROBLEM FORMULATION 
 We consider the feedback loop of Fig . where the plant is with one control input and one sensor output by u and , respectively . The disturbed by a zero mean white noise which is by and identity covariance matrix , i . e . I . Moreover , as in Fig . , the plant the vector valued signal upon which the performance measure is . The relationship between the set of and is by a transfer function matrix as : 
 , 
 where the dimensionality of each is determined by the of corresponding pair of and . So , , and are the for G , G , G 
 and G , respectively . 
 Assumption . . Every entry of the transfer function matrix in is proper with no unstable hidden . Moreover , G , which the single input single output open loop system from u to , is strictly proper . The initial of the plant by x ,..., are jointly with and independent of the disturbance a finite differential entropy . 
 As in Fig . , the output of the plant ,, is into a binary word by the E and over the error free channel . Such transmission is with a random delay . Let denote the delay experienced by the binary word at the . We assume that is an independent and identically distributed i . i .. 
 process which a bounded support at each time step , i . e ., h ,..., , N where hi hi i , ,..., . In order to avoid unnecessary notational complexity and without loss of generality , we set h as h and as . The marginal distribution of the delay is assumed to be known and by where . 
 Such introduce a channel with the following input output relationship : 
 i i , N 
 where is defined as 
 S , i : i i 
 for every , i N . the of by , we can imply form that is a vector comprised of binary which the output of the channel at time . Note that is a random variable depending on the channel delay . We assume 
 that i is at the controller side if i . Moreover , under , binary over the considered channel are not necessarily received in the same order they were . It should be also that the channel does not 
  
 Fig . : Considered with a detailed model of and control scheme 
 allow for any data loss . The average data rate across the channel is defined as 
 , 
 where i the length of the binary word i . 
 A more detailed presentation of the feedback path in the of Fig . is provided by Fig . . As , the controller is comprised of a and a lossless component . The 
 part E the symbol yE according to the following dynamics : 
 , 
 where e e the side information at the . : 
 is a deterministic map and As a fixed countable set . At each time 
 instant , the is assumed to know the time experienced by previous binary and the time delay of the current binary word to be sent over the channel . Therefore , is known at the N . This that yE can be reconstructed perfectly later at the if is by yE and only those of which will be already available at the at time . Note that access to at the at the time is not assured . So the lossless O the binary 
 symbol based on 
 , 
 in which is a sequence the of for which the associated binary will have the by the time , i . e ., yE i : i N , i , i 
 h i . Moreover , o o , and is an arbitrary 
 deterministic where . So the 
 of the sequence . Note that since no dropout during data transmission , 
 will certainly have been received at the by the time . In addition , A is a countable set of prefix free binary code , which the input alphabet of the channel at each time instant . 
 On the receiver side , is available as the input to the lossless . This , 
 shown by O , as 
 , 
 where is a sequence comprised of of that have time indices less than or equal to the time index of in , i . e ., where , N . Such selection of data for lossless is in accordance 
 with the information in for . Furthermore , 
 , where , an arbitrary deterministic . It should be noted that according to the channel model , is a random variable the of and for all N . Moreover , based on the definition of and , the information provided by is enough for the lossless 
 to reconstruct every element of yE i i perfectly . Therefore 
 yE i i , N 
 where is defined as in . Indeed for such reconstruction , the knowledge of the delay is at the . Hence , we further assume that the is provided by through 
 for example . Finally , the controller the control input via 
 u 
 where the side information available at the at is in the well defined set . So . Moreover , 
 R , tu , is an arbitrary deterministic where tu is the of . It should be noted that o in o o is defined as o , e . We state some additional of the setting above in the following 
 . 
 Remark . It can be from that u and are of where for every N . It thus from the definition of and assuming no dropout in the considered channel that . This that the controller access to the and amount of sensor information when the channel delay is zero and , respectively . 
 Remark . It can be from the definition of in that can have at most at each time step . So the number of the that can be received at the at each time instant to the set ,..., . Therefore , since the channel input is a scalar process , a single input multiple output channel . Moreover ,, as a stochastic process , cannot be i . i . because in the considered channel , no binary word is received at the more than once . 
 For further analysis , we consider the following assumption . 
 Assumption . . At each time instant N , the side information pair e , together with , and consequently , are statistically independent of x ,. Therefore , it can 
 be from the dynamics of the system that I u ; hi for any hi ,..., with hi . Moreover , upon knowledge of , di and Si , the is invertible . It that for each i N , there a deterministic such that 
 . 
 Remark . In Appendix A , we will prove that for the architecture of Fig . , any and non invertible with E , O , O and , can be by another set of with the same input output relationship and lower average data rate where the is invertible . 
 For the purpose of the information rate in of spectral of the of the system , we use the following notion of stability : 
 Definition . A scalar strongly asymptotically wide sense stationary if its covariance matrix is asymptotically equivalent to the covariance matrix of the wide 
 sense stationary process , say , to which it , i . e ., and are 
 asymptotically equivalent . Furthermore , in an , all internal are and their cross covariance matrices are asymptotically equivalent to the cross covariance matrices of corresponding to be to . 
 Clearly , ness ness but not vice ; for both and . For each scheme satisfying and rendering the of Fig . , the steady state variance of the a random variable which on the realization of . The same goes for the average data rate . We make explicit such dependence by writing and , and consider the of these over all of as our performance measure and data rate of interest , respectively . Such of performance and rate , 
 by and Ra respectively , are as : 
  
  
  
 support set for possible of the delay . Moreover , and indicate that the average data rate and steady state variance are of delay . 
 Generally speaking , we are interested in finding the minimal Ra for which a bounded 
 is feasible . Let denote the average steady state variance can be , when the random delay , with the , is present in the channel . Hence , is by the average steady state variance all possibly nonlinear and time u that render the of Fig . . Note that is defined as in Remark . Under the condition that Assumption . , 
 the problem of our interest is to find 
 Ra Ra , 
 D 
 where , , and the average state variance of the all of the delay . The feasible set of the optimization problem in is 
 comprised of all controller and controller by , satisfying 
 Assumption . and rendering the of Fig . . 
  
 Fig . : Auxiliary system equivalent to the main in the random delay case 
 Remark . It is straightforward to see from that the concatenation of the pair and the channel in the of Fig . is equivalent to a controller pair with the same and side information that a time delay with same as in , on its received data , and that is by a delay free channel . So the system in Fig . is equivalent to the feedback loop of Fig . in which the and the plant are the same and the have the same as in Fig . . 
 The equivalence pointed out in Remark between of Fig . and the of Fig . 
 will assist us a lower bound on the average data rate Ra in the next Section . 
 . LOWER BOUND PROBLEM IN THE PRESENCE OF RANDOM DELAY 
 In this section , we establish a lower bound on Ra . To do so , we derive and that describe the relationship between the flow of information and system performance in the of Fig . . Therefore , we will update fundamental in , for the case where the channel delay is randomly distributed . As the first result , we show how the average data rate Ra is bounded from below in the following theorem . 
 Theorem . Consider the feedback loop in Fig . for which . and . hold . Then 
  
 where I .;. . conditional mutual information . According to , the average directed information rate across the forward channel u in the of Fig . over all possible of the channel delay . 
 Proof . It can be from , Theorem . that , for each realization h of the , the 
 average data rate in the feedback loop of Fig . is bounded from below as 
 . 
 Based upon the chain rule of mutual information , the bound in can be as 
 , 
 where the definition of li is given in Remark . From the dynamics of the plant , we can easily conclude that the sequence is only a function of and , once is given . Furthermore , it from that upon the knowledge of , side and will be 
 the only u i , i N . Latter together with the fact that Assumption . for the system of Fig . yield the conclusion that the rightmost term of to zero , i N . So we have 
  
 for the of Fig . . Now by both sides of with respect to the delay , as in , and that the feedback loop of Fig . is equivalent to the system of Fig . , based on Remark , our claim immediately . 
 The next lemma that joint of two the directed information rate between them when these are connected through a channel with random delay . 
 Lemma . Suppose that the of Fig . Assumption . and Assumption . . For this system , if ,, u , a jointly second order set of , then the following : 
 , 
 where and uG symbolize the u , respectively , in a way that ,, uG , are jointly with the same first and second order cross as ,, u ,. 
 Proof . According to , Lemma . , the directed information rate from sensor output to the 
 control input in the auxiliary of Fig . is bounded as : 
 I u I uG , 
 where I u and I uG are defined as in . We conclude based on , the dynamics of the plant and the system of Fig . satisfying Assumption . that 
 , , i N . This together with the chain rule of mutual information lead to 
 . 
 Now the proof is complete by taking average over all possible of the delay from both sides of and considering that based on Remark , the system of Fig . is equivalent to the feedback loop in Fig . . 
 If the above are stationary as well , then the average directed information rate can be stated in of the average power spectral density of the involved . The next lemma will state such result formally . 
 Lemma . Suppose that the control input u in the of Fig . is for every realization 
 of the channel delay . For each realization , assume that there a in such a way that 
 . Let further consider the sensor with u . Then 
 the average directed information rate is equal to an integral term as : 
 , 
  
 where is a process that independent . Such a random process is as 
 , u u , u , E u , 
 for each realization of the random delay . Moreover , the steady state power spectral density of u . 
 Proof . It can be from , Lemma . that in the of Fig . , the following 
 for the directed information rate : 
  
 in a process with independent and I u is defined 
 as in . The calculated as : 
 n , u , , E u , . 
 As already before , based on the plant dynamics , the knowledge of will render 
 dependent only on and wi for any i N . Moreover , according to , knowing 
 , one can determine u i by only out di and . Since , based on 
 Assumption . , x , and , e are independent , the following is : 
 E u , E u , . 
 From , it can be that is actually equal to as in for the of Fig . . Now , our claim is given by taking average from both sides of upper and that based on Remark , the in Fig . and Fig . are equivalent . 
 We are now ready to present a lower bound on Ra . A corollary : 
 Corollary . Suppose that the of Fig . Assumption . . Then Ra is lower bounded as : 
  
 where and u are defined as in and the is restricted to all and Assumption . , u with as stated in Lemma . 
 Proof . The claim immediately from Theorem and Lemma . 
 V . LOWER BOUND PROBLEM IN THE CASE OF THE CONSTANT DELAY 
 In this section , we consider the same as in Section but with a channel that a known constant delay , say , on the data . The corresponding feedback loop is by Fig . . The problem we investigate here is a special case of the problem in where the channel delay is constant and therefore , there is only one realization for the channel delay . In this case , we consider the notation Ra and . In Appendix A , we prove that finding is feasible if 
  
 Fig . : Considered in the constant delay case 
 , . We show that in order to obtain a lower bound on , one can minimize the directed information rate over an auxiliary scheme formed of and an channel with feedback and delay . and related to the delay free version of this optimization derived in will be extended to the case with a constant channel delay . We start by a lower bound on the average data the following theorem . 
 Theorem . Suppose that the feedback system of Fig . . and . . Then the average data lower bounded as : 
 , 
 where is the directed information rate across the forward channel u with 
 constant delay see , Definition for the formal definition . 
 Proof . Considering that at any N for the of Fig . , we can conclude the claim immediately from Theorem . 
 The directed information rate in will be reduced if the involved are jointly . This result is by the following lemma . 
 Lemma . Suppose that . and . hold for the of Fig . . Furthermore , consider ,, u , as a jointly second order set of random . Denote the u by and uG , respectively , where ,, uG , are jointly with the same first and second order cross as ,, u ,. Then 
 . 
 Proof . Recall that , N , for the considered case with constant channel delay . The claim immediately from Lemma . 
 It can be from Lemma that by directed information rate over a scheme u jointly , one can obtain a lower bound on . Now , we will 
 show that the directed information rate can be stated in of power spectral of the involved if such meet certain . 
 Lemma . Suppose that u is an process with where . 
 Moreover , assume that u is jointly and with the sensor output . Then the directed information rate between u expressed as 
  
 where a process with independent defined by 
 , u u , u , E u , . 
 Furthermore , the steady state power spectral density of u . 
 Proof . Immediate from Lemma by that for the of Fig . at every time instant N . 
 It can be from Theorem and Lemma that the rate performance pair by any and control scheme satisfying Assumption . which the of Fig . is attainable with a lower or equal rate if there a scheme that , u jointly with x , while rendering the system . Due to the of x , and the fact that the plant is , a jointly pair , u can be produced by a control scheme comprised of and an noise source . Such a scheme is in Fig . . The of Fig . all of the and that hold for the system of Fig . . However , the arbitrary are by the auxiliary feedback loop of Fig . . 
  
 Fig . : The structure giving the lower bound in the constant delay case 
 In addition , for such an , a channel with noiseless one sample feedback as communication channel . The control scheme in the of Fig . is 
 via the following dynamics : 
 , 
 where is a zero mean white noise with variance and independent of x ,, and By . It should be that Assumption . for the initial x , the the disturbance the of Fig . . Furthermore , the initial of , and the delay are deterministic . As the system in Fig . is a special case of the structure of Fig . , we use for in Fig . that have in the of Fig . . 
 Theorem . If the of Fig . Assumption . and Assumption . and 
 , , then 
 , 
 where and represent the steady state variance of z and the steady state power spectral density of u in Fig . , respectively . Moreover , the feasible set for the optimization in is the set comprised of the noise with that render the system of Fig . 
 internally stable and well with . 
 Proof . See Appendix . 
 Theorem that doing the optimization in over the auxiliary system of Fig . , with the channel and delay , will give a lower bound on the minimal data rate to achieve a certain performance level in the arbitrary possibly nonlinear and time structure of Fig . . The following show how the lower bound derived in can be simplified to a bound which is easier to compute . 
 Lemma . For the of Fig . , let describe by 
 , 
 where is fixed and the steady state power spectral density of . Moreover , suppose that the pair , B , J the feedback loop of Fig . internally stable and well . Then for any , there another pair with a filter , say J , and a proper one , say B , that the system of Fig . internally stable and well , and the steady state power spectral density of z in a way that the following : 
  
 Proof . See Appendix . 
 Intuitively speaking , the of Theorem and Lemma imply that can be bounded from below by a logarithmic term as in which is a function of channel in the 
 of Fig . . Such an intuition will assist us with a lower bound which is appealing in the following corollary . 
 Corollary . Take the feedback loop of Fig . into account as an that 
 . and . . Then for every , , the following : 
 , 
 in which st and symbolize the steady state z in the auxiliary system of Fig . , respectively . For the optimization problem in , a candidate solution is an filter 
 pair , together with noise variance that cause the system in Fig . to become 
 internally stable and well . 
 Proof . See Appendix . 
 . UPPER BOUND PROBLEM IN THE PRESENCE OF CONSTANT DELAY 
 In this section , we show that for any , , one can always find a scheme that with an average data rate which a distance of about . per sample from the theoretical lower bound . For such a scheme , we propose a design approach which the that together with an with feedback and delay , render the directed information rate over the channel equal to the lower bound on . 
 Definition . We call a scheme with input output relationship as in in the constant channel delay case linear if and only if its dynamics can be as : 
 , 
 where By proper with deterministic initial condition . Moreover , a zero mean i . i . random sequence independent of x ,. The initial state of the one step delay feedback channel is assumed to be deterministic . 
 The realization of linear source can be carried out by entropy together with . First , an the 
 following relationship between , in and , and , in : 
 yE 
 yE , 
  
 h , , 
 in which by , we denote a uniform with resolution , : i ; i . Additionally , a dither signal whose access are provided to both and . The and its complementary formalize entropy for the lossless at the and , respectively . The following lemma an interesting property of when being set up in an feedback loop . 
 Lemma . Consider the feedback loop in Fig . and suppose that the plant is by a proper real rational transfer function matrix in which the transfer function from scalar and strictly proper . For such a system , assume that the input output relationship 
  
 Fig . : setup in the feedback path 
 of the in the feedback path is given by with finite and positive quantization step size . Moreover , take the disturbance into account as a white noise process jointly second order with , the initial state of . Then if the an i . i . process with a uniform distribution over , and independent of , , the i . i ., uniformly distributed over , and independent of , . 
 Proof . See Appendix E . 
 It can be from above that combining the in with the in in a setting as in Fig . will lead to a linear scheme for the of Fig . as 
 long as the same criteria as for the dither in Lemma . If so , the 
  
 Fig . : The based linear scheme 
 scheme is a linear based scheme . If such a scheme is on the feedback path of the main of Fig . , the average data rate is bounded from above by a certain value which is shown in the following lemma . 
 Lemma . Suppose that Assumption . for the of Fig . . Then the existence of an based linear source scheme rendering the of Fig . is certified in such a way that the average data rate 
 . 
 In , the variance of the quantization error noise of the based linear source scheme is set as . Moreover , st the steady state variance of the signal t 
 in . 
 Proof . See Appendix . 
 Now , through the following theorem , we use the result of Lemma to show that based linear can lead to an upper bound on the desired minimal average data rate . 
 Theorem . Let Assumption . hold for the closed loop system of Fig . . Then for each , , one can always find an based linear source scheme satisfying Assumption . and rendering the feedback loop of Fig . in such a way that is and the average data rate is bounded as 
 , 
 where the definition of is given in . 
 Proof . See Appendix . 
 In the following remark , we state how the upper bound derived in Theorem can be considered as an upper bound on Ra in the case of random channel delay . 
 Remark . The upper bound in will be an upper bound on Ra in the random channel delay case if and control are linear based designed as in the proof of Theorem for the delay where the have at 
  
 Fig . : Three possible for the delay component in the case with constant channel delay 
 their sending only for at each time instant N . Clearly , this is due to the fact that at every time step N , is available at the . Such an upper bound does not seem to be tight since imposing a delay of on data is actually a worst case scenario . 
 The derived in this section and the previous section limit the desired average data rate in the of Fig . . In this system , the constant delay is induced by the digital 
 communication channel between the controller and the controller . One concern is the effect of delay location on the derived . The following lemma a step in this issue by showing how the system change when the time delay block is to a different location in the feedback loop of Fig . . 
 Lemma . Consider the of Fig . and two other each of which by moving the delay component in the of Fig . to either the measurement path between the sensor and the controller or the actuation path between the controller and the plant . Fig . the where the time delay in these . Then are not necessarily equivalent across the if the only difference between them is the delay location . However , the equivalence can be assured by the side information to change across the 
 . 
 Proof . See Appendix . 
 . NUMERICAL SIMULATION 
 Take the following transfer function into account as the model the generalized the of Fig . : 
  
 Let us set the disturbance initial x in such a way that Assumption . is satisfied . We calculated lower and upper on as derived in and . For these , we made use of the equivalence between the of Fig . and Fig . , shown in the proof of Lemma , in that we adopted the method in which performance optimization similar to the one for such as the of Fig . . The are for three different of channel delay , , , , with respect a range from to for each . Moreover , we designed actual linear based , and for each the latter interval , we the of Fig . . To do so , we the giving the lower bound on according to the procedure in , Theorem . . The are 
 in Fig . . In this figure , the to as and present the lower 
  
 Fig . : on in and actual data and for different of time delay h 
 and upper on , respectively . We can compare among with different of channel delay as well . As shown , greater is associated with channel delay , as according to . how the change in response to in the delay is one of the main of this simulation study . We can observe from the plotted in Fig . that fixed , increasing the delay will enlarge the on . In other , the greater the delay is , the higher average data rate is to be used in order to achieve a fixed quadratic performance level . Moreover , Fig . that the lower upper bound converge to the minimum data rate for mean square stability as . From , we know that the minimal data rate of the of Fig . is only a function of unstable of the plant . On the other hand , we use the equivalent system of Fig . for the purpose of calculating . So the observation with convergence of to the minimal data rate for stability comes from the fact that time delay into the model of the plant Ga will not affect its unstable . 
 Simulation are in Fig . as well . The to as OR and OE present the average data and by actual linear . Furthermore , sample long have been considered for the dither . The task 
 in all is done by memory less which do not take the past information of the dither into account as prior knowledge for . In addition to the average data rate , the entropy of the output of the been for the setup . The gap of around . per sample between the measured entropy and the lower bound that for each , , , . per sample of the gap between the actual and lower bound is by the with uniform dither and the remainder . per sample to sample by sample . It can be that the actual and have the same as the of in the previous paragraph . The most prominent property is related to the behaviour of the and as a function of channel delay , i . e ., for a system with greater time delay in the channel , 
 higher are to guarantee quadratic performance . 
 . 
 In this paper , the trade off between average data rate and performance in control been studied . Two have been , each of which an plant with disturbance and initial , and scalar control input and sensor output . Moreover , both of them have causal , but otherwise arbitrary , on their feedback which are responsible for and control . The only difference between the two considered is the model of the channel that out data transmission between the and the controller . In one case , the digital communication channel is and information to be are exposed to random delay . In the other system , the channel is error free as well but it is and constant delay on data . For the case with random channel delay , we considered for rate and performance which show the average behaviour of the system over all of the delay . We have shown that for such a setup , data rate is lower bounded by average directed information rate from the sensor output to control input , and u are jointly , the average directed information rate would be . Moreover , we have shown that u satisfy certain , the average directed information rate between them is a function of the average power spectral of these over all of the channel delay . We have shown that value of this function over all arbitrary and that cause system have those and lower the average data rate to attain a quadratic performance . 
 For the constant delay case , which is a special case of the system with random channel delay , we by the minimal average data rate that a certain performance level . the fundamental information and derived for the random delay case , we that this desired minimal average data rate is bounded from below when coder and the channel behave as a concatenation of proper and an channel with feedback and delay . Then we that by such with simply linear based , one can achieve any legitimate performance level by actual which are at most . per sample higher than the lower bound . The through the simulation show that and empirical are increasing of channel delay for a fixed performance level . It delay in the channel higher minimal average data rate that is for a certain level of quadratic performance . 
 Future research will concern with finding closed form solution for the lower and upper bound in the case of random channel delay , finding analytic expression for the desired data rate , lower and upper with shorter gap between them , with model 
 and vector quantization . 
 APPENDIX A 
 INVERTIBILITY OF THE 
 Lemma . Consider a scheme through that a non invertible , 
 and let be defined as . For such scheme , assume that u u and f , N , where . Then there another 
 scheme the control input u u with an invertible in such a way that f , N . 
 Proof . Suppose that in represent a non invertible at a way that upon knowledge of and Si , perfect reconstruction of from been possible for all i 
 k . Then there exist , As such that . 
 Let S and S be associated with and respectively . Two possible can occur . In the first case , S and S are unequal , i . e ., S S . known at the at each time step , this case does not contradict the invertibility . That is due to the fact that the knowledge determine whether u is by or . However , the situation is not the same in the case where S S . Since both and are vector valued , that at least one entry of is not equal to the entry with the same dimension in . The corresponding of and that are not equal to each other are by , , , . Since S S , for each , both and have been exposed to the same delay , say , . So if we denote the output of the that to by , then . It should be noted a positive integer which is at most equal to the size of the set As . Let 
 represent the conditional probability of at the given at 
 time . The set E , can be defined with exactly the same as E , but different from it in the sense that E only at time with probability . This only at input instead of either or . Let us define , ; . Then 
 R E , 
 aa 
 n , 
  
 n , E , 
 in which aa from the definition of entropy and , can be based on the fact that the function is monotonically decreasing , and from the definition of for the scheme E ,. So E , E , for , and consequently f . 
 The above procedure can be for pair with the same as , to make sure that there are no two of the reproduction into one identical u at time instant . Such iteration will then yield an invertible . In other , 
 when the pair E , is used , knowing is equivalent to knowing with 
 u i u i and i f i , i . Our main claim now by the above 
 for every . 
 APPENDIX B 
  
 A . Feasibility proof for and 
 Suppose that in the standard architecture in Fig . ,, x Assumption . . Considering the of x the 
 fact , we can imply from some in that : 
 , 
 in which the steady state variance of is the set of all proper which render the system of Fig . internally stable and well . The considered that finding is feasible . Since can be , for every , , there K which for the system of Fig . . K to this system in a stable setting which is a special 
  
 Fig . : Standard feedback loop over which is defined 
 case of the in Fig . with and where the steady state variance of , is finite . Therefore , since K , it can bring internal stability and well to the feedback loop of Fig . in the presence of any additive noise with steady sate 
 variance . So and can be , when taking 
 into account as an with finite variance for the system of Fig . . It should be noted that 
 , depend only on K . Now by choosing and the variance 
 for the , there K rendering the of Fig . internally 
 stable and well in a way that . Then 
 the following can be for the structure of Fig . : 
 . 
 So considering inequality and concavity of logarithm , we can deduce that the problem of finding in is feasible for every . The feasibility of the problem of finding in is immediately from for any . 
 B . Proof of Theorem 
 Due to the validity of , one can always find at least one control pair , say and , that while satisfying Assumption . , the of Fig . in such a 
 way that and 
  
 In and are the of , and u in Fig . , respectively . Moreover , 
 the and in stem from Theorem if in Lemma and Lemma are satisfied . Therefore , , are jointly of , as in Lemma and the steady state power spectral density of as in Lemma . The 
 pair , with stated in Lemma can be by a scheme which 
 and is comprised of linear with a unit gain noisy channel and delay 
 h as : 
 , 
 in which a noise with zero mean and independent of . Since is a linear and causal , we can redescribe as 
 . 
 It from causality in that , and are lower triangular matrices with and on the top left . This together with the fact that , are jointly allow us to conclude that based on transitivity of asymptotic equivalence for and sum of the matrices in , the and are asymptotically equivalent to of lower triangular matrices . Furthermore , as in will bring internal stability and well ness to the corresponding . Now let us set a concatenation of linear with the same behaviour as steady state behaviour of in for the auxiliary system of Fig . . Moreover , suppose that a variance equal to . So based on the asymptotic equivalence between the matrix , choosing , and as above will render the system of Fig . well and internally stable . More specifically , the latter set of and the noise will give to which and converge . Therefore , for the control input u and error signal z in the feedback loop of Fig . , and hold . Then based on Lemma , the directed information rate in the of Fig . can be expressed as 
  
 First , we can deduce that any pair E , with stated above a counterpart comprised of filter , and the white noise in architecture of Fig . in such a way that I y u I and . Secondly , the main problem is finding the 
  
 Fig . : The system whose internal stability the internal stability of the auxiliary system in Fig . 
 all E With all of this in mind , it can be from and that the lower bound for would be equal to the rightmost term of which 
 the proof . 
 C . Proof of Lemma 
 The necessary and sufficient condition for the feedback loop of Fig . to be internally stable and well is that every entry of the transfer function matrix from input ,, , to z , y ,, u in the system of Fig . to . Such a transfer function matrix , which we denote by , is as : 
  
 G 
  
 G 
 T , 
  
  
  
 M 
 where 
 M , . 
 Now , let us shift the delay block in the system of Fig . to the plant model in a way that for 
 the newly system , the plant is by 
 . 
 Such an auxiliary is by Fig . . Except for the plant model , everything 
  
 Fig . : The equivalent system with the same as the of Fig . 
  
 Fig . : The auxiliary feedback loop the internal stability of the of Fig . 
 in the feedback loop of Fig . is assumed to be the same as in the system of Fig . . The 
 internal stability and well ness of the feedback loop of Fig . is if and only 
 if every entry of the transfer function matrix , say Ta , from in 
 Fig . to . It is straightforward to see that Ta . So an equivalence between internal stability and well ness of the system of Fig . and the of Fig . . In other , every triplet rendering the feedback loop of Fig . internally stable and well , will bring internal stability and well ness to the of Fig . as well . One other implication of Ta is that an commonly for of Fig . and Fig . will lead to an identical . This is due to the of 
 exposed to and stationary . Furthermore , those lead to the following H norm for and variance of the output z in the of Fig . : 
  
 , 
 in which , . Likewise , the and variance of the the of Fig . is in of H as : 
  
 , 
 where Ma and Na , . It from and that 
 and . Therefore , upon the same triplet ,, , the channel and the variance of the output performance will be the same for the of Fig . and Fig . . 
 According to , Lemma . , for any pair , B , J that the feedback loop of Fig . internally stable and well , there another pair with the same as for B , J in this lemma . Then our follow immediately from the above between the of Fig . and the of Fig . . 
 D . Proof of Corollary 
 The feasibility of , to , , the existence of a triplet , say , that to for the system of Fig . . In the latter triplet , is assumed to be a proper filter and . This together with the definition 
 of and in and , respectively , the following : 
 . 
 Moreover , the triplet with the in Lemma . Therefore , another triplet , say , in such a way that it 
 internal stability and well ness , intact , and 
  
 for the feedback loop of Fig . . Note that is a filter while only needs to be proper . Now the fact that for any , , the definition of in , and the claim of Theorem complete the proof . 
 E . Proof of Lemma 
 the transfer function matrix from to in Fig . . Since is related toby , we can conclude of being proper and real rational , and a strictly proper open loop transfer function . Now the via and in mind , we can deduce our claim immediately from , Lemma . . 
 F . Proof of Lemma 
 Let us assume that a linear source scheme is in the feedback path of the main system in Fig . . Due to the feasibility of finding , which satisfaction of Assumption . , we can conclude the existence of together with an , say , render the of Fig . . It from some of internal 
 stability that the system will still be stable if one the only as . This that in the case of unity feedback , internal stability and ness are for the open loop system . We come immediately to the conclusion that based on , Corollary . and statistical of the dither in Lemma . 
 G . Proof of Theorem 
 Considering the feasibility of finding , of Lemma , lemma , and Lemma , and invertibility of the , we conclude the claim by following the same as in , 
 Theorem . . 
 H . Proof of Lemma 
 One of the common feedback loop across the considered in Fig . is the 
 is by state space difference as : 
  
 Ax Bu 
 G : 
 , 
 where plant and u ,, and defined as in 
 . Moreover , A , B , B , C , C , D , D , and D are time invariant matrices of appropriate . According to the recursion in , the and of the plant at each time instant i N can be expressed in of initial , disturbance and control as : 
  
 i B i wi B i 
 z i D i wi D i 
 i D i wi D i , 
 where the involved matrices are defined as 
 B i Ai B Ai B ... B 
 B i Ai B Ai B ... B 
 D i B B ... D 
 D i B B ... D 
 D i B B ... D 
 D i B B ... . 
 For the case where the time delay is by the error free digital channel between the controller and the controller , the relationship between the control input and the sensor output is based on . The dynamics by can be 
 in the constant channel delay case as : 
 , 
  
 u 
 where and represent causal , but otherwise arbitrary , at each N . It 
 from that can be stated as an arbitrary function , say , of , i . e ., 
 . Then from and by induction , we can conclude that at each time instant 
 k N , is a function of is a function of , and is a function of . 
 In the second case , it is the link between the controller and the plant that the 
 time delay . For such a setting , e and yield a scheme with following dynamics : 
 , 
  
 u , . 
 It from that in this case , can be expressed as , N , where is an arbitrary which is by and . 
 Substituting such an expression into an by induction , we can ,, and as 
 of , and , 
 respectively . 
 As the third case , we focus on a structure in which the delay is by the path between the sensor and the controller . In this situation , the scheme is by causal 
 and , and side e and , as : 
 , 
  
 u 
 Taking the same as for the previous , we derive , 
 where is a causal and a function of and . Then considering 
 and based on induction , we come to the conclusion that for the closed loop system considered in 
 this case , is a function of is a function of , 
 and is a function of for all N . 
 According to the above , system , sensor output , and the each time instant that such are not necessarily equal across the three studied above if the share the design for and control and side information and have the same initial and exogenous . So of each signal change by the delay component in the of Fig . . However , it is straightforward to see from the structure of the ,, the equivalence over can be under the condition that everything is the same across 
 the except for side information which can be considered as decision variable . 
  
 and . Interplay between transmission delay , average data rate , and performance in output feedback control over digital communication , in Control Conference , May . 
 and . A Achievable performance of zero delay variable rate in control with channel delay , in th Annual Conference on Decision and Control . 
 ... Han , and Survey on recent in control , on 
 Industrial , vol . , no . , . 
 . and . Feedback control under data rate : An overview , 
 of the , vol . , no . , . 
 and O Network induced in control a survey , 
 on Industrial , vol . , no . , . 
 . and . Control and communication in real time , of 
 the , vol . , no . , . 
 A .. and A . Estimation and control over communication . Springer Science Business 
 Media , . 
 .. and . A Feedback control in the presence of noisy : Bode like fundamental 
 of performance , on Automatic Control , vol . , no . , . 
 .. A and . Fundamental of disturbance attenuation in the presence of side 
 information , on automatic control , vol . , no . , . 
 E . I . Silva ,. and A framework for control system design subject to average data rate 
 , on Automatic Control , vol . , no . , . 
 , An achievable data rate region subject to a stationary performance constraint for , 
 on Automatic Control , vol . , no . , . 
 E . I . Silva ,. and . A . Encina , A characterization of the minimal average data rate that a given closed loop performance level , on Automatic Control , vol . , no . , . 
 . and . control with minimum directed information : Semidefinite approach , on Automatic Control , vol . , no . , . 
 . A and . Zero delay rate distortion via filtering for vector valued 
 , Journal of Selected in Signal to appear , . 
 . and Rate cost in control , . . Available : : . . v 
 and infinity stabilization for singular cascade control with state delay and 
 disturbance , on Industrial , vol . , no . , . 
 . A O . Yin , and Adaptive indirect fuzzy sliding mode controller for control subject to time network induced time delay , on Fuzzy , vol . , no . , . 
 . Cheng , and Fuzzy model based cost control of nonlinear , 
 on Fuzzy , vol . , no . , . 
 . Lam , and . Mao , Stability analysis of continuous time switched with a random switching 
 signal , on Automatic Control , vol . , no . , . 
 . Shi , and Network based robust H H control for linear with two channel 
 random packet and time , on cybernetics , vol . , no . , . 
 .. Pang ,. and Output control for : A model based prediction 
 approach , on Industrial Electronics , vol . , no . , . 
 . Li and . Shi , Network based predictive control for constrained nonlinear with two channel packet , 
 on Industrial Electronics , vol . , no . , . 
 . Wen , and . Cheng , Wide area damping controller for power system : A predictive control approach , on Control Technology , vol . , no . , . 
 . ` in controller design for with delay and quantization , in th Conference on 
 Decision and Control . 
 .. Han , and . Yang , Optimal communication network based H control with packet for a class of discrete time neural with distributed time delay , on neural and learning 
 , vol . , no . , . 
 E . and control under round robin communication protocol , 
 on Industrial Electronics , vol . , no . , . 
 .. A .. Teel ,. Van de , and control with communication : between transmission , and performance , on Automatic control , vol . , no . , 
 . , . 
 . and .. Wang , On the rate cost of linear control with random communication , in 
 International Symposium on Information Theory , June . 
 .. Cover and . A of information theory . Sons , . 
 . E . I . Silva , and Fundamental and mutual and directed 
 in closed loop , . . Available : : . . 
 H H controller design for input delay and preview based on state decomposition approach , 
 .. dissertation , Department of Human , Metropolitan University , Japan , . 
 E Control and communication with signal to noise ratio . dissertation , Department of 
 Automatic Control , University . 
 .. om ,¨ Introduction to stochastic control theory . Courier Corporation , . 
 .. Gray , and circulant matrices : A review , and in and Information 
 Theory , vol . , no . , . 
 . A A course in H control theory . Berlin ; New York : Springer , . 
  
 ﻿The characterization of `p-compressible random sequences is revisited and extended to the case of stationary and ergodic processes. The main result of this work offers a simpleto-check necessary and sufficient condition for a stationary and ergodic sequence to be `p-compressible in the sense proposed by Amini, Unser and Marvasti [1, Def. 6]. Furthermore, for non `p-compressible random sequences, we provide a closed-form expression for the best k-term relative approximation error given a rate of coefficients as the block-length tends to infinity.
 I.	INTRODUCTION
 Defining notions of compressibility for a stochastic process, meaning that every realization can be well-approximated in some sense by its best k-term sparse version [2], has been a topic of recent interest [1], [3], [4]. Quantifying compressibility for random sequences can play an important role in regression, reconstruction (for instance in the classical compressed sensing analysis and reconstruction setting [3, Th. 2]), inference and decision-making problems. One important case is defining such a compressibility notion for i.i.d. processes where the probability is equipped with a density function. In this context, every realization of the process is clearly non-sparse (almost-surely), and conventional ways of defining compressibility for finite dimensional signals, based on the power-law decay of the best k-term approximation error (or sequences that belong to the weak-`p ball), are not applicable either, as shown in [1], [3].
 Motivated by this, Amini et al. [1] and Gribonval et al. [3] have recently introduced new definitions for compressible random sequences. These notions are not based on the typical absolute approximation error decay pattern of the signals, but on a relative `p-best k-term approximation error behavior. In particular, Amini et al. [1] formally define the concept of `p-compressible process (details in Section II below). This new definition provides a meaningful way of categorizing i.i.d. random sequences (or distributions) in terms of the probability that almost all the `p-relative energy of the process is concentrated in an arbitrarily small sub-dimension of the coordinate (innovation) domain, as the block-length tends to infinity. Under this umbrella, they provide two important results using the theory of order statistics [1]. On the one hand, [1, Theorem 3] shows that a concrete family of i.i.d. heavytail distributions is `p-compressible (including the generalized Pareto, students‘s t and log-logistic), while on the other side,
 [1, Theorem 1] demonstrates that families with exponentially decaying tails (such as Gaussian, Laplace, Generalized Gaussian) are not `p-compressible. Therefore, it is interesting to ask about the compressibility of i.i.d processes not considered in this analysis. In this direction, we highlight the work of Gribonval et al. [3], which under an alternative notion of relative `p-compressibility (involving almost sure convergence instead of convergence in measure criterion adopted in [1]) and a different analysis setting (fixed-rate instead of the variable rate used in [1]), elaborates an exact dichotomy between compressible and non-compressible i.i.d. sequences. Therefore, it is an interesting direction to connect Amini et al. [1] `pcompressibility with the more refined almost sure convergence analysis of the `p best k-term relative approximation error in [3, Prop. 1], with the idea of completing the analysis of [1, Ths. 1 and 3].
 To address this question, as well as to extend the analysis to more general random sequences, this work precisely categorizes all `p-compressible random sequences, in the sense of Amini et al.[1], within the family of stationary and ergodic processes [5]. Our main result (Theorem 1) offers a necessary and sufficient condition for an ergodic process to be `p-compressible, for any arbitrary p > 0. Furthermore, for the case of non `p-compressible ergodic processes, we provide a closed-form expression for an achievable rate v/s `p-approximation error function. The key element in the proof is the application of the ergodic theorem [5] and the derivation of intermediate almost-sure convergence results (Lemma 1 and 2 in Section IV) that match the approximation result presented by Gribonval et al. [3, Prop. 1] developed for the i.i.d. case. A corollary of Theorem 1 implies a necessary and sufficient condition to categorize i.i.d. random sequences in terms of `pcompressibility, which completes the analysis presented in [1, Ths. 1 and 3].
 II.	PRELIMINARIES
 For a finite-dimensional vector xn = (x1,..,xn) in Rn, let (xn,1,..,xn,n) ? Rn denote the ordered vector such that
 |xn,1| = |xn,2| = ...|xn,n|. For some p > 0 and k ?
 {1,..,n}, let
 ?p(k,xn) = (|xn,1|p + ... + |xn,k|p)p1	(1)
 denote the `p-norm of the best k-term approximation of xn, where by definition ||xn||`p = ?p(n,xn). In addition,
  
 denotes the best k-term `p-approximation error of xn, in the sense that if   is the collection of k-sparse signals, then sp(k,xn) is the solution of minx˜n?Skn ||xn - x˜n||`p. For the analysis of infinite sequences, Amini et al. [1] and Gribonval et al. [3] proposed the following relative best k-term `p-distortion indicator:
  
 From this, Amini et al. [1] introduced a notion of critical dimension (for a finite-length signal) and a notion of `pcompressibility for infinite sequences.
 Definition 1: [1, Def. 4] For xn ? Rn and d ? (0,1), let us define ?p(d,xn) = min{k ? {1,..,n} : s˜p(k,xn) = d}. (4)
 Then, a sequence (xn)n?N ? RN is called `p-compressible if, ?d ? (0,1)
 	 ,	(5)
 where xn = (x1,..,xn) is the truncated finite-block vector of (xn)n?N.
 A. Approximation Error for Random Sequences
 Let X1,..,Xn,.. be a random sequence with values in (R,B(R)) and characterized by its consistent family of finite-dimensional probabilities {µn ? P(Rn) : n = 1} [5], where Xn = (X1,..,Xn) ~ µn and P(Rn) denotes the space of probability measures for the Borel measurable space (Rn,B(Rn)). As a short-hand, we denote by P =
 {µn : n = 1} the process distribution of (Xn)n?N. Let us define the measurable set An,kd	=
 {xn ? Rn : s˜p(k,xn) = d} = {xn ? Rn : ?p(d,xn) = k},
 (6)
 where the last equality is by (4). Then in analogy with Definition 1, Amini et al. [1] proposed the following:
 Definition 2: [1, Defs.5 and 6] Let us consider a process
 (Xn)n?N (equipped with  and d ? (0,1). Then
  
 is the critical number of terms with which the set typical with respect to µn. With this, the process (Xn)n?N (and P, respectively) is said to be `p-compressible, if , ?d ? (0,1),
 	 .	(8)
 Alternatively, we can consider the following fixed-rate notions:
 Definition 3: Let (Xn)n?N be a process characterized by
 P, and let us consider  and d ? (0,1).
 We say that the rate-distortion pair (r,d) is `p-achievable for (Xn) with  probability, if there exists a sequence of positive integers (kn) such that  and
 	 	(9)
 Definition 4: The rate-distortion approximation function of (Xn), with  probability is given by:
  
 inf	{(r,d) is `p-achievable for (Xn) with  probability}. r?[0,1]
 (10)
 In the next section, we will study the class of stationary and ergodic processes [5], where the best k-term approximation properties measured in terms of the function  in (10) will be characterized in closed-form. Furthermore, it will be shown for this class of random sequences that
  
 for all  and d ? (0,1).
 III.	ANALYSIS OF ERGODIC PROCESSES
 Let (Xn)n?N be a stationary and ergodic sequence (process) with distribution P = {µn : n = 1}, where we denote by µ ?
 P(R) its shift-invariant distribution [5]. For simplicity , we assume that  where ? denotes de Lebesgue measure
 [5]. Then µ is equipped with a probability density function (pdf) and dµ(x) = dµ d?(x)d?(x).
 For a measure v on (R,B(R)), we say that a measurable function f : (R,B(R)) -? (R,B(R)) is integrable with respect to v if [5]
 Z
 	|f(x)|dv(x) < 8	(11)
 R
 and L1(v) denotes the collection of v-integrable functions.
 We are in the position to state the main result:
 THEOREM 1: Let (Xn)n?N be a stationary and ergodic process with marginal shift-invariant distribution µ ? P(R) such that , and let us consider an arbitrary p > 0. Then we have the following dichotomy:
 i)	If fp(x) = xp ?/ L1(µ): then  and ?d ? (0,1),
 	 .	(12)
 i.e., (Xn)n?N is `p-compressible.
 ii)	If fp(x) ? L1(µ): then (Xn)n?N is not `p-compressible. Furthermore, if we introduce the induced probability measure in (R,B(R)) by:
 	 ,	(13)
 then ?d ? (0,1) and 
 	 .	(14)
 where Bt = (-8,-t] ? [t,8) ? B(R) and t > 0 is solution of:
 	vp(Bt) = 1 - dp.	(15)
 (The proof is presented in Section IV).
 A. Discussion and Interpretation of Theorem 1
 1)	Theorem 1 offers a necessary and sufficient condition for a stationary and ergodic process to be `p-compressible in the sense elaborated in Definition 2.
 2)	In the case of non `p-compressible processes, i.e., when fp(x) ? L1(µ), Theorem 1 offers, what we call, the achievable rate-distortion region for the process, given by the set of critical rate-distortion pairs:
  
 This region depends solely on the shift invariant measure µ ? P(R) and its induced measure vp ? P(R) in (13).
 3)	In both scenarios i) and ii), the critical rate  for a stationary and ergodic processes is independent of . The justification is that asymptotically as n goes to infinity, the characterization of  implies to compute probabilities on events that belong to the tail s-field of the process, which is known to be trivial (i.e., their events have zero or one probability) for the case of ergodic processes [5]. Therefore, we obtain P-almost-sure convergence results that make irrelevant the role of  in the characterization of  (see Section
 IV for details).
 4)	A natural order among stationary and ergodic process can established from Theorem 1.
 Proposition 1: If (Xn)n?N is `p-compressible for some p > 0, then (Xn)n?N is `q-compressible for all q = p. Proof: If fp(x) ?/ L1(µ), then fq(x) ?/ L1(µ) for all q = p .
 Proposition 2: If (Xn)n?N is not `p-compressible for p > 0 then (Xn)n?N is not `q-compressible for all q = p. Proof: If fp(x) ? L1(µ), then fq(x) ? L1(µ) for all q = p.
 5)	For the emblematic i.i.d. scenario , we want to highlight in more detail the results by Amini et al. [1] related to `1compressibility in the sense of (8). [1, Theorem 1] says that if µ is such that for some ? < 0, EX~µ(e?·X) < 8 then the i.i.d. process is not `1-compressible. On the other hand, [1, Theorem 3] says that if µ belongs to the domain of attraction of a-stable distribution [5, Chap. 9.11, pp. 207-213] with a ? (0,1), then the process is `1-compressible. First for p = 1, Theorem 1 provides a refined result, revealing a richer (indeed, the complete) family of distributions that are not `1-compressible. In fact, in addition to distributions that go to zero exponentially and consequently f1(x) ? L1(µ) (Gaussian, Laplacian, Gamma, etc.), heavy tail distributions whose density functions are lower dominated by a power-law decay of the form   with q > 1 (for instances, student‘s t with q degrees of freedom), are not `1-compressible either3. On the other hand, concerning [1, Th. 3], it is simple to verify that any µ that is in the domain of attraction of an a-stable law with a < 1 [5, Ch.9] satisfies that EX~µ(|X|) = 8 and consequently, part i) of Theorem 1 covers this family of `1-compressible i.i.d. processes.
 6)	More generally, from Theorem 1 we can state the following:
 Corollary 1: Let (Xn)n?N be a stationary and ergodic process. If ?? > 0 where EX1~µ(e-?X) < 8, then (Xn)n?N is not `p-compressible for any p > 0.
 Corollary 2: Let (Xn)n?N be an ergodic process with invariant distribution   and density fµ(x) =  dµd?(x), ?x ? R. If fµ(x) decays as |x|-(t+1) for some t > 04, then (Xn)n?N. is lp-compressible, if and only if, p = t.
 7)	For the proof of Theorem 1, we derive almost-sure convergence results (see Lemma 1 and 2 in Section IV). In the case when fp(x) ? L1(µ): if (kn) is such that  
 µ(Bt) for some t	>	0, then limn?8 s˜p(kn,Xn)	=
 pp 1 - vp(Bt), P - a.s.. Furthermore, for 
 for some t > 0, it follows that  , P-a.s.. In the case when , then limn?8 s˜p(kn,Xn) = 0, P - a.s. These results are consistent and extend the result by Gribonval et al. [3, Prop. 1], which for the i.i.d. case shows the same almostsure convergence limit for the object s˜p(kn,Xn). Their proof was based on the Wald‘s lemma of order statistics (see details in [3, Th. 6]), while ours is based on the application of the ergodic theorem.
 8) Theorem 1 implies another interesting dichotomy
 Corollary 3: If for some  and for some d ? (0,1) it holds that
  ,
 then the latter also holds for all  and for all d ? (0,1). Likewise, if for some , for some r ? (0,1) and for all d ? (0,1) the pair (r,d) is `p-achievable for (Xn) with  probability, then all pairs (¯r,d) with r¯ ? (0,1) and d ? (0,1) are `p-achievable for   probability.
 IV.	PROOF OF THEOREM 1
 Let us first state the following:
 Proposition 3: For any  and d ? (0,1):
 	 .	(17)
 Proof: Considering  as a short-hand, by definition in (7)  , where from (10) it follows that  .  
 3µ is lower dominated by g(x), if there exits x0 > 0 and C0 > 0 such that for any x such that |x| > xo then fµ(x) = C0 · g(x).
 4We say that fµ(x) decays as |x|-(t+1) if there exists xo > 0 and
 0	1	2 < 8 and limx?8 fµ(x) = ? ?
 ,
  ; and otherwise,
 Let us first consider the case when fp(x) ? L1(µ). For the rest, it is important to note that given that , then for all r ? (0,1) there exists t > 0 such that µ(Bt) = r, and for all
 d ? (0,1) there exists t > 0 such that pp 1 - vp(Bt) = d. 5 For (X1,..,Xn) ~ µn, we can define:
 	 ,	(18)
 where from the ergodic theorem [5, Th. 6.28], ?t = 0,
  
 and  
 The second almost sure convergence is from the assumption that fp(x) ? L1(µ). Then, we can state the following:
 Lemma 1: For any r ? (0,1) and sequence (kn)n?N such that  , we have that
 q 
 	lim s˜p(kn,Xn) = p 1 - vp(Bt), P - a.s.	(21)
 n?8
 where Bt is solution of µ(Bt) = r. In addition, ?d ? (0,1),
 	 	(22)
  
 where Bt is solution of pp 1 - vp(Bt) = d. (The proof is given in Section I-A)
 In order to prove (14), let us fix d ? (0,1). Then there exists t, such that (15) holds and from Lemma 1 if (kn)n is such that  , then limn?8 s˜p(kn,Xn) = d, P - a.s..
 Let us consider an arbitrary t < t˜ such that µ(Bt˜) > µ(Bt), then again from Lemma 1, if a sequence (k˜n) is such that knn ? µ(Bt˜), then limn?8 s˜p(k˜n,Xn) =
  
 pp 1 - vp(Bt˜) < d, P - a.s.. Consequently, s˜p(k˜n,Xn) convergences almost surely to a distortion strictly less than d, and then for all :
 	 	(23)
 Hence from the definition of  in (7), we have that   eventually in n, which implies that
 	 .	(24)
 This upper bound is valid for any t < t˜ such that µ(Bt˜) > µ(Bt), then
 =	inf	µ(Bt˜) t<t˜
 µ(Bt˜)>µ(Bt)
 	= µ(Bt).	(25)
 5In general, the achievability condition on t > 0 for the rate (i.e., ?r ? (0,1) ?t > 0 such that µ(Bt) = r) and the distortion (?d ? (0,1) ?t > 0
 pp 1 - vp(Bt) = d) are not unique.
 The first inequality from Proposition 3 and the last equality from the fact that the function fµ(t) = µ(Bt) is continuous as .
 To derive a lower bound, let us consider an arbitrary r˜ ? (0,µ(Bt)). We know that there exists Bt˜ with t > t˜ such that µ(Bt˜) = r˜. Again from Lemma 3, for all (k¯n) such that
    , then limn?8 s˜p(k¯n,Xn) = pp 1 - vp(Bt˜) = d > d˜ , P - a.s. Therefore,
 	 .	(26)
 This result implies that eventually in , and
 consequently,
 	 .	(27)
 On the other hand, from (26), we have that it is necessary that . This last inequality and (27) are valid for any r˜ ? (0,µ(Bt)), then   and µ(Bt) =
 n
 , which from (25) proves (14).
 Moving to the case where fp(x) ?/ L1(µ), we have that
 EX~µ(|X|p) = 8, then from the ergodic theorem [5] ?t > 0,
  
 (28)
 and 	(29)
 In	other	words,	(29)	means	that limn?8 s˜p(n(Xn,Bt),Xn) = 0, P - a.s. Furthermore, we have the following:
 Lemma 2: Let us consider an arbitrary r ? (0,1) and
 (kn)n?N such that  , then
 lim s˜p(kn,Xn) = 0, P - a.s.	(30)
 n?8
 (The proof is presented in Section I-B)
 Let fix an arbitrary r > 0 and  , then from Lemma 2 we have that ?d > 0:
 	 .	(31)
 Then	for	all 	and	d	?	(0,1),   and therefore 
 kn eventually in n. From this,  for all r > 0, which concludes the result from Proposition 3.
 V.	ACKNOWLEDGMENT
 The work of J. F. Silva is supported by awards of CONICYT Fondecyt Grant 1140840 and the CONICYT Basal Grant AMTC. The work of M. S. Derpich is supported by CONICYT Fondecyt Grant 1140384, and UTFSM internal research grant
 23.13.65.
 APPENDIX I COMPLEMENTARY RESULTS
 A. Proof of Lemma 1
 Proof: To begin let us prove the fixed-rate result in (21).
 It will be first important to concentrate in the case where
 0 and show in this context that
 	lim s˜p(kn,Xn) = 1. P - a.s.	(32)
 n?8
 For any t > 0, let us define the sets
  
 (33)
 where from (19) and (20), P(At n Bt) = 1 because
  
 	lim s˜p(n(Xn,Bt),Xn) = qp 1 - vp(Bt), P - a.s.	(34)
 n?8
 Let us fix an arbitrary d > 0 and let d = 1 - d and Bt
 such that pp 1 - vp(Bt) = d for some t > 0. Let us take an arbitrary (xn)n ? At n Bt. From the zero-rate assumption on (kn) and the definition of At, ?N > 0 such that ?n = N, kn < n(xn,Bt), which implies that s˜p(n(xn,Bt),xn) < s˜p(kn,xn). Therefore considering that (xn) ? Bt,
 lim inf s˜p(kn,xn) = d = 1 - d.	(35) n?8
 Doing the same process for the collection  , we have that ?(xn) ? Tm=1(A m n B m),
 	 .	(36)
 Then from the sigma additivity P(Tm=1 Atm n Btm) = 1, which proves the result in (32).
 Equipped with this result, for an arbitrary r ? (0,1) let us consider (kn) such that limn kn/n = r. We know that there exists to > 0 such that r = µ(Bto), and for this to we consider Ato and Bto as defined in (33). Then for any (xn) ? AtonBto it follows that
 	 .	(37)
 Furthermore, from definition of the ordered sequence, it is simple to verify that (see Eq.(1)):
  k	n p	n,B	n p| =
 (38) This is where the zero-rate result in (32) is used. In particular,
 if we consider (xn)n?N ? (Ato n Bto) n Tm=1 Atm n Btm, from (38), (36) and the fact that |kn - n(xn,Bto)| is o(1) in
 (37), we have that
 lim s˜p(kn,xn) = lim s˜p(n(xn,Bto),xn) = qp 1 - vp(Bto
  
 the last equality in (39) from definition of Bto in (33). Finally from (19) and (20), 
 1 which proves (21).
 Concerning the fixed-distortion result in (22), for d ? (0,1)
  
 let t > 0 be such that pp 1 - vp(Bt) = d. Let us consider an arbitrary t > t¯ where µ(Bt¯) < µ(Bt) and consequently vp(Bt¯) < vp(Bt) (µ and vp are mutually absolutely continuous). Again for this t¯, we use the sets in (33), where for all  , it follows that   such that  .
 Considering  , it follows by definition in
 (4) that:
 (40)
 then= liminfn?8 ?p(d,xn)/n. We can do the same for the numerable collection:
   such that 
 where for any (xn) ? Ttm?C Atm n Btm,
 µ(Bt) = sup µ(Btm) = lim inf ?p(d,xn)/n.	(42) tm?C	n?8
 The first equality follows from the continuity of the function fµ(t) = µ(Bt) with respect to  . Then from the fact that P(Ttm?C Atm n Btm) = 1, we have that, µ(Bt) = liminfn?8 ?p(d,Xn)/n, P-a.s. Finally, proving that limsupn?8 ?p(d,Xn)/n = µ(Bt) P-a.s. follows an equivalent symmetric argument and we omit it.  
 B. Proof of Lemma 2
 Proof: For r > 0 let us consider r¯ ? (0,r) and t > 0, such that µ(Bt) = r¯ and (kn) such that  . Considering the sets   and
  , we have
 that P(At n Bt) = 1 from (28) and (29). Let us fix an arbitrary (xn) ? At n Bt. Considering that r < r¯ , then   eventually in n, and therefore   eventually, which implies
 from definition of Bt that limn?8 s˜p(kn,xn) = 0. The fact the event At n Bt happens P-almost surely concludes the result.	 
 ﻿
 This paper studies the performance of a feedback control loop closed via an error-free digital communication channel with transmission delay. The system comprises a discrete-time noisy linear timeinvariant (LTI) plant whose single measurement output is mapped into its single control input by a causal, but otherwise arbitrary, coding and control scheme. We consider a single-input multiple-output (SIMO) channel between the encoder-controller and the decoder-controller which is lossless and imposes random time delay. We derive a lower bound on the minimum average feedback data rate that guarantees achieving a certain level of average quadratic performance over all possible realizations of the random delay. For the special case of a constant channel delay, we obtain an upper bound by proposing linear source-coding schemes that attain desired performance levels with rates that are at most 1.254 bits per sample greater than the lower bound. We give a numerical example demonstrating that bounds and operational rates are increasing functions of the constant delay. In other words, to achieve a specific performance level, greater channel delay necessitates spending higher data rate.
 I.	INTRODUCTION
 Taking communication imperfections into account for analysis and design has proved to be an interesting topic within the area of control theory during recent years. This interest is motivated by advantages of communication networks over point-to-point wiring and, on the other side, by the complexity that communication constraints impose on classical control problems [3]. Time delay, packet dropout and data rate constraints (quantization) are among prominent challenges [4]–[7].
 Using an information-theoretic approach, [8], [9] report primary derivations related to system performance. In these works, it is shown that the presence of a finite-capacity communication channel in a strictly causal feedback loop introduces a new performance limitation which differs from conventional Bode’s formula by a constant quantifying channel information rate. Moreover, the authors derive inequalities among entropy rate of internal signals (inside the loop) and external signals (outside the loop), resulting in a general performance bound which is affected by finite feedback capacity. Inspired by [8], [9], lower and upper bounds are derived on the minimum data rate that guarantees achieving a prescribed level of quadratic performance in [10]–[12]. These works consider noisy linear time-invariant (LTI) plants with Gaussian disturbances, controlled over an error-free digital channel without delay. In particular, [12] shows that over all causal mappings which represent coding and control, the average data rate is bounded from below by the directed information rate generated by the mappings that render the sensor input and control output jointly Gaussian. Moreover, it is proved in [12] that in an auxiliary LTI structure, the minimum signal-to-noise ratio (SNR) which guarantees stability and meeting a quadratic performance requirement gives the lower bound on the desired minimal data rate. For the upper bound analysis, [12] suggests employing entropy-coded dithered quantizers (ECDQs). Such a simple coding scheme is designed based on the aforementioned SNR-constrained optimization giving the lower bound. Inspired by [10] and [12], the authors of [13] present a method based upon semidefinite programming (SDP) to characterize the trade-off between directed information rate and linear quadratic Gaussian (LQG) performance in rate-constrained networked control systems (NCSs) with fully-observable multiple-input multiple-output (MIMO) plants. In [14], the authors derive a lower bound on the zero-delay rate distortion function associated with vector-valued Gauss-Markov processes and mean-square error distortion constraint. Based on the separation principle, this bound is in fact the lower bound on the minimum data rate required for attaining LQG performance in control of fully observable plants. Then [14] utilizes the optimal realization that corresponds to the foreshadowed class of vector-valued Gaussian sources to derive an upper bound on zero-delay rate distortion function using variable-length entropy coding with lattice quantization. Similar ideas are employed in [15] for establishing bounds on minimum mutual informations, across a delay-free channel, that guarantee achieving specific linear quadratic regulator (LQR) performance levels. Specifically, [15] derives the lower bound based on Shannon’s lower bound and power entropy inequalities whereas the upper bound is
 established via variable-length coding and lattice-based quantization methods.
 NCSs subject to network-induced delays are generally analyzed according to two methodologies: robustness and adaptation [5]. The aim in the robustness framework is deriving conditions for certain stability or performance requirements by constructing Lyapunov-Krosovskii functionals that do not incorporate time-stamp information as a variable. For instance, in [16], stabilization
 and H8 performance conditions for a singular cascade NCS are obtained. Fuzzy-model-based control is another approach in the robustness framework, where the rules are based on the size of delays, and the controller is required to be robust over the delay range [17], [18]. In the adaptation framework, one method is modelling NCSs as stochastic switched systems. The recent results on stability and H2/H8 performance of Markov jump linear systems (MJLSs) are reported in [19] and [20], respectively. The second approach in this framework is predictive control; a method which is currently quite popular in NCSs. According to this technique, the actuator selects among a sequence of control commands based on the transmission delays experienced by them [21]–[23].
 In all the aforementioned results on system performance, either the effect of channel delay is neglected, or the rate limitation is not taken into account. However, looking into the literature, one can find works investigating performance issues in NCSs with both rate constraints and network-induced delays (see, e.g., [24]–[27]). Even so, a few has utilized the informationtheoretic approach to treat systems with such limitations. For example, [28] derives bounds on the minimum individual (non-asymptotic) rate needed to guarantee meeting an individual performance requirement (boundedness of the maximum `2-norm of states).
 In this paper, we study the performance of a discrete-time LTI plant with Gaussian initial state in a loop with Gaussian exogenous inputs and random or constant channel delay on the feedback path. For the setup with random delay in the channel, we seek the infimum average data rate required to achieve a prescribed qudratic performance level. We show that the average data rate over all possible realizations of the delay is lower bounded by the average directed information rate. We prove for the random channel delay case that under certain stationarity assumptions, the average directed information rate can be stated in terms of average power spectral densities of the involved signals. We obtain a lower bound on the desired minimal average data rate which is stated as the average of a function of the power spectral densities of feedback path signals over all possile realizations of the delay. To establish all these results, we utilize the tools adopted in [10] and [12]. However, compared to [12] and [10], the channel is not delay-free in our setup. In other words, we extend the information inequalities in [12] to the case where there exists a random time delay between the sensor output and the control input.
 For the setup with known constant delay in the channel, we show that the above lower bound on the infimum average data rate required for attaining quadratic perfromance is equal to a function of infimum SNR of the channel over schemes comprised of LTI filters and AWGN channels with feedback and delay that meet the quadratic performance constraint. Our contribution in this case is showing how the presence of the channel delay affects the scheme yielding the lower bound. This gives an insight to the interplay between time delay, average data rate and performance in the considered NCS. We also prove that even over a channel with a constant delay, any admissible performance level can be achieved by an EDCQ-based linear coding scheme which generates an average data rate at most (approximately) 1.254 bits per sample away from the corresponding lower bound. We illustrate via a numerical example that lower and upper bounds as well as empirical rates and entropies are all increasing functions of channel delay. This in turn implies that channels with larger delays demand higher average data rates to allow for attaining a certain system performance.
 Compared to our previous works in [1] and [2], first, we here study the case of random channel delay and second, we employ a simpler proof than information inequalities and identities in [1] and [2]. In this work, we also show the effect of having a delay at different places in the loop on system signals. The last departure from our previous results is that we incorporate some eliminated proofs of [1] into this paper.
 The remainder of the paper is organized as follows. Section II introduces the notation. Section III formulates the main problem. Section IV analyzes the lower bound problem for the setup with random channel delay. Section V derives a lower bound on the desired minimal data rate in the case of constant channel delay. The analysis of upper bound problem in the constant delay case is presented in Section VI where the equivalence between systems with different delay locations is investigated. A numerical example is given in Section VII. Finally, Section IX concludes the
 paper.
 II.	NOTATION
 By R, we denote the set of real numbers whose subset R+ represents the set of strictly positive real numbers. The set N0 is defined as N0 ,N ? {0} where N symbolizes the set of natural numbers. The time index of every considered signal, denoted by k in most cases, belongs to N0. Symbols E, log, |.|, and k.k2 represent operators for expectation, natural logarithm, magnitude and H2-norm, respectively. Moreover, ?min(S) and ?max(S) are respectively the largest and smallest eigenvalues of the square matrix S for which the element on the i-th row and j-th column is denoted by [S]i,j. In addition, ßk is shorthand for ß(0),...,ß(k) where ß(k) denotes the k-th sample of a discrete-time signal. Furthermore, for the time-dependent set a(i),i ? N0, ak is defined as ak , a(0) × ··· × a(k). However, if a is a fixed set, then ak , a × ··· × a
 (k times).
 Random variables and processes are vector valued, unless otherwise stated. Take v and q into account as two random variables with known marginal and joint probability distribution functions (PDFs). Their joint PDF is represented by f(v,q) while the marginal PDFs of v and q are symbolized by f(v) and f(q), respectively. The conditional PDf of v given q is denoted by f(v|q) and Ev(.) is the operator for the expectation with respect to the distribution of v. We define the differential entropy of v and the conditional differential entropy of v given q as h(v) , -Ev(logf(v)) and h(v|q) , -Ev,q(logf(v|q)), respectively. The mutual information between v and q is symbolized by by I(v;q) and
 defined as I(v;q) , -Ev,q(log(f(v)f(q)/f(v,q))). Moreover, the definition of the conditional
 mutual information between random variables v and q given the random variable r is given by I(v;q|z) , I(v,r;q) - I(r;q). All the information-theoretic definitions presented in this
  
 Fig. 1: Considered NCS with a channel imposing random delay
 paragraph are standard and follow [29].
 We call the random process ? asymptotically wide-sense stationary (AWSS) if limk?8 E[?(k)] =
 ?? and C? , R?(0) and limk?8 E[(?(k + t) - E[?(k + t)])(?(k) - E[?(k)])T ] = R?(t) hold,
 where ?? is a finite constant. Accordingly, the steady-state covariance matrix and the steady-
 state variance of ? are defined as s?2 , trace(C?), respectively. For the scalar random sequence
  , we define the covariance matrix as  .
 Assume that Pn,Qn ? Rn×n are square matrices. Then the sequences  and  are
 called asymptotically equivalent if and only if they satisfy the following expression for finite %:
  .
 III.	PROBLEM FORMULATION
 We consider the feedback loop of Fig. 1 where the plant is LTI with one control input and one sensor output denoted by u ? R and y ? R, respectively. The plant G is disturbed by a vectorvalued zero-mean white noise which is represented by w ? Rnw and has identity covariance matrix, i.e. Cw = I. Moreover, as depicted in Fig. 1, the plant outputs the vector-valued signal z ? Rnz upon which the performance measure is characterized. The relationship between the mentioned set of inputs and outputs is described by a transfer-function matrix as follows:
 	  ,	(1)
 where the dimensionality of each Gij is determined by the dimensions of corresponding pair of inputs and outputs. So nz ×nw, nz ×1, 1×nw and 1×1 are the dimensions for G11, G12, G21
 and G22, respectively.
 Assumption III.1. Every entry of the transfer-function matrix in (1) is proper with no unstable hidden modes. Moreover, G22, which describes the single-input single-output (SISO) open-loop system from u to y, is strictly proper. The initial states of the plant denoted by x0 = {x(-hmax),...,x(0)} are jointly Gaussian with and independent of the disturbance signal w and has a finite differential entropy.
 As depicted in Fig. 1, the output of the plant, y, is processed into a binary word by the encoder E and transmitted over the error-free channel. Such transmission is accompanied with a random delay. Let h(k) denote the delay experienced by the binary word yq(k) constructed at time k at the encoder. We assume that h(k) is an independent and identically distributed (i.i.d.)
 process which has a bounded support at each time step, i.e., h(k) ? {h1,...,hm}, ?k ? N0 where hi < hi+1 (i = 1,2,...,m-1). In order to avoid unnecessary notational complexity and without loss of generality, we set h1 as h1 = 0 and hm as hm = hmax. The marginal distribution of the delay is assumed to be known and described by Pr{h(k) = hj} = aj where .
 Such characteristics introduce a channel with the following input-output relationship:
 	uq(k) = [yq(i)]i?S(k),	k ? N0	(2)
 where S(k) is defined as
 	S(k) , {i : i + h(i) = k}	(3)
 for every k,i ? N0. Denoting the cardinality of S(k) by s(k), we can imply form (2) that uq(k) is a vector comprised of s(k) = hmax +1 binary words which specifies the output of the channel at time k. Note that s(k) is a random variable depending on the channel delay. We assume
 that yq(i) is discarded at the decoder-controller side if i < 0. Moreover, under aforementioned circumstances, binary words transmitted over the considered channel are not necessarily received in the same order they were emitted. It should be also emphasized that the channel does not
  
 Fig. 2: Considered NCS with a detailed model of coding and control scheme
 allow for any data loss. The average data rate across the channel is defined as
 	 ,	(4)
 where R(i) indicates the expected length of the binary word yq(i).
 A more detailed presentation of the feedback path in the NCS of Fig. 1 is provided by Fig. 2. As depicted, the encoder-controller is comprised of a lossy and a lossless component. The lossy
 part E outputs the symbol yE according to the following dynamics:
 	 ,	(5)
 where ?e(k) ? ?e(k) symbolizes the side information at time k at the lossy encoder. Ek :
   is a deterministic map and As represents a fixed countable set. At each time
 instant, the encoder is assumed to know the time delays experienced by previous binary words and the time delay of the current binary word to be sent over the channel. Therefore, hk is known at the encoder ?k ? N0. This implies that yE(k) can be reconstructed perfectly h(k) steps later at the decoder if yq(k) is constructed by using yE(k) and only those samples of  which will be already available at the decoder at time k + h(k). Note that having access to  at the decoder at the time k +h(k) is not assured. So the lossless encoder O outputs the binary
 symbol yq based on
 	 ,	(6)
 in which yEf(k) is a sequence comprising the elements of yEk-1 for which the associated binary words will have reached the decoder by the time k + h(k), i.e., {yE(i) : i ? N0,i = k - 1,i +
 h(i) = k + h(k)}. Moreover, ?o(k) ? ?o(k), and   is an arbitrary
 deterministic mapping where k - hmax + h(k) + 1 = f(k) = k + 1. So f(k) - 1 specifies the
 cardinality of the sequence yEf(k). Note that since no dropout occurs during data transmission,
   will certainly have been received at the decoder by the time k + h(k). In addition, A(k) is a countable set of prefix-free binary code words, which specifies the input alphabet of the channel at each time instant.
 On the receiver side, uq(k) is available as the input to the lossless decoder. This decoder,
 shown by O-1, generates uD as
 	 ,	(7)
 where uqf(k) is a sequence comprised of elements of that have time indices less than or equal to the largest time index of yq in uq(k), i.e.,  where m(k) = maxS(k), ?k ? N0. Such selection of data for lossless decoding is in accordance
 with the information utilized in (6) for encoding. Furthermore,  
  , where k - hmax + 1 = g(k) = k + 1, represents an arbitrary deterministic mapping. It should be noted that according to the channel model, g(k) is a random variable denoting the cardinality of uqf(k) and   holds for all k ? N0. Moreover, based on the definition of uqf and (6), the information provided by   is enough for the lossless decoder
 to reconstruct every element of {yE(i)}i?S(k) perfectly. Therefore
 	uD(k) = [yE(i)]i?S(k),	k ? N0	(8)
 where S(k) is defined as in (3). Indeed for such reconstruction, the knowledge of the delay is required at the decoder. Hence, we further assume that the decoder is provided by S(k) through
 for example timestamping. Finally, the decoder-controller gives the control input via
 	u(k) = Dk(ukD,?dk).	(9)
 where ?d(k) signifies the side information available at the decoder at time k and is contained in the well-defined set ?d(k). So ?d(k) satisfies ?d(k) ? ?d(k). Moreover,  
 R, k - hmax + 1 = tu(k) = k + 1, is an arbitrary deterministic mapping where tu(k) is the cardinality of Sk. It should be noted that ?o(k) in ?o(k) ? ?o(k) is defined as ?o(k) , ?e(k)n ?d(k). We state some additional properties of the setting described above in the following
 remarks.
 Remark 1. It can be implied from (5)-(9) that u(k) and uk are functions of   where lk = maxSk for every k ? N0. It thus follows from the definition of S(k) and assuming no dropout in the considered channel that k - hmax = lk = k. This implies that the controller has access to the largest and smallest amount of sensor information when the channel delay is zero and hmax, respectively.
 Remark 2. It can be implied from the definition of S(k) in (3) that uq(k) can have at most hmax+1 entries at each time step. So the number of the words that can be received at the decoder at each time instant belongs to the set {0,...,hmax +1}. Therefore, since the channel input yq is a scalar process, (2) describes a single-input multiple-output (SIMO) channel. Moreover, S(k), as a stochastic process, cannot be i.i.d because in the considered channel, no transmitted binary word is received at the decoder more than once.
 For further analysis, we consider the following assumption.
 Assumption III.2. At each time instant k ? N0, the side information pair (?e(k),?d(k)) together with h(k), and consequently S(k), are statistically independent of (x0,w(k)). Therefore, it can
 be implied from the dynamics of the system that I(u(k);y(k - hi) | uk-1) = 0 for any hi ? {1,...,hmax} with k - hi < 0. Moreover, upon knowledge of ui, ?di and Si, the decoder is invertible. It means that for each i ? N0, there exists a deterministic mapping Qi such that
  .
 Remark 3. In Appendix A, we will prove that for the architecture of Fig. 2, any encoder and non-invertible decoder with mappings E, O, O-1 and D, can be replaced by another set of mappings with the same input-output relationship and lower average data rate where the decoder is invertible .
 For the purpose of expressing the information rate in terms of spectral densities of the signals of the system, we use the following notion of stability:
 Definition 1. A scalar AWSS process x is called strongly asymptotically wide-sense stationary (SAWSS) if its covariance matrix is asymptotically equivalent to the covariance matrix of the wide
 sense stationary (WSS) process, say x¯, to which it converges, i.e.,  and  are
 asymptotically equivalent. Furthermore, in an SAWSS NCS, all internal signals are SAWSS and their cross-covariance matrices are asymptotically equivalent to the cross-covariance matrices of corresponding WSS processes to be converged to.
 Clearly, SAWSS-ness implies AWSS-ness but not vice versa; for both signals and systems. For each coding scheme satisfying (5)-(9) and rendering the NCS of Fig. 1 SAWSS, the steady-state variance of the output z is a random variable which depends on the realization of h(k). The same goes for the average data rate. We make explicit such dependence by writing   and R(hk), and consider the means of these variables (over all realizations of hk) as our performance measure and data rate of interest, respectively. Such notions of performance and rate, represented
 by  and Ra respectively, are formulated as follows:
  
 (10)
  
 where H denotes the support set for possible realizations of the delay h(k). Moreover, R(hk) and   indicate that the average data rate and steady-state variance are functions of delay.
 Generally speaking, we are interested in finding the minimal Ra for which having a bounded
   is feasible. Let Dinf(hr) denote the smallest average steady-state variance of z that can be achieved, when the random delay h(k), with the aforementioned properties, is present in the channel. Hence, Dinf(hr) is obtained by minimizing the average steady-state variance of z over all (possibly nonlinear and time-varying) settings u(k) = Kk(ylk) that render the NCS of Fig. 1 SAWSS. Note that lk is defined as in Remark 1. Under the condition that Assumption III.1 holds,
 the problem of our interest is to find
 	Ra(D) = inf Ra,	(11)
 sza2 =D
 where D ? (Dinf(hr),8), and   represents the average staedy-state variance of the output z over all realizations of the delay. The feasible set of the optimization problem in (11) is
 comprised of all encoder-controller and decoder-controller pairs described by (5)-(9), satisfying
 Assumption III.2 and rendering the NCS of Fig. 1 SAWSS.
  
 Fig. 3: Auxiliary system equivalent to the main NCS in the random delay case
 Remark 4. It is straightforward to see from (5)-(9) that the concatenation of the decodercontroller pair and the channel in the NCS of Fig. 1 is equivalent to a decoder-controller pair with the same mapping and side information that applies a time delay with same properties as characterized in (2), on its received data, and that is followed by a delay-free channel. So the system depicted in Fig. 1 is equivalent to the feedback loop of Fig. 3 in which the encoder and the plant are the same and the inputs have the same properties as in Fig. 1.
 The equivalence pointed out in Remark 4 between systems of Fig. 1 and the NCS of Fig. 3
 will assist us deriving a lower bound on the average data rate Ra in the next Section.
 IV.	LOWER BOUND PROBLEM IN THE PRESENCE OF RANDOM DELAY
 In this section, we establish a lower bound on Ra(D). To do so, we derive inequalities and identities that describe the relationship between the flow of information and system performance in the NCS of Fig. 1. Therefore, we will update fundamental derivations in [2], [12] for the case where the channel delay is randomly distributed. As the first result, we show how the average data rate Ra is bounded from below in the following theorem.
 Theorem 1. Consider the feedback loop depicted in Fig. 1 for which Assumptions III.1 and III.2 hold. Then
 	 	(12)
 where I(.;. | .) represents conditional mutual information. According to [30],  specifies the average directed information rate across the forward channel from y to u in the NCS of Fig. 1 over all possible realizations of the channel delay.
 Proof. It can be implied from [12, Theorem 3.1] that, for each realization h8 of the delays, the
 average data rate (4) in the feedback loop of Fig. 3 is bounded from below as
 	 .	(13)
 Based upon the chain rule of mutual information, the bound in (13) can be restated as
 	 ,	(14)
 where the definition of li is given in Remark 1. From the dynamics of the plant, we can easily conclude that the sequence  is only a function of x(0) and w, once ui-1 is given. Furthermore, it stems from (2)-(9) that upon the knowledge of yli, side informations   and   will be
 the only variables describing u(i), ?i ? N0. Latter observations together with the fact that Assumption III.2 holds for the system of Fig. 1 yield the conclusion that the rightmost term of (14) amounts to zero, ?i ? N0. So we have
 	 	(15)
 for the NCS of Fig. 3. Now by averaging both sides of (15) with respect to the delay realizations, as in (10), and noting that the feedback loop of Fig. 3 is equivalent to the system of Fig. 1, based on Remark 4, our claim follows immediately.  
 The next lemma shows that joint Gaussianity of two signals lowers the directed information rate between them when these are connected through a channel with random delay.
 Lemma 1. Suppose that the NCS of Fig. 1 satisfies Assumption III.1 and Assumption III.2. For this system, if (x(0),w,u,y) represents a jointly second-order set of processes, then the following holds:
 	 ,	(16)
 where yG and uG symbolize the Gaussian counterparts of y and u, respectively, in a way that (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y).
 Proof. According to [12, Lemma 3.1], the directed information rate from sensor output to the
 control input in the auxiliary NCS of Fig. 3 is bounded as follows:
 	I8(y ? u) = I8(yG ? uG),	(17)
 where I8(y ? u) and I8(yG ? uG) are defined as in (13). We conclude based on (2)-(9), the dynamics of the plant and the system of Fig. 3 satisfying Assumption III.2 that  
 ui-1,yli) = 0,?i ? N0. This together with the chain rule of mutual information lead to
 	 .	(18)
 Now the proof is complete by taking average over all possible realizations of the delay from both sides of (18) and considering that based on Remark 4, the system of Fig. 1 is equivalent to the feedback loop in Fig. 3.	 
 If the above Gaussian signals are stationary as well, then the average directed information rate can be stated in terms of the average power spectral density of the involved signals. The next lemma will state such result formally.
 Lemma 2. Suppose that the control input u in the NCS of Fig. 1 is SAWSS for every realization
 of the channel delay. For each realization, assume that there exists a µ > 0 in such a way that
  . Let further consider the sensor output y jointly AWSS with u. Then
 the average directed information rate is equal to an integral term as follows:
 ,
 (19)
 where ? is a Gaussian AWSS process that has independent samples. Such a random process is described as
 	?(k) , u(k) - u˜(k),u˜(k) ,E[u(k) | ylk,uk-1]	(20)
 for each realization of the random delay. Moreover, Su? represents the steady-state power spectral density of u.
 Proof. It can be deduced from [12, Lemma 3.2] that in the NCS of Fig. 3, the following holds
 for the directed information rate :
 	 	(21)
 in which n is a Gaussian AWSS process with independent samples and I8(y ? u) is defined
 as in (13). The noise n is calculated as follows:
 	n(k) , u(k) - uˆ(k),uˆ(k) ,E[u(k) | yk,uk-1].	(22)
 As already mentioned before, based on the plant dynamics, the knowledge of ui-1 will render
   dependent only on x(0) and wi for any i ? N0. Moreover, according to (2)-(9), knowing
 yli, one can determine u(i) by only figuring out ?di and  . Since, based on
 Assumption III.2, (x0,w) and (?d,?e) are independent, the following is yielded:
  	(23) E[u(k) | yk,uk-1] = E[u(k) | ylk,uk-1].
 From (23), it can be concluded that n(k) is actually equal to ?(k) as in (20) for the NCS of Fig. 3. Now, our claim is given by taking average from both sides of upper (23) and noting that based on Remark 4, the systems in Fig. 3 and Fig. 1 are equivalent.  
 We are now ready to present a lower bound on Ra(D). A corollary follows:
 Corollary 1. Suppose that the NCS of Fig. 1 satisfies Assumption III.1. Then Ra(D) is lower bounded as follows:
 	 	(24)
 where ? and u? are defined as in (20) and the infimum is restricted to all mappings staisfying (5)(9) and Assumption III.2, and producing signals y and u with propoerties as stated in Lemma 2.
 Proof. The claim follows immediately from Theorem 1 and Lemma 2.	 
 V.	LOWER BOUND PROBLEM IN THE CASE OF THE CONSTANT DELAY
 In this section, we consider the same NCS as described in Section III but with a channel that imposes a known constant delay, say h steps, on the transmitted data. The corresponding feedback loop is depicted by Fig. 4. The problem we investigate here is a special case of the problem formalized in (11) where the channel delay is constant and therefore, there is only one realization for the channel delay. In this case, we consider the notation Ra(D) = R(D) and Dinf(hr) = Dinf(h). In Appendix B-A, we prove that finding R(D) is feasible if D ?
  
 Fig. 4: Considered NCS in the constant delay case
 (Dinf(h),8). We show that in order to obtain a lower bound on R(D), one can minimize the directed information rate over an auxiliary coding scheme formed of LTI filters and an AWGN channel with feedback and delay. Inequalities and identities related to the delay-free version of this optimization derived in [12] will be extended to the case with a constant channel delay. We start by deriving a lower bound on the average data rate R in the following theorem.
 Theorem 2. Suppose that the feedback system of Fig. 4 satisfies Assumptions III.1 and III.2. Then the average data rate R is lower bounded as follows:
 	 ,	(25)
 where  is the directed information rate across the forward channel from y to u with
 constant delay h (see [30, Definition 1] for the formal definition).
 Proof. Considering that lk = h holds at any k ? N0 for the NCS of Fig. 4, we can conclude the claim immediately from Theorem 1.	 
 The directed information rate in (25) will be reduced if the involved signals are jointly Gaussian. This result is formalized by the following lemma.
 Lemma 3. Suppose that Assumptions III.1 and III.2 hold for the NCS of Fig. 4. Furthermore, consider (x(0),w,u,y) as a jointly second-order set of random processes. Denote the Gaussian counterparts of y and u by yG and uG, respectively, where (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y). Then
  .
 Proof. Recall that lk = h, ?k ? N0, for the considered case with constant channel delay. The claim follows immediately from Lemma 1.	 
 It can be implied from Lemma 3 that by minimizing directed information rate over a scheme that renders y and u jointly Gaussian, one can obtain a lower bound on R(D). Now, we will
 show that the directed information rate can be stated in terms of power spectral densities of the involved processes if such signals meet certain stationarity conditions.
 Lemma 4. Suppose that u is an SAWSS process with   where µ > 0.
 Moreover, assume that u is jointly Gaussian and AWSS with the sensor output y. Then the directed information rate between u and y is expressed as
 	 	(26)
 where ? represents a Gaussian AWSS process with independent samples defined by
 	?(k) , u(k) - u˜(k),u˜(k) ,E[u(k) | yk-h,uk-1].	(27)
 Furthermore, Su? denotes the steady-state power spectral density of u.
 Proof. Immediate from Lemma 2 by noting that lk = h holds for the NCS of Fig. 4 at every time instant k ? N0.	 
 It can be implied from Theorem 2 and Lemma 4 that the rate-performance pair yielded by any coding and control scheme satisfying Assumption III.2 which renders the NCS of Fig. 4 SAWSS is attainable with a lower or equal rate if there exists a scheme that generates (y,u) jointly Gaussian with (x0,w) while rendering the system SAWSS. Due to the Gaussianity of (x0,w) and the fact that the plant is LTI, a jointly Gaussian pair (y,u) can be produced by a coding-control scheme comprised of LTI filters and an AWGN noise source. Such a scheme is depicted in Fig. 5. The NCS of Fig. 5 satisfies all of the assumptions and conditions that hold for the system of Fig. 4. However, the arbitrary mappings are replaced by proper LTI filters B and J in the auxiliary feedback loop of Fig. 5.
  
 Fig. 5: The LTI structure giving the lower bound in the constant delay case
 In addition, for such an NCS, a delayed AWGN channel with noiseless one-sample-delayed feedback serves as communication channel. The coding-control scheme in the NCS of Fig. 5 is
 described via the following dynamics:
 	  ,	(28)
 where ? is a zero-mean white Gaussian noise with variance s?2 and independent of (x0,w), and B = [Br By]. It should be emphasized that Assumption III.1 holds for the initial states x0, the plant G and the disturbance input w in the NCS of Fig. 5. Furthermore, the initial states of the filters B and J, and the delay blocks are deterministic. As the system depicted in Fig. 5 is a special case of the structure of Fig. 4, we use apostrophes for presenting signals in Fig. 5 that have counterparts in the NCS of Fig. 4.
 Theorem 3. If the NCS of Fig. 4 satisfies Assumption III.1 and Assumption III.2 and D ?
 (Dinf(h),8) holds, then
 	 ,	(29)
 where sz20 and Su0 represent the steady-state variance of z0 and the steady-state power spectral density of u0 in Fig. 5, respectively. Moreover, the feasible set for the optimization in (29) is the set comprised of all LTI filters B and the noise ? with  that render the system of Fig. 5
 internally stable and well-posed with J = 1.
 Proof. See Appendix B-B.	 
 Theorem 3 implies that doing the optimization in (29) over the auxiliary LTI system of Fig. 5, with the AWGN channel and delay, will give a lower bound on the minimal data rate required to achieve a certain performance level in the arbitrary (possibly nonlinear and time-varying) structure of Fig. 4. The following results show how the lower bound derived in (29) can be simplified to a bound which is easier to compute.
 Lemma 5. For the NCS of Fig. 5, let describe   by
 	 ,	(30)
 where  is fixed and Sr denotes the steady-state power spectral density of r. Moreover, suppose that the pair (B,J) = (B1,J1) renders the feedback loop of Fig. 5 internally stable and well-posed. Then for any ? > 0, there exists another pair with a biproper filter, say J2, and a proper one, say B2, that renders the system of Fig. 5 internally stable and well-posed, and preserves the steady-state power spectral density of z0 in a way that the following holds:
 	 	(31)
 Proof. See Appendix B-C.	 
 Intuitively speaking, the results of Theorem 3 and Lemma 5 imply that R(D) can be bounded from below by a logarithmic term as in (31) which is a function of channel SNR in the NCS
 of Fig. 5. Such an intuition will assist us with deriving a lower bound which is computationally appealing in the following corollary.
 Corollary 2. Take the feedback loop of Fig. 4 into account as an NCS that satisfies Assump-
 tions III.1 and III.2. Then for every D ? (Dinf(h),8), the following holds:
 	 ,	(32)
 in which st2 and sz20 symbolize the steady-state variances of t and z0 in the auxiliary system of Fig. 5, respectively. For the optimization problem in (32), a candidate solution is an LTI filter
 pair (B,J) together with noise variance   that cause the system in Fig. 5 to become
 internally stable and well-posed.
 Proof. See Appendix B-D.	 
 VI.	UPPER BOUND PROBLEM IN THE PRESENCE OF CONSTANT DELAY
 In this section, we show that for any D ? (Dinf(h),8), one can always find a scheme that guarantees attaining   with an average data rate which has a distance of about 1.254 bits per sample from the theoretical lower bound. For such a scheme, we propose a design approach which utilizes the filters that together with an AWGN with feedback and delay, render the directed information rate over the channel equal to the lower bound on R(D).
 Definition 2. We call a coding scheme with input-output relationship as in (5)-(9) in the constant channel delay case linear if and only if its dynamics can be restated as follows:
 	  ,	(33)
 where B = [Br By] and J are proper LTI filters with deterministic initial condition. Moreover, ? represents a zero-mean i.i.d random sequence independent of (x0,w). The initial state of the one-step-delay feedback channel is assumed to be deterministic.
 The realization of linear source coding schemes can be carried out by using entropy-coded dithered quantizers (ECDQs) together with LTI filters. First, implementing an ECDQ causes the
 following relationship between (uq,yq) in (2) and (6), and (r,t) in (33):
 yE(k) = Fq(t(k) + d(k))
 yq(k) = Ok(yE(k),d(k))
 (34)
 uD(k) = Ok--1h(uq(k),d(k - h)) rh(k) = uD(k) - d(k - h),
 in which by Fq, we denote a uniform quantizer with resolution ? ? R+, Fq : R ? {i?;i ? Z}. Additionally, d(k) represents a dither signal whose access are provided to both encoder and decoder. The mapping Ok and its complementary Ok-1 formalize entropy coding for the lossless parts at the encoder and decoder, respectively. The following lemma presents an interesting property of ECDQs when being set up in an LTI feedback loop.
 Lemma 6. Consider the feedback loop depicted in Fig. 6 and suppose that the plant G˜ is described by a proper real rational transfer-function matrix in which the transfer function from rh to t is scalar and strictly proper. For such a system, assume that the input-output relationship
  
 Fig. 6: ECDQ setup in the feedback path
 of the ECDQ in the feedback path is given by (34) with finite and positive quantization step size ?. Moreover, take the disturbance w˜ into account as a white noise process jointly second-order with x˜0, the initial state of G˜. Then if the dither d is an i.i.d process with a uniform distribution over (-?/2,?/2) and independent of (w,˜ x˜0), the error r-t is i.i.d, uniformly distributed over (-?/2,?/2) and independent of (w,˜ x˜0).
 Proof. See Appendix B-E.	 
 It can be implied from above that combining the LTI filters in (33) with the ECDQ in (34) in a setting as depicted in Fig. 7 will lead to a linear coding scheme for the NCS of Fig. 4 as
 long as d(k) meets the same criteria as for the dither in Lemma 6. If so, the obtained coding
  
 Fig. 7: The proposed ECDQ-based linear coding scheme
 scheme is called a linear ECDQ-based coding scheme. If such a scheme is implemented on the feedback path of the main NCS of Fig. 4, the average data rate is bounded from above by a certain value which is shown in the following lemma.
 Lemma 7. Suppose that Assumption III.1 holds for the NCS of Fig. 4. Then the existence of an ECDQ-based linear source-coding scheme rendering the NCS of Fig. 4 SAWSS is certified in such a way that the average data rate satisfies
 	 .	(35)
 In (35), the variance of the quantization error (noise) of the ECDQ-based linear source-coding scheme is set as  . Moreover, st2 represents the steady-state variance of the signal t
 in (33).
 Proof. See Appendix B-F.	 
 Now, through the following theorem, we use the result of Lemma 7 to show that utilizing ECDQ-based linear coding schemes can lead to an upper bound on the desired minimal average data rate R(D).
 Theorem 4. Let Assumption III.1 hold for the closed-loop system of Fig. 4. Then for each D ? (Dinf(h),8), one can always find an ECDQ-based linear source-coding scheme satisfying Assumption III.2 and rendering the feedback loop of Fig. 4 SAWSS in such a way that sz2 = D is resulted and the average data rate is bounded as
 	 ,	(36)
 where the definition of ?0(D) is given in (32).
 Proof. See Appendix B-G.	 
 In the following remark, we state how the upper bound derived in Theorem 4 can be considered as an upper bound on Ra(D) in the case of random channel delay.
 Remark 5. The upper bound in (36) will be an upper bound on Ra(D) in the random channel delay case if coding and control schemes are linear ECDQ-based schemes designed as in the proof of Theorem 4 for the delay hmax where the decoder-controllers have buffers installed at
  
 Fig. 8: Three possible locations for the delay component in the case with constant channel delay
 their inputs sending only yq(k - hmax) for prcessing at each time instant k ? N0. Clearly, this is due to the fact that at every time step k ? N0, yq(k - hmax) is available at the decoder. Such an upper bound does not seem to be tight since imposing a delay of hmax steps on transmitted data is actually a worst-case scenario.
 The bounds derived in this section and the previous section limit the desired average data rate R(D) in the NCS of Fig. 4. In this system, the constant delay is induced by the digital
 communication channel between the encoder-controller and the decoder-controller. One concern is the effect of delay location on the derived bounds. The following lemma takes a step in addressing this issue by showing how the system signals change when the time delay block is moved to a different location in the feedback loop of Fig. 4.
 Lemma 8. Consider the NCS of Fig. 4 and two other systems each of which yielded by moving the delay component in the NCS of Fig. 4 to either the measurement path (between the sensor and the encoder-controller) or the actuation path (between the decoder-controller and the plant). Fig. 8 depicts the locations where the time delay occurs in these cases. Then systems are not necessarily equivalent across the cases if the only difference between them is the delay location. However, the equivalence can be assured by allowing the side information to change across the
 cases.
 Proof. See Appendix B-H.	 
 VII.	NUMERICAL SIMULATION
 Take the following transfer function into account as the model describing the generalized plant G in the NCS of Fig. 4:
 	 	(37)
 Let us set the disturbance signal w and initial states x0 in such a way that Assumption III.1 is satisfied. We calculated lower and upper bounds on R(D) as derived in (32) and (36). For computing these bounds, we made use of the equivalence between the NCSs of Fig. 5 and Fig. 12, shown in the proof of Lemma 5, in that we adopted the method in [12] which solves SNR-performance optimization problems similar to the one defining ?0(D) for such systems as the NCS of Fig. 12. The bounds are computed for three different values of channel delay, h = {0,1,2}, with respect to D varying over a range from Dinf(h) to 50 for each h. Moreover, we designed actual linear ECDQ-based coding schemes, and for each selected D in the latter interval, we simulated the NCS of Fig. 4. To do so, we utilized the filters giving the lower bound on R(D) according to the procedure suggested in [12, Theorem 5.1]. The results are
 demonstrated in Fig. 9. In this figure, the curves referred to as LB and UB present the lower
  
 Fig. 9: Bounds on R(D) in (32) and actual data rates and entropies for different values of time delay h
 and upper bounds on R(D), respectively. We can compare Dinf(h) among cases with different values of channel delay as well. As shown, greater Dinf(h) is associated with larger channel delay, as expected according to [31]. Evaluating how the bounds change in response to changes in the delay is one of the main purposes of this simulation study. We can observe from the bounds plotted in Fig. 9 that when D is fixed, increasing the delay will enlarge the bounds on R(D). In other words, the greater the delay is, the higher average data rate is to be used in order to achieve a fixed quadratic performance level. Moreover, Fig. 9 shows that the lower (upper) bound curves converge to the minimum data rate required for mean square stability as D grows larger. From [32], we know that the minimal data rate guaranteeing stabilizability of the NCS of Fig. 5 is only a function of unstable poles of the plant G. On the other hand, we use the equivalent system of Fig. 12 for the purpose of calculating bounds. So the observation with convergence of bounds to the minimal data rate needed for stability comes from the fact that incorporating time delay into the model of the plant Ga will not affect its unstable poles.
 Simulation results are illustrated in Fig. 9 as well. The curves referred to as OR and OE present the average data rates and entropies achieved by using actual linear coding schemes. Furthermore, 106-sample-long realizations have been considered for the dither. The coding task
 in all utilized schemes is done by memory-less Huffman coders which do not take the past information of the dither into account as prior knowledge for coding. In addition to the average data rate, the entropy of the output of the quantizer has been estimated for the aforementioned setup. The gap of around 0.4 bits per sample between the measured entropy and the lower bound indicates that for each h ? {0,1,2}, 0.4 bits per sample of the gap between the actual rates and lower bound is caused by replacing the AWGN with uniform dither and the remainder 0.25 bits per sample corresponds to sample-by-sample coding. It can be observed that the actual rates and entropies have the same properties as the properties of bounds mentioned in the previous paragraph. The most prominent property is related to the behaviour of the achieved rates and entropies as a function of channel delay, i.e., for a system with greater time delay in the channel,
 higher rates are required to guarantee quadratic performance requirements.
 VIII.	CONCLUSIONS
 In this paper, the trade-off between average data rate and performance in networked control systems has been studied. Two setups have been investigated, each of which incorporates an LTI plant with Gaussian disturbance and initial states, and scalar control input and sensor output. Moreover, both of them have causal, but otherwise arbitrary, mappings on their feedback paths which are responsible for coding and control. The only difference between the two considered systems is the model of the channel that carries out data transmission between the encodercontroller and the decoder-controller. In one case, the digital communication channel is SIMO and information to be exchanged are exposed to random delay. In the other system, the channel is error-free as well but it is SISO and imposes constant delay on transmitted data. For the case with random channel delay, we considered notions for rate and performance which show the average behaviour of the system over all realizations of the delay. We have shown that for such a setup, data rate is lower bounded by average directed information rate from the sensor output to control input, and if y and u are jointly Gaussian, the average directed information rate would be lowest. Moreover, we have shown that when y and u satisfy certain stationarity assumptions, the average directed information rate between them is a function of the average power spectral densities of these signals over all realizations of the channel delay. We have shown that infimum value of this function over all arbitrary coders and controllers that cause system signals have those Gaussianity and staionarity properties lower bounds the infimum average data rate required to attain a prescribed quadratic performance.
 For the constant delay case, which is a special case of the system with random channel delay, we approximated (by deriving bounds) the minimal average data rate that certifies attaining a certain performance level. Employing the fundamental information inequalities and identities derived for the random delay case, we showed that this desired minimal average data rate is bounded from below when coder-controllers and the channel behave as a concatenation of proper LTI filters and an AWGN channel with feedback and delay. Then we showed that by approximating such schemes with simply implementable linear ECDQ-based coding schemes, one can achieve any (legitimate) performance level by actual rates which are at most 1.254 bits per sample higher than the lower bound. The results illustrated through the simulation show that bounds and empirical rates are increasing functions of channel delay for a fixed performance level. It means larger delay in the channel necessitates higher minimal average data rate that is needed for achieving a certain level of quadratic performance.
 Future research will concern with finding closed-form solution for the lower and upper bound problems in the case of random channel delay, finding analytic expression for the desired infimum data rate, deriving lower and upper bounds with shorter gap between them, plants with model
 uncertainties and vector quantization.
 APPENDIX A
 INVERTIBILITY OF THE DECODER
 Lemma 9. Consider a coding scheme described through (5)-(9) that has a non-invertible decoder,
 and let R?(k) be defined as . For such scheme, assume that u(k) = u0(k) and R?f(k) = R?f0(k), ?k ? N0, where  . Then there exists another
 coding scheme constructing the control input u(k) = u0(k) with an invertible decoder in such a way that R?f(k) = R?f0(k), ?k ? N0.
 Proof. Suppose that mappings in (7)-(9) represent a non-invertible decoder at time k in a way that upon knowledge of and Si, perfect reconstruction of   from ui has been possible for all i =
 k - 1. Then there exist uD1,uD2 ? As such that .
 Let S1 and S2 be associated with uD1 and uD2 respectively. Two possible cases can occur. In the first case, S1 and S2 are unequal, i.e., S1 6= S2. Since S is known at the decoder at each time step, this case does not contradict the invertibility. That is due to the fact that the knowledge of S would determine whether u(k) is caused by uD1 or uD2. However, the situation is not the same in the case where S1 = S2. Since both uD1 and uD2 are vector-valued variables, uD1 =6 uD2 means that at least one entry of uD1 is not equal to the entry with the same dimension in uD2. The corresponding elements of uD1 and uD2 that are not equal to each other are denoted by pairs (uD1j,uD2j), 1 = j = m, 1 = m = hmax + 1. Since S1 = S2, for each j, both uD1j and uD2j have been exposed to the same delay, say hj, 0 = hj = hmax. So if we denote the output of the lossy encoder that corresponds to uDnj by yEnj, then uDnj(k) = yEnj(k - hj). It should be noted that n is a positive integer which is at most equal to the size of the set As. Let pnj
 represent the conditional probability of having yEnj at the encoder given   at
 time k -hj. The encoder-decoder set (E¯,D¯) can be defined with exactly the same properties as (E,D) but different from it in the sense that E¯ outputs only yE1j at time k -hj with probability p1j + p2j. This means having only uD1j at time k as decoder input instead of receiving either uD1j or uD2j. Let us define tj , k - hj;k = hj. Then
 R?(tj) |(E,D) = R?0(tj)
 (=aa) - X pnj lnpnj - p1j lnp1j - p2j lnp2j
 n/?{1,2}
 (38) (= -ab)	X pnj lnpnj - (p1j + p2j)ln(p1j + p2j)
 n/?{1,2} (=ac) R?(tj) |(E¯,D¯)
 in which (aa) results from the definition of entropy and R?(k), (ab) can be concluded based on the fact that the function -ln(pnj) is monotonically decreasing, and (ac) follows from the definition of R?(k) for the scheme (E¯,D¯). So R?(tj) |(E¯,D¯) = R?(tj) |(E,D) for tj = 0, and consequently R?f(k) = R?f0(k).
 The above procedure can be iterated for evey pair with the same characteristics as (uD1,uD2) to make sure that there are no two inputs of the reproduction decoder mapped into one identical u(k) at time instant k. Such iteration will then yield an invertible decoder. In other words,
 when the pair (E¯,D¯) is used, knowing   is equivalent to knowing   with
 u(i) = u0(i) and R?f(i) = R?f0(i), ?i = k. Our main claim now follows by repeating the above
 for every k = 0.	 
 APPENDIX B
 PROOFS
 A. Feasibility proof for  and ?0(D)
 Suppose that in the standard architecture depicted in Fig. 10, G, x0 and w satisfy Assumption III.1 and K follows u(k) = Kk(yk-h). Considering the Gaussianity of x0 and w and the
 fact that G is LTI, we can imply from some results in [33] that:
 	 ,	(39)
 in which   denotes the steady-state variance of output z and ? is the set of all proper LTI filters which render the system of Fig. 10 internally stable and well-posed. The assumptions considered for G guarantee that finding Dinf(h) is feasible. Since Dinf(h) can be obtained, for every ? ? (0,D - Dinf(h)), there exists K1 ? ? which gives  for the system of Fig. 10. Applying K1 to this system results in a stable setting which is a special
  
 Fig. 10: Standard feedback loop over which Dinf(h) is defined
 case of the NCS depicted in Fig. 5 with J = 1 and r = t = K1y0 where the steady-state variance of , is finite. Therefore, since K1 ? ?, it can bring internal stability and well-posedness to the feedback loop of Fig. 5 in the presence of any additive noise ? with steady-sate
 variance . So  and  can be concluded, when taking ?
 into account as an AWGN with finite variance s?2 for the system of Fig. 5. It should be noted that
 ?t,?z = 0 depend only on K1. Now by choosing ? = (D - Dinf(h))/3 and the variance  
 (D - Dinf(h))/(3?z) for the AWGN, there exists K1 ? ? rendering the NCS of Fig. 5 internally
 stable and well-posed in a way that . Then
 the following can be obtained for the structure of Fig. 5:
 	 .	(40)
 So considering Jensen’s inequality and concavity of logarithm, we can deduce that the problem of finding  in (29) is feasible for every D > Dinf(h). The feasibility of the problem of finding ?0(D) in (32) is inferred immediately from (40) for any D > Dinf(h).
 B. Proof of Theorem 3
 Due to the validity of D > Dinf(h), one can always find at least one coding-control pair, say Eˆ and Dˆ, that while satisfying Assumption III.2, renders the NCS of Fig. 4 SAWSS in such a
 way that and
 	 	(41)
 In (41), processes zˆ, yˆ and uˆ are the counterparts of z, y and u in Fig. 4, respectively. Moreover,
 the inequalities and identities in (41) stem from Theorem 2 if conditions in Lemma 3 and Lemma 4 are satisfied. Therefore, (yˆG,uˆG) are jointly Gaussian counterparts of (y,ˆ uˆ) as in Lemma 3 and Su? represents the steady-state power spectral density of uˆG as in Lemma 4. The
 pair (yˆG,uˆG) with conditions stated in Lemma 3 can be generated by a scheme which certifies
   and is comprised of linear filters with a unit-gain noisy channel and delay
 h as follows:
 	 ,	(42)
 in which ?ˆG(k) denotes a Gaussian noise with zero mean and independent of  . Since Lk is a linear and causal mapping, we can redescribe  as
 	 .	(43)
 It follows from causality in (43) that ?k ? N, Bk and Gk are lower triangular matrices with Bk-1 and Gk-1 on the top left corners. This together with the fact that (yˆG,uˆG) are jointly SAWSS allow us to conclude that based on transitivity of asymptotic equivalence for products and sum of the matrices in [34], the sequences {Qk} and {Pk} are asymptotically equivalent to sequences of lower triangular Toeplitz matrices. Furthermore, using Lk as in (42) will bring internal stability and well-posed-ness to the corresponding NCS. Now let us set J = 1 and B as a concatenation of linear filters with the same behaviour as steady-state behaviour of Lk in (42) for the auxiliary system of Fig. 5. Moreover, suppose that ? has a variance equal to s?2ˆG. So based on the asymptotic equivalence between the matrix representations of L and {Lk}, choosing J, B and ? as above will render the system of Fig. 5 well-posed and internally stable. More specifically, the latter set of filters and the noise will give WSS processes to which uˆG and zˆG converge. Therefore, for the control input u0 and error signal z0 in the feedback loop of Fig. 5, Su0 = Su? and   hold. Then based on Lemma 4, the directed information rate in the NCS of Fig. 5 can be expressed as
 	 	(44)
 First, we can deduce that any pair (E,ˆ Dˆ) with properties stated above has a counterpart comprised of LTI filter B, J = 1 and the white Gaussian noise ? in architecture of Fig. 5 in such a way that I8(y0 ? u0) = I8(yˆ ? uˆ) and . Secondly, the main problem is finding the
  
 Fig. 11: The LTI system whose internal stability guarantees the internal stability of the auxiliary system in Fig. 5
 infimum of R over all mappings (E,ˆ Dˆ). With all of this in mind, it can be implied from (44) and (41) that the lower bound for R(D) would be equal to the rightmost term of (29) which
 completes the proof.
 C. Proof of Lemma 5
 The necessary and sufficient condition for the feedback loop of Fig. 5 to be internally stable and well-posed is that every entry of the transfer function matrix from input [?,w,?1,?2]T to outputs [z0,y0,r,u0]T in the system of Fig. 11 belongs to RH8 [35]. Such a transfer function matrix, which we denote by T, is described as follows:
 	
 	?G12Jz-hM	G11 + G12Jz-hByMG21	G12z-h(1 - Brz-1)M	G12Jz-hByM?
 	
 	??G22Jz-hM	G21(1 - Brz-1)M	G22z-h(1 - Brz-1)M	G22Jz-hByM??
 T = ?	?, (45)
 	?	M	G21ByM	G22z-hByM	ByM	??
 ?
 	?	?
 	JM	G21JByM	(1 - Brz-1)M	JByM
 where
 	M , (1 - Brz-1 - G22Jz-hBy)-1.	(46)
 Now, let us shift the delay block in the system of Fig. 5 to the plant model in a way that for
 the newly obtained system, the plant is described by
 	  .	(47)
 Such an auxiliary NCS is depicted by Fig. 12. Except for the plant model (47), everything
  
 Fig. 12: The equivalent system with the same ?0(D) as the NCS of Fig. 5
  
 Fig. 13: The auxiliary feedback loop characterizing the internal stability of the NCS of Fig. 12
 in the feedback loop of Fig. 12 is assumed to be the same as in the system of Fig. 5. The
 internal stability and well-posed-ness of the feedback loop of Fig. 12 is guaranteed if and only
 if every entry of the transfer-function matrix, say Ta, from   in
 Fig. 13 belongs to RH8. It is straightforward to see that Ta = T. So an equivalence holds between internal stability and well-posed-ness of the system of Fig. 12 and the NCS of Fig. 5. In other words, every triplet   rendering the feedback loop of Fig. 12 internally stable and well-posed, will bring internal stability and well-posed-ness to the NCS of Fig. 5 as well. One other implication of Ta = T is that using an stabilizing   commonly for NCSs of Fig. 5 and Fig. 12 will lead to an identical  . This is due to the properties of
 LTI systems exposed to Gaussian and stationary inputs. Furthermore, those properties lead to deriving the following H2-norm expressions for SNR and variance of the output z0 in the NCS of Fig. 5:
 (48)
 ,
 in which N , JByz-h(1 - Brz-1)-1. Likewise, the SNR and variance of the output z in the NCS of Fig. 12 is formalized in terms of H2-norms as follows:
 (49)
 ,
 where Ma = M and Na , JBy(1 - Brz-1)-1. It follows from (48) and (49) that  
   and . Therefore, upon using the same stabilizing triplet (B,J,s?2), the channel SNR and the variance of the output characterizing performance will be the same for the NCSs of Fig. 5 and Fig. 12.
 According to [12, Lemma 4.1], for any pair (B,J) = (B1,J1) that renders the feedback loop of Fig. 12 internally stable and well-posed, there exists another pair with the same properties as for (B2,J2) in this lemma. Then our claims follow immediately from the above equivalences between the NCS of Fig. 12 and the NCS of Fig. 5.
 D. Proof of Corollary 2
 The feasibility of obtaining  , caused by D belonging to (Dinf(h),8), certifies the existence of a triplet, say  , that leads to for the system of Fig. 5. In the latter triplet, B? is assumed to be a proper LTI filter and  . This together with the definition
 of  and  in (29) and (30), respectively, yields the following:
 	 .	(50)
 Moreover, the triplet   with aforementioned properties meets the conditions in Lemma 5. Therefore, another triplet, say  , exists in such a way that implementing it brings
 internal stability and well-posed-ness, keeps  intact, and yields
 	 	(51)
 for the LTI feedback loop of Fig. 5. Note that J˜? is a biproper filter while B˜? only needs to be proper. Now the fact that (51) holds for any ?,? > 0, the definition of ?0(D) in (32), and the claim of Theorem 3 complete the proof.
 E.	Proof of Lemma 6
 Let G˜h denote the transfer-function matrix from [w˜ r]T to [z˜ t]T in Fig. 6. Since rh is related to r by rh = rz-h, we can conclude that G˜h meets the conditions of being proper and real rational, and containing a strictly proper SISO open-loop transfer function from r to t. Now having the schemes described via (33) and (34) in mind, we can deduce our claim immediately from [12, Lemma 5.1].
 F.	Proof of Lemma 7
 Let us assume that a linear source coding scheme is implemented in the feedback path of the main system in Fig. 4. Due to the feasibility of finding ?0(D), which necessitates satisfaction of Assumption III.1, we can conclude the existence of proper LTI filters B and J that together with an AWGN, say ?, render the NCS of Fig. 4 SAWSS. It stems from some properties of internal
 stability that the system will still be stable if one keeps the latter filters B and J and only sets ? as ? = 0. This signifies that in the case of unity feedback (t = r), internal stability and wellposed-ness are guaranteed for the open-loop system between r and t. We come immediately to the conclusion that (35) holds based on [10, Corollary 5.3] and statistical characteristics of the dither mentioned in Lemma 6.
 G.	Proof of Theorem 4
 Considering the feasibility of finding ?0(D), results of Lemma 5, lemma 6, and Lemma 7, and invertibility of the decoder, we conclude the claim by following the same steps as in [12,
 Theorem 5.1].
 H.	Proof of Lemma 8
 One of the common feedback loop components across the considered cases in Fig. 8 is the
 LTI plant G which is described by state-space difference equations as follows:
 ?
 ?????x(k + 1) = Ax(k) + B1w(k) + B2u(k)
 	G :	z(k) = C1x(k) + D11w(k) + D12u(k)	(52)
 ?????y(k) = C2x(k) + D21w(k),
 where x ? Rnx represents plant states and u, w, y and z are inputs and outputs defined as in
 (1). Moreover, A, B1, B2, C1, C2, D11, D12, and D21 are time-invariant matrices of appropriate dimensions. According to the recursion in (52), the states and outputs of the plant at each time instant i ? N0 can be expressed in terms of initial conditions, disturbance and control inputs as follows:
 ?
 ?????x(i) = Aix(0) + B1(i)wi-1 + B2(i)ui-1
 	z(i) = C1Aix(0) + D11(i)wi + D12(i)ui	(53)
 ?????y(i) = C2Aix(0) + D21(i)wi + D22(i)ui-1,
 where the involved matrices are defined as
 B1(i) = [Ai-1B1 Ai-2B1 ...B1]
 B2(i) = [Ai-1B2 Ai-2B2 ...B2]
 D11(i) = [C1Ai-1B1 C1Ai-2B1 ...C1B1 D11]
 D12(i) = [C1Ai-1B2 C1Ai-2B2 ...C1B2 D12]
 D21(i) = [C2Ai-1B1 C2Ai-2B1 ...C2B1 D21]	(54)
 D22(i) = [C2Ai-1B2 C2Ai-2B2 ...C2B2].
 For the case where the time delay is imposed by the error-free digital channel between the encoder-controller and the decoder-controller, the relationship between the control input and the sensor output is characterized based on (5)-(9). The dynamics described by (5)-(9) can be
 summarizd in the constant channel delay case as follows:
 yq(k) = Ek(yk,?ek)
 	uq(k) = yq(k - h)	(55)
 u(k) = Dk(ukq,?dk),
 where Ek and Dk represent causal, but otherwise arbitrary, mappings at each k ? N0. It follows
 from (55) that uk can be stated as an arbitrary function, say Nk, of  , i.e., uk =
  . Then from (53) and by induction, we can conclude that at each time instant
 k ? N0, x(k) is a function of   is a function of  , and y(k) is a function of  .
 In the second case, it is the link between the decoder-controller and the plant that induces the
 time delay. For such a setting, Ek, Dk, ?e(k) and ?d(k) yield a scheme with following dynamics:
 yq(k) = Ek(yk,?ek)
 	uq(k) = yq(k)	(56)
 u(k) = Dk-h(ukq-h,?dk-h).
 It follows from (56) that in this case, uk can be expressed as  , ?k ? N0, where Mk is an arbitrary mapping which is specified by   and  .
 Substituting such an expression into (53) an by induction, we can rederive x(k), z(k), and y(k) as
 functions of  , and  ,
 respectively.
 As the third case, we focus on a structure in which the delay is introduced by the path between the sensor and the encoder-controller. In this situation, the coding scheme is described by causal
 mappings Ek and Dk, and side informations ?e(k) and ?d(k), as follows:
 yq(k) = Ek(yk-h,?ek)
 	uq(k) = yq(k)	(57)
 u(k) = Dk(ukq,?dk).
 Taking the same steps as for the previous cases, we derive ,
 where Sk is a causal mapping and a function of   and  . Then considering (53)
 and based on induction, we come to the conclusion that for the closed-loop system considered in
 this case, x(k) is a function of   is a function of  ,
 and y(k) is a function of   for all k ? N0.
 According to the above observations, comparing system states x, sensor output y, and the output z at each time instant indicates that such signals are not necessarily equal across the three cases studied above if the systems share the design (mappings for coding and control and side information) and have the same initial conditions and exogenous inputs. So values of each signal change by relocating the delay component in the NCS of Fig. 4. However, it is straightforward to see from the structure of the variables describing processes x, z, and y that the equivalence over cases can be obtained under the condition that everything is the same across
 the cases except for side information which can be considered as decision variable.
 REFERENCES
 [1]	M. Barforooshan, J. Østergaard, and M. S. Derpich, “Interplay between transmission delay, average data rate, and performance in output feedback control over digital communication channels,” in American Control Conference (ACC), May 2017, pp. 1691–1696.
 [2]	M. Barforooshan, J. Østergaard, and P. A. Stavrou, “Achievable performance of zero-delay variable-rate coding in rateconstrained networked control systems with channel delay,” in IEEE 56th Annual Conference on Decision and Control (CDC), Dec. 2017, pp. 5991–5996.
 [3]	X.-M. Zhang, Q.-L. Han, and X. Yu, “Survey on recent advances in networked control systems,” IEEE Transactions on
 Industrial Informatics, vol. 12, no. 5, pp. 1740–1752, 2016.
 [4]	G. N. Nair, F. Fagnani, S. Zampieri, and R. J. Evans, “Feedback control under data rate constraints: An overview,”
 Proceedings of the IEEE, vol. 95, no. 1, pp. 108–137, 2007.
 [5]	L. Zhang, H. Gao, and O. Kaynak, “Network-induced constraints in networked control systems—a survey,” IEEE
 Transactions on Industrial Informatics, vol. 9, no. 1, pp. 403–416, Feb 2013.
 [6]	J. Baillieul and P. J. Antsaklis, “Control and communication challenges in networked real-time systems,” Proceedings of
 the IEEE, vol. 95, no. 1, pp. 9–28, 2007.
 [7]	A. S. Matveev and A. V. Savkin, Estimation and control over communication networks.	Springer Science & Business
 Media, 2009.
 [8]	N. C. Martins and M. A. Dahleh, “Feedback control in the presence of noisy channels: Bode-like fundamental limitations
 of performance,” IEEE Transactions on Automatic Control, vol. 53, no. 7, pp. 1604–1615, 2008.
 [9]	N. C. Martins, M. A. Dahleh, and J. C. Doyle, “Fundamental limitations of disturbance attenuation in the presence of side
 information,” IEEE Transactions on automatic control, vol. 52, no. 1, pp. 56–66, 2007.
 [10]	E. I. Silva, M. S. Derpich, and J. Østergaard, “A framework for control system design subject to average data-rate
 constraints,” IEEE Transactions on Automatic Control, vol. 56, no. 8, pp. 1886–1899, 2011.
 [11]	——, “An achievable data-rate region subject to a stationary performance constraint for LTI plants,” IEEE Transactions
 on Automatic Control, vol. 56, no. 8, pp. 1968–1973, 2011.
 [12]	E. I. Silva, M. S. Derpich, J. Østergaard, and M. A. Encina, “A characterization of the minimal average data rate that guarantees a given closed-loop performance level,” IEEE Transactions on Automatic Control, vol. 61, no. 8, pp. 2171–2186, 2016.
 [13]	T. Tanaka, P. M. Esfahani, and S. K. Mitter, “LQG control with minimum directed information: Semidefinite programming approach,” IEEE Transactions on Automatic Control, vol. 63, no. 1, pp. 37–52, 2018.
 [14]	P. A. Stavrou, J. Østergaard, and C. D. Charalambous, “Zero-delay rate distortion via filtering for vector-valued Gaussian
 sources,” IEEE Journal of Selected Topics in Signal Processing (to appear), 2018.
 [15]	V. Kostina and B. Hassibi, “Rate-cost tradeoffs in control,” 2016. [Online]. Available: http://arxiv.org/abs/1612.02126v2
 [16]	Z. Du, D. Yue, and S. Hu, “H-infinity stabilization for singular networked cascade control systems with state delay and
 disturbance,” IEEE Transactions on Industrial Informatics, vol. 10, no. 2, pp. 882–894, 2014.
 [17]	M. A. Khanesar, O. Kaynak, S. Yin, and H. Gao, “Adaptive indirect fuzzy sliding mode controller for networked control systems subject to time-varying network-induced time delay,” IEEE Transactions on Fuzzy Systems, vol. 23, no. 1, pp. 205–214, 2015.
 [18]	R. Lu, H. Cheng, and J. Bai, “Fuzzy-model-based quantized guaranteed cost control of nonlinear networked systems,”
 IEEE Transactions on Fuzzy Systems, vol. 23, no. 3, pp. 567–575, 2015.
 [19]	J. Xiong, J. Lam, Z. Shu, and X. Mao, “Stability analysis of continuous-time switched systems with a random switching
 signal,” IEEE Transactions on Automatic Control, vol. 59, no. 1, pp. 180–186, 2014.
 [20]	L. Qiu, Y. Shi, F. Yao, G. Xu, and B. Xu, “Network-based robust H2/H8 control for linear systems with two-channel
 random packet dropouts and time delays,” IEEE transactions on cybernetics, vol. 45, no. 8, pp. 1450–1462, 2015.
 [21]	Z.-H. Pang, G.-P. Liu, D. Zhou, and M. Chen, “Output tracking control for networked systems: A model-based prediction
 approach,” IEEE Transactions on Industrial Electronics, vol. 61, no. 9, pp. 4867–4877, 2014.
 [22]	H. Li and Y. Shi, “Network-based predictive control for constrained nonlinear systems with two-channel packet dropouts,”
 IEEE Transactions on Industrial Electronics, vol. 61, no. 3, pp. 1574–1582, 2014.
 [23]	W. Yao, L. Jiang, J. Wen, Q. Wu, and S. Cheng, “Wide-area damping controller for power system interarea oscillations: A networked predictive control approach,” IEEE Transactions on Control Systems Technology, vol. 23, no. 1, pp. 27–36, 2015.
 [24]	Y. Nakahira, “LQ vs. `8 in controller design for systems with delay and quantization,” in IEEE 55th Conference on
 Decision and Control (CDC), Dec. 2016, pp. 2382–2389.
 [25]	Q.-L. Han, Y. Liu, and F. Yang, “Optimal communication network-based H8 quantized control with packet dropouts for a class of discrete-time neural networks with distributed time delay,” IEEE transactions on neural networks and learning
 systems, vol. 27, no. 2, pp. 426–434, 2016.
 [26]	K. Liu, E. Fridman, K. H. Johansson, and Y. Xia, “Quantized control under round-robin communication protocol,” IEEE
 Transactions on Industrial Electronics, vol. 63, no. 7, pp. 4461–4471, 2016.
 [27]	W. M. H. Heemels, A. R. Teel, N. Van de Wouw, and D. Nesic, “Networked control systems with communication constraints: Tradeoffs between transmission intervals, delays and performance,” IEEE Transactions on Automatic control, vol. 55, no. 8,
 pp. 1781–1796, 2010.
 [28]	J. Zhang and C.-C. Wang, “On the rate-cost of Gaussian linear control systems with random communication delays,” in
 2018 IEEE International Symposium on Information Theory, June 2018, pp. 2441–2445.
 [29]	T. M. Cover and J. A. Thomas, Elements of information theory.	John Wiley & Sons, 2012.
 [30]	M. S. Derpich, E. I. Silva, and J. Østergaard, “Fundamental inequalities and identities involving mutual and directed
 informations in closed-loop systems,” 2013. [Online]. Available: http://arxiv.org/abs/1301.6427
 [31]	K. Hashikura, “H2/H8 controller design for input-delay and preview systems based on state decomposition approach,”
 Ph.D. dissertation, Department of Human Mechatronics Systems, Tokyo Metropolitan University, Japan, 2014.
 [32]	E. Johannesson, “Control and communication with signal-to-noise ratio constraints,” Ph.D. dissertation, Department of
 Automatic Control, Lund University, Sweden, 2011.
 [33]	K. J. Astr°	om,¨	Introduction to stochastic control theory.	Courier Corporation, 2012.
 [34]	R. M. Gray, “Toeplitz and circulant matrices: A review,” Foundations and Trends in Communications and Information
 Theory, vol. 2, no. 3, pp. 155–239, 2006.
 [35]	B. A. Francis, A course in H8 control theory.	Berlin; New York: Springer-Verlag, 1987.
 ﻿ This paper novel on the characterization of the the causal information function for arbitrarily distributed one sided stationary th order source . It is first shown that on the of the to the causal stated for two sided stationary do not apply to the commonly used family of asymptotic average single letter distortion criteria . Moreover , we show that , in general , a reconstruction sequence cannot be both jointly stationary with a one sided stationary source sequence and causally related to it . This that , in general , the causal for one sided stationary cannot be by a stationary distribution . However , we prove that for an arbitrarily distributed one sided stationary source and a large class of distortion criteria , the search for can be restricted to which yield the output sequence y jointly stationary with the source after . Finally , we improve the definition of the stationary causal previously by and for two sided stationary and show that for a two sided source ..., , , ,... for the associated one sided source , ,.... This that , for the quadratic case , the practical zero delay by for approaching achieve an operational data rate which by less than . log p e . per sample . 
 The information Rit for a given one sided random source be defined as the of the mutual information rate , Section . 
 between source and reconstruction y such that a given fidelity criterion does not exceed a distortion value . If one to this definition the restriction that the output can only depend causally upon the source , one what is known as the causal , , non anticipative or sequential . All these are equivalent and will be as , defined in 
 where the is taken over all joint of y given x such that the causality which will be to as the short causality constraint 
 hold and which yield distortion not greater than , for some fidelity criterion . Notice that , if one is given a two sided random source process x ..., , , ,... instead , and one is interested only in and the x , then the causality may be stated as 
 as done in . This notion of causality will be to as the long causality constraint . 
 The motivation for considering in this work one sided instead of two sided and thus instead of from the aim of building which operate with zero delay the same motivation behind the causality constraint . To see this , notice that the causality constraint for two sided to the situation in which source in the infinite past exist and are available to the . This may require an infinite delay before actually beginning to encode and decode . By contrast , the causality constraint the case when the source is a one sided process and only upon x 
 Remark . It is important to highlight at this point that even though the causality condition can also be applied to a two sided source process , it would not ensure causality in that case . To see why , consider the situation in a binary i . i .. source where each the or with equal probability . Suppose y is built as , where the exclusive OR operator . It is easy to see that , even though y non causally on x . 
 The above observation that if the source is two sided but only the x are and the process is one sided y , then one needs to impose instead the more general 
 which . Besides causality , these guarantee that even if the source is a two sided process , its and reconstruction proceeds as if it were a one sided process . 
 Notice that and . For this reason , will be to as the strong causality 
 As we shall see in and , this situation , where at can take only as input , significant due to the unavoidable need to deal with transient phenomena . 
 The operational significance of from its relation to the causal operational , as . The latter is defined as the of the average data which are achievable by a sequence of causal , yielding a distortion not greater than . is important because every zero delay source code suitable for such as low delay streaming or control , must be causal . 
 An is said to be achievable if it the under the same , . As far as the are aware , the of not been yet , for any source and distortion measure , and thus the gap between and is unknown in general . However , it is known that , Section 
 and for it is possible to construct causal with an operational data rate exceeding by less than approximately . sample . sample for zero delay , once the statistics which realize the latter are known . This the importance of the 
 To the best of the knowledge , no closed form are known for , except when considering mean squared error distortion and for i . i .. or AR , either scalar , Section or vector valued . However , there exist various structural of the causal that have been found in literature when or is assumed to admit a stationary realization . 
 Indeed , the of the of the causal a crucial role in the computation of for st order and distortion in . It also been a key implicit assumption in , and an explicit assumption in works such as and . In particular , for a stationary two sided random source , Definition the stationary causal 
 where the is taken over all of y given x which yield a one sided reconstruction y jointly stationary with x , satisfying and an asymptotic average distortion constraint on . For the case of a source , it was shown in that an operational data rate exceeding by less than . log p e . sample was 
 achievable a entropy uniform surrounded by linear time invariant operating in steady state . These illustrate the relevance of whether or in which the causal a stationary realization . 
 To the best of our knowledge , the only work which given an answer to this question in a general framework is . Under a set of in Section below , it is shown in , Theorem that the search for the causal for a large class of two sided and distortion criteria can be restricted to which are jointly stationary with the source . Unfortunately , as we show in Section , the on the fidelity criteria in leave out some common such as the family of asymptotic average single letter fidelity criteria , and the statement of , Theorem an assumption whose validity to be proved . More importantly , the entire analysis of is built for two sided the causality constraint , which the question of whether its could apply to one sided as well , with the causality constraint . 
 In this paper we give an answer to these and use the to prove some novel of the causal associated with the of its . Specifically , our main are the following : 
 We show in Theorem that if a pair of one sided jointly stationary , with the latter depending causally on the former according to but otherwise arbitrarily distributed , then it must also satisfy the 
 which is a fairly restrictive condition . In particular , as we show in Theorem , jointly and y causally upon x , then joint x is an i . i .. or st order process . This in stark contrast with what was shown in for two sided stationary and a of what is stated in , Theorem . . 
 Despite the above , we show in Theorem that for any th order one sided stationary source x and a large class of distortion , the search for the causal as defined in can be restricted to output causally related to the source and jointly stationary with it after , and such that . We refer 
 to such of as being quasi jointly stationary this notion is formally in Definition below . A consequence of this result is that for any th order stationary the corresponding one sided 
 stationary source x . The relevance of this finding is that for stationary and asymptotic distortion , an operational data rate exceeding and thus by less than approximately . sample , when operating causally , and . bit sample , in zero delay operation , is achievable by a scalar as in . 
 The remainder of this paper with Section , in which the leading to , Theorem are and the of that theorem are . In Section we prove that , in general , it is not possible to have two one sided which are jointly stationary and , at the same time , satisfy the causality constraint . Section our main theorem Theorem , which that the search for the causal for one sided th order stationary can be restricted to . Finally , main of this work . All are in section the Appendix , which also some technical by these . 
 Notation : the real , the , is the set of natural positive , and N , , ,.... For every , the ceiling operator the integer not less than . We use non for scalar random 
 For a random one sided process x we will sometimes use the short hand this meaning is clear from the context . When convenient , we write a random sequence , as the column vector , the so that the index goes above the one , thus the usual index order in a column vector . The entry on the th row and th column of a as ,, with being the sub matrix ,. 
 For a random a given alphabet set , we write to denote a sigma algebra associated : , to denote its probability distribution or probability measure . We describe the fact same probability distribution as , state independent . We write the condition in which two random a , are independent given a third random chain notation a . a set of probability , then the set of all random whose probability distribution to . The expectation operator is as E . We write as a shorthand for . The mutual information between two is defined as , Lemma . 
 where the is over , and ,, and , are the joint and marginal of and , respectively . If , have joint and marginal probability density ,, and , respectively , then 
 The conditional mutual information I a ; is defined via the chain rule of mutual information I a ; , I a ;, I a ;. The mutual information rate between two 
 x and y is defined as in . The variance of a real valued random as 
 . The auto correlation function of a random process x is , , E , ,. 
 The following of the mutual information any random a ,, will be and to throughout this work : 
 Fact . Let a ,, be three random with an arbitrary joint distribution . Then , there a random element a equivalently , a joint distribution a ,, such that 
 In order to assess whether or to what extent , Theorem could provide support to the made in , e .. , , , , it is necessary to take a closer look at the made in and the statement of its Theorem . For that purpose , the first part of this section is an exposition of the and leading to , Theorem . The second part is an analysis which the of , Theorem and its inapplicability to the case in which the source and reconstruction are one sided . At the same time , this section also and part of the notation to be in the remainder of this paper for convenience , a summary of these is in Table I below . 
 Throughout , the search in the associated with various of i . e ., causal rate distortion is stated over of joint probability between source and reconstruction as opposed to the usual , in which the search is over conditional , see and , Chapter , . Since the distribution of the source is given , it is that for every k k , all the joint , to be considered same given distribution of the source for the corresponding block , say . This requirement can be as that , for a set of admissible joint 
 where and are , respectively , the to which and belong . In , this admissibility requirement is in the definition of the of which meet the distortion constraint , next . 
 The fidelity criterion for every pair of k k is expressed in as to belong to a non empty set of hereafter to as distortion feasible set , a condition written as . In this definition , the number an 
 admissible distortion level . Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation . 
 As above , the admissibility requirement is expressed in the in . . The latter equation can be written as 
 We believe this re exposition of to be valuable in itself since on the one hand , it the minimal set of to formulate and understand its Theorem , and on the other hand , it an clearer presentation than the one found in an translation from , which is not easy to read due to its notation , some mathematical and the low resolution of its available form . 
 The analysis in considered both discrete and continuous time , but here we only refer to the discrete time scenario . 
 In . and . , the distortion feasible are assumed to satisfy the concatenation condition 
 With this , . defined the epsilon entropy of the set of as 
 where the is taken over all of random such that the causality 
 are satisfied . Then . the message generation rate as 
 when the limit . An alternative message generation rate is also considered in by the set of distortion admissible process as : 
 Definition . The set of all two sided random process x , y for which there exist k k such that and 
 when the limit , where the is taken over all of satisfying the causality 
 Notice that these imply and differ from the latter in that here the reconstruction y is a two sided random process . 
 Now assume that Xi and , for all i , for . Define , for any given non negative sequence such that , the distribution 
 , E a , E , k k , E k k k k . 
 The actual term employed in is epsilon entropy of the message where the term message to the random in . 
 Then , the analysis of the lower bound in can be confined to jointly stationary of random 
 For convenience , Table I a summary of the and notation so far , together with some which will be defined in the following . 
 We now discuss three of Theorem which are relevant when trying to establish whether the causal of a one sided stationary source a stationary realization . 
 Limitation : The first obvious limitation is that even if source and reconstruction are , every distortion criterion which only their positive time part cannot be expressed by a distortion feasible set given by Definition if the , satisfy condition in Theorem . To see this , notice that if , then such distortion criterion which non positive times would require , to admit all joint probability satisfying . Combining this with condition in Theorem that every set , , with , which to imposing no restriction on the distortion at all . 
 It is natural to think that such elemental shortcoming could be by simply condition in Theorem by a one sided version of the form : 
 For every t ,, t such that t t : , t and , t are identical . 
 Leaving aside the fact that this alternative condition is not sufficient for Theorem to hold , it is worth pointing out that , the commonly family of asymptotic single letter fidelity 
 Distortion feasible set . The set of all joint which satisfy a given constraint given by see before 
 Generic distortion feasible set of probability for of one sided 
 Q , , ,... The set of all joint of of one sided random 
 , , ,... and The of causally related one sided of see Definition . 
 C The set of one sided of causally related according constraint see Definition . to the short causality 
 The set of causal for of the form the long causality constraint see Definition Such satisfy 
 criteria can not be expressed by a distortion feasible set given by Definition , as the following lemma its proof can be found in Appendix A . 
 Lemma . Let be any given distortion functional which as argument a joint distribution , and a non negative real value . Let AD be the set of all of stationary , with pair wise , which satisfy the asymptotic 
 Then , there exist an infinite collection of distortion feasible , k k k satisfy 
 ing such that the associated given by Definition AD . N 
 Limitation : The second limitation associated with Theorem is that its application 
 one to prove its condition , i . e ., the unproven supposition that The only work we are aware of which upon Theorem is , and , accordingly , , Theorem . , which that a similar equality . Unfortunately , as shown in , the proof of , Theorem . is flawed . 
 We note that Lemma in Section A below two alternative sufficient for an equality similar to but for one sided to hold . 
 Limitation : The third limitation of Theorem for its applicability to one sided is the fact that the entire framework built in is stated for two sided and , crucially , for the corresponding causality restriction given by chain . This difference cannot be simply while Theorem to remain valid . Indeed , as we show in the next section Theorem , a pair of random can be jointly stationary and at the same time satisfy the causality chain only if is independent is given . Moreover , we prove that joint and causality are incompatible when the source is a th order one sided process with . 
 In this section we address the question of whether there a one sided reconstruction process y jointly stationary with a source x and which also the causality constraint . 
 Each source random sample i to some given set source alphabet and is to have an arbitrary distribution . Recall that a random process , the reconstruction alphabet and Y ,, is said to be jointly stationary with x if and only if , for every , the distribution of does not depend on , for , ,.... 
 The next theorem that , for such one sided , joint and causality may hold together only if is independent of when is given . 
 Theorem . If x and y are jointly stationary and y is causally related to x according to , then 
 Proof . If does not hold for if y and x are jointly stationary , then N 
 does not hold , which to not satisfying for , the proof . 
 To illustrate how restrictive condition is , the next theorem that , for a th order stationary source x , causality and joint is possible only if x is i . i .. or . Recall that a random vector or scalar valued process th order if is the non negative integer such that 
 Theorem . Suppose x is a zero mean stationary process , and assume that , for some 
 Proof . Since and yN are jointly and the latter causally upon the former , it that 
 for some lower triangular matrix A ai ,, A i ,, i , ,...,. On the other hand , the fact that and yN are jointly stationary that and are matrices . From , considering the on the first and second of and 
 a , a , a , a , a , . a , , 
 Therefore , , ,..., , which for a stationary sequence that 
 E . For random the latter is equivalent to the , which a st order process if or an i . i .. process if . This the proof . 
 In the next section we will see that if x is th order , then it is possible to build a pair causally related according to such that x , y is stationary . Moreover , we will show in Theorem below that the minimization associated with the causal can be restricted to such . 
 In this section we show that for any th order one sided stationary source x the search for the causal as defined in and for a large class of distortion criteria can be restricted to output y causally related to the source , jointly stationary with it after , and such 
 Definition Set of quasi jointly stationary process . The set of is composed of all joint , y of of one sided random which satisfy 
 Notice that Q to the set of joint associated with all jointly stationary one sided process . 
 One can define a distortion feasible set for of one sided , say , from the finite length distortion feasible ,, in more than one manner . A minimal condition we shall require for such definition is the following . 
 Assumption . The distortion feasible set of for of one sided the following : 
 If , then the given probability distribution of the source process , say 
 If is any given pair of one sided , and there an infinite collection of increasing k k such that , for all , 
 For any pair of , and if , then the ¨ , , , , 
 Notice that if this assumption and if the in Definition were restricted to be positive , then we would have see Definition . However , the one way in Assumption allow to be than . 
 Definition Set of Causal . Define as the set of all one sided random which satisfy the causality constraint 
 The set of causally related one sided process C is defined likewise but for one sided 
 provided the exist . The causal with if in one k and k . By contrast , from in that the latter is associated with the less general distortion feasible set see Definition . 
 Since our main result will be stated with the assumption that , we develop next two sufficient for such equality to hold . 
 We begin by a useful construction of a pair of from a finite length sequence and some if the of the former . 
 Proposition . Let , be given , with stationary and such that the causality condition for , ,... Build the as : 
 For every N , choose the conditional distribution of given as 
 The second equality in b together with the fact that is stationary imply that , and thus . On the other hand , we have that 
 where the first inequality due to Proposition , in the Appendix it successively to the 
 The fact that , the definition of and the condition imply that , for all N . The latter together with Assumption 
 On the other hand , together with the fact that for , ,.., 
 Proposition with a ,,, according to the in and in , we readily obtain that , a , 
 We now state a technical lemma which is akin to , Theorem but for one sided , the proof of which can be found in Appendix A . 
 Lemma . Let be a stationary one sided source and suppose the distortion feasible , and satisfy Assumption and the condition 
 Next , we propose a possible definition of general enough to encompass the asymptotic fidelity criteria by . For that purpose , we need to define 
 Notice that with such construction , does not necessarily satisfy Assumption . Also , the distortion feasible , with the specific form given by do not necessarily satisfy the condition . 
 This definition , based on the limit of a sequence of distortion , is clearly capable of the general asymptotic single letter criteria of while satisfying Assumption . Recall that , as shown in Lemma , it is not possible to do this with the distortion feasible set from , given by Definition . In addition , the construction of provided by for several specific criteria commonly found in the literature , such as the one in and in the definition of a rate distortion achievable pair in ,. . 
 We are now in the position to provide two independent that are sufficient to ensure the proof is given in Appendix A . 
 Lemma . Consider the same given in the statement of Lemma . If , in addition , any of the two following 
 With the above , we can state the main result of this section , akin to Theorem but for one sided and for the corresponding causality condition given by the proof is in Appendix A : 
 Theorem . Let the source x be a one sided stationary th order process and suppose that , where and are as defined in and , respectively . Furthermore , suppose that the distortion feasible , , satisfy Assumption and The shift invariance condition : 
 Then the minimization in the definition of in can be restricted to of 
 x , y with which , in addition , satisfy , . N 
 aside the obvious difference between Theorem and Theorem from the fact that the former as one sided and the latter two sided , it is worth drawing a parallel between these two . The requirement of Assumption in Theorem is than the requirement of to conform to Definition in Theorem . Thus , Theorem for a class of fidelity criteria . The assumption that can be seen as the 
 equivalent of condition in Theorem to the setting of one sided . The same is true with condition in Theorem with respect to condition in Theorem . However , no are stated in which suffice for condition in Theorem to hold . In contrast , we have provided Lemma , which two independent under which is satisfied . The other in Theorem differ from those in Theorem . Condition in Theorem is than condition in Theorem . Condition in Theorem is absent in Theorem , and is in our proof as a consequence of the transient behavior from treating one sided see Theorem in Section . 
 Remark . Among the distortion criteria which satisfy the of Theorem , we find the family of asymptotic single letter of by , , : , 
 a distortion feasible set to Definition , and hence it is not covered by Theorem . 
 As pointed out by Remark in the Introduction , if now one that x is the positive time part of a two sided stationary process , then the fact that does not guarantee 
 that y causally on x . For the latter to hold in this situation , it is that . This that for this case , the definition of the causal as stated in needs to be extended to 
 Notice that when the source a negative time part , to , and thus strong becomes equal to . 
 The above raise the question of whether Theorem can be extended for the case in which the one sided source is the positive time part of a two sided stationary process . This considering strong instead of , or , equivalently , the strong causality constraint instead of the short causality constraint . 
 It turns out that Theorem can indeed be extended for this situation , thanks to the following proposition , the proof of which can be found in Section A . 
 Proposition . Suppose that x is the positive time part of a two sided that 
 Then there or , equivalently , one can construct a one sided random process such that 
 Theorem Extension of Theorem . Let the source x be the positive time part of a two sided stationary th order process . Under the same made in the statement of Theorem , the in the definition of can be restricted to of 
 Proof . The only difference between the of and is that they consider the causality and , respectively . First , notice that 
 and thus strong . On the other hand , from Theorem , the yielding can be carried out considering only of satisfying and 
 n , x , y . But as a consequence of Proposition , for every such x , y , there a process such that and 
 This that the minimization associated with strong can be restricted to satisfying , and proving the first claim of the theorem and that strong . 
 The latter and the reverse inequality that strong , concluding the proof . 
 The purpose of this section is to establish a correspondence between in . As we discuss next , drawing an appropriate comparison between these two causal two to the definition of already on page . 
 The first modification of extending to account for arbitrary fidelity criteria in an arbitrary distortion feasible set P , . 
 The second modification is necessary in order to make a lower bound to the corresponding operational data rate . To see why , it is necessary to recall how lower the operational data rate of x and it as y . For this purpose , let be the random binary sequence produced by the from time let be the length of in . Since the code must be uniquely , the bit string the Kraft inequality , § . . In general , can be by not only but also x , and thus 
 where a from , Theorem . . and is a consequence of the data inequality , Theorem . . . Thus , lower the operational data rate E as tightly 
 which is precisely the causality constraint for one sided . But , as we have shown in and , such causality constraint is , in general , incompatible with the joint of 
 . As a consequence , since such joint is by , chain cannot hold . This that when the causality constraint for a two sided source established by is 
 Following these , we propose here the following definition of . 
 Definition An and More General Definition of . For any sided 
 stationary source , redefine the causal stationary in , Definition as 
 where Q is the set of all one sided jointly stationary random , and is the set of all of random which satisfy the causality constraint . 
 We can now state the following corollary of Theorem , the proof of which can be found in Appendix A : 
 One important consequence of this result from the fact that , for a th order Gauss stationary source and quadratic distortion , can be found by a convex optimization problem over frequency response of linear time invariant around an additive noise channel , and . 
 The operational relevance of Corollary is that when the latter channel is by an entropy scalar , one a source scheme whose operational rate by at most . sample when operating causally and by at most . sample when operating with zero delay , section . Thanks to Corollary , it turns out that the operational data rate of such scheme within the same with respect to 
 We have shown that , in general , the causal information rate distortion function for one sided stationary cannot be by a reconstruction which is jointly stationary with the source . Nevertheless , if the source is th order , then the search for the causal can be restricted to which are jointly stationary with the source from the th sample . This led us to prove that actually with for a large class of distortion criteria . This that for Gauss and quadratic distortion , can be found by the convex optimization problem derived in . It also that for the same source and distortion , a zero delay average data rate exceeding by not more than approximately . sample is achievable with the scheme in . 
 Proof of Lemma . We will resort to a contradiction argument , and thus start by supposing that there AD . 
 Since is non negative , there must exist a pair of random and a value 
 From the definition of AD , any other pair of which exactly as 
 everywhere except on a single positive index , say , in which , P , and 
 Definition , there a pair of l , l with l such that l l and 
 This , together with the fact that the , satisfy , that 
 where , l l . Hence , any pair of random with pair wise given by 
 together with the collection of satisfy the of Definition , and thus . However , 
 meaning that . This the initial supposition that AD , the proof . 
 By the definition of , we have that o , No , there such that 
 and build the as in Proposition . The mutual information rate between and 
 Since this inequality is satisfied for all o , o , it that , the 
 Proof of Lemma . Since the of Lemma are satisfied , we have that . Therefore , it to show that . 
 , for all . Since all the latter for all No ,, we obtain 
 Since this inequality for all o , o , it that , the first part of the proof . 
 We shall now prove that Assumption and the continuity of . The continuity assumption on that 
 od , for all . By the definition of , we have that , for every , o , there a pair of such that 
 Also , since and from the definition of in , it that there a 
 Since this inequality for all , o , o , and that od when , it that Rit , the proof . 
 Proof of Theorem . Since , it that for all o , there a finite No such that 
 Thus , for all o and for all No there a pair of such that 
 The fact that n as one to define the stationary extension of n such that 
 Starting from we build the as in Proposition . From this construction 
 and the assumption on the distortion feasible given by , we have that 
 Now suppose a random variable uniformly distributed over , ,.. and independent of . Let and define the pair of 
 A similar construction , for two sided , was in the proof of , Theorem seemingly for the first time , building upon . The same idea was in the proof of , Theorem . . Here we adapt it for the case of one sided . 
 are jointly stationary . Thus th order stationary . These imply , in view of and , that the pair may not be causally related according to . However , since see Proposition , we have that does satisfy the 
 x ¨ , ,.. , , ,... a ¨ , ,.. , , ,... b 
 The pair further two important . First , Lemma that the of are i . e Second , as we show in Lemma , the mutual information rate of the pair lower , for all . Thus , for every No ,, 
 is for every o and o , it readily that the search for 
 the on the of can be confined to such , the proof . 
 Lemma . Let be the defined in , which are built from 
 see and see and and the text between these and satisfy and . Then . N 
 Proof . By construction , are jointly stationary . Thus , all that remains to prove is that see Definition . For this purpose , notice that for all i , 
 where all the stem from the chain rule of mutual information , a because I a ; , and is a consequence of the fact that ¨ , and 
 chain . The mutual information in the middle of can be upper bounded as 
 where the are due to the chain rule of mutual information , a because mutual information is non negative , and ¨ . 
 the definition of ¨ ,¨ see we have that , for the case i , 
 are so as to streamline the presentation of the following . The between all these is in Fig . . Notice that for these the translate into 
 where a because mutual information is non negative and is due to and the fact that 
 Figure . Schematic representation of the change of in . Each dot one element in the and , with time increasing from left to right . 
 Lemma . Let the defined in , built from and and and the text between these . Then 
 where a is due to the non negativity of mutual information and because . But 
 see . Substituting this into we obtain that for , 
 The proof is by taking the limit as and substituting in it . 
 Proof of Proposition . From Fact , there such that and which 
 Proposition with a ,,, corresponding to the under the in and , we obtain directly , the proof . 
 Proof of Corollary . We will first show that and then that the reverse inequality is true as well . 
 can be confined to of which satisfy and such that and . For each such pair , one can construct the pair of 
 where the last equality from the fact that and that and the definition of . In addition , the 
 Therefore , for every pair of which the associated with , there another pair which the in the definition of and the same information rate . This that . 
 In order to show that , consider any pair satisfying and such that . Construct the pair of as 
 the existence of such pair is by the first condition in the statement of Theorem . From Assumption , this construction 
 Proposition to and with in the proposition assigned according to the under and , we obtain that 
 Proof of Proposition . The mutual information between a , a and b , b is given by 
 where all stem from the chain rule of mutual information . 
 Since mutual information is non negative , it that I a ;, if and only if both I a ; and I a ;, are zero . The proof is by that the statement I u ; is equivalent to the chain u . 
  
 ﻿ We derive closed form for the statistics of the power gain as a function of frequency of microwave indoor . We obtain our within a framework that is general enough to be compatible with several popular channel , such as the channel model and those by the . . a and . . a task . As in all these , our channel description is based upon and with possibly mixed and random . Our consist of closed form for the second order statistics of the channel power frequency response , where statistical involve over ray and arrival times . These reveal that the of the spectral power gain between any two and to zero as the difference between these to infinity if and only if the cluster arrival rate goes to infinity . They also show that the variance to squared mean ratio of the narrow band power gain exactly the same behavior with respect to the center frequency . We then use these to obtain closed form for the variance and the second order moment of the aggregate channel power gain over any given interval of . This us to express the channel spectral diversity as a function of model and . In addition , we illustrate how these allow one to devise automatic cluster identification which , from empirical of the second order spectral statistics of the channel power gain , can confirm or deny the existence of in a given scenario . 
 Index Fading , channel model , statistical channel modeling 
 I . INTRODUCTION 
 S 
 wireless channel allow one to predict the statistics of radio propagation over an ensemble of with similar . This is particularly useful in complex , heterogeneous , and time , such as office , residential , and industrial indoor . 
 One of the most popular for indoor wireless is that in , which as the basis for several other channel . Building upon it , the . . a task group accepted a channel model for indoor , and similar have been adopted for the . . a standard . Similar to the model , the . . a and . . a channel consist of a discrete time description of the impulse response of a wireless channel , in which are grouped into . 
 Some of the useful time domain delay statistics of wireless channel are the power delay profile , the average delay , and the delay , which , under suitable , allow one to determine spectral statistics such as coherence and average power gain . These , which are associated with second order statistics of the channel impulse or frequency response , have been extensively and in the literature . 
 Another set of stochastic of wireless , which received relatively less attention , is those derived from the second order statistics of the channel power frequency response i . e ., its squared frequency response magnitude , which is by . An early analysis of these statistics and their application can be found in , where it is that the transmission over two with the right frequency spacing may benefit from the negative correlation between each of the received . Simplified for the spatial and spectral of and for its power over a frequency interval are in , which are then via . Building upon this result and , it been possible to assess fading depth statistics and relate them to the and to some of the propagation environment , . 
 . Personal use is permitted , but republication redistribution permission . See : index . for more information . 
 Although the of the second order statistics of are helpful in the performance of wireless communication , closed form for these statistics further . First and naturally , these closed form can be substituted into other , thus one to relate them with performance , which are also in closed form . Second , explicit parametric will provide useful insight into the relationship between the performance of wireless communication and certain . In particular , the second order statistics of allow one to refine probabilistic drawn from first order statistics alone . Moreover , if the power received over some band is known to distribute according to some parametric probability density function , then the first and second order statistics of this power one to identify , in some , the of the distribution . 
 This it possible to calculate fade depth statistics from the second order statistics of . 
 In this regard , it was already noted in and that the fade depth as the width of . In relation to this question , and an information theoretic approach and measured data , it been shown in that the number of significant of the covariance matrix of the random channel impulse response and , hence , the diversity order of the channel scale approximately linearly with the . This increase in diversity , which to the reduction of the relative channel power variance as the , been to reach a saturation point , , . explicit for this channel power variance as a function of would allow one to predict the associated saturation and relate it to environmental . 
 Available closed form that allow one to analytically obtain the second order statistics of are rather limited . Although the analysis in and does obtain some intermediate closed form , the actual evaluation of these statistics is carried out by . In addition , one of the underpinning the in and is the between the and arrival times of , which is known to contradict channel , . Under the assumption of uncorrelated and arrival times , and considering uniformly distributed arrival times , closed form for the of have been derived in . Exact closed form for all the joint between and , for any and , as a function of the of the channel impulse response have been found in , assuming that component are independent jointly complex and that arrival times are fixed . However , although popular channel such as the model consider complex component , other amplitude have been in the literature see , e .., . Moreover , the conditional independence assumption in does not hold for channel where the impulse response , such as the and its several , . In a recent paper , analytical for the of the channel frequency response squared magnitude and the variance of the power over any frequency band have been derived for the . . a channel model , conditioned to fixed and given ray and cluster arrival times . The corresponding statistics over random arrival times were via . To the best of the knowledge , no closed form are available in the literature for second order unconditioned statistics of the spectral power gain of wireless channel such as the . . a and . . a , as well as for the other like . 
 In this paper , we derive closed form exact for the second order statistics of for a general class of of the channel model . This class , as special , those accepted by the . . a task group and the . . a channel modeling subgroup , , . Unlike , our include considering the randomness of both ray and arrival times . As in all like , our channel description is based upon and , with different power decay among and within . Our analysis framework is general enough to encompass model such as mixed within , as well as in which the first ray of each cluster statistics different from the other these two are considered in , , and . To this end , we first derive , in Section , a general , exact , and closed form expression for the of . This expression is determined by four , corresponding to the second and fourth order moment delay of the cluster and ray by their respective arrival . We use these to analyze the effect of a finite infinite number of , as well as the of only one cluster or , equivalently , no at all . We then use the former result to obtain , in Section , a expression for the variance of the channel power gain over any given interval of . Our predict a positive lower bound for the ratio between the variance of and its squared mean . This bound only as the arrival rate of in the model to infinity , which is a result that is confirmed via . Some preliminary on this topic were in . In addition , the allow one to devise an automatic cluster identification algorithm , which is capable of or the presence of in an ensemble of channel impulse . This algorithm is shown in an example in can be considered an alternative to other cluster identification , such as those in and . 
 . CHANNEL MODEL 
 Here , we formulate the wireless channel model framework to be throughout this paper . The idea is to establish a framework that is general enough to encompass most of the in the available literature . With that idea in mind , and before proceeding , we will first review some of the salient of several of the model , from which we will then define the set of that our will be based upon . 
 A . Brief Review of Like Channel 
 In all and of the model , the channel is by the following random impulse response , : 
  
 where 
  
 ti , Ti ti , i ,..., ; ,..., 
 is the random arrival time of the path or ray in the cluster , Ti is the random arrival time of the cluster , and ti , is the random delay of the path in the cluster relative to Ti . Cluster and relative arrival times are ordered , i . e ., Ti i and ti , ti , i . By definition , the cluster with its first ray ; thus 
 ti , i ,..., . 
 Each random coefficient ai , in the amplitude of the path in cluster i . These random are formed as : 
 ai , pi i ,..., ; ,..., . 
  
 Each is a random variable the amplitude of the cluster , whereas each real valued random variable ai , the amplitude or gain of the component or ray within the cluster relative to that cluster amplitude . The meaning of the random pi , in on whether a complex or real , representation is used . For the former , pi , , where fi , are uniformly independent and identically distributed i . i .. over , , , . For the real representation , which is as more suitable for , pi , are binary random taking from , with equal probability . In both 
 E pi , i ,. 
 B . and Analysis Framework 
 To establish a framework to encompass the various of the like channel present in , , and and possible future , we will formulate now the least restrictive set of under which our analysis and are valid . To do so , the following three are necessary . 
 Definition Bounded Density Process : We will state that a sequence of random arrival times , with xi i , is bounded density with the following . 
 The times satisfy the 
  
  
 There a constant such that , for every that is sufficiently small 
 ; ,... 
  
  
 In other , condition in the given definition that the time just after an arrival at and , hence , the arrival density at , at most , on the value of . 
 In turn , condition that the arrival density at any is bounded . 
 Definition Sequence i . i .. Under Another Sequence : Let be a bounded density process , with x possibly being equal to . We will say that a sequence of random is i . i . under if the following two are satisfied . 
 The following hold : 
 ; , ,... 
 For every and , E 
  
 , where , ,... if x is 
  
 random , and , ,... if x . 
  
 Notice that condition in Definition to the conditional independence between and arrival times by all channel considered in , , , and , except the . . a model for industrial under in and . Similarly , taking as the cluster or relative ray and as the cluster or relative ray arrival times , condition all the amplitude considered in , , and , the in which the amplitude of the first ray or cluster a special form , as in body surface to body surface , . . and body surface to external , . . in . 
 We will need one more definition to treat arrival time differently from plain , such as mixed in . To this end , we introduce the following single and joint arrival density . 
 Definition Arrival Density : Let be a bounded density sequence of arrival times , with x possibly being equal to zero . For any , denote the number of in falling inside 
 , by the random variable , i . e ., 
  
 where , for , . We define the single and joint arrival density of as 
  
  
 , if x is random 
 , if x a 
 , 
 ,, if x is random 
 , z 
 if x 
 b 
 where , for i , 
 a 
 , 
  
 , 
 b 
  
 One case in which arrival times do not conform to a process is in the model in , which is further considered in and . In that case , within each cluster are as mixed ; in which case , the ray times are i . i .. with the following : 
 with probability with probability 
  
 where and are two different arrival . 
 the given , we can now establish the scope of validity of our by of the following assumption . 
 Assumption : Cluster and ray arrival times and are random and as . 
 The following independence hold : 
 , Ti , Ai , ti ,, ai , i , ,..., ; 
 , ,..., a , pi ,, unless i and b , ai ,, unless i c 
 Ti , Ai , 
 i , ,..., ; ,..., d 
 i ,, ,..., ; 
 where probabilistic independence , and the chain notation a a 
 independent known . 
 Ti is a bounded density sequence , with T that is possibly equal to zero , i . i . times exponentially distributed with exponent , and arrival density and ,. 
 ti , m is a bounded density sequence with ti , and arrival density and , for every i . 
 is i . i .. under , with order moment delay profile , as shown in a at the bottom of the page , for , . 
 is i . i .. upon , with order moment delay profile , as shown in b at the bottom of the page , where , for every i . 
  
 Notice that , in the in Assumption , and , , capture the moment delay between and within , respectively , without taking arrival into account . In turn , the single and joint arrival of are by and ,, whereas the single and joint relative arrival of are given by and ,, respectively . 
 The and in Assumption are satisfied for all the channel in , , and . Indeed , by choosing cluster and ray arrival times as , with each ai , conditioned to ti , being distributed with second moment g , and deterministic Ai with the form Ai b Ti ,, , we obtain the widely used model . A similar choice with distributed Ai and ai the model by the . . a task group excluding its large scale fading term . The decay of the by can be also chosen to match those that the . a standard for high in some scenario such as office and industrial . 
  
 , if T and 
 and a is Mixed do not Give Rise to a Mixed Distribution : It is worth at this point that the mixed , which are by the conditional given in , do not give rise to what is known as a mixed distribution . More precisely , if the random exponent selection in place after each arrival , then the probability any unit length time interval , cannot be written as , where the probability mass function with parameter . The exact probability distribution been derived by the first author in a work currently in preparation . 
 Another assumption that will facilitate the forthcoming analysis is related to the number of and . Although in , as in , , and , the number of and the number of within each cluster are finite , we will consider infinitely many and per cluster . As acknowledged in , the latter choice is more realistic . Of course , this assumption that the decay fast enough with increasing delay . We will make this requirement precise by assuming that the conditional of the defined in a and b satisfy 
  
 and work in the sequel with the channel impulse response 
  
 with ai , and ti , as defined in Section A . In addition , as in , we will adopt a real model for the impulse response ; hence , the pi , in are i . i .. random taking the or with equal probability . This is done mainly to simplify the notation , and it is worth that all forthcoming also hold for the complex representation . All that is is that and Assumption hold . 
 From , the squared magnitude of the channel frequency response is 
  
 A deterministic frequency dependent gain for each ray , which is a feature in , , and , can be easily incorporated to our model by simply multiplying by the squared magnitude of this gain . For ease of notation , and because the effects of this factor on the resulting statistics can be easily added afterward , we will not include it in our . 
 In the following and under Assumption , we will derive the exact closed form for the second order statistics of and of the channel power over any given frequency interval as a function of the moment delay b , b , g , and g . 
 . OF 
 Here , we will obtain closed form for the mean , the , and the coefficient of over any interval of angular . These rely on several technical , which can be found in the Appendix . 
 A . Value of 
 From and the zero mean and independence of the pi established in , it is easy to verify that 
 . 
 , d , 
 in the Appendix , it is readily found that 
  
 Thus , the value of is the same for all , being equal to the product of the average energy of the cluster Ai and the ray relative ai ,. 
 B . of 
 Denote the of by 
  
 The following theorem , which is the main result of this paper , an exact closed form expression for , in of the arrival density , , , and and the moment delay b , b , g , and g defined in Assumption . 
 Theorem : . Define the effective moment delay profile 
 , , b b 
 , , g g 
 for , , and let , respectively , denote 
 their . Then , for any R 
 Proof : See the Appendix . Remark : Theorem that the variance of the channel power gain the same variation with as does its O , O with O . Since the overall behavior of the latter is to decrease as O , it that the variability of the channel power gain is smaller at higher . Notice that this reduction does not come from the frequency dependence affecting each component which is typical of real indoor propagation since we have not included it in our model . However , had this dependence been included , the variance to squared mean ratio of would still exhibit the decay with by Theorem . Instead , the origin of this behavior can be more intuitively understood by looking at in the Appendix and by that E is independent of . In , if we choose , then T T becomes the only frequency dependent term . Now , T is a sum of of the random . At , T turns into a sum of . By contrast , if we let increase so that the of the random extend smoothly over an interval that is several times than p , then the phases of the will be approximately distributed uniformly over , p . This will reduce each of the in the sum . Thus , as , and so does E . 
 Remark : If the effective moment delay and are smooth , then all the frequency dependent in vanish if to infinity while keeping , , leaving only 
 . This that the between two infinitely distant frequency is greater than zero , which , at first sight , may seem . Nevertheless , although this to be the first time that such behavior is proven to exist for like channel , it is consistent with the saturation of the channel diversity order as the , as in and . Further treatment of this phenomenon is in B and . As will be in the following , this nonzero asymptotic is due to that the arrival rate of is finite . 
 We now apply these to some specific . 
 Infinitely Many : Suppose the arrival of and are scaled by and , respectively , to obtain arrival 
  
 where 
 , if T otherwise . 
 Let us also suppose that this is done while the original so that the total impulse response power is , which one to scale g and b by and , respectively . This that g and b are scaled by and , respectively . Denote the resulting scaled moment profile by , and . 
 Substituting them into , the behavior of the involved in the frequency independent part of as cluster and ray going to infinity is given by the following : 
 d 
 Therefore , for any given and smooth , lim 
  
 c , if the cluster arrival rate to infinity and the right hand side of b is zero . With this , as , in the case in which have which , B B and if the first cluster randomly i . e ., if , then the of the simpler form 
  
  
 . 
 squared magnitude of the and the at and . 
 Only One Cluster No : Consider the case in which do not exhibit the presence of . This situation can be under the framework defined by Assumption by supposing that there is only one cluster with deterministic arrival time T and amplitude A , and such that its first ray at time t , . To ensure that there are no other , we may take E Ak . With these , we obtain , and , 
 ; thus , the becomes 
 . 
 Nonzero Asymptotic : As previously in Remark , Theorem that , for finite cluster arrival , the of , i . e ., , , to a positive value as i . e ., if O to , and not to zero . To the best of our knowledge , this is the first time that this result is derived or revealed , at least for an like channel model . A simple and intuitive explanation of this phenomenon is provided at the beginning of Section , in of the variance of the aggregate channel power gain over a given band . It is worth that such asymptotic behavior cannot be if one the of the complex frequency response , i . e ., by looking at the second order statistics of instead of those of . Indeed , by the independence a and d and then Lemma , it becomes easy to show that 
 . 
 This simple expression which we believe to be novel , that , if the moment delay of and within them are smooth , then lim E . 
 C . Correlation Coefficient of 
 The correlation coefficient of , which is as , , is given by 
 . 
 From , it readily that can be written as 
  
 It was already shown that , for any given smooth moment delay profile , the between two and O to zero as O if the cluster arrival density goes to infinity . We shall see in Section A that even when this arrival density is finite , , O if the moment delay change so that the number of with a significant amplitude to infinity . 
 . SECOND ORDER STATISTICS OVER A FREQUENCY BAND 
 Here , we will use Theorem to derive closed form for the mean and the variance of the total channel power over any given frequency band , i . e ., 
  
 B 
 , , , r 
 where and are in per second . 
 A . Value of the Channel Power Over a Frequency Band 
  
 Denote the channel power over frequency band is constant for by 
 . Given that E 
 see , it from that 
  
 where 
 . 
 B . Variance of the Channel Power Over a Frequency Band 
 The variance of the channel power over be directly from the of as 
 Before proceeding , it is worth that readily that , if , u is bounded , then 
  
 since E linearly with . This one to provide a simple and intuitive explanation for the fact that , u i . e ., the of does not vanish when the two , u are infinitely distant from one another . Recall from theorem that , when , the total impulse response power . Then , if the impulse response is amplitude by a finite number of cluster , it is clear that the ratio variance of impulse response power over the square of the impulse response power will not be zero . In view of , this will imply that . That is , the of between a pair of infinitely distant will not be zero whenever the total number of with random amplitude remains finite . As , this an alternative and more intuitive explanation to what was already in Section B . 
 We now return to the variance of . From , , can be written as 
  
 where 
  
 . b Substituting into 
  
 the change of O u , u and along diagonal over the square , ,, we obtain 
  
 We note that is a measure of the fluctuation of the received power over a wireless channel and that the latter its fade statistics . The fact that fade depth with channel been in the literature , both from empirical data see , e .., and from , . However , a closed form formula fade depth and the classical channel model used here have , to the best of our knowledge , not been to date . The derived allow us to directly deal with this issue , provided that the fading distribution can be fully determined from its second moment an assumption that was successfully applied in . The explicit dependence between in the following example . 
 V . EXAMPLE 
 Here , we will illustrate the application of the in and to the classical model . In particular , we will show that our analytical found in and accurately predict the of and the variance of the channel power gain over any given band . In addition , we will show how the presence of can be confirmed or by the statistics of and the derived in this paper . 
 A . Model 
 In the model , the ai , are distributed conditioned to ti ,, and the Ai are deterministic exponentially . The of cluster and ray are given by 
 . 
 For the fourth order , we have 
  
 where the last equation since the fourth moment E x of a distributed random related to its second order moment as E x E x . Both cluster and relative ray times are exponentially i . i .. with and , respectively . In addition , the arrival times of the first cluster and the relative arrival time of the first ray in every cluster are , by definition , zero . Therefore , the effective moment profile defined in take the following form : 
  
 B . 
 Recall from that , can be written as 
 . 
 In this case , from , we have 
 . 
 If , the decomposition into partial of the term 
  
 one to write 
  
 Substituting this result into 
  
 Fig . with dashed line the empirical of , of the channel model with four different of , which are in Table I . Each of these was after random of a channel impulse response . On the same plot , the theoretical of , by are solid , for each set of . It is shown that , in all , the data very tightly . 
 The set A correspond to those in the . . a . Notice that , for the of this set , as O is , the spectral by approximately when O . rad , and then at about , which is precisely the behavior determined by provided K is much than K and . From , the next corner separation frequency , beyond which , almost to diminish , place when , which in this case to rad , in agreement with what is shown in Fig . . The from those of the previous one in that the arrival are halved , whereas the decay are doubled . This the same and and the same ratio as the set A . Thus , the , K , and are also the same as those by the set A . Accordingly , the only difference with respect with the set A is that the corner are reduced by a factor of 
 . In addition , as by , reducing to one fourth of 
  
 Fig . . Spectral of as a function of the frequency rad for the model separation O around a central frequency with the four of shown in Table I . in dashed line are over channel . Each theoretical curve in solid line was . 
 TABLE I 
 OF FOR THE MODEL USED FOR THE IN Section V 
  
 its value in set A the corner frequency associated with K by , which is sufficiently higher than to have a small but noticeable effect see the slight bump in the and rad . With the of set , these two corner are two away from one another , with their presence becoming clearly visible in the corresponding plot in Fig . . Intuitively , the existence of two corner separation beyond which the spectral or to decrease can be associated with the temporal resolution associated with O . More precisely , when O is too small , all in the impulse response are added with roughly the same phase in the sum for O and for O , yielding a high correlation . As O is , a point is at which some within the impulse response contribute with different random phases in , which to the channel power gains . This reduction when O is large enough so that all within the impulse response are added with different phases , only when O to make also the within each cluster add with different random phases . 
 C . Correlation Coefficient 
 The variance of is directly by for an arbitrary frequency , which 
 . 
 By substituting this and into , the correlation coefficient the form in , shown at the bottom of the page . By substituting the of , K , and K into this expression and after some algebra , one that the correlation coefficient 
 . 
 Thus , extending what was found for the of in Section B , we see here that the lower asymptote of the spectral power correlation as O not only when but also when the product . 
 Moreover , for any , we have that , O if and only if . 
 D . or no 
 When a sufficiently large number of of the impulse response of a wireless channel , the only model that can be directly are the ray arrival rate and the decay exponent . In contrast , the arrival rate and the decay exponent are not readily observable . Indeed , the very existence of is not unquestionably evident from the impulse response . 
 Here , based upon the , we propose a quantitative method for and . By doing this , it is possible to assert the presence of if the value is comparable to or smaller than or the absence of them if 
  
 As previously , the decay the arrival rate can be directly from as done in , e .., and . Additional for finding the two and can be by O , O at two or more different . For the channel model considered in this 
 If there were no , the impulse response would be as if there were only a single cluster , beginning at , with decay exponent . 
 In practice , it will be also necessary to identify the power scaling factor of the impulse response , namely , the value of b at . This can be easily done by taking the empirical mean of the squared magnitude of the first ray in the impulse response . 
 example , the two can be from , 
 i . e ., 
  
 The left hand sides of these L and L can be directly from the . Denote these as and , respectively . In principle , one could express and in closed form as of L and L , and then evaluate for L and L . However , is in this case the solution to the following quadratic equation : 
 L L L 
 resulting from substituting into . Therefore , the approximate nature of and can turn a real positive root of into a complex valued or negative one . Moreover , even if one could measure L and L with infinite precision , there can be still two real valued of greater than . To overcome these , the following algorithm is . 
 Find each of of 
 for L and L . Denote these as ,...,, respectively , where , . From these , calculate 
 the , i , i ,...,. These will be the preliminary for . 
 Let xi , i ,...,, be the corresponding preliminary for . 
 For each i ,...,, let i xi and i . 
 Then , apply a least curve fitting optimization algorithm , with and as unknown , to match against the empirical of , at several . Use i and i as initial . Define the residual matching as i 
 Pick the of i and i associated with the minimum i . 
 This technique for and was applied to impulse for the channel model , with 
 Fig . . Left Typical impulse response for the channel model with the of top set E . , . , , and . and bottom set , . , . from the same cluster are shown with the same color . Top right Twelve of and , each from the of the impulse for each set of , which are by the method in Section . Bottom right of from and s range along them , the standard deviation of the empirical estimate of 
 c , from channel . 
 two of . For the first of these set E , . , . , , and . . Notice that this , i . e ., the arrive at the same rate as the within them , making it very difficult to distinguish one cluster from another in the time domain . 
 It may be that the chosen and decay and are the same as those from the . . a used in Section . In turn , the product been chosen about ten times than that used in Section , so that the resulting dense train of it even harder to recognize the presence of . A typical channel impulse response for these is shown in Fig . top left . As , with the chosen , arrive so densely that it is virtually impossible to tell one from the other . After of independent of these impulse , of L and L were by . From the corresponding empirical of ,, which are for of evenly distributed from to rad , the and were the method , yielding the shown in the scatter plot of Fig . top right . The average error magnitude of these is less than of their true . More importantly , in all , the existence of is unambiguously revealed since all of are greater than . This more remarkable after that , in this case , the impulse are such that are almost totally , making it virtually impossible to distinguish one from the other . 
 The for a scenario in which there are no is shown in Fig . bottom . In this case , the used for the model : , . , and . A typical channel impulse response from these , as the one shown on Fig . bottom left , the same overall decay exponent and a similar net density of as in the previous case . The corresponding of and , which are by the algorithm here , are shown in the scatter plot in Fig . top right . Here , for of of the channel impulse response , the corresponding empirical of , for of evenly distributed over , rad were . Notice how , in all , the of are well below , unambiguously revealing the absence of . 
 The possibility of correctly the true of a channel , and in the case from the empirical estimate of ,, e ..,,, channel , will ultimately depend upon the accuracy of the latter estimate . To illustrate this dependence , the theoretical O , O is plotted in Fig . right bottom , for the parameter E and . The within five standard of the estimate above and below each of the O , O is shown as shaded . The standard deviation shown to what is when , is calculated from channel , which is the same number to obtain the shown in Fig . top right . Given the significant overlap of these and since the variance of , is proportional to , it is clear that , in this case , a reliable decision about which of the parameter better the data cannot be from much less than channel . 
 We end by that the parameter estimation algorithm been for its simplicity and with the purpose of the potential applicability of the derived in and . Therefore , the search for better estimation , which must certainly exist , goes beyond the scope of this paper . 
 E . Second Order Statistics of Total Power Over a Band 
  
 The variance of the channel power over a band , rad is by substituting into , with the change of u and u as : 
  
  
 these , the change of , and the identity , we obtain , after some manipulation , that 
 The relative variability of with respect to the average total power better by the variance , i . e ., 
  
 where the equality is by substituting and into . 
 and reveal several interesting about the channel power variance , which are in the following . 
 Except for the first term on the of , all the other grow above some frequency . It is a simple although long exercise of algebra to show that the frequency above which the growth rate all these is significantly than 
 W is given by , . . If , then the variance of the total power be well as 
  
 where we have used . Thus , in agreement with what was shown in B and , the variance of the power over an asymptotically infinite only when the number of significant to infinity . 
 The narrow band channel power variance P E P is from by O 
 . small enough , then all the in 
 grow proportional to W . It is straightforward but lengthy to show that the maximum value for 
 this to be a reasonable approximation is 
 G min 
 smaller than the latter threshold , it that 
  
 and therefore 
  
 Thus , unless comparable to or than , no reduction in channel power gain variance is to be . 
 Notice also that , if there is a single cluster with deterministic amplitude and infinite duration i . e ., if we let and , the narrow band channel power variance should coincide with that of a channel in which case should distribute exponentially . Indeed , if we substitute the for K and K in , fix , and let , then it is easy to verify that E , which is precisely the variance to squared expectation ratio of an exponentially distributed random variable . 
 The in the dashed line in Fig . display the value of E for a band of at rad from of the model for each set of given in Table I . The corresponding of E by are plotted in the same figure with solid . It is shown that , in all , the theoretical and simulation are almost indistinguishable , confirming the accuracy of our . It is also shown that , in all , as from , from zero , the channel power variance remains almost constant until a certain value , near the threshold defined previously . Beyond that threshold , all the decay a saturation point is , in a way that is consistent with what was found in . 
 . CONCLUSION 
 We have derived the general that characterize the second order statistics of the frequency response power gain in wireless indoor . Our are applicable to several well established channel in the literature , all based upon the model . In particular , the closed form formula here for the of the squared frequency response magnitude of the channel one to predict the variance of the aggregate channel power gain over any frequency band . This an approximation 
  
 Fig . . variance of the channel power gain over a at rad , for the model with the four of A ,,, given in Table I . are over channel . The theoretical curve was and . 
 to the diversity order of over narrow band in such . Our also allow one to obtain an upper bound for this measure of spectral diversity , and they show how and why this limit from a finite number of with a significant amplitude in the channel impulse response . In addition , the derived explicitly reveal how the statistical of the in the channel impulse response affect the spectral of the channel power gain . This one to use these spectral statistics to identify the presence or absence of . A simple procedure to accomplish this task been and its effectiveness been by it to of the channel model . 
 APPENDIX 
 Proof of Theorem : For notational simplicity , we will temporarily adopt a single indexing nomenclature for the path arrival times ti ,. More precisely and with a slight abuse of notation , we define the infinite random set 
 . For our , it will not be necessary to define a between the i the index . Indeed , it will be sufficient to note that the double index matching condition i , , which i and is equivalent to the single index condition . 
 the single index notation , the first term on the of is from , which 
  
 Consider the in different from all other 
 . Then 
  
  
 where a from , and the last equality from In , a is a consequence of the independence the fact that all pi have zero mean see . Proceeding established in , whereas directly from Lemma . similarly , it is easy to show that each term in the summation Proceeding similarly with the frequency dependent function of in which one or more is not to any T , we obtain other index to zero . Thus , one must only consider the 
 and , which yield , respectively , each of the following : 
 notation , we have 
 Substituting these for T and T into , by the result and into , 
 th moment delay profile of , and the single and joint effective arrival and , are as in 
 Proof of Lemma : Let E ,, with as in Definition . Recall the of i and from . If x , then 
  
 where Proposition in the Appendix been used . The last term in the latter expression is the difference between the rightmost term in and the squared of . the effective moment delay profile defined in , the of can be written as 
  
 With the change of 
 O , 
 one can write as , the proof . 
 Lemma : Let be an random process , with x possibly being equal to zero , and let be a random sequence i . i .. under . Then , for any given N 
  
 provided that the exist , where 
 E m , if x 
 x 
 f 
 E , , ,..., if x is random 
 where a from Lemma and and . Else 
  
 For the double sum in the lemma statement , if x , then 
  
  
 where a from and in the Appendix and from and . In turn , was the fact that and are bounded for . 
 Finally , if x is random , it readily by Lemma in the Appendix that 
  
 which the proof . 
 Lemma : Let be such that . Then 
 . 
 Proof : We have that 
 . 
 Then , since 
  
 Similarly 
 . 
  
 Lemma : Let the arrival times be an sequence with x randomly distributed . Define function as in . Then 
  
 Proof : We have that 
  
  
 where the discrete random variable to the number of in , . Its expectation 
 E 
  
  
 where the last inequality from Lemma . Thus 
  
 Hence 
 . 
 On the other hand 
 E . 
 Therefore 
 Substituting this result into , thus the proof . Lemma : Let the arrival times distribute as in 
 Lemma , and define , as in . Assume that , is bounded for all . Then 
  
 Proof : We have that 
  
 For the expectation within the first integral , we have 
  
 By dividing by and taking the limit as , and then substituting and Lemma , it that 
 . 
 On the other hand , for , we have 
 E 
 , 
 , 
  
  
  
  
  
  
 . 
 that , E and that the integrand in is symmetric , and proceeding with as in , we conclude that 
  
 Additionally , since 
 , , E 
 it from that 
  
 The proof is upon substituting this result into . Proposition : Let , be as in Definition , and let : be a bounded function . Then 
  
 Proof : If x is random , then , is bounded , and the result trivially . Otherwise , x , in which case 
  
 the proof . 
  
  
 ﻿ We present empirical on the achievable gains stemming from the use of wireless remote radio in a typical urban environment . Our work is based on simultaneous path loss of the base station and links to outdoor street level . We statistically characterize the increase in received power , when a is added to improve the coverage by a base station . We consider diverse coverage for the mobile terminal , the effect of position with respect to the intended . We also compare the power gains that would be in practice from combining the from the base with those of the , such as selection combining and maximum ratio combining . We conclude that under practical , the of will depend very strongly on the existence of line of sight links between the and the intended . For at low , below the clutter , only in a street canyon position with respect to the will obtain a benet . Our data also that the gains in noise ratio when maximum ratio combining are only marginally better than those of the much simpler selection combining . 
 I . INTRODUCTION 
 One of the great for today wireless communication is to provide adequate spatial coverage in a cost effective way , while and interference adequate for high frequency re use . To meet this challenge , during the last few there been a growing interest in the study of , relay and low complexity , . These may constitute relatively simple , low cost and easy to install when to the deployment of an additional base station to serve mobile within the cell . 
 While are around a small coverage base station with low transmit power connected to a wired network , a is connected to a through a wireless link , being able to repeat or re code the data by amplify or decode and forward , among . A repeater may be thought of as an amplify and forward relay with no or ability . In this context , there are two basic of : wireless , and remote radio , also known as ber , connected to a by an optical ber . 
 will suffer large and small scale fading at both and links . In contrast , for wireless , achievable gains will only depend on the quality of the connection , which it an attractive solution to provide connectivity to wireless in dense urban , where the links to the may experience shadowing and ber connectivity for the may be readily available . 
 Proper placement of a is obviously a fundamental factor in the compromise between the desired gains and the cost associated with its installation . In this regard , in an urban environment it is reasonable to expect that while coverage will improve with antenna height , this will at the same time have a negative impact on deployment and on channel interference , thus affecting frequency reuse in a large system , . If the is to cover an area of a size comparable to a small cell , then it is reasonable to assume that this is best by its antenna in normally used by base i . e above the clutter height . In such well established propagation such as those of and al . will be adequate to predict the coverage by the . Alternatively , they may be positioned at lower with the aim of in more limited , i . e . generating within a large cell . Coverage in growing interest as it can provide local in signal to noise ratio and thus higher data , . This may be without generating excessive interference in neighboring and at lower implementation when to a base station , , , particularly when below the surrounding clutter height . Numerous empirical have been for the statistical characterization of path loss in relatively radio links . Analytical based on optical geometry have also been for in urban , . Goldsmith al . , based on a collection of , a mathematical description of the radius of coverage of a . Such may be used to characterize the link portion of a repeater . al . based on outage capacity for a very short range outdoor indoor environment . In , the different of and in cellular , concluding that careful placement can improve capacity substantially by transferring from heavily loaded to lightly loaded . 
 The . Relay Task Group and the WINNER Consortium have the use of both empirical and theoretical to predict path for wireless links with transmission below the clutter of , also applicable to links . While the accuracy of for diverse been the subject of extensive , their use in wireless more than treating each link individually , since the joint statistics of and links may not correspond to those of independent random . Statistics for power gains stemming from the use of or based on simultaneous in urban , as here , have to the best of our knowledge not been . 
 In this work , we report on path loss of the and links in a typical outdoor urban environment . We evaluate diverse related to the performance improvement that can be when a in an area by a base station . We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user . Our will be useful to calculate the achievable in coverage and in transmission experienced by mobile . were carried out in an area within a range of of the , which is at lamp post height , considering that this type of setting will be typical for a practical deployment of low complexity . To avoid our statistics we randomly chose and for the at various from the . The maximum range was by the requirement that at all , particularly those , the received power would be within the by our channel sounding system . We note that our system was capable of measuring path that would considerably exceed those compatible with a typical link budget in a wireless network . We also considered two of . One was chosen so as to maximize the likelihood of to the and the other chosen in close proximity to the , but from direct street view by construction . In this way we were able to consider the effect of blocking a , as may occur when a surrounding construction is after the placement of the ber repeater . Thus our statistics allow us to quantify the of a for randomly mobile within a given range considering in coverage and in transmission . From the joint data of path loss for the and links we were able to compare the effectiveness of Selection Combining and Maximal Ratio Combining at the mobile terminal . To this effect we considered typical transmit and antenna gains at the and and calculated received signal power at the under the condition of equal noise power for both . It was found that the statistical gain of over is less than under all tested , due to the random power imbalance between the links . 
 Our also us to calculate between for the and links . The low validate an approach based on considering them as independent random . 
 . MEASUREMENT SCENARIO AND 
 A . Description of the Urban Environment Tested 
 The measurement campaign extended over a period of during summertime . The urban area used as test bed in a Mar , Chile , a mix of high rise and two story with ranging between and , built on a plane region at sea level . of received power at were carried out at street level . The area is by nearby that a transmitter at a location typical for a base station covering a relatively large urban area . The transmitter height was above the measurement region at a distance of to the position . The was mounted in two on the exterior of a high building . The streets in the measurement area are lined with with ranging between and . All links were non line of sight , with due to the surrounding . All were within a radius of centered at the low height . Regarding the distance of these to the , were within , between and , and between and , which us to also obtain range dependent statistics . A schematic description of the terrain and of our measurement scenario are in . and . 
 Base Station 
  
 m 
  
  
  
 . . . . . 
 Figure . Terrain 
  
 Figure . Placement of and 
 B . Measurement System 
 At the position , a continuous wave transmitter based on a oscillator at . with output power through a sector type antenna . The antenna used had azimuth and elevation at . All were within this . At the , the antenna used was a vertically dipole with gain , a signal at . with output power . Power at the were carried out an model NA spectrum analyzer that simultaneously tracked the at . and . . A noise of in a was low noise . The antenna used was a gain vertically dipole at a . height . The measurement system included a computer that acquired the spectrum analyzer at a rate of per second . Within the selected area all power the noise by at least . 
 C . Measurement Procedure 
 The was at an altitude of at two different : one at a street intersection so as to simultaneously illuminate two streets and the other one within of the placement but with no direct path to the adjacent streets . In the case there are and user , while in the second all are . The measurement campaign for both involved moving the along a straight path of about . at each of the that were selected at random within of the , as in Fig . . This was done a very slow moving vehicle in such a way that approximately equally spaced power were collected . At the frequency , this more than received power at spaced at least half a apart . From these we the path loss at each receiver position , which the calculation of average path loss and small scale fade statistics at each of the . 
 D . Data Analysis and Received Power Gain 
 To evaluate achievable power gains under realistic we calculated the received power at the mobile station assuming typical for transmit and antenna gains . the path for all measured we then the cumulative distribution function of the received power at the , considering the alone , the alone , the with the under a selection combining scheme and the with the considering a maximum ratio combiner . We assumed the existence of a with of transmission power . The and are assumed to be the same as the used in our with gains of and , respectively . For the we assumed that in most practical the would be attached to a wall and consequently only illuminate a sector of in azimuth . All our are in fact within a sector of such angle . We thus considered that a typical , sector antenna would be used , consistent with in . For the transmit at the we considered two that cover what could be considered the high and low of a practical deployment : and . 
 To quantify the gains we dene as the increase in received power that can be to at a given availability level . Let be the received power at the . We denote the of power received from the alone as and the of the power received under as . The gain in received power when is then : 
  
 The same type of of course to the gain through . We will use to calculate , at various availability , the power gain resulting from the use of a . 
 . EMPIRICAL 
 In this section discuss the that can be when a low altitude height . We consider a random selection of within a radius of the . In order to verify that our selected urban do not deviate from those by for wireless , we our path loss with those by such . For the links between and , we found a good match to the COST model and to the alternative WINNER path loss model for the case in an operating frequency of . . With respect to these , our measured were slightly higher , and on average , with ... of and respectively . The measured for the street canyon links were far lower than those for links at the same distance , from to , with an average of . They were however higher by about on average than those by the advanced model for Type with both and below . For the collection of mobile that we considered , we also empirical for both and links according to a log distance model , with free space at . The resulting were and log respectively . We note that most of our links were partially by relatively large . To evaluate the effect of choosing a position , we considered two . One had an uncluttered path to two streets , while the other was in close proximity but by construction . The placement is here as Non and the second as O . The position links to some of the chosen . For a setting such as this , where both are below clutter , links are as being in a . Correspondingly , we use this notation and refer to the links as non street canyon . In the radius , half of the links are of the street canyon type while all O links are non street canyon . When considering a radius all links to the become of type . This according to the link is at which can be in practice to benet most from a given placement . 
 As an example of the achievable gains we present in Fig . the of the received power at the within the radius , for the case where the at for both . All other are as in section . As seen , the for and are virtually indistinguishable , the shift being less that . We observe that the resulting from the use of a very much depend on its placement . For example , at coverage , the will guarantee a power of at least . an will only provide a slight improvement , the same power to of , while an will increase this coverage to . this calculation under the assumption of a transmit power of in coverage to and respectively . In what we summarize the power gains for various the importance of proper placement of the with respect to the intended . 
  
 Figure . of received power at for coverage radius and power . height was . 
 A . Gains as Function of Coverage Radius 
 To evaluate the effect of coverage range we divided the where the power were carried out into three with of up to , and , to the . Table I the power gain as in with a at three availability for two transmit and the two . We recall that for radii of or less the placement street canyon type links to all , while only of the are in that condition for the radius . The consequence , clearly in the table , is that the gains available to of the sharply when the coverage radius is reduced to , i . e . when all of them are in a street canyon with respect to the . Both Fig . 
  
 Figure . of received power at street canyon for coverage radius and power . 
 and Table I illustrate the importance of proper of the . We further explore this below . 
 B . Gains for Street Canyon and Non Street Canyon Links 
 Based on our above we divided the for all links with up to length into street canyon and canyon , rather than by distance as before . Table the for the power gain under this , again the previously detailed in section . The illustrate that very considerable power gains are available for street canyon links , even at low . In contrast , the row corresponding to the non street canyon links very modest gains regardless of the position . The latter were considering only that are of non street canyon type under both , so that are based on a common set of data . 
 To illustrate the gains achievable under the most favorable and , we present in Fig . the for received power considering all street canyon within the range of , at a power of . When this with Fig . , it becomes clear that the street canyon are those that contribute most to the in received power . For example the will provide at least to of . This same minimum power will be available to of if a is included . this for an assumed transmit power coverage to virtually . 
 Finally we calculated correlation of for the and links , for O and as 
  
 where and represent respectively the path , the small scale , from the and 
 Table I 
 SELECTION COMBINING GAIN FOR A LOW ALTITUDE . 
  
 Table 
 SELECTION COMBINING GAIN , FOR STREET AND NON STREET CANYON . 
  
 the to a position . E and E are the statistical of these path at the distance under consideration , from a linear regression of path loss . distance . and are the standard of the with respect to these . 
 In both O and these were found to be below . . When small scale are out , this to . . 
 . 
 Our empirical suggest that a important gains in received power even when as low as and operating at the relatively low transmit power of . However , these gains are basically associated with the existence of or , equivalently , street canyon links to , and thus heavily depend on site and user . In all , the gains achievable are only marginally than those by a , to the fact that , in practice , the likelihood of the and the base providing comparable is very small . 
 Virtually no correlation . was between the of the user link and the base user link . This treating them as independent random when modeling a wireless system by , the appropriate statistical description of the individual links . 
  
  
 ﻿ This paper discrete time control subject to average data rate . We focus on a situation where a noisy linear system been designed assuming transparent feedback and , due to implementation , a source scheme with unity signal transfer function to be in the feedback path . For this situation , and by on a class of source built around entropy , we develop a framework to deal with average data rate in a tractable manner that from both information and control . As an illustration of the of our framework , we apply it to study the interplay between stability and average data in the considered architecture . It is shown that the class of can achieve mean square stability at average data that are , at most , . per sample away from the absolute minimum rate for stability established by and . This rate penalty is by the simplicity of our approach . 
 Index Average data rate , control , perfect reconstruction , signal to noise ratio . 
 I . INTRODUCTION 
 P 
 control often use nontransparent communication links and , thus , communication arise . Such include random , data loss and data rate quantization , , and . This paper on average data rate . 
 It might be that the communication capacity of modern is in general sufficiently large , so as to make quantization irrelevant see , e .., and . However , there exist where the communication assigned to a particular relevant control signal are limited and , hence , quantization effects become important . Quantization is a highly nonlinear operation on and , accordingly , it is hard to analyze . 
 In control theory , quantization usually been as an undesirable effect that should be for see , e .., . This in contrast to the by information theory , where quantization is considered as an integral part of see , e .., and . This line of reasoning recently been brought to the control arena see , e .., , , , , and . An alternative view of quantization , to , also been inside the control community ; see , e .., , , , and . 
 In a discrete time control framework , a key problem is being able to characterize the minimal average data rate in , e .., per sample that one to achieve a given control objective . In the context of noiseless digital , i . e ., that transmit data without or at a constrained rate , this question is related to rate distortion theory i . e ., source ; see , e .., , , , and . The associated design problem is how to quantize a signal , with the average data rate , whilst a degree of fidelity or performance . A typical performance measure is the mean square error , but other are also possible . For instance , discrete to address black and white control such as , e .., and observability . 
 Standard in information theory and in particular in rate distortion theory rely upon arbitrarily long which incur arbitrarily long time . In addition , most of the general on rate distortion theory do not take stability nor causality into account , . It , thus , becomes clear that standard rate distortion theory is not useful to deal with control . Some progress been made in the information theory community towards a causal rate distortion theory , but most use that , even though causal , allow for arbitrary , . Only recently , established upper on the zero delay causal rate distortion function for stationary . However , the best provided in are of algorithmic nature , and derived for open loop . Thus , stability are not in . This is also the case of the in , where sequential quantization of is . 
 . 
 The discussion in the previous paragraph the work in , , and specially relevant , even though the focus in those works only on stability . The first that pointed out that there a data rate under which memoryless control cannot keep the state of a noiseless plant bounded were in and . The were later extended in and and with memory , and adaptive scaling so and . A landmark result was in , where the focus on noisy plant subject to mild on the noise statistics . It was shown in that it is possible to find causal , and such that the resulting closed loop system is mean square stable if and only if the average data rate in per sample , say , 
  
 where the th unstable plant pole . The above result a fundamental limitation in control closed over digital , when the problem of interest is mean square stabilization . similar to arise as to different e .., observability , deterministic stability and under different on the and see , e .., , , , , and . Indeed , the quantity on the right hand side of is a fundamental measure of the difficulty of a system , as in and . 
 All known to date that achieve stability at average data arbitrarily close to use complex nonlinear time that , in principle , have infinite memory . The consideration of with limited or no memory is much more involved and no explicit are currently available see , also , , Section . An alternative simpler approach been in , although for the scalar plant case only . 
 Almost all the work to above on stability only . A performance approach been in and . In that work , for separation and certainty equivalence have been in the context of quadratic stochastic for fully with data rate in the feedback path . If the a specific recursive structure , then certainty equivalence and a quasi separation principle hold . This result is interesting , but does not give a computable characterization of the optimal . A similar drawback is by the in . In that work , performance related are expressed in of the so sequential rate distortion function a rate distortion function with causality , which is difficult to compute in general . For fully first order , an expression for the sequential rate distortion function . However , it is not clear from the in whether or not the sequential rate distortion function is tight see , Section . Related work can be found in , where estimation are . 
 The main contribution of this paper is a novel , though restricted , bridge between information theory and control theory . The link is restricted it for a specific class of based on entropy see , e .., , , and . Nevertheless , the link is useful , one to address control system design subject to average data rate in a systematic manner . Our approach is constructive and based upon standard building . As such , it on average data that are to be achievable with conceptually simple . An additional feature of our approach is that it does not rely on asymptotic e ., high rate or high vector . 
 As both a motivation for our approach , and also to illustrate a , we linear system been designed assuming transparent feedback and , due , a source unity signal transfer function to be in the feedback path . For this situation , we discuss how to obtain on the minimal average data rate that one to attain a certain performance level , and also provide a detailed characterization of the interplay between stability and average data . It is shown that the class of can achieve mean square stability at average data that are to be at most . per sample away from the absolute minimum in . This rate penalty is by the simplicity of our approach . 
 A key result in the paper is that , when the class of source is employed , average data rate can be enforced by imposing signal to noise ratio in a related additive noise channel see , also , and . Our , thus , establish a formal relationship between and average data in noiseless digital . As such , our work goes beyond and where no such relationship is . Early of the in this paper can be found in and . 
 The remainder of the paper is organized as . Section notation . Section the setup considered in the paper . Section a lower bound on average that the remainder of the paper . class of source considered in the paper and average data rate to . Section on the interplay between stability and average data rate . Section . For ease of reference , the Appendix basic information theoretic . 
 . NOTATION 
 stand for the , the non negative , 
 the strictly positive , and the non negative , respectively . the magnitude of the complex scalar the conjugate transpose of the matrix is the set of all strictly proper and stable real rational transfer , and is the set of all stable , and minimum phase real rational transfer . The usual norm in is written 
 . If is an asymptotically wide sense stationary process , then denote its stationary variance , its stationary power spectral density , and the corresponding spectral factor , respectively . If is a discrete time signal , then is the th sample , and is shorthand for . 
 If is a family of , then . 
 If is a set that does not depend on any index , then 
 times , as usual . We write if and only if and are independent . We write , and form a chain see Appendix . and stand for the expectation and probability of , respectively . Definition of information theoretic and related notation is given in Appendix . 
  
 Fig . . closed over a digital channel . 
 . PROBLEM SETUP 
 This paper on the of Fig . , where is a given linear time invariant system such that 
 exogenous , is a signal related to closed loop performance , is a signal available for measurement , and is a control input . We assume that been designed so as to achieve satisfactory performance when . The feedback path , however , an error free digital channel . Hence , the quantization of the signal becomes mandatory . This task is by an that the sequence of binary . Once these are available at the end , a the signal that is fed back to . The situation above naturally if , for example , to the interconnection of an plant and an controller that been designed without taking into account data rate in the feedback path . 
 Throughout this paper we assume that the following . 
 Assumption . : 
 a is a proper real rational transfer function , single input single output and strictly proper , 
 and , then the feedback system of Fig . is internally stable and well . 
 The initial state of , say , is a second order random variable , and is a second order process with spectral factor . 
  
 Assuming that the loop is stable when is consistent with our setup where been designed supposing transparent feedback . Assuming to be strictly proper the well of the of Fig . for all causal between and . This assumption can be removed at the expense of additional care . However , removing the constraint of being single input single output , additional effort 
 for our approach to be useful . 
 The remainder of this paper at building a framework to study the interplay between the average rate at which the channel are , and the performance and stability of the of Fig . . To that end , we begin by first a general lower bound on average data in feedback . 
 . AVERAGE DATA RATE 
 A . Background 
 The use of digital communication the of . separation theorem that this process can be into two : source and channel see , also , . Source with the representation of continuous a countable alphabet and , as such , quantization . On the other hand , channel on the reliable and efficient communication of digital data over an underlying channel . We note that separation , and is useful , for point to point where causality and are not an issue . If causality are , then separation does not hold in general see , also , . Nonetheless , the study of causal source in isolation a key open problem in information theory , , . 
 The study of optimal source or quantization is the subject of rate distortion theory . Rate distortion theory does not take channel into account , and an digital link between the sending and , . In this paper , we adopt a purely source perspective and consider source whose output have a variable instantaneous length , but a bounded average length see , also , . We note , however , that bounded average data does not guarantee bounded instantaneous data , . for this to happen are in . 
 Without loss of generality , we consider source with the structure in Fig . . In that figure , is a , a reproduction , and the and form a lossless pair also entropy coder entropy pair ; see , e .., , Chapter . The continuously valued random into a countable set of . These are then by the into a countable set of prefix free binary that , in general , at every time instant . At the end , the the output from the binary by the , and the reproduction the back into real . A precise characterization of and is provided below . 
 B . General Source 
 In this paper we focus on single input single output within feedback , as in Fig . . Accordingly , we consider , reproduction and that are causal and , moreover , operate on a sample by sample basis , without delay . We also assume that side information is available at both the and sides . The new side information that becomes available at time instant is by , for the source , and by for the source . Such side information is in suitably defined and , where and . We also define the set 
 , which the common side 
  
 Fig . . General source scheme used within a feedback loop . 
 information that becomes available at both the and sides at instant . 
 In Fig . , the dynamic system is assumed to be such that its scalar output 
  
 where is a possibly time deterministic , is the initial state of , is the scalar input , and , with , is an exogenous random process . We also characterize the output of the via 
  
 where is the input to the , is as before , 
 is a possibly time deterministic , and is a fixed countable set . The are then used by the to construct the binary via 
  
 where , is as before , 
 is a time deterministic , and is a countable set of prefix free binary . The output of the is to the end assuming ideal digital communication consistent with the source point of view adopted in this paper . Once becomes available at the end , the via 
  
 where is a time that 
  
  
 for any . Condition the fact that the considered here operate in real time , without delay . Finally , the reproduction its output via 
  
 where is as before , and is a 
 possibly time deterministic . 
 Before the reception of are available at the side . It , thus , that the length 
 , measured in , of any binary description of the output symbol see , Chapter , , and 
  
 where conditional entropy see Appendix . The gap between both sides of on how efficient the is at . It is known that there exist such that 
  
 That is , the gap in is smaller than bit when suitable are employed e .., 
 . 
 In this paper we focus on the time average of . 
 Definition . : The average data rate of the source via 
  
 C . Lower on Average Data 
 We will now study a lower bound on that only on the joint statistics of the source input and its output . 
 This bound will play a key role in the remainder of this paper . 
 Our require the following assumption . 
 Assumption . : The and in Fig . are causal , by , and such that . 
 Assumption . can be thought as being a fairness assumption . Indeed , the that the source only past and present , and side information not related to the message being sent , to construct the current output value . In other , we assume that the channel is the only link from to . 
 Definition . : The source scheme by is said to have an invertible reproduction 
 in short , an invertible there a deterministic such that . 
 If a source scheme an invertible , then knowledge of is equivalent to knowledge of . 
 The generality of source with invertible is next . 
 Lemma . : Consider any source scheme by . Define . If 
 and the corresponding is not invertible , then there another causal source 
 nat . 
 For the gap is actually upper bounded by 
 , where is the conditional probability of the most likely symbol of the alphabet of , given . 
 scheme , with an invertible , such that and 
 . 
 Proof : Assume that the such that it been possible to use to recover from , for all such assumption is not at time instant . If at time instant cannot be inverted 
 i . e ., if there no deterministic such that 
 , then there exist such that . Denote 
 by the conditional probability of the output of being at time instant , given . Consider now another pair that like , except for the fact that instead of either or at time instant . At time instant , the value with conditional probability , given . Thus 
  
 from the definition of and that of entropy , from inequality , and from the definition of and that of . 
 By the above procedure until there are no two of that are into the same value at time instant , one a source scheme where knowing is equivalent to knowing and 
 The result upon the above for every . 
 Given , it from Lemma . that one can always focus on source with invertible , 
 without loss of any generality . 
 The next result will be used to prove the main result in this section . 
 Lemma . : Consider a source scheme inside a feedback loop , as in Fig . . If Assumption . and the is invertible , then the chain and , conditioned upon true . 
 Proof : Given , it from that there a deterministic such that . Since 
 , it immediately that and are independent upon knowledge of , thus proving our first claim . The second claim is immediate upon that upon . 
 We are now in a position to state the main result of this section . 
 Definition . : The directed mutual information rate across 
 or between 
  
 Appendix . 
 Theorem . : Consider a source scheme inside a feedback loop , as in Fig . . If Assumption . and the is invertible , then . 
 Proof : we have 
  
 where from Property in Appendix and the fact that , from the fact that the is invertible , from , the fact that the is invertible , and the second claim of Lemma . , and from and the first claim of Lemma . . The result now . 
 Theorem . that , when causality are , directed mutual information rate across a source scheme as a lower bound on the associated average . The result a physical quantity average data rate to an information theoretic quantity directed mutual information rate . Theorem . also that the appropriate information theoretic definition of average data in causal is the directed mutual information rate . However , showing that the , over all joint input and output that satisfy a causality constraint , of the directed mutual information rate across a source scheme an tight lower bound on the corresponding average data rate , remains an open problem see , also , . 
 To our knowledge , Theorem . , for the first time , a characterization of the relationship between directed mutual information rate and the operational rate of source within feedback . The result in the literature that is to Theorem . is , Theorem . However , that result is derived for entropy only see Section , as opposed to the general causal source 
  
 Fig . . a Independent source scheme and equivalent . 
 considered here . Related are , Lemma . . and , Theorem . . where feedback data are . However , those do not focus on operational data , and assume no feedback between the at the physical of the . 
 Other relevant and related works are and . In , the study fundamental directed mutual information across within feedback , and Bode like fundamental that arise due to finite capacity communication see , also , . On the other hand , a relationship between operational data and directed mutual information rate from a channel perspective . In that work , the show that the , over all joint channel input and output that satisfy a causality constraint , of the directed mutual information rate across a channel capacity with feedback for that channel . Despite all the work to above , no relationship besides that of Theorem . between average operational data and directed mutual information rate from a source perspective , and valid in general , is currently available in the literature . 
 V . A CLASS OF SOURCE 
 This paper at a bridge between control and information , when a specific class of source is employed . This section such class . 
 A . Independent Source 
 In order to obtain a simple yet useful framework for the study of the of Fig . , we will focus on the following class of source . 
 Definition . : A source scheme is said to be independent Assumption . , its reproduction is invertible , and the or quantization noise sequence , defined via 
  
 , where is a second order zero mean i . i .. sequence , finite differential entropy and the a deterministic initial state ; see Fig . a . 
 The class of independent source is restrictive . However , it is a sensible choice when data rate arise in that have already been designed to perform satisfactorily in the absence of quantization . In such , which include the situation of interest in this paper see Section , 
  
 Fig . . Considered closed over an independent source scheme . 
 it is desirable to introduce quantization effects in an additive fashion so as not to alter the nominal design see , also , . We will describe a practical independent source scheme in Section . 
 We begin our study of independent source by that the following . 
 Lemma . : Any independent source scheme can be written as shown in Fig . , where and are auxiliary , is as in Definition . , and 
 are auxiliary with deterministic initial , such that 
 . Moreover , in Fig . , 
 Proof : Our first claim upon 
 . To prove our second claim , we note that the on and imply that there exist deterministic , with and invertible , such that see 
 Fig . , 
 , and , 
 . Hence 
  
 The proof is upon in . 
 Since Lemma . , the system that when an independent source scheme is employed in the feedback loop of Fig . can be written as shown in Fig . . Note that the error free digital channel of Fig . is in the independent source scheme of Fig . . In Section , we will 
 make the channel explicit again . 
 A key feature of independent source is that the directed mutual information rate across them can be bounded by the directed mutual information rate that would arise if all random were by . To be precise , we introduce the following definition . 
 Definition . : Consider an system with random and random initial state . If is a state , input , or output variable of the system , then to the signal that would arise in the place of , when all and initial are by jointly random or the same first and second order cross , and the same statistical dependence , as in the original situation ; is the counterpart of . 
 Lemma . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
  
  
 where and are the auxiliary in Fig . see Lemma . , entropy rate , and relative entropy see Appendix . in hold and are . 
 Remark . : If the of Lemma . hold and , in addition , is jointly , then see , Chapter and 
  
 Thus , if is jointly , then is by choosing to be . 
 The relevance of Lemma . in that the characterization of directed mutual information rate under is straightforward . 
 Theorem . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
  
 to as the stationary signal to noise ratio of the scheme . 
 Proof : Proceeding as in the proof of Lemma . see we conclude that 
  
 . the proof . 
 Theorem . explicit upper on the directed mutual information rate across any independent source scheme in a stable and causal feedback loop . These are , essentially , expressed in of the spectral of the auxiliary and in the scheme of Fig . . Interestingly enough , there a one to one correspondence between the of an independent source scheme , and upper on the directed mutual information rate across it . Given Theorem . , we can infer that there a link between the of an independent source scheme and the associated average data rate . A precise characterization of such link will be given in Section . 
 We note that also a relationship between directed mutual information rate and Bode like , when . , Theorem . to feedback with arbitrary disturbance and initial state . Indeed , , Theorem . can be from the inequality in upon assuming to be jointly distributed . 
 We end this section by showing that , for any given independent source scheme , there another independent source scheme , with the same noise color and the same directed mutual information rate across it , such that the gap between the right hand side of and can be made arbitrarily small . 
 Theorem . : Consider the of Fig . , where the source scheme is independent and a fixed noise source . Suppose that Assumption . and define Since and , we have from the Bode integral Theorem and the definition of that 
 . 
 . Proceeding as in , proof of Lemma ,. , we conclude that the last term in can be made arbitrarily small with 
 . Our last claim now from the last claim of Theorem . . If for some , then is not admissible since it and , thus , the representation of the independent source scheme of Fig . would be internally unstable . 
 B . Entropy 
 Entropy are that have convenient that make them suitable for use as building when dealing with average data rate , , and . In particular , we show below that can be used to construct the noise source that an independent source scheme . 
 The structure of an is shown in Fig . . In that figure , is a dither signal which is assumed available at both the sending and , the pair is as in 
  
 where is the quantization step a designer choice . The output of the 
  
  
 Fig . . Entropy . 
  
 Fig . . Entropy inside a feedback loop . 
 where , and the output of the 
 is given by 
  
 quantization noise as an independent i . i .. source is common in the signal literature . In general , this model is not exact , but becomes exact in when the dither is appropriately chosen see , e .., and . The next theorem that this key property remains valid when are in strictly causal feedback . 
 Theorem . : Consider the setup of Fig . , where the is as above and a finite quantization step . Assume that is a proper real rational transfer function , that the open loop transfer function from to is single input and strictly proper , that the closed loop system is internally stable and well when , that the signal is a second order process , and that the initial state of , say , is a second order random variable . If is such that , then 
 the noise is such that 
 . 
 Proof : Similar to the proof of Theorem in see , Chapter for . 
 Remark . : The definition of and that Theorem . irrespective of how the pair is chosen . In particular , it if the pair is ; see . It is also worth that , if the dither is not at the side , then only moment independence is . For the remainder of the paper , the following consequence of Theorem . is relevant . 
 Corollary . : Consider the system of Fig . with and as in Lemma . . If an , with dither chosen as in Theorem . and finite quantization step , is used as the link from to , then the system of Fig . becomes an independent scheme . 
  
 Fig . . Explicit of an independent source scheme that an as the link between and . 
 Proof : Given Lemma . and Theorem . , it to show that the resulting source scheme Assumption . and an invertible . Since in the present situation , 
 Assumption . is satisfied . Also , since , its initial state is deterministic and , by definition of , 
 and , we conclude that knowledge of is equivalent to knowledge of . The invertibility of 
 the thus , and the proof is . 
 When an is used as the link between and in the system of Fig . , the resulting scheme can be as in Fig . . Fig . . From that figure , it is clear that feedback from to the input of in Fig . does not require explicit feedback around the digital channel . Recall that we 
 consider error free digital . 
 Remark . : , and share a common source of randomness the dither . In principle , this that both the and must share information about the dither . This an additional degree of implementation complexity , but to the best of our knowledge , there is no other simple way of satisfying Assumption . . In practice , one can use synchronized number with the same . 
 Average Data in Independent Source That Use : We are now in a position to present an upper bound on the average data rate in independent source that use . We start with the following result . 
 Theorem . : Consider the of Fig . , where the source scheme is independent . Suppose that Assumption . , and that the link between the auxiliary and see Fig . and Lemma . is an with dither chosen as in Theorem . and finite quantization step . Then , and there an 
 pair such that . 
 Proof : By definition of 
 . Also , from Theorem . and its proof we have that and . the above , our first claim from the proof of Theorem in . The second claim from the first , and . A direct proof can be by the definition of , and the fact that knowledge of is equivalent to knowledge of see proof of Corollary . , to show 
 equality in all but the first inequality of . 
 Theorem . that inside independent source one to achieve average data that are close to the absolute lower established in Theorem . . The worst case gap , which in the inefficiency of the and is smaller than , is intrinsic to any scalar lossless coder and cannot be removed , unless one high rate regime ; , block entropy which may introduce unbounded ; , , or the scheme to operate in a nonstationary fashion by time , . In practice the gap may be smaller than ,. , . 
 A useful corollary of . and . is next . 
 Corollary . : Consider the setup and of Theorem . . There an such that 
  
 Remark . : Theorem . and Corollary . are only existence type . The implementation of inside outside the scope of this paper , and we refer the reader to , Remark . for related . 
 Corollary . a closed form upper bound on the average data rate in an independent source scheme that an . The bound is given in of spectral of the output , and two additional constant . The second term in , i . e ., per sample i . e ., per sample , to the divergence of the quantization noise distribution from and because generate uniform quantization noise not noise ; see , also , . This term can also be given an alternative interpretation in of the space filling loss when a finite dimensional instead of an infinite dimensional one , with spherical quantization . We refer the interested reader to , , and for further . As before , the term bit due to the inefficiency of the inside the . Interestingly , our result without on the external signal nor on the initial state . 
 Remark . : If the of Corollary . hold and , in addition , is , then a lower bound for in is given by the first term on the right hand side of . That is , becomes tight up to per sample see Remark . and Theorem . . Independent Source With Memoryless : So far , we have considered where the pair is to exploit all past and present , binary , and side information . Such have unrestricted memory and its implementation the knowledge of the conditional distribution of , given . That distribution can be difficult to characterize . In order to simplify implementation , it is common to consider without memory see , also , . 
 Definition . : An is said to be memoryless the associated pair is such that 
 and , for all . 
 When a memoryless , the can only exploit the knowledge of to encode . Thus , must be by . Again , it is possible to design such that compare with 
 We now present a definition and two that allow one to state the counterpart of Corollary . for the case of independent source that use memoryless . 
  
 Theorem . : Consider the setup and of Theorem . . If the is memoryless , then and there an pair such that 
 . 
 Proof : The result from the proof of Theorem in and . 
 Theorem . : Consider the of Fig . , where the scheme is independent . If Assumption . , then 
  
 where all are as in Lemma . and Theorem . . 
 Proof : The result proceeding as in the proof of Theorem . see in . 
 Corollary . : Consider the setup and of Theorem . . There a memoryless such that 
  
 Proof : Immediate from . and . . 
 Remark . : In analogy to Remark . , we note that if the of Corollary . hold , and is , then 
 becomes tight up per sample . 
 The inside a memoryless less information to encode than the inside an with unrestricted memory . As a consequence , average data achievable with memoryless will be , in general , than those achievable with that have unrestricted memory . This . and . , as inequality . However , the conclusion is pessimistic when are used inside independent source . In this case , Theorem . that the rate penalty , as measured by the gap between the right hand sides of and , when a memoryless instead of an with memory , can be made arbitrarily small if an appropriate choice for the auxiliary and is made . Hence , without being unduly conservative , it to study the to give upper on the average data rate of independent source , irrespective of whether they use with unrestricted memory or not . 
 C . Discussion 
 Consider the of Fig . when the source scheme is independent and an as the link between the auxiliary and see Fig . . For this setup , the of A one to restate control average data rate as control stationary . This one to design subject to average data rate in a systematic fashion that standard quadratic optimization . For instance , if performance is measured by of the stationary variance of the error signal , then , irrespective of whether the unrestricted memory or not , the minimal average data rate to achieve a performance level , say , see . and . 
  
 In , the optimization is carried out with respect to all , 
 , and that guarantee stability in an appropriate sense , and is the minimal stationary of the source coder that one to achieve the performance level , i . e ., 
  
 Once is which , in principle , is a standard quadratic control problem , one readily a bound on see , also , Section . 
 The framework provided here is conservative since it is based on that are not tight in general . However , the framework that are , by construction , to be achievable with conceptually simple building , and do not rely on any asymptotic approximation . As above , the framework also the door to use standard synthesis to deal with average data rate in control system design , as in . These are the main that distinguish our approach from the literature in Section I . 
 . EXAMPLE : MEAN SQUARE STABILIZATION 
 We now illustrate the approach outlined in on the minimal average data rate for stability , 
 i . e ., we consider in . The case of finite is due to space see . 
 A . Mean Square Stability Subject to 
 We start by the interplay between on the source coder and stability . By virtue of Theorem . , we focus on the feedback system of Fig . , and use the following notion of stability . 
 Definition . : Consider the linear system 
 , where , is the system state at time instant where is a second order random variable , and are constant matrices of appropriate , and is a second order process . The system is said to be mean square stable there exist finite and a finite and positive semi definite both not depending on , such that 
 small with a sufficiently large and cannot be made equal to zero since , by assumption ,. Well known , Lemma ,. allow one to conclude that there such that the gap in is arbitrarily small . The 
 result , thus The existence of the family also by the in . 
 Theorem . that , for independent source , the minimal that is compatible with , i . e ., 
 , is only a function of the unstable of . Hence , for any given , the condition is necessary 
 and sufficient to be able to find , and a noise variance equivalently , a quantization step such that the resulting is and the . 
 We conclude this section with a simple corollary of Theorem 
 . . 
 Corollary . : Consider the setup and of Theorem . . If , then the stationary variance of the error signal unbounded . 
 Proof : The result is immediate since , by Assumption . , and , by Theorem 
 We see from Corollary . that the study of on for are insufficient to give performance . This fact is unsurprising , and consistent with in and . 
 B . Mean Square Stability Subject to Average Data Rate 
 We now return to the of Fig . when the source scheme is independent and an . Given the definition of an independent source scheme , it that the notion of in Definition . is still valid in this setting . It is also clear that , provided Assumption . , 
 , and regardless of the pair , the considered is and . 
 Corollary . : Consider the setup and of Theorem . . Then , irrespective of whether the memory or not , the following . 
 a The minimal average data rate compatible with , i . e ., as defined in , 
  
 . 
 a Consider the case of with unrestricted memory . Equation and Theorem . yield 
  
 The upper bound in upon Theorem . in . The lower bound from , Theorem . . 
 The case of memoryless similarly . 
 For any , and sufficiently large and , 
 . Also , irrespective of the memory in the , there such that 
 see , and . Thus , 
 and the result 
 upon choosing . 
  
 Corollary . lower and upper on the minimal average data rate that is compatible with in the considered , when an independent source scheme is employed . A feature of our proposal is that with unrestricted memory do not provide any advantage over memoryless at least from a point of view ; see , also , discussion at the end of Section . This is relevant in practice since the implementation of with unrestricted memory is prohibitive . Indeed , in order to design an independent scheme that stability at an average data rate smaller than , it to use a memoryless with sufficiently large quantization step , an pair designed , and and that , for the situation studied in Section A , guarantee at sufficiently close to . By doing so , however , the performance of the will be see Corollary . . This is consistent with in and . 
 The of Corollary . show that , when an independent source scheme is employed in the of Fig . , can be at average data that are to be no than the absolute bound in and plus per sample . This 
 extra rate is , in our view , a fair price to be if one oneself to the conceptually simple source considered in this paper . We note however that the upper bound in is a worst case upper bound . As , in practice one can expect to achieve at no than see and . 
 Our can also be used to provide upper on the average data rate that is to achieve , when memoryless source are employed in the considered 
 . Indeed , if one , and a memoryless 
 , then the resulting source scheme no memory and it is easy to show that and the pair can be chosen so as to achieve at an average data rate satisfying 
  
 Relation can be with the of , Section even though on a different notion of stability . That work that there exist memoryless that guarantee stability with bounded but otherwise unknown data , whist a computable upper bound on the minimal average data rate compatible with . 
 . CONCLUSION 
 is designed assuming transparent feedback and , at a later design stage , a unity signal transfer function source scheme is tobe rate feedback path have on closed loop performance . To address this problem , we have on a class of source and , by doing so , we have established a bridge between information and control . A key result of our work is that , for the considered class of , average data rate can be enforced by imposing signal to noise ratio in a related additive noise communication channel . As an application of our , we studied the interplay between average data in the considered setup . For that problem , the class of was shown to achieve mean square stability at average data that are to be less than . per sample above the absolute minimum established in . 
 We refer the reader to and for of the framework in this paper to beyond stabilization . A key open problem not in this work is how to incorporate average data rate into control causal but otherwise unrestricted source . 
 APPENDIX 
 The following and are standard and can be found in . Unless otherwise stated , all are assumed to be random with well defined joint probability density or mass . The of is . to the conditional of , given . mean with respect to ; 
 for independent of . 
 The differential entropy of is defined via 
 . The conditional differential entropy of , given , is defined via . 
 equality 
 . 
 If is with finite variance , 
 . 
 If and are discrete , then we use to denote the conditional entropy of given . The are analogue to the continuous case . 
 . 
  
 Conditional mutual information between discrete is defined as in the continuous case . 
 The relative entropy between and or divergence of the distribution of with respect to that of is defined via . Given joint 
 and , the conditional relative entropy is defined via . 
 equality , 
 . 
 If is the counterpart of see Definition . , then . If , are the of then 
 . 
 If is uniformly distributed on , and is zero mean with variance , then 
 . 
 The form a chain 
 . If , conditioned upon then we write . 
 If is a deterministic function of , then . 
 If , then equality 
 . 
 If , then 
 equality as well . 
 The entropy rate of a stochastic process is defined via . A useful fact is the following . 
 If is an asymptotically process with stationary 
 , then equality , in addition , is asymptotically ; see , Lemma . . 
  
  
 ﻿ We show that the structural similarity index , which is used in image to assess the similarity between an image representation and an original reference image , can be as a locally quadratic distortion measure . We , furthermore , show that recent of Linder and on the rate distortion function under locally quadratic distortion are applicable to this distortion measure . We finally derive the high resolution and provide a simple method to numerically compute an approximation of the of real . 
  
 . Introduction 
 A vast majority of the work on source with a fidelity criterion i . e ., rate distortion theory on the mean squared error fidelity criterion . The fidelity criterion is used mainly due to its mathematical tractability . However , in a human observer it been noted that distortion which include some of human perception generally perform better than the . A great number of perceptual distortion are distortion and , unfortunately , even for simple , their corresponding rate distortion , that is , the minimum to attain a distortion equal to or smaller than some given value , are not known . However , in certain it is possible to derive their . For example , for a process with a weighted squared error criterion , where the are restricted to be linear time invariant , the complete was first found in and later by several , . Other include the special case of locally quadratic distortion for fixed rate vector and under high resolution , which are extended to variable rate vector in , , and applied to perceptual audio in , . 
 In , Wang al . the structural similarity index as a perceptual measure of the similarity between an image representation and an original reference image . The index into account the between the image and its representation as well as the first and second order . It been shown that this index a more accurate estimate of the quality than the . The index was used for image in and was cast in the framework of compression of and image in . The relation between the rate of a fixed rate uniform and the distortion measured by the index was first in . In particular , for several of source and under high resolution , upper and lower on the index were provided as a function of the operational rate of the . 
 In this paper , we present the high resolution for with finite differential entropy and under an index distortion measure . The is particularly important for and within the image area , since it a lower bound on the number of that any coder , for example and so forth , will use when an image into a representation , which an index not smaller than a level . Thus , it one to compare the performance of a architecture to the optimum performance theoretically attainable . The is and does not appear to admit a simple closed form expression . However , when the rate is high , that is , when each of the image is by a high number of , say more than . , then we are able to find a simple expression , which is asymptotically as the bit rate exact . For finite and small bit , our an approximation of the true . 
 In order to find the , we first show that the index can be as a locally quadratic distortion measure . We then show that recent of Linder and on the under locally quadratic distortion are applicable , and finally obtain a closed form expression for the high resolution . We end the paper by showing how to numerically approximate the high resolution of real . 
 . 
 In this section , we present an important result on rate distortion theory for locally quadratic distortion and also present the index . We will need these when proving our main , that is , and in Section . 
 . . Rate Distortion Theory for Locally Quadratic Distortion . Let be a realization of a source vector process and let be the corresponding reproduction vector . A distortion measure , is said to be locally quadratic if it a series i . e ., it of all in a neighborhood around the of interest and furthermore , if the second order of its series dominate the distortion asymptotically as corresponding to the high resolution regime . In other , if , is locally quadratic , then it can be written as , O y , where is an input dependent positive definite matrix and where to , the quadratic term i . e ., is . We use upper to the stochastic process generating a use to denote the differential entropy of , provided it . The determinant of a and E the expectation operator . 
 The for locally quadratic distortion and smooth was found by Linder and and is given by the following theorem . 
 Theorem see . Suppose , some mild technical see a in Section . A in 
 , then 
 n peD 
 R log n 
  
  
 , 
 where is the of in per block under distortion ,, and the differential entropy of . The distribution of image and image of natural can in general be sufficiently well by smooth , . Thus , the regularity of Theorem are satisfied for many naturally . 
 . . The Structural Similarity Index . Let , where . We define the following empirical : the sample 
 mean ni , the sample , and the 
  
 sample cross variance . We define and similarly . 
 The index studied in is defined as . 
 x , y C , 
 where , i , . The index between and , where positive close to indicate a small perceptual distortion . We can define a distortion measure as one minus the index , that is , 
 d , y C , 
 which between and and where a value close to a small distortion . The index is locally applied the image . Then , all block are to yield the index of the entire image . We treat each block as an dimensional vector where N . 
 . 
 In this section , we present the main theoretical of this paper . We will first show that , is locally quadratic and then use Theorem to obtain the for the index . 
 Theorem .,, as defined in , is locally quadratic . 
 Proof . See the appendix . 
 Theorem . The high resolution for the the distortion measure ,, defined in and where and EX , is given by 
 n 
 log peD 
 D 
  
 E log a log a 
 n 
 log , 
  
 where a and are given by 
  
 a C , 
  
 b n C C . 
 Proof . Recall from Theorem that , is locally quadratic . Moreover , the weighting matrix in , which is also known as a sensitivity matrix , is given by A . , see the appendix . In the appendix , it is also shown that is positive definite since a , a , for all , where a and are given by and , respectively . From A . , it that 
  
 E log a log a . 
 At this point , we note that the main technical for Theorem to be applicable is boundedness in the following sense : , 
 E log , and E trace and furthermore uniformly bounded third order partial of ,. The first two are satisfied by the of the Theorem . The next two follow since all of are bounded for all see the proof of Theorem . Moreover , due to the positive stabilization C and C , trace is clearly bounded . Finally , it was established in the proof of Theorem that the third order of , are uniformly bounded . Thus , the proof now simply by in . 
 . . the Rate Distortion Function . In this section we propose a simple method for the in practice based on real . Conveniently , we do not need to encode the in order to find their corresponding high resolution . Thus , the in this section as well as the in the previous are independent of any specific architecture . 
 In practice , the source statistics are often not available and must therefore be found from the image data . Towards that end , one may assume that the individual i Mi where i the image total number of in the image of the image constitute approximately independent of a vector process . In this case , we can approximate the expectation by the empirical arithmetic mean , that is , 
 M 
 i log a i 
 log a i i , 
 where a i and i that the a in and are used on the 
 Table : n E log log for some bit grey and block sizes N , , , and . 
 Image 
 Baboon . . . 
 Pepper . . . 
 Boat . . . 
 . . . 
 F . . . 
  
 Table : in dim or equivalently per for different bit grey and block sizes N , , and . 
 Image 
 Baboon . . . 
 Pepper . . . 
 Boat . . . 
 . . . 
 F . . . 
 x i . Several of n E log log are shown in Table , for various commonly considered in the image literature . 
 In order to obtain the high resolution of the image , according to Theorem , we also need the differential entropy of the image , which is usually not known a in practice . Thus , we need to numerically estimate , for example , by the average empirical differential entropy over all of the image . In order to do this , we apply the two dimensional on each of the of the image in order to reduce the correlation within the since the is an orthogonal transform , this operation will not affect the differential entropy . Then we use a entropy estimation approach to approximate the marginal differential of the within a . Finally , we approximate by the sum of the marginal differential , which the in Table . 
 . 
 In this section , we use the on the and measure the corresponding of the reconstructed . In particular , we use the coder implementation available via the function in . Then , we compare these operational to the information theoretic high resolution as in the previous section . We are interested in the high resolution region , which to small , i . e ., close to zero or equivalently large i . e ., close to one . Figure the high resolution for , below . , corresponding to above . . Notice that the rate becomes negative at large i . e ., small , which because the high resolution assumption is clearly not satisfied and the are therefore 
  
 Baboon 
 Boat 
 Figure : High resolution under the similarity measure , , for different and an block size . 
 not accurate . Thus , it does not make sense to evaluate the asymptotic of Theorem at large . 
 . Discussion 
 The information theoretic high resolution by Theorem a lower bound on the achievable minimum rate for a given distortion value . As in , the high resolution could require the use of optimal compounding , which may not be feasible in some . Thus , the of whether the in Theorem is achievable and how to achieve it , remain open . Nevertheless , we can obtain a loose estimate of how close a practical scheme could get to the high resolution by the operational performance of , for example , the . Figure the operational for the coder used on the image and block sizes of . For comparison , we have also shown the . It may be that the operational curve is up to above the corresponding a similar behavior is for the other four in the test set . 
 The gap between the and the operational based on as can be in Figure can be by the following . First , the coder at a frequency weighted rather than the index . Second , is a practical algorithm with reduced complexity and is therefore not optimal even for the weighted . Third , the differential entropy as well as the expectation of the log of the determinant of the sensitivity matrix are found based on a finite amount of image data . Thus , they are only of the true . Finally , the becomes exact in the asymptotic limit where the rate 
  
  
  
 Figure : Operational the coder on the image under the similarity measure , , for block size . For comparison we have also shown the high resolution thin line . 
 towards infinity i . e ., for small . At finite , it is an approximation . Nevertheless , within these , the numerical evaluation of the here that significant compression gains could be by an optimal image coder , at least at high rate . To obtain further insight into this question , the corresponding under distortion for the image is shown in Figure . We can see that the excess rate of with respect to the at high is not greater than . . This that a like algorithm at distortion could reduce at least a fraction of the bit rate gap seen in Figure . 
 It is interesting to note that , in the case , we have I , which that log . 
 Thus , the difference between the and the , under high resolution , is constant e .., independent of the bit rate . In fact , if the is measured per dimension , then the rate difference is given by the in Table , that is , n E log log . It that the is simply a version of the at high . Moreover , the gap between the the fact that , in general , a representation of an image which is optimal is not necessarily also optimal . 
 . 
 We have shown that , under high resolution , the for a range of natural under the commonly used index a simple form . In fact , the only upon the differential entropy of the source image as well as the value of a function of the sensitivity matrix of the image . Thus , it is independent of any specific 
  
  
 Figure : Operational the coder on the image under the distortion measure . For comparison we have also shown the high resolution thin line . The horizontal axes on the top and the bottom show the and , respectively . 
 architecture . Moreover , we also provided a simple method to estimate the in practice for a given image . Finally , we the operational performance of the image coder to the and by approximate numerical that potentially significant perceptual rate distortion could be by optimal . 
 Appendix Proof of Theorem 
 We need to show that the second order of the series of , are in the high resolution limit where . In order to do this , we show that the series of the zero and first order vanish whereas the of the second and third order are nonzero . Then , we upper bound the remainder due to , by its second order series . This upper bound is established via the third order partial of ,. We finally show that the second order decay more slowly towards zero than the remainder . 
 Let us define C C and C C and let h . It that , and we note that the second order partial with respect to and for any i ,, are given by 
 h g g 
 . 
 y 
 Clearly , where that the expression , y yis at the , . Since 
 xi , it is easy to show that , for all i . Thus , the of the first order of the series of , are zero . Moreover , it from A . that h 
 g , for all i ,. With this , and after some algebra , it can be shown that 
 h 
 if i , 
  
 if i . 
 A . 
 We now let denote the partial derivative respect to note that from 
 generalized product rule f . When at that , this and 
 g 
 to 
 g are both zero . For the third order of , 
 we have , for all i ,,, 
  
 . A . x 
 C 
 Moreover , if i and i , we obtain 
 g 
 s 
 , 
 A . 
 whereas if any two indices are equal , for example , i , we obtain 
 g x 
 xi 
 . 
 Finally , if i , we obtain 
 g 
  
 an dimensional ball of radius centered at , let , and let T be the second order series of , centered at i . e ., at . It that 
 T i , i , i , A . where is given by half the second order partial of ,, that is see A . , 
  
  
  
 B n C .... ... 
  
  
  
 n C ... ... 
  
  
 ... 
  
 n ,... 
  
 A . 
 which full rank and is well defined for . This can be as 
 B a I , A . 
 where I is the identity matrix , is the all matrix , 
  
 a C , A . 
  
 b n C C . A . 
 Thus , a and i a , i ,..., . Since is symmetric , the quadratic form is lower bounded by 
 min , A . 
 where min min i ni min a , a , which that is positive definite . 
 On the other hand , it is known from theorem that for any , the remainder R , where 
 R , T , A . 
 is upper bounded by 
 R i , A . 
 i ,, k 
 where 
 h 
 f , A . 
 that is , is upper bounded by the over the set of third order of the series of . Since for real , the are finite , and since , i , , it from A . A . that the third order are uniformly bounded therefore finite . Moreover , for all such that e , it A . , A . , and A . that 
 lim lim min A . 
  
 A . 
 min 
  
 , A . 
 where A . since i ,..., i , and the sum in A . over all possible of third order partial of a vector of length , that is , 
 i ,, n . Furthermore , A . . Finally , by use . from A . and the fact that i 
 the fact bounded by A . . Since the limit of A . and is zero , we deduce that the second order of the series of , are asymptotically as 
 y to . This the proof . 
  
 The work of . is by the Research Council for Technology and Production , Grant no . . The work of . is by the Project no . and the Project no . ACT . 
  
  
 ﻿We show that the structural similarity (SSIM) index, which is used in image processing to assess the similarity between an image representation and an original reference image, can be formulated as a locally quadratic distortion measure. We, furthermore, show that recent results of Linder and Zamir on the rate-distortion function (RDF) under locally quadratic distortion measures are applicable to this SSIM distortion measure. We finally derive the high-resolution SSIM-RDF and provide a simple method to numerically compute an approximation of the SSIM-RDF of real images.
  
 1.	Introduction
 A vast majority of the work on source coding with a fidelity criterion (i.e., rate-distortion theory) concentrates on the mean-squared error (MSE) fidelity criterion. The MSE fidelity criterion is used mainly due to its mathematical tractability. However, in applications involving a human observer it has been noted that distortion measures which include some aspects of human perception generally perform better than the MSE [1]. A great number of perceptual distortion measures are nondifference distortion measures and, unfortunately, even for simple sources, their corresponding rate-distortion functions (RDFs), that is, the minimum bitrate required to attain a distortion equal to or smaller than some given value, are not known. However, in certain cases it is possible to derive their RDFs. For example, for a Gaussian process with a weighted squared error criterion, where the weights are restricted to be linear time-invariant operators, the complete RDF was first found in [2] and later rederived by several others [3, 4]. Other examples include the special case of locally quadratic distortion measures for fixed rate vector quantizers and under high-resolution assumptions [5], results which are extended to variable-rate vector quantizers in [6, 7], and applied to perceptual audio coding in [8, 9].
 In [10], Wang et al. proposed the structural similarity (SSIM) index as a perceptual measure of the similarity between an image representation and an original reference image. The SSIM index takes into account the crosscorelation between the image and its representation as well as the images first- and second-order moments. It has been shown that this index provides a more accurate estimate of the perceived quality than the MSE [1]. The SSIM index was used for image coding in [11] and was cast in the framework of 1-compression of images and image sequences in [12]. The relation between the coding rate of a fixed-rate uniform quantizer and the distortion measured by the SSIM index was first addressed in [13]. In particular, for several types of source distributions and under high-resolution assumptions, upper and lower bounds on the SSIM index were provided as a function of the operational coding rate of the quantizer [13].
 In this paper, we present the high-resolution RDF for sources with finite differential entropy and under an SSIM index distortion measure. The SSIM-RDF is particularly important for researchers and practitioners within the image coding area, since it provides a lower bound on the number of bits that any coder, for example, JPEG, and so forth, will use when encoding an image into a representation, which has an SSIM index not smaller than a prespecified level. Thus, it allows one to compare the performance of a coding architecture to the optimum performance theoretically attainable. The SSIM-RDF is nonconvex and does not appear to admit a simple closed-form expression. However, when the coding rate is high, that is, when each pixel of the image is represented by a high number of bits, say more than 0.5bpp, then we are able to find a simple expression, which is asymptotically (as the bit rate increases) exact. For finite and small bit rates, our results provides an approximation of the true SSIM-RDF.
 In order to find the SSIM-RDF, we first show that the SSIM index can be formulated as a locally quadratic distortion measure. We then show that recent results of Linder and Zamir [7] on the RDF under locally quadratic distortion measures are applicable, and finally obtain a closed form expression for the high-resolution SSIM-RDF. We end the paper by showing how to numerically approximate the high-resolution SSIM-RDF of real images.
 2.	Preliminaries
 In this section, we present an important existing result on rate-distortion theory for locally quadratic distortion measures and also present the SSIM index. We will need these elements when proving our main results, that is, Theorems 2 and 3 in Section 3.
 2.1. Rate-Distortion Theory for Locally Quadratic Distortion Measures. Let x ? Rn be a realization of a source vector process and let y ? Rn be the corresponding reproduction vector. A distortion measure d(x, y) is said to be locally quadratic if it admits a Taylor series (i.e., it possesses derivatives of all orders in a neighborhood around the points of interest) and furthermore, if the second-order terms of its Taylor series dominate the distortion asymptotically as y ? x (corresponding to the high-resolution regime). In other words, if d(x, y) is locally quadratic, then it can be written as d(x, y) = (x - y)TB(x)(x - y) + O(x - y3), where B(x) is an input-dependent positive-definite matrix and where for y close to x, the quadratic term (i.e., (x - y)TB(x)(x - y)) is dominating [7]. We use upper case X when referring to the stochastic process generating a realization x and use h(X) to denote the differential entropy of X, provided it exists. The determinant of a matrix B is denoted det(B) and E denotes the expectation operator.
 The RDF for locally quadratic distortion measures and smooth sources was found by Linder and Zamir [7] and is given by the following theorem.
 Theorem 1 (see [7]). Suppose d(x, y) and X satisfy some mild technical conditions (see conditions (a)–(g) in Section II.A in
 [7]) , then
 	n	2peD
 Dlim?0R(D) + 2log2 n 
 (1)
 1
 = h(X) + 2 Elog2(det(B(X))),
 where R(D) is the RDF of X (in bits per block) under distortion d(x, y), and h(X) denotes the differential entropy of X. (The distribution of image coefficients and transformed image coefficients of natural images can in general be approximated sufficiently well by smooth models [14, 15]. Thus, the regularity conditions of Theorem 1 are satisfied for many naturally ocurring images.)
 2.2. The Structural Similarity Index. Let x, y ? Rn where n = 2. We define the following empirical quantities: the sample
 mean1))(x µxµ) (1(x/n-)µ)ni==-01(xxi, the sample varianceTx/(n-1))-(nµ2x/(n-sx21)), and the(1/(n -
 T x	x
 sample cross-variance sxy = syx  (1/(n - 1))(x - µx)T(y - µy) = (xT y/(n - 1)) - (nµxµy/(n - 1)). We define µy and sy2 similarly.
 The SSIM index studied in [10] is defined as.
 	SSIM	x, y   µ2x2+µxµµ2yy++CC11s2x2s+xys+y2 C+2C2,	(2)
 where Ci > 0, i = 1,2. The SSIM index ranges between -1 and 1, where positive values close to 1 indicate a small perceptual distortion. We can define a distortion “measure” as one minus the SSIM index, that is,
 	d	x, y  1 -  µ2x2+µxµµ2yy++CC11s2x2s+xys+y2 C+2C2,	(3)
 which ranges between 0 and 2 and where a value close to 0 indicates a small distortion. The SSIM index is locally applied to N × N blocks of the image. Then, all block indexes are averaged to yield the SSIM index of the entire image. We treat each block as an n-dimensional vector where n = N2.
 3.	Results
 In this section, we present the main theoretical contributions of this paper. We will first show that d(x, y) is locally quadratic and then use Theorem 1 to obtain the highresolution RDF for the SSIM index.
 Theorem 2. d(x, y), as defined in (3), is locally quadratic.
 Proof. See the appendix.	 
 Theorem 3. The high-resolution RDF R(D) for the source X under the distortion measure d(x, y), defined in (3) and where h(X) < 8 and 0 < EX2 < 8, is given by
 n
 limR(D) + 2log2(2peD)
 D ?0
 1
 = h(X) + 2E(n - 1)log2(a(X)) + log2(a(X) + b(X)n)
 n
 +  2log2(n),
 (4)
 where a(X) and b(X) are given by
 	1	1
 	a(X) = n - 1 · 2sx2 + C2 ,	(5)
 	1	1	1	1
 	b(X) = n2 · 2µ2x + C1 - n(n - 1) · 2sx2 + C2 .	(6)
 Proof. Recall from Theorem 2 that d(x, y) is locally quadratic. Moreover, the weighting matrix B(X) in (1), which is also known as a sensitivity matrix [5], is given by (A.8), see the appendix. In the appendix, it is also shown that B(x) is positive definite since a(x) > 0,a(x) + b(x)n > 0, for all x, where a(x) and b(x) are given by (5) and (6), respectively. From (A.9), it follows that
 Elog2(det(B(X)))
 (7) = E(n - 1)log2(a(X)) + log2(a(X) + b(X)n).
 At this point, we note that the main technical conditions required for Theorem 1 to be applicable is boundedness in the following sense [7]: h(X) <  < ,
 E[log2(det(B(X)))] < 8, and E(trace{B-1(X)})3/2 < 8 and furthermore uniformly bounded third-order partial derivatives of d(X,Y). The first two conditions are satisfied by the assumptions of the Theorem. The next two conditions follow since all elements of B(x) are bounded for all x (see the proof of Theorem 2). Moreover, due to the positive stabilization constants C1 and C2, trace{B(x)}-1 is clearly bounded. Finally, it was established in the proof of Theorem 2 that the third-order derivatives of d(X,Y) are uniformly bounded. Thus, the proof now follows simply by using (7) in (1).	 
 3.1. Evaluating the SSIM Rate-Distortion Function. In this section we propose a simple method for estimating the SSIMRDF in practice based on real images. Conveniently, we do not need to encode the images in order to find their corresponding high-resolution RDF. Thus, the results in this section (as well as the results in the previous sections) are independent of any specific coding architecture.
 In practice, the source statistics are often not available and must therefore be found empirically from the image data. Towards that end, one may assume that the individual vectors {x(i)}Mi=1 (where x(i) denotes the ith N × N subblock of the image and M denotes the total number of subblocks in the image) of the image constitute approximately independent realizations of a vector process. In this case, we can approximate the expectation by the empirical arithmetic mean, that is,
 M
 	Elog2(det(B(X))) ˜ M 1  i=1(n - 1)log2(a(x(i)))	(8)
 + log2(a(x(i)) + b(x(i))n),
 where a(x(i)) and b(x(i)) indicates that the functions a and b defined in (5) and (6) are used on the ith subblock
 Table 1: Estimated (1/2n)E[log2(det(B(X)))] + log2(N) values for some 512 × 512 8-bit grey images and block sizes n = N2,N = 4,8, and 16.
 Image	N = 4	N = 8	N = 16
 Baboon	-4.57	-4.77	-5.00
 Pepper	-3.16	-3.51	-4.12
 Boat	-3.66	-3.99	-4.45
 Lena	-3.13	-3.49	-4.08
 F16	2.83	3.14	3.65
 	-	-	-
 Table 2: Estimated (1/n)h(x) (in bits/dim or equivalently bits per pixel (bpp)) for different 512 × 512 8-bit grey images and block sizes n = N2, N = 4, 8 and 16.
 Image	N = 4	N = 8	N = 16
 Baboon	6.18	6.06	6.03
 Pepper	4.75	4.55	4.49
 Boat	5.10	4.92	4.88
 Lena	4.63	4.41	4.38
 F16	4.32	4.14	4.13
 x(i). Several estimates of (1/2n)E[log2(det(B(X)))]+log2(N) using (8) are shown in Table 1, for various images commonly considered in the image processing literature.
 In order to obtain the high-resolution RDF of the image, according to Theorem 3, we also need the differential entropy h(X) of the image, which is usually not known a priori in practice. Thus, we need to numerically estimate h(X), for example, by using the average empirical differential entropy over all blocks of the image. In order to do this, we apply the two-dimensional KLT on each of the subblocks of the image in order to reduce the correlation within the subblocks(since the KLT is an orthogonal transform, this operation will not affect the differential entropy.) Then we use a nearestneighbor entropy-estimation approach to approximate the marginal differential entropies of the elements within a subblock [16]. Finally, we approximate h(X) by the sum of the marginal differential entropies, which yields the values presented in Table 2.
 4.	Simulations
 In this section, we use the JPEG codec on the images and measure the corresponding SSIM values of the reconstructed images. In particular, we use the baseline JPEG coder implementation available via the imwrite function in Matlab. Then, we compare these operational results to the information theoretic estimated high-resolution SSIM RDF obtained as described in the previous section. We are interested in the high-resolution region, which corresponds to small d(x, y) values (i.e., values close to zero) or equivalently large SSIM values (i.e., values close to one). Figure 1 shows the high-resolution SSIM-RDF for d(x, y) values below 0.27, corresponding to SSIM values above 0.73. Notice that the rate becomes negative at large distortions (i.e., small rates), which happens because the high-resolution assumption is clearly not satisfied and the approximations are therefore
  
 	Baboon	  Lena
 PepperF16 Boat
 Figure 1: High-resolution RDF under the similarity measure d(x, y) = 1 - SSIM(x, y) for different images and using an 8 × 8 block size.
 not accurate. Thus, it does not make sense to evaluate the asymptotic SSIM-RDF of Theorem 3 at large distortions.
 5.	Discussion
 The information-theoretic high-resolution RDF characterized by Theorem 3 constitutes a lower bound on the operationally achievable minimum rate for a given SSIM distortion value. As discussed in [17], achieving the high-resolution RDF could require the use of optimal compounding, which may not be feasible in some cases. Thus, the questions of whether the RDF obtained in Theorem 3 is achievable and how to achieve it, remain open. Nevertheless, we can obtain a loose estimate of how close a practical coding scheme could get to the high-resolution SSIM-RDF by evaluating the operational performance of, for example, the baseline JPEG. Figure 2 shows the operational RDF for the JPEG coder used on the Lena image and using block sizes of 8 × 8. For comparison, we have also shown the SSIM-RDF. It may be noticed that the operational curve is up to 2bpp above the corresponding SSIM-RDF (a similar behavior is observed for the other four images in the test set).
 The gap between the SSIM-RDF and the operational RDF based on JPEG encoding as can be observed in Figure 2 can be explained by the following observations. First, the JPEG coder aims at minimizing a frequency-weighted MSE rather than maximizing the SSIM index. Second, JPEG is a practical algorithm with reduced complexity and is therefore not ratedistortion optimal even for the weighted MSE. Third, the differential entropy as well as the expectation of the log of the determinant of the sensitivity matrix are empirically found—based on a finite amount of image data. Thus, they are only estimates of the true values. Finally, the SSIM-RDF becomes exact in the asymptotic limit where the coding rate
  
 SSIM-JPEG
   SSIM-RDF
 Figure 2: Operational RDF using the JPEG coder on the Lena image under the similarity measure d(x, y) = 1 - SSIM(x, y) for block size 8 × 8. For comparison we have also shown the high-resolution SSIM-RDF (thin line).
 diverges towards infinity (i.e., for small distortions). At finite coding rates, it is an approximation. Nevertheless, within these limitations, the numerical evaluation of the SSIMRDF presented here suggests that significant compression gains could be obtained by an SSIM-optimal image coder, at least at high-rate regimes. To obtain further insight into this question, the corresponding RDF under MSE distortion (MSE-RDF) for the Lena image is shown in Figure 3. We can see that the excess rate of JPEG with respect to the MSE-RDF at high rates is not greater than 1.4bpp. This suggests that a JPEG-like algorithm aimed at minimizing SSIM distortion could reduce at least a fraction of the bit rate gap seen in Figure 2.
 It is interesting to note that, in the MSE case, we have B(x) = I, which implies that log2(|det(B(x))|) = 0.
 Thus, the difference between the SSIM-RDF and the MSERDF, under high-resolution assumptions, is constant (e.g., independent of the bit-rate). In fact, if the MSE is measured per dimension, then the rate difference is given by the values in Table 1, that is, (1/2n)E[log2(det(B(X)))] + log2(N). It follows that the SSIM-RDF is simply a shifted version of the MSE-RDF at high resolutions. Moreover, the gap between the curves illustrates the fact that, in general, a representation of an image which is MSE optimal is not necessarily also SSIM optimal.
 6.	Conclusions
 We have shown that, under high-resolution assumptions, the RDF for a range of natural images under the commonly used SSIM index has a simple form. In fact, the RDF only depends upon the differential entropy of the source image as well as the expected value of a function of the sensitivity matrix of the image. Thus, it is independent of any specific
  
 MSE-JPEG MSE-RDF
 Figure 3: Operational RDF using the JPEG coder on the Lena image under the MSE distortion measure. For comparison we have also shown the high-resolution MSE-RDF (thin line). The horizontal axes on the top and the bottom show the PSNR and MSE, respectively.
 coding architecture. Moreover, we also provided a simple method to estimate the SSIM-RDF in practice for a given image. Finally, we compared the operational performance of the baseline JPEG image coder to the SSIM-RDF and showed by approximate numerical evaluations that potentially significant perceptual rate-distortion improvements could be obtained by using SSIM-optimal encoding techniques.
 Appendix Proof of Theorem 2
 We need to show that the second-order terms of the Taylor series of d(x, y) are dominating in the high-resolution limit where y ? x. In order to do this, we show that the Taylor series coefficients of the zero- and first-order terms vanish whereas the coefficients of the second- and third-order terms are nonzero. Then, we upper bound the remainder due to approximating d(x, y) by its second-order Taylor series. This upper bound is established via the third-order partial derivatives of d(x, y). We finally show that the second-order terms decay more slowly towards zero than the remainder as y tends to x.
 Let us define f  ((2µxµy + C1)/(µ2x + µ2y + C1)) and g  ((2sxy + C2)/(sx2 + sy2 + C2)) and let h = f g. It follows that d(x, y) = 1 - h and we note that the second-order partial derivatives with respect to yi and yj for any i, j, are given by
 	?2h	?2 f	?2g	?f ?g	?f ?g
 	= g	+ f	+	+	.
 	?y ?y	?y ?y	?y ?y	?y ?y	?y ?y
 Clearly f |y=x = g|y=x = 1, where (·)|y=x indicates that the expression (?µy/?yi = 1/n,?s·y2)/?yis evaluated at the pointi = (2/(n-1))(yi-µy), andy =?syxx. Since/?yi =
 (1/(n - 1))(xi - µx), it is easy to show that ?f/?yi|y=x = ?g/?yi|y=x = 0, for all i. Thus, the coefficients of the zeroand first-order terms of the Taylor series of d(x, y) are zero. Moreover, it follows from (2 f/?yi?yj|y=x + ? A.1) that ?2h/?yi?yj|y=x =
 ? 2g/?yi?yj|y=x, for all i, j. With this, and after some algebra, it can be shown that
  ?2h 
 ?yi?yj	y=x ?????	x	-	x if i=/ j,
 =
 ????if i = j.
 (A.2)
 We now let h(m) denote the mth partial derivative of h with respect to some m variables and note that from Leibniz
 generalized product rule [(1) f (2) + 3(3)g(2) f (1) + gf(3)(3)18f|y. When evaluated at=]xit follows that+ g(3)|y=x sinceh(3)fy(1)==|yg f=xx, this(3)and+
 3g
 reduces to h |y=x =
 g(1)|y=x are both zero. For the third-order derivatives of f ,
 we have, for all i, j,k,		
 ?3 f	12	µ
  	  	 2 .	(A.3) x
 	?yi?yj?yk	y=x	n	2µ2 + C1
 Moreover, if i=/ j =/ k and i=/ k, we obtain
 	?3g	4	1
 	?yi?yj?yk y=x = -n(n	 1)2	2s2 + C  2
 ×	xix ,
 (A.4)
 whereas if any two indices are equal, for example, i=/ j = k, we obtain
  ?3g   µx ?yi?yj?yj y=x2
 	4	xi	µx (1	1/n)
  .
 Finally, if i = j = k, we obtain
 	 ?3g		 12	µx (1  1/n)
 	?yi?yi?yi	y=x	 	2
 Let B be an n-dimensional ball of radius  centered at x, let ? = y - x, and let T2(?) be the second-order Taylor series of d(x,x + ?) centered at x (i.e., at ? = 0). It follows that
 T2(?)  -12   i,j ?2?yh	i?yx, yj  y=x?i?j = ?TB(x)?,	(A.7) where B(x) is given by half the second-order partial derivatives of d(x, y), that is (see (A.2)),
 ?1 ··· 1?
 1
  
 B(x) = n2 2µx2 1+ C1 ?????1... ···...	1...?????		
 	? -1	n -1 1
 	1	1
 -
 	n 2sx2 + C2 ???????????nn --11... 11	n--1...11
  	···
 ···
 ...
 ···	1
 n -1 1????????????, n -... 1
 -1
 (A.8)
 which has full rank and is well defined for 1 < n < 8. This can be rewritten as
 	B(x) = a(x)I + b(x)J,	(A.9)
 where I is the identity matrix, J is the all-ones matrix,
 	1	1
 	a(x) = n  - 1 2sx2 + C2 ,	(A.10)
 	1	1	1	1
 	b(x) = n2 2µ2x + C1 - n(n - 1) 2sx2 + C2 .	(A.11)
 Thus, B(x) has eigenvalues ?0 = a(x) + b(x)n and ?i = a(x), i = 1,...,n - 1. Since B(x) is symmetric, the quadratic form ?TB(x)? is lower bounded by
 	?TB(x)? = ?min?2,	(A.12)
 where ?min = min{?i}ni=-01 = min{a(x) + nb(x),a(x)} > 0, which implies that B(x) is positive definite.
 On the other hand, it is known from Taylor’s theorem that for any y ? B, the remainder R2(?), where
 	R2(?)  d(x,x + ?) -T2(?),	(A.13)
 is upper bounded by
 	R2(?) < f  ?i?j?k,	(A.14)
 i,j,k
 where
 		?3h	
 	f = ysup	k ,	(A.15)
 that is, f is upper bounded by the supremum over the set of third-order coefficients of the Taylor series of h. Since for real images, the pixel values are finite, and since Ci > 0, i = 1,2, it follows from (A.3)–(A.6) that the third-order derivatives are uniformly bounded and f is therefore finite. Moreover, for all ? such that ?2 = e, it follows using (A.7), (A.12), and (A.14) that
 ?lim?0  RT22((??)) = ?lim?0 	 ?min? 3n3f	(A.16)
 3
 	  	(A.17)
 ??0 ?min ?2
 n3f
  	0,	(A.18)  	 
 where (A.16) follows since |?i?j?k| = maxi?{1,...,n}|?i|3, and the sum in (A.14) runs over all possible combinations of third-order partial derivatives of a vector of length n, that is,
 i,j,k 1 = n3. Furthermore, (3 < ?A.173. Finally, () follows by use ofA.18) follows from(A.12) and the fact that |?i|
 the fact that f is bounded by (A.15). Since the limit of (A.18) exists and is zero, we deduce that the second-order terms of the Taylor series of d(x, y) are asymptotically dominating as
 y tends to x. This completes the proof.
 Acknowledgments
 The work of J. Østergaard is supported by the Danish Research Council for Technology and Production Sciences, Grant no. 274-07-0383. The work of M. Derpich is supported by the FONDECYT Project no. 3100109 and the CONICYT Project no. ACT-53.
 
 ﻿In [1, Theorem III.6] it is claimed that, for a one-sided random source x , the 
 search for the non-anticipative (i.e., causal) rate distortion function can be restricted to reconstructions y81 which are jointly stationary with x81 . In this technical report we show that the proof of [1, Theorem III.6] is invalid because it relies on [1, Theorem III.5], the proof of which, as we also show, 
 The manuscript [1] utilizes [2, Theorem 4] to prove the claim that, for one-sided sources x81 , the non-anticipative (i.e., causal) rate-distortion function can be realized by a reconstruction process y81 which is jointly stationary with x81 . To do so, it relies on [1, Theorem III.5]. 
 In this note we argue that the proof of [1, Theorem III.5], and hence that of [1, Theorem III.6], are flawed. For that purpose, we will first recall the assumptions and definitions utilized in [2]. After that, we will present the definitions introduced in [1] and show, under the conditions stated there, the requirements needed by [2, Theorem 4] (the basis of [1, Theorem III.6]) of are not met. 
 Throughout [2], the search in the infimizations associated with various types of “nonanticipatory” (i.e., causal) rate-distortion functions is stated over sets of joint probability distributions between source and reconstruction (as opposed to the usual definitions, in which the search is over conditional distributions, see [3, Chapter 10], [4]). Since the distribution of the source is given, it is required that for every k2 > k1 ? Z, all the joint distributions   to be considered yield xkk21 having the 
 same (given) distribution of the source for the corresponding block, say  . This requirement can be formalized as requiring that  , for a set of admissible joint distributions Pk1,k2 
 where  and   are, respectively, the alphabets to which   and ykk21 belong, and  is a 
 s-algebra over . In [2], this admissibility requirement is embedded in the definition of the sets of distributions which meet the distortion constraint, described next. 
 The fidelity criterion for every pair of integers1 k1 = k2 is expressed in [2] as requiring  to belong to a non-empty set of distributions (hereafter referred to as distortion-feasible set) WDk1,k2, a condition written as  . In this definition, the number D = 0 represents an 
 admissible distortion level. Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation. 
 As mentioned above, the admissibility requirement   is expressed in the distortionfeasible sets in [2, eqn. (2.1)]. The latter equation can be written as 
 In [2, eqs. (2.4) and (2.5)], the distortion-feasible sets are assumed to satisfy the “concatenation” condition 
 With this, [2, eqn. (2.9)] defined the “nonanticipatory epsilon entropy” of the set of distributions2 
 are satisfied. Then [2, eq. (2.13)] defines the “nonanticipatory message generation rate” as 
 The analysis in [2] considered both dicrete- and continuous-time processes, but here we only refer to the discrete-time 
 The actual term employed in [2] is “nonanticipatory epsilon entropy of the message” where the term “message” 
 An alternative “nonanticipatory message generation rate” is also considered in [2] by defining the set of distortion-admissible process distributions WD as follows: 
 Definition 1. The set (WD) consists of all two-sided random process pairs   for which there exist integers ··· < k-1 < k0 < k1 < ··· such that limi?±8 ki = ±8 and 
 (when the limit exists), where the infimum is taken over all pairs of processes  satisfying the causality Markov chains 
 The proof of [1, Theorem III.6] relies on the claim stated in [1, Theorem III.5], namely, that an equality similar to 
 We demonstrate that the proof of [1, Theorem III.5] is not valid (and hence that of [1, Theorem III.6] 
 is flawed). We do this by showing next that [1, Theorem III.5] has two problems, namely: a) one of 
 the causal IRDFs considered in it does not coincide with HD0 , and b) the proof of [1, Theorem III.5] 
 The already mentioned first problem of [1, Theorem III.5] as a basis for [1, Theorem III.6] follows 
 -?na(D) as ( [1, II.9]) from the fact that [1] defines its alternative causal IRDF function R 
 where (as defined in the text just below equation (II.6) in [1]) Q1,8(D) is the set of conditional distributions of y81 given x81 such that   satisfies the causality Markov chains 
 -?e(D) = -?Rna(D) (although this equality is not explicitly Thanks to (12), it readily follows that R 
 Since the only causal IRDF defined in [2] as an inf lim is HD0 , one must conclude that [1] regards -?Re(D) as equivalent to -H?D0 . However, in view of Definition 1 and (8), such equivalence is not valid 
 (since the distortion feasible sets of Definition 1 are not compatible with the distortion constraint (13)). 
 it does not mean that   equals HD0 . As a consequence, one of the necessary conditions for [2, Theorem 4] is not shown to hold. 
 The second issue with [1, Theorem III.5] is the validity of its proof. To begin with, the only 
 argument used in it is that the source is stationary and [2, Theorem 2]. However, the latter theorem 
 only says that HD0 = HD0 , and thus the proof of [1, Theorem III.5] presented there is flawed. 
 Although not referred to in that proof, the reverse inequality claimed in [1, Lemma III.4] would be all that is required to show that  . However, the proof of [1, Lemma III.4], reproduced 
 and then that the claim follows by taking the infimum over Q1,8(D). The problem with this reasoning is that (16) does not follow from (15). A rigorous reasoning reveals that when taking the limit as 
 Thus, one cannot choose to infimize the RHS of this inequality over Q1,8(D) and expect the in- 
 equality to hold, since one can easily find a pair of processes   whose conditional distribution -? 
 Unfortunately, the latter is not true since, as already mentioned, Q1,8(D) allows pairs of random 
 processes   such that E[ , for all n ? N (thus reaching the limit distortion (13) from above), and thus such that  , for all n ? N. Therefore, (18) does not hold. 
 leading to an inequality in the same direction as the one provided by [2, Theorem 2], i.e., that 
 ﻿In [1, Theorem III.6] it is claimed that, for a one-sided random source x , the
 search for the non-anticipative (i.e., causal) rate distortion function can be restricted to reconstructions y81 which are jointly stationary with x81 . In this technical report we show that the proof of [1, Theorem III.6] is invalid because it relies on [1, Theorem III.5], the proof of which, as we also show,
 is flawed.
 INTRODUCTION
 The manuscript [1] utilizes [2, Theorem 4] to prove the claim that, for one-sided sources x81 , the non-anticipative (i.e., causal) rate-distortion function can be realized by a reconstruction process y81 which is jointly stationary with x81 . To do so, it relies on [1, Theorem III.5].
 In this note we argue that the proof of [1, Theorem III.5], and hence that of [1, Theorem III.6], are flawed. For that purpose, we will first recall the assumptions and definitions utilized in [2]. After that, we will present the definitions introduced in [1] and show, under the conditions stated there, the requirements needed by [2, Theorem 4] (the basis of [1, Theorem III.6]) of are not met.
 A BRIEF REVIEW OF [2]
 Throughout [2], the search in the infimizations associated with various types of “nonanticipatory” (i.e., causal) rate-distortion functions is stated over sets of joint probability distributions between source and reconstruction (as opposed to the usual definitions, in which the search is over conditional distributions, see [3, Chapter 10], [4]). Since the distribution of the source is given, it is required that for every k2 > k1 ? Z, all the joint distributions   to be considered yield xkk21 having the
 same (given) distribution of the source for the corresponding block, say  . This requirement can be formalized as requiring that  , for a set of admissible joint distributions Pk1,k2
 defined as
 	 ,	(1)
 where  and   are, respectively, the alphabets to which   and ykk21 belong, and  is a
 s-algebra over . In [2], this admissibility requirement is embedded in the definition of the sets of distributions which meet the distortion constraint, described next.
 The fidelity criterion for every pair of integers1 k1 = k2 is expressed in [2] as requiring  to belong to a non-empty set of distributions (hereafter referred to as distortion-feasible set) WDk1,k2, a condition written as  . In this definition, the number D = 0 represents an
 admissible distortion level. Notice that such general formulation of a fidelity criteria does not need a distortion function and does not necessarily involve an expectation.
 As mentioned above, the admissibility requirement   is expressed in the distortionfeasible sets in [2, eqn. (2.1)]. The latter equation can be written as
 	WDk1,k2 ? Pk1,k2.	(2)
 In [2, eqs. (2.4) and (2.5)], the distortion-feasible sets are assumed to satisfy the “concatenation” condition
 	 .	(3)
 With this, [2, eqn. (2.9)] defined the “nonanticipatory epsilon entropy” of the set of distributions2
 WDk1,k2 as
 	 ,	(4)
 where the infimum is taken over all pairs of random sequences   such that the
 causality Markov chains
 	x 	(5)
 are satisfied. Then [2, eq. (2.13)] defines the “nonanticipatory message generation rate” as
  	(6) 1
 The analysis in [2] considered both dicrete- and continuous-time processes, but here we only refer to the discrete-time
 scenario.
 2
 The actual term employed in [2] is “nonanticipatory epsilon entropy of the message” where the term “message”
 refers to the random ensembles in  .
 (when the limit exists).
 An alternative “nonanticipatory message generation rate” is also considered in [2] by defining the set of distortion-admissible process distributions WD as follows:
 Definition 1. The set (WD) consists of all two-sided random process pairs   for which there exist integers ··· < k-1 < k0 < k1 < ··· such that limi?±8 ki = ±8 and
 	 .	(7)
 N
 With this, [2, eq. (2.14)] defines
 	 	(8)
 (when the limit exists), where the infimum is taken over all pairs of processes  satisfying the causality Markov chains
 	x .	(9)
 THE PROBLEMS WITH [1]
 The proof of [1, Theorem III.6] relies on the claim stated in [1, Theorem III.5], namely, that an equality similar to
 	-?	 
 	HD0 = HD0	(10)
 holds.
 We demonstrate that the proof of [1, Theorem III.5] is not valid (and hence that of [1, Theorem III.6]
 is flawed). We do this by showing next that [1, Theorem III.5] has two problems, namely: a) one of
 -?
 the causal IRDFs considered in it does not coincide with HD0 , and b) the proof of [1, Theorem III.5]
 is invalid.
 A. The First Problem
 The already mentioned first problem of [1, Theorem III.5] as a basis for [1, Theorem III.6] follows
 -?na(D) as ( [1, II.9]) from the fact that [1] defines its alternative causal IRDF function R
 	-?Rna(D) , ,	(11)
 -?
 where (as defined in the text just below equation (II.6) in [1]) Q1,8(D) is the set of conditional distributions of y81 given x81 such that   satisfies the causality Markov chains
 	x 	(12)
 and the asymptotic distortion constraint
 	 	(13)
 Next, [1] states in its equation (III.2) that [2] defined
 	-?Re(D) , .	(14)
 -?e(D) = -?Rna(D) (although this equality is not explicitly Thanks to (12), it readily follows that R
 stated in [1]).
 -?
 Since the only causal IRDF defined in [2] as an inf lim is HD0 , one must conclude that [1] regards -?Re(D) as equivalent to -H?D0 . However, in view of Definition 1 and (8), such equivalence is not valid
 (since the distortion feasible sets of Definition 1 are not compatible with the distortion constraint (13)).
 na(D) = -?Rna(D) (and hence),
 Therefore, when in [1, Theorem III.5] it is stated that R
 -?
 it does not mean that   equals HD0 . As a consequence, one of the necessary conditions for [2, Theorem 4] is not shown to hold.
 B. The Second Problem
 The second issue with [1, Theorem III.5] is the validity of its proof. To begin with, the only
 argument used in it is that the source is stationary and [2, Theorem 2]. However, the latter theorem
 	-?	 
 only says that HD0 = HD0 , and thus the proof of [1, Theorem III.5] presented there is flawed.
 Although not referred to in that proof, the reverse inequality claimed in [1, Lemma III.4] would be all that is required to show that  . However, the proof of [1, Lemma III.4], reproduced
 below, is clearly invalid. It starts by noting that, by definition,
 	 .	(15)
 Then it proceeds by saying that “taking the limit on both sides we obtain”
 	 	(16)
 -?
 and then that the claim follows by taking the infimum over Q1,8(D). The problem with this reasoning is that (16) does not follow from (15). A rigorous reasoning reveals that when taking the limit as
 n ? 8, (15) translates to
 	n ?8	1	n?8 n	1	1	1	1	such that	 y  
 -?
 Thus, one cannot choose to infimize the RHS of this inequality over Q1,8(D) and expect the in-
 equality to hold, since one can easily find a pair of processes   whose conditional distribution -?
 belongs to Q1,8(D) and yet(because the normalized expectations on
 the LHS of (13) are allowed to reach the limit D from above).
 In order to arrive to (16), one should first show that
 	 .	(18)
 -?
 Unfortunately, the latter is not true since, as already mentioned, Q1,8(D) allows pairs of random
 processes   such that E[ , for all n ? N (thus reaching the limit distortion (13) from above), and thus such that  , for all n ? N. Therefore, (18) does not hold.
 Indeed, the latter reasoning reveals that
 	 ,	(19)
 leading to an inequality in the same direction as the one provided by [2, Theorem 2], i.e., that
 na(D) = -?Re(D).
 R
 ﻿This paper deals with control system design subject to average data-rate constraints. By focusing on SISO LTI plants, and a class of source coding schemes, we establish lower and upper bounds on the minimal average data-rate needed to achieve a prescribed performance level. We also provide a specific source coding scheme, within the proposed class, that is guaranteed to achieve the desired performance level at average data-rates below our upper bound. Our results are based upon a recently proposed framework to address control problems subject to average data-rate constraints.
 	1	Introduction
 The study of control systems subject to communication constraints has recently received much attention in the control community (see, e.g., the papers in the special issue [1]). Within this framework, a key question relates to the trade-offs between control objectives and communication constraints. This paper focuses on the interplay between average data-rate constraints (in bits per sample) and stationary performance in a class of networked control systems (NCSs).
 When stability is the sole control objective, the results of [2] guarantee that, for a noisy LTI plant model and subject to mild conditions on the noise sources statistics, it is possible to find causal coders, decoders and controllers such that the resulting closed loop system is mean square stable, if and only if the average data-rate is greater than the sum of the logarithm of the absolute value of the unstable plant poles. This result establishes a fundamental separation line between what is achievable in NCSs over digital channels and what is not, when the problem of interest is mean square stability (see also the thorough discussion in the survey paper [3]).
 When performance bounds subject to average data-rate constraints are sought, there are relatively fewer results available. There exist lower bounds on the mean square norm of the plant state that make explicit the fact that, as the average data-rate approaches the absolute minimum for stability, the performance becomes arbitrarily poor when disturbances are present [2,3]. This holds irrespective of how the coder, decoder and controller are chosen. Unfortunately, it is unclear whether or not these bounds are tight in general [3].
  
 
 A more general performance-oriented approach has been pursued in [3,4]. In those works, conditions for separation and certainty equivalence have been investigated in the context of quadratic stochastic problems for fully observed plants with data-rate constraints in the feedback path. If the encoder has a specific recursive structure, then certainty equivalence and a quasi-separation principle hold [3]. This result is interesting, but [3] does not give a computable characterization of the optimal encoding policies. A similar drawback is shared by the results reported in [4]. In that work, performance related results are expressed in terms of the so-called sequential rate-distortion function, which is very difficult to compute in general. For fully observed Gaussian first order autoregressive systems, [4] provides an expression for the sequential rate-distortion function. However, it is not clear from the results in [4] whether or not the sequential rate-distortion function is operationally tight (see Section IV-C in [4]).
 Other works related to the performance of control systems subject to data-rate constraints are reported in [5] and [6]. The first work focuses on noiseless state estimation subject to data-rate constraints under three different criteria. The case most relevant to this work uses an asymptotic (in time) quadratic criterion to measure the state reconstruction error. For such a measure, it is shown in [5] that the bound established in [2] is sufficient to achieve any prescribed asymptotic distortion level. This is achieved, however, at the expense of arbitrarily large estimation errors for any given finite time. This feature of the solution makes the conclusions in [5] too optimistic. On the other hand, [6] considers non-linear stochastic control problems over noisy channels, and a functional (i.e., not explicit) characterization of the optimal control policies is presented.
 In this paper, we focus on SISO LTI plants. Our main contribution is a characterization of upper and lower bounds on the minimal average data-rate that allows one to attain a given performance level (as measured by the stationary variance of the plant output). To that end, we focus on a specific class of source coding schemes that contains, as special cases, the coding schemes studied in [7,8]. We also provide a specific source coding scheme, within the proposed class, that is guaranteed to achieve the desired performance level at average data-rates below our upper bound. Instrumental to our results is the characterization of the minimal signal-to-noise ratio (SNR) that guarantees a prescribed closed loop performance level in a related class of NCSs (cf. [9,10]).
 The remainder of this paper is organized as follows: Section 2 presents the problem of interest in this paper, and defines the class of considered source coding schemes. Section 3 establishes, for the class of source coding schemes introduced in Section 2, a relationship between average data-rates and an internal SNR. Section 4 focuses on the interplay between SNR constraints and closed loop performance. These results are then used in Section 5 to present our main contribution. Section 6 draws conclusions.
 Notation: R denotes the set of real numbers, R+ denotes the set of strictly positive real numbers,   stands for the magnitude (absolute value) of x. Rp is the set of
 all proper and real rational SISO transfer functions,RH2	RH8 contains all stable transfer functions in, and U8 contains all the biproper andRp, contains all the strictly proper transfer functions in RH8
 minimum-phase transfer functions in RH8. L1 and L2 are defined as usual, and the associated norms are denoted by ||·||1 and ||·||2, respectively [11].
  
 Figure 1: Networked control system.
 	2	Setup and Problem Definition
 	2.1	The setup
 This paper focuses on the NCS of Figure 1. In that figure, G is a SISO LTI plant, u is the control input, y is the plant output, and d is an input disturbance. The feedback path in Figure 1 comprises an error-free digital channel, and thus quantization becomes mandatory. This task is carried out by an encoder whose output correspond to the binary words sc. These symbols are then mapped back into real numbers by a decoder. It is clear that the encoder and decoder also embody a controller for the plant. As will become clear as we proceed, a distinction between the controller task and the encoder-decoder task is rather artificial in our setup.
 We introduce the following assumptions on G and d:
 Assumption 1 The plant G is SISO, LTI, strictly proper, free of unstable hidden modes, and has no poles or zeros on the unit circle. The initial state of the plant xo, and the disturbance d, are such that
 (xo,d) is jointly second order Gaussian; d is stationary and has spectral factor ?d ? U8.  In this paper we focus on source coding schemes (i.e., encoder-decoder pairs) of the following type:
 Definition 1 The source coding scheme of Figure 1 is said to be linear if and only if its input y and output u are related via
 	u = Tqq + Tyy,	(1)
 where q is a second order zero-mean i.i.d. sequence, q is independent of (d,xo), and Tq,Ty ? Rp are the transfer functions of LTI systems with deterministic initial states.
 The class of linear coding schemes extends the class of so-called independent and i.i.d. coding schemes introduced in [7,8]. We acknowledge that it is a restricted class of coding schemes. However, its simplicity allows one to actually address control system design problems subject to average data-rate constraints.
 Any linear source coding scheme can be written as shown in Figure 2(a), where E and H¯ are proper transfer functions. The design of E and H¯ is however non-trivial. Indeed, it is easy to see that optimally designing E and H¯ amounts to solving an optimal control problem subject to sparsity constraints [12]. To the best of our knowledge, the only known sufficient conditions (see [12]) that allow one to pose such problems as convex ones are not satisfied in our case.
 Motivated by the previous discussion, we introduce the alternative rewriting of a linear source coding scheme shown in Figure 2(b), where H,E,C,F ? Rp are the transfer functions of filters with
  
  
 Figure 2: Two alternative representations of a linear source coding scheme.
  
 Figure 3: The NCS of Figure 1 when a linear source coding scheme is employed.
 deterministic initial states. In the scheme of Figure 2(b), F is redundant. However, as will become clear below, this over-parametrization of linear source coding schemes allows one to construct a convex optimization problem that is equivalent to that of optimally designing H¯ and E in Figure 2(a).
 When the linear source coding scheme of Figure 2(b) is used in the NCS of Figure 1, the feedback system of Figure 3 arises. For future reference we define 
 	 : the loop of Figure 3 internally stable and well-posed.	(2)
 Remark 1 In the remainder of this paper, when we refer to a linear source coding scheme, it must be understood that we refer to the specific architecture of Figure 2(b), where q satisfies the conditions in
 	Definition 1.	
 	2.2	Average data-rate constraints
 In this section we make a connection between the average data-rate across a linear source coding scheme, and the stationary second order properties of the auxiliary signals w and v in Figure 2. In
  
 	encoderlossy	encoderlossless	decoderlossless reproductiondecoder
 Figure 4: The link between v and w in a linear source coding scheme.
 order to do so, we first make the relationship between those signals and the channel symbols sc in Figure 1 explicit.
 Without loss of generality, we assume that the link between v and w in Figure 2 is given by the scheme of Figure 4 [14]. In that figure, E, D, H and H-1 are causal systems such that 
 	s(k) = Ek(vk,SEk), 	sc(k) = Hk sk,SHk ,
 	s k	1	k,SHk	,	w(k) = Dk(sˆk,SDk ),	(3)
 ˆ( ) = Hk- sc
 where Ek, Dk, Hk, Hk-1 are (possibly nonlinear time-varying) deterministic mappings, and SE(k), SD(k), SH(k) correspond to side information that becomes available at time instant k at the encoder side, decoder side, and both at the encoder and decoder sides, respectively. The range of Ek is assumed to be countable, and that of Hk, to be a countable set of prefix-free binary words [15]. The mappings Hk and Hk-1 are chosen so as to satisfy
 	H )	(4)
 for any , and any k ? N0. Condition (4) makes explicit the fact that the blocks H and H-1 act as a transparent link between the output of E and the input of D in Figure 4. Since we assume an error free digital channel, and (4) holds, it follows that ˆs = s.
 It is clear that, when one uses the scheme of Figure 4 as the link between v and w in a linear source coding scheme, one needs to focus on mappings Ek,Dk,Hk,Hk-1 such that the process
 	q  w - v	(5)
 satisfies the conditions in Definition 1. We also note that, since ˆs = s, the feedback link between w and the encoder side in Figure 2 does not require a physical channel. (To make w available at the encoder side, it suffices to replicate D at the encoder side.)
 We denote the expected length of the symbol sc(i) by R(i), and define the average data-rate across the considered source coding scheme as
 	R .	(6)
 We will work under the following assumptions:
 Assumption 2
 (a)	SD is independent of (xo,d).
 (b)	The mapping Di is such that, ?i ? N0, there exists another deterministic mapping). gi such that   (i.e., D is invertible upon knowledge of SD
 Assumption 2(a) is motivated by the sensible requirement that the block D in Figure 4 uses only past and present symbols, and side information not related to the message being sent, to construct its current output. On the other hand, if, for some mappings Ek and Dk, Assumption 2(b) does not hold, then one can define another set of mappings that achieve a coding noise q with the same statistics as in the original situation, but at the expense of a lower average data-rate R.  Accordingly, if one aims at minimizing R, then one can focus, without loss of optimality, on mappings Ek and Dk satisfying Assumption 2(b).
 	2.3	Problem definition
 The main focus of this paper is on the minimal average data-rate that allows one to attain a given performance level, as measured by the stationary variance of the plant output y. With the definitions introduced above, we are now in a position to formally define the problem of interest as follows: Problem 1 Consider the NCS of Figure 1, suppose that Assumption 1 holds, that the source coding scheme is linear, and that the scheme of Figure 4 is used as the link between v and w. Denote by Dinf the minimum stationary variance of y that is achievable in the NCS of Figure 3 when q = 0. Find, for a given performance level D ? (Dinf,8),
 RD  2inf R,	(7) sy=D
 where  is the stationary variance of y, and the optimization is carried out with respect to:
 •	All causal blocksin Definition 1, andE,DD,Hsatisfies Assumption 2(b).,H-1 described by (3)-(4), such that the coding noise q (see (5)) is as
 •	All side information processes SE, SD, SH satisfying Assumption 2(a).
 •	All filters (H,C,F,E) ? S (see (2)).	
 Remark 2and the auxiliary noiseThe definition of independent source coding scheme guarantees that, providedq satisfies the conditions in Definition 1, the NCS considered in Problem 1(H,C,F,E) ?
 	Sis mean square stable.	
 In order to solve Problem 1, we will first establish a lower bound on the average data-rate across a linear source coding scheme in terms of stationary second order properties of the auxiliary signals w and v (see Figure 4). The existence of this bound motivates Section 4, where we study the optimal design of linear source coding schemes subject to SNR constraints. These results are then used in Section 5 to give both upper and lower bounds on RD.
 3 Bounding the Average Data-Rate Across Linear Source Coding Schemes
 This section extends the results of Section V-A in [8] to show that, when a linear source coding scheme is used in the NCS of Figure 1, the minimum average data-rate across it (subject to a performance constraint) is bounded from below by a simple function of the minimum ratio between the stationary variances of v and q (subject to the same performance constraint). To do so, we start by noting that the following holds:
 Theorem 1 Consider the NCS of Figure 1 where the source coding scheme is linear, and the link betweenhold, thenv and w is given by the scheme of Figure 4. If (H,C,F,E) ? S and Assumptions 1 and 2
 	R ,	(8)
 where I8(v ? w) denotes the directed mutual information rate [16,17] betweenw, and sq2 is the variance of q. Moreover, equality holds in the lastv and w, Sw is the stationary power spectral density of inequality if and only if q is Gaussian.
 Proof: The first inequality follows immediately from Theorem 1 in [8]. The second inequality follows from Part 3 of Lemma 5.2 in [7], and from the proof of Theorem 3 in [8] (see also Theorem 4.6
 	in [17]).	
 Theorem 1 gives a lower bound on the average data-rate across a linear source coding scheme in terms of a simple function of the spectrum of w and the equivalent noise variance sq2. This key result, upon which the remainder of this paper is based upon, can be further simplified. Note that Jensen’s inequality yields
 	 ,	(9)
 where sv2 is the stationary variance of v, and ? is the SNR of the linear source coding scheme. By finding the minimal SNR ? subject to a performance constraint (e.g., an upper bound on the stationary variance of the plant output y), one is also calculating an upper bound on the minimal value of the right hand side of (8), subject to the same performance constraint. If, in addition, the optimal solution to the former SNR minimization problem is such that the gap between the left and right hand sides of the inequality in (9) is arbitrarily small (or can be made so without compromising optimality), then, by virtue of Theorem 1, one would immediately get a lower bound on RD by solving the much simpler SNR minimization problem. The following result guarantees that this is actually the case:
 Lemma 1 Consider the NCS of Figure 1, where the source coding scheme is linear and has a fixed noise source q. Suppose that Assumption 1 holds and define  . If the choice (H,C,F,E) =
 (H0,C0,F0,E0) ? S(H,C,F,Eis such that) = (sy2 H=1s,Cy,2 01,Fand1,Ef1=) ?f0S, then, for any arbitrarily smallsuch that sy2 = sy,2 0 f = f0 and, in addition,? > 0, there exist
 a choice of filters,
  .
 	Proof:	The proof of this result goes along the lines of the proof of Theorem 4 in [8].	
 	4	Optimal Performance Subject to SNR Constraints
 Motivated by the discussion preceding Lemma 1, we will now focus on the following problem:
 Problem 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumption 1 holds. Define, where pi is the ith unstable pole of G. Find, for a given G ? (?inf,8),
 	sy2G 	inf	q2	+ sy2,	(10)
 (H,C,F,E)?S,s ?R ?=G
 	where all the symbols are as defined before.	
 Remark 3 It follows from Theorem 17 in [10] that ?inf corresponds to the minimal SNR compatible with mean square stability in the NCS of Figure 3. 
 We note that Problem 2 is concerned with the best achievable performance, as measured by sy2, that is achievable when an upper bound G is placed on the SNR ?. As shown in Lemma 4 below, this problem is equivalent to the problem of finding the minimal achievable SNR ? subject to an upper bound on sy2. Our momentary change of focus is only due to technical reasons (see Remark 4 below). The next lemma states necessary conditions for the 4-tuple (H,C,F,E) to belong to S:
 Lemma 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumption 1 holds. Define
 1
 	S  1 - C - HEG,	(11)
 	and consider a coprime factorization of G over RHY N8, i.e., consider= 1.	If (H,C,F,EX, Y), ?NS, , thenD ? RHF ?8, withRH2,
 X, D
 	HSGE	-	8
 	 ,	(12)
 where pi is the ith unstable pole of G.
 Proof: It is clear from Figure 3 that F is in open loop. Thus F must be stable to ensure the internal stability of the loop. On the other hand, q models a possibly non-linear and discontinuous mapping between v and w and, accordingly, F must be strictly proper for the architecture to be
 In Figure 3, denote the open loop transfer function from y to u by K, i.e.,
 well-posed (the same conclusion applies to(H,C,F,E) ?must contain as non-minimum phase (NMP) zeros all the unstable poles ofS, then K must be an admissible one degree-of-freedom controller forC). Therefore, F must belong to RHK2. HEG. (1H,E,GThus- C)-S1Kand. If
 (1 - GK)-1
 	(1-CS)-=1 (Ssee Lemma 3.1.3 in [18]). Denote all the unstable poles ofK(1 - C)-1 we have from the Bode Integral Theorem [18] that, for anyH,E and G by ¯piF, i??RH{1,2···,	,np¯}.
 Since
 	 .	(13)
 To complete the proof, we note that HSGE is the closed loop transfer function from a disturbance at the input of H to the output y. By invoking the Youla-Kucera parametrization [19], we thus conclude that HSGE must be as claimed.	
 By virtue of Lemma 2, we are now ready to present a lower bound on the best achievable performance  . To that end, we will make use of the following mild additional assumption:
 	Assumption 3 In Problem 2, EH = 0	at the optimum.	
 If EH were identically zero at the optimum, then optimal performance would be achieved by leaving G in open loop. The cases where this happens are clearly of no interest in a networked control setting.
 Theorem 2 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumptions 1 and 3 hold. Then,
 
 	sy2G = Q?finf?RHM8 JG(f,Q)  JG,inf,	(14)
 where 
 	 , and  ,	(15)
 and
 	 ,	(16)
 where WQ  NY ?x -ND?xQ, N,Y,D and pi are as in Lemma 2,  
  , where ci denotes the ith NMP zero of G. Moreover, the optimization problem on the
 right hand side of (14) is convex (i.e., JG(·,·) is a convex function of its arguments, and M × RH8 is a convex set).
 Proof: Under our assumptions we have that, for any (H,C,F,E) ? S and sq2 ? R+, sy2 and ? exist and are given by
 	  ,	(17)
 	 	(18)
 and also that
 A simple contradiction-based argument shows that, since Assumption 3 holds, the SNR constraint in (10) is active at the optimum. Hence, at the optimum,
 	 + 1	(19)
 and
 	 	(20)
 	 ,	(21)
 where the first equality follows upon solving (18) for sq2 with ? = G, and the inequality follows from the Cauchy-Schwartz inequality and the fact that HSGE+1 = (1-C)S. If one defines f  |(1 - F)S| and2.
 notes that |?| = 1, then the first part of the result follows immediately from (21), (19) and Lemma
 To complete the proof we need to show that the optimization problem on the right hand side of
 (14)Lemma 4 in [20] thatis convex. Since WJGQ(·is an afine function of,·) is a convex function of its arguments. The proof is thus completed.Q, and both RH8 and M are convex, it follows from
 Theorem 2 shows that one can calculate a lower bound on the best achievable performance subject to an SNR constraint by solving a convex optimization problem. Convexity guarantees, among other things, that the problem of finding the bound0 JG,inf, and also Qd ? RH8 and fd ? M such that, for any d > ,
 	JG(fd,Qd) = JG,inf + d,	(22)
 can be addressed numerically using standard algorithms [21].
 Remark 4 As an alternative to Problem 2, one can also consider the problem of finding the minimal SNR subject to a performance constraint. By doing so, and by proceeding as in the proof of Theorem 2, one arrives at an auxiliary optimization problem whose convexity we have not been able to prove yet.
 
 We will next show that a solution of the optimization problem on the right hand side of (14) actually yields a solution to Problem 2. To that end, we start by noting that the following holds:
 Lemma 3 Consider Qd ? RH8 and fd ? M satisfying JG(fd,Qd) = JG,inf + d.
 1.	For any 1 > 0, there exist Fˆd,1 ? RH2 such that  and
 	 .	(23)
 2.	Consider X,N,Y and D as in Lemma 2, and define 
 Kd  (X - QdNn)-1(Y - QdD),
 z
 Sd  den{Kd}(1 - GKd),
 (24)
 (25)
 	 .	(26)
 There exists Ed,2 ? RH8 such that, for any 2 > 0,
 	.	(27)
 Proof:
 1.	Equation (23) follows by mimicking the proof of Lemma 1 in [22], and by using the definition of
 M.
 2.	The Youla-Kucera parametrization guarantees that Kd is an admissible one degree-of-freedom controller for G. Thus, (1 - GKd)-1 has as zeros all the unstable poles of both G and Kd. Accordingly, Sd is stable, Sd(8) = 1, and Sd has as NMP zeros the unstable plant poles only. Given the definition of Fˆd,1, we thus conclude that Fd,1 ? RH2 and that (1 - Fd,1)-1 is also stable. Since, by assumption, ?d is stable, we have that num{Kd}?d(zn(1 - Fd,1))-1 ? L2 is analytic outside and on the unit circle and can thus be approximated to any degree of accuracy
 	2 > 0 by a stable and proper Ed,2.	
 With the aid of Lemma 3 we can state the main result of this section:
 Theorem 3 Consider the NCS of Figure 1 where the source coding scheme is linear, and recall the notation introduced in Lemma 3. If Assumptions 1 and 3 hold, then? RHsy2G = JG?,infM. Moreover, for any  => 0, there exists1 1,2,d > 0 such that choosingˆd,1 satisfying the conditions in Part 1 of Lemma 3, andQd 8 and fd satisfying
 	JG(fd,Qd)	JG,inf +d, Fd,	as in (26) with F
 Ed,2 ? RH8 satisfying (27), guarantees that the choice of parameters
 (28)
 (29)
 (30)
 defines a 4-tuple  , and yields sy2 = JG,inf +  whilst achieving ? = G.
 	Proof:	Our first claim follows immediately from the second, which is proved below.
 Since H,E ? RH8 and F,C ? RH2 (recall the definition of den{·}), it follows that the proposed choice of parameters defines a well-posed feedback loop. We next show that the proposed choice of parameters achieves internal stability. To do so, we first note that the open loop transfer function from y to u is given by HE(1-C)-1 = KGd. As such, there exist no unstable pole-zero cancellations between. From the proof of Lemma 3, we know that Kd is an admissible one degree-of-freedom controller for
 Kd and G [18]. Moreover, it is clear by construction (see (28) and (29)) that there are no unstable pole-zero cancellation among any two transfer functions in the set {H,E,(1-C)-1}. The above facts imply that the closed loop is internally stable.
 We next show that . Equation (23) implies
 	  = 1.	(31)
 Thus, since||HS1 -dGFˆ?d,d1is stable since||22f<d ?GM+ 1, it follows that there exists a su, whereKd is admissible forFˆd,1 satisfies the conditions in Part 1 of Lemma 3. On the other hand,G and Hffi?ciently smalld is stable. As a consequence of the above,1 > 0 such that ||(1 - F)Sd||22s=q2
 in (30) is finite and positive, as required.
 The fact that our choice of parameters guarantees ? = G follows upon using (30) in (18), and noting that S = Sd.
 To complete the proof, it remains to show that the proposed set of parameters is such that  JG,inf +  for any  > 0. By definition of Ed,2 (see (27)), the choice for E in (28) is such that there exists ?E ? L2 such that
 	 .	(32)
 Thus,
 	 	(33)
 and the numerator of the second term on the right hand side of (20) can be written as
 	 	(34)
 where a, satisfying 0 = a(2) < 8, ?2 ? R+, is given by
  ,
 In (34), the first equality follows upon using (32) and (33), the inequality follows from the CauchySchwartz inequality, and the last equality follows from the fact that our choice of parameters guarantees
 	thatKd), and from our choice forS = Sd and thus num{ 	}	d,1GK. By using the above facts and the identityW	(use the definition of WQd and
 HSGE + 1 = (1 - C)S we can write (20) as
 .	(35) 	
 Now, note that (23) implies1 = v1. The latter inequality and (31) allow one to
 rewrite, for sufficiently small
 	 ,	(36)
 where ß, satisfying 0 = ß(1) < 8, ?1 ? R+, is given by
 	 .	(37)
 The result follows immediately from (36). (Note that 2 depends upon 1 and, hence, has to be chosen, once 1 is chosen.)	
 Theorem 2 provides a characterization of the solution of Problem 2. In the next section, we will use this result to characterize bounds on RD.
 5 Bounds on the Minimal Average Data-rate needed to Achieve a Given Performance Level
 This section presents a characterization of the solution of Problem 1. In particular, we present both upper and lower bounds on RD, and also a specific implementation of a linear source coding scheme that achieves an average data-rate that is guaranteed to be below our upper bound.
 We start by stating the following auxiliary result:
 Lemma 4 Consider the NCS of Figure 1 where the source coding scheme is linear, and suppose that Assumptions 1 and 3 hold. Define
 	?D 	inf	2	+ ?,	(38)
 (H,C,F,E2)?S,sq?R sy=D
 where all symbols are as defined before. Then, ?[sy2]G = G.
 Proof: By solving Problem 1 one finds a set of parameters such that sy2 = ˆsy2˜G and ? = G (recall that Assumption 3 guarantees that the SNR constraint is active at the optimum). It is therefore immediate to see that, by definition, ?[sy2]G> G is impossible. Assume now that Qo and fo are the parameters that minimize J?[sy2]G (f,Q). Then, since the SNR constraint in Problem 2 is active at the optimum, we have that
 	?[sy2]G < G ? J?[sy2]G (Qo,fo) > JG(Qo,fo),	(39)
 	which contradicts the optimality of Qo and fo. We thus see that ?[sy2]G = G.	
 Lemma 4 shows that Problem 2 is equivalent to the problem of finding ?D in (38). That is, if some choice of the parameters (H,C,F,E) and   solve Problem 2 and thus achieve a performance level and an SNR ? = G, then the same parameters optimize the SNR ? subject to the constraint
   .
 We are now in a position to present the main result of this paper:
 Theorem 4 Consider the setup and assumptions of Problem 1. If, in addition, Assumption 3 holds and  , then R . Moreover, there exists a linear source coding scheme such that  , while satisfying for any  > 0.
 	Proof:	Our first claim is immediate from Theorems 1 and 3, and Lemmas 1 and 4.
 We now prove our second claim. Use Theorem 3 to find filters (H,C,F,E) = (Ho,Co,Fo,Eo) and a noise variance sq2 = so2 such that ? = G and sy2 = D* +  for the desired value of  > 0. Consider an entropy coded dithered quantizer (ECDQ) [23] as the link between v and w, i.e., pick blocks E, D, H and H-1 in Figure 4 such that
 s(k) = Q(v(k) + dh(k)),	sc(k) = Hk(s(k),dh(k)),
 sˆ(k) = Hk-1(sc(k),dh(k)),	w(k) = sˆ(k) - dh(k),
 where dh is a dither signal available at both the encoder and decoder sides (accordingly, SE(k) = SD(k) = SH(k) = dh(k)), Q : R ? {i?;i ? Z} corresponds to a uniform quantizer with step size ? ? R+, Hk corresponds to the mapping describing an entropy-coder (also called loss-less encoder [15, Ch.5]) whose output symbol is chosen using the conditional distribution of s(k), given dh(k), and Hk-1 corresponds to the mapping describing the entropy-decoder that is complementary to the entropy-coder at the encoder side. If dh is an i.i.d. sequence, independent of (xo,d), and uniformly distributed on
 (-?/2,?/2), with ? = 12so2, then our claim follows from Corollary 3 in [8].	 Theorem 4 establishes a lower bound on the minimal average data-rate that is needed to achieve a given performance level, when linear source coding schemes are employed to control SISO LTI plants. This lower bound is not tight, but the worst case gap is given by  + ln2 nats per sample
 (i.e., ˜ 1.254 bits per sample). The first term of this gap corresponds to the divergence of the ECDQq from Gaussianity, and appears due to the fact that ECDQs generate uniform quantization noise
 and not Gaussian coding noise. The second term arises because practical entropy-coders (see proof of Theorem 4) are not perfectly efficient [15, Chapter 5]. (A detailed discussion of these facts can be found in [7,8].) Since we constrain ourselves to a simple class of source coding schemes, this gap seems inescapable but, in our view, it is a fair price to be paid given the simplicity of our approach.
 A key aspect of our results is that they are built upon the solution of an SNR optimization problem. This is a key feature of our work, and allows one to easily provide average data-rate guarantees in feedback loops, by using standard control system design techniques.
 Remark 5 If in Problem 1 one removes the performance constraint , then the problem reduces to the calculation of the minimal average data-rate that is compatible with mean square stability, say
 RMSS. By using Theorem 17 in [10], and proceeding as in the proof of Theorem 4, it follows that, for the setup and assumptions of the latter theorem,
  RMSS ,
 where pi is ith unstable pole of G. That is, independent source coding schemes can achieve MSS at average data-rates that are at most   nats per sample away from the absolute lower bound established in [2].
 	6	Conclusions
 This paper has studied control systems closed over noiseless digital channels. By focusing on a class of source coding schemes, we have established lower and upper bounds on the minimal average data-rate needed to achieve a prescribed performance level. Instrumental to our result was the characterization of the minimal SNR that guarantees a given closed loop performance in a related LTI control system.
 Future work should focus on situations that include causal but otherwise unrestricted source coding schemes. Extensions to the MIMO case are also under study by the authors.
 
 ﻿We derive closed-form expressions for the secondorder statistics of the power gain (as a function of frequency) of wideband microwave indoor channels. We obtain our results within a framework that is general enough to be compatible with several popular channel models, such as the Saleh–Valenzuela (SV) channel model and those proposed by the IEEE 802.15.3a and IEEE 802.15.4a task groups. As in all these models, our channel description is based upon clusters and rays with (possibly mixed) Poisson arrivals and random amplitudes. Our results consist of closed-form expressions for the second-order statistics of the channel power frequency response, where statistical averages involve expectations over ray amplitudes and arrival times. These expressions reveal that the autocovariance of the spectral power gain between any two frequencies decreases and tends to zero as the difference between these frequencies tends to infinity if and only if the cluster arrival rate goes to infinity. They also show that the variance-to-squared-mean ratio of the narrow-band power gain exhibits exactly the same behavior with respect to the center frequency. We then use these results to obtain closed-form expressions for the variance and the second-order moment of the aggregate channel power gain over any given interval of frequencies. This allows us to express the channel spectral diversity as a function of model parameters and bandwidth. In addition, we illustrate how these equations allow one to devise automatic cluster identification algorithms which, from empirical estimates of the second-order spectral statistics of the channel power gain, can confirm or deny the existence of clusters in a given scenario.
 Index Terms—Fading channels, Saleh–Valenzuela (SV) channel model, statistical channel modeling, ultrawideband channels.
 I. INTRODUCTION
 S
 TOCHASTIC wireless channel models allow one to predict the statistics of radio propagation conditions over an ensemble of scenarios with similar characteristics. This is particularly useful in complex, heterogeneous, and time-varying environments, such as office, residential, and industrial indoor scenarios.
 One of the most popular models for indoor wireless channels is that proposed in [1], which has served as the basis for several other channel models. Building upon it, the IEEE 802.15.3a task group accepted a channel model [2] for ultrawideband indoor communications, and similar models have been adopted for the IEEE 802.15.4a standard [3]. Similar to the Saleh–Valenzuela (SV) model, the IEEE 802.15.3a and IEEE 802.15.4a channel models consist of a discrete-time description of the impulse response of a wireless channel, in which multipath components are grouped into clusters. 
 Some of the useful time-domain delay statistics of wireless channel models are the power delay profile (PDP), the average delay, and the RMS delay, which, under suitable assumptions, allow one to determine spectral statistics such as coherence bandwidth and average power gain. These parameters, which are associated with second-order statistics of the channel impulse or frequency response, have been extensively discussed and characterized in the literature [4]–[7].
 Another set of stochastic properties of wireless channels, which has received relatively less attention, is those derived from the second-order statistics of the channel power frequency response (i.e., its squared frequency response magnitude, which is denoted by |H(j?)|2). An early analysis of these statistics and their application can be found in [8], where it is argued that the transmission over two carriers with the right frequency spacing may benefit from the negative correlation between each of the received powers. Simplified expressions for the spatial and spectral autocorrelation coefficients of |H(j?)|2 and for its power over a frequency interval are presented in [9], which are then evaluated via simulations. Building upon this result and using simulations, it has been possible to assess fading depth statistics and relate them to the bandwidth and to some features of the propagation environment [10], [11].
 0018-9545 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
 Although the simulations of the second-order statistics of |H(j?)|2 are helpful in assessing the performance of wireless communication systems, having closed-form expressions for these statistics provides further benefits. First (and naturally), these closed-form expressions can be substituted into other formulas, thus allowing one to relate them with performance indexes, which are also in closed form. Second, explicit parametric formulas will provide useful insight into the relationship between the performance of wireless communication systems and certain parameters. In particular, the second-order statistics of |H(j?)|2 allow one to refine probabilistic approximations drawn from first-order statistics alone. Moreover, if the power received over some band is known to distribute according to some parametric probability density function (pdf), then having the first- and second-order statistics of this power allows one to identify, in some cases, the parameters of the distribution [12].
 This makes it possible to calculate fade-depth statistics from the second-order statistics of |H(j?)|2.
 In this regard, it was already noted in [9]–[11] and [13] that the fade depth decreases as the width of B increases. In relation to this question, and using an information-theoretic approach and measured data, it has been shown in [14] that the number of significant eigenvalues of the covariance matrix of the random channel impulse response and, hence, the diversity order of the channel scale approximately linearly with the bandwidth. This increase in diversity, which relates to the reduction of the relative channel power variance as the bandwidth increases, has been reported to reach a saturation point [12], [13], [15]. Having explicit expressions for this channel power variance as a function of bandwidth would allow one to predict the associated saturation bandwidth and relate it to environmental parameters.
 Available closed-form results that allow one to analytically obtain the second-order statistics of |H(j?)|2 are rather limited. Although the analysis in [9] and [11] does obtain some intermediate closed-form expressions, the actual evaluation of these statistics is carried out by simulations. In addition, one of the simplifying assumptions underpinning the results in [9] and [11] is the uncorrelation between the amplitudes and arrival times of multipath components, which is known to contradict channel measurements [1], [16]. Under the assumption of uncorrelated amplitudes and arrival times, and considering uniformly distributed arrival times, closed-form expressions for the autocorrelation of |H(j?)| have been derived in [17]. Exact closed-form expressions for all the joint moments between |H(j?1)| and |H(j?2)|, for any frequencies ?1 and ?2, as a function of the PDP of the channel impulse response have been found in [18], assuming that multipath-component amplitudes are independent jointly complex Gaussian and that arrival times are fixed. However, although popular channel models such as the SV model [1] consider complex Gaussian multipath component amplitudes, other amplitude distributions have been reported in the literature (see, e.g., [2]). Moreover, the conditional independence assumption in [18] does not hold for channel models where the impulse response exhibits clusters, such as the SV and its several extensions [1]–[3], [16]. In a recent paper [12], analytical expressions for the autocorrelation of the channel-frequency-response squared magnitude |H(j?)|2 and the variance of the power over any frequency band have been derived for the IEEE 802.15.4a channel model, conditioned to fixed and given ray and cluster arrival times. The corresponding statistics over random arrival times were obtained via simulations. To the best of the authors’ knowledge, no closed-form expressions are available in the literature for second-order unconditioned statistics of the spectral power gain of wireless channel models such as the IEEE 802.15.3a and IEEE 802.15.4a, as well as for the other SV-like models.
 In this paper, we derive closed-form exact expressions for the second-order statistics of |H(j?)|2 for a general class of extensions of the SV channel model. This class includes, as special cases, those accepted by the IEEE 802.15.3a task group and the IEEE 802.15.4a channel modeling subgroup [2], [3], [16]. Unlike [12], our expressions include expectations considering the randomness of both ray amplitudes and arrival times. As in all SV-like models, our channel description is based upon clusters and rays, with different power decay profiles among and within clusters. Our analysis framework is general enough to encompass model features such as mixed Poisson arrivals within clusters, as well as models in which the first ray of each cluster exhibits statistics different from the other rays (these two features are considered in [3], [16], and [19]). To this end, we first derive, in Section III, a general, exact, and closed-form expression for the autocovariance of |H(j?)|2. This expression is determined by four functions, corresponding to the second- and fourth-order moment delay profiles of the cluster and ray amplitudes multiplied by their respective arrival rates. We use these expressions to analyze the effect of having a finite/infinite number of clusters, as well as the implications of having only one cluster (or, equivalently, no clusters at all). We then use the former result to obtain, in Section IV, a closedform expression for the variance of the channel power gain PB integrated over any given interval of frequencies B. Our derivations predict a positive lower bound for the ratio between the variance of PB and its squared mean. This bound only vanishes as the arrival rate of clusters in the model tends to infinity, which is a result that is confirmed via simulations. (Some preliminary results on this topic were reported in [20].) In addition, the obtained equations allow one to devise an automatic cluster identification algorithm, which is capable of verifying or discarding the presence of clusters in an ensemble of channel impulse responses. This algorithm is shown in an example in Section V and can be considered an alternative to other cluster identification algorithms, such as those proposed in [21] and [22].
 II. CHANNEL MODEL
 Here, we formulate the wireless channel model framework to be utilized throughout this paper. The idea is to establish a framework that is general enough to encompass most of the SVlike models proposed in the available literature. With that idea in mind, and before proceeding, we will first review some of the salient features of several extensions of the SV model, from which we will then define the set of assumptions that our results will be based upon.
 A. Brief Review of SV-Like Channel Models
 In all versions and extensions of the SV model, the channel is represented by the following random impulse response [1]–[3], [16]:
 	 	(1)
 where
 ?
 ti,m = Ti + ti,m	?i ? {1,...,Nc};	?m ? {1,...,Nr}
 is the random arrival time of the mth path (or ray) in the ith cluster, Ti = 0 is the random arrival time of the ith cluster, and ti,m = 0 is the random delay of the mth path in the ith cluster relative to Ti.  Cluster and relative arrival times are ordered, i.e., Ti = Tj ?? i = j and ti,m = ti,n ?? m = n ?i. By definition, the cluster begins with its first ray; thus
 	ti,1 = 0	?i ? {1,...,Nc}.
 Each random coefficient ai,m in (1) represents the amplitude of the mth path in cluster i. These random numbers are formed as follows:
 ai,m=pi,mAiai,m	?i ? {1,...,Nc};	?m ? {1,...,Nr}.
 (2)
 Each  is a random variable representing the amplitude of the ith cluster, whereas each real-valued random variable ai,m = 0 denotes the amplitude (or gain) of the mth multipath component (or ray) within the ith cluster relative to that cluster’s amplitude. The meaning of the random numbers {pi,m} in (2) depends on whether a complex or real basebandi,m representation is used. For the former, pi,m = ejf , where {fi,m} are uniformly independent and identically distributed (i.i.d.) over [-p, p] [1], [3], [16]. For the real baseband representation, which is regarded as more suitable for ultrawideband applications, {pi,m} are binary random variables taking values from {-1,1} with equal probability [2]. In both cases
 	E[pi,m] = 0	?i,m.	(3)
 B. Assumptions and Analysis Framework
 To establish a framework to encompass the various features of the SV-like channel models present in [1]–[4], [16], and [23] (and possible future ones), we will formulate now the least restrictive set of assumptions under which our analysis and results are valid. To do so, the following three definitions are necessary.
 Definition 1 (Bounded-Density Markovian Process): We will state that a sequence of random arrival times  , with xi = xj ?? i = j, is bounded-density Markovian with the following conditions.
 The	interarrival	times  	satisfy	the
 Markov chains
  
 There exists a constant ? < 8 such that, for every ? > 0 that is sufficiently small
 Pr{xk+1 - xk < ?|xk = x} =	?	? R+;	k = 1,... ?	x	0
 ?
 
 In other words, condition 1 in the given definition requires that the interarrival time just after an arrival at x (and, hence, the arrival density at x) depends, at most, on the value of x.
 In turn, condition 2 guarantees that the arrival density at any   is bounded.
 Definition 2 (Sequence i.i.d. Under Another Sequence): Let   be a bounded-density Markovian process, with x1 possibly being deterministically equal to 0. We will say that a sequence of random variables  is i.i.d under  if the following two conditions are satisfied.
 The following Markov chains hold:
 	 ;	k = 1,2,...
 For	every  	and	m ? N,	E[?jm|xj = x] =
 ?
 , where S = {1,2,...} if x1 is
 ?
 random, and S = {2,3,...} if x1 = 0 deterministically.
 
 Notice that condition 1 in Definition 2 corresponds to the conditional independence relationships between amplitudes and arrival times shared by all channel models considered in [1]–[4], [16], [23], and [24], except the 802.15.4a model for industrial environments under LOS in [3] and [16]. Similarly, taking {?k}k as the cluster (or relative ray) amplitudes and {xk}k as the cluster (or relative ray) arrival times, condition 2 encompasses all the amplitude distributions considered in [1]– [4], [16], and [23], including the cases in which the amplitude of the first ray or cluster takes a special form, as in CM3 (body surface to body surface, 3.1–10.6 GHz) and CM4 (body surface to external, 3.1–10.6 GHz) in [24].
 We will need one more definition to treat arrival-time distributions differently from plain Poisson processes, such as mixed Poisson arrivals [described in (6)]. To this end, we introduce the following single and joint arrival density functions.
 Definition 3 (Arrival Density Functions): Let  be a bounded-density Markovian sequence of arrival times, with x1 possibly being equal to zero deterministically. For any ? = 0, denote the number of elements in  falling inside
 [x, x + ?) by the random variable , i.e.,
  
 where, for sets, | · | denotes cardinality. We define the single and joint arrival density functions of  as
 ?¯x(x)
 
 	?¯(1)x (x),	if x1 is random
 =¯(2)x (x) + d(x), if x1 = 0 deterministically (4a) ?
 ?¯¯x(y, z)
 	????¯¯(1)x (y, z),	if x1 is random
 	=	?¯¯(2) y, z	d y ?¯(2) z
 if x1 = 0 deterministically
 (4b)
 where, for i ? {1,2}
 	 	(5a)
   ,
 Pr
 	 ?2	,
 (5b)
 
 One case in which arrival times do not conform to a Poisson process is in the model proposed in [19], which is further considered in [3] and [16]. In that case, rays within each cluster are described as having mixed Poisson arrivals; in which case, the ray interarrival times are i.i.d. with the following pdf:
 with probability ß with probability 1 - ß
 (6)
 where ?1 and ?2 =   are two different Poisson arrival rates.
 Using the given definitions, we can now establish the scope of validity of our results by means of the following assumption.
 Assumption 1: Cluster and ray arrival times and amplitudes are random and as follows.
 The following independence relationships hold:
 	pj,m ? {Ti,Ai,ti,n,ai,n}	?i,j ? {1,...,Nc};
 ?n,m ? {1,...,Nr} (7a) pj,n ? pi,m, unless j = i and n = m (7b) aj,n ? ai,m, unless j = i (7c)
 {Ti,Ai} ? {tj,n,aj,n}
 	?i,j ? {1,...,Nc};	?n ? {1,...,Nr} (7d)
 ?i,j,k ? {1,...,Nc};
 where ? denotes probabilistic independence, and the Markov chain notation a ? b ? c means “a and c are
 independent when b is known.”
 {Ti} is a bounded-density Markovian sequence, with T1 that is possibly deterministically equal to zero, i.i.d interarrival times exponentially distributed with exponent ?, and arrival density functions ?¯T(·) and ?¯¯T(·,·).
 {ti,m}8m=1 is a bounded-density Markovian sequence with ti,1 =   and arrival density functions ?¯t(·) and ?¯¯t(·,·) for every i ? N.
   is i.i.d. under , with mth-order moment delay profile functions, as shown in (8a) at the bottom of the page, for m ? {1,2}.
   is i.i.d. upon , with mth-order moment delay profile functions, as shown in (8b) at the bottom of the page, where m ? {1,2} for every i ? N.
 
 Notice that, in the definitions in Assumption 1, functions bm(·) and gm(·), m = 1,2 capture the moment delay profiles between clusters and within clusters, respectively, without taking arrival rates into account. In turn, the single and joint arrival rates of clusters are represented by functions ?¯T(·) and ?¯¯T(·,·), whereas the single and joint relative arrival rates of rays are given by ?¯t(·) and ?¯¯t(·,·), respectively.
 The requirements and suppositions in Assumption 1 are satisfied for all the channel models described in [1]–[4], [23], and [24]. Indeed, by choosing cluster and ray arrival times as Poisson processes, with each |ai,m| conditioned to ti,m being Rayleigh distributed with second moment g2(t) = exp(-t/?), and deterministic Ai’s with the form Ai = b0 exp(-Ti/G), G, ? > 0, we obtain the widely used SV model [1]. A similar choice with lognormally distributed amplitudes |Ai| and |ai| yields the model proposed by the IEEE 802.15.3a task group [2] (excluding its large-scale fading term). The decay profiles of the amplitudes [represented by the functions b and g of (8)] can be also chosen to match those that the IEEE 802-15.4a standard recommends for high frequencies in some scenario types (such as office and industrial scenarios) [16].
  
 	 ,	if T1 = 0 deterministically and x = 0
 0 deterministically and(8a) is randomRemark 1 (Mixed Poisson Arrivals do not Give Rise to a Mixed Poisson Distribution): It is worth mentioning at this point that the mixed Poisson arrivals, which are characterized by the conditional pdf given in (6), do not give rise to what is known as a mixed Poisson distribution [25]. More precisely, if the random exponent selection in (6) takes place after each arrival, then the probability of having n arrivals in any unit-length time interval [x, x + 1] cannot be written as ßpn(?1) + (1 - ß)pn(?2), where pn(?) denotes the Poisson probability mass function with parameter ?. (The exact probability distribution has been derived by the first author in a work currently in preparation.)	
 Another assumption that will facilitate the forthcoming analysis is related to the number of clusters and rays. Although in (1), as in [1], [2], and [16], the number of clusters Nc and the number of paths within each cluster Nr are finite, we will consider infinitely many clusters and rays per cluster. As acknowledged in [1], the latter choice is arguably more realistic. Of course, this assumption requires that the amplitudes decay fast enough with increasing delay. We will make this requirement precise by assuming that the conditional moments of the amplitudes defined in (8a) and (8b) satisfy
  
 and work in the sequel with the channel impulse response
 	 	(9)
 with ai,m and ti,m as defined in Section II-A. In addition, as in [2], we will adopt a real baseband model for the impulse response; hence, the coefficients pi,m in (2) are i.i.d. Bernoulli random variables taking the values 1 or -1 with equal probability. This is done mainly to simplify the notation, and it is worth mentioning that all forthcoming results also hold for the complex baseband representation. (All that is required is that (3) and Assumption 1 hold.)
 From (9), the squared magnitude of the channel frequency response is
  
 A deterministic frequency-dependent gain for each ray, which is a feature discussed in [4], [16], and [19], can be easily incorporated to our model by simply multiplying |H(j?)|2 by the squared magnitude of this gain. For ease of notation, and because the effects of this factor on the resulting statistics can be easily added afterward, we will not include it in our expressions.
 In the following and under Assumption 1, we will derive the exact closed-form expressions for the second-order statistics of |H(j?)|2 and of the channel power over any given frequency interval as a function of the moment delay functions b2, b4, g2, and g4.
 III. AUTOCOVARIANCE OF |H(j?)|2
 Here, we will obtain closed-form expressions for the mean, the autocovariance, and the autocorrelation coefficient of |H(j?)|2 over any interval of angular frequencies. These results rely on several technical lemmas, which can be found in the Appendix.
 A. Expected Value of |H(j?)|2
 From (10) and the zero-mean and independence properties of the polarities {pi} established in (7), it is easy to verify that
 	  .	(11)
 Applying(2),theindependencerelationships(7d),andLemma1
 (in the Appendix), it is readily found that
  
 Thus, the expected value of |H(j?)|2 is the same for all frequencies, being equal to the product of the average energy of the cluster amplitudes {Ai} and the ray relative amplitudes {ai,m}.
 B. Autocovariance of |H(j?)|2
 Denote the autocovariance of |H(?)|2 by
  
 The following theorem, which is the main result of this paper, provides an exact closed-form expression for c(?1, ?2) in terms of the arrival density functions ?¯T, ?¯¯T, ?¯t, and ?¯¯t and the moment delay functions b2, b4, g2, and g4 defined in Assumption 1.
 Theorem 1: LetclustersandraysdistributeasinAssumption1. Define the effective moment delay profile functions
 Bm(x) =? ?¯T(x)bm(x)	?(y, z) = ?¯¯T(y, z)b2(y)b2(z)
 Gm(x) =? ?¯t(x)gm(x)	F(y, z) =? ?¯¯t(y, z)g2(y)g2(z) (14)
 for m ? {2,4}, and let, respectively, denote
 their Fourier transforms. Then, for any	T ? R
 Proof: See the Appendix.	 Remark 2: Theorem 1 reveals that the variance of the channel power gain |H(j?)|2 exhibits the same variation with ? as does its autocovariance c(((T + O)/2),((T - O)/2)) with O. Since the overall behavior of the latter is to decrease as O grows, it follows that the variability of the channel power gain is smaller at higher frequencies. Notice that this reduction does not come from the frequency dependence affecting each multipath component (which is typical of real indoor propagation channels [26]) since we have not included it in our model. However, had this dependence been included, the variance-to-squared-mean ratio of |H(j?)|2 would still exhibit the decay with ? predicted by Theorem 1.  Instead, the origin of this behavior can be more intuitively understood by looking at (37) in the Appendix and by recalling that E[|H(?)|2] is independent of ?. In (37), if we choose ?2 = ?1 = ?, then T2(?1 + ?2) = T2(2?) becomes the only frequency-dependent term. Now, T2(2?) is a sum of expectations of the random variables  . At ? = 0, T2 turns into a sum of nonnegative terms. By contrast, if we let ? increase so that the pdfs of the random variables 2  extend smoothly over an interval that is several times larger than 2p, then the phases of the exponentials will be approximately distributed uniformly over [0,2p]. This will reduce each of the expectations in the sum. Thus, T (2?) decreases as ? grows, and so does E[| (?1)|2 (?2)|2].
 Remark 3: If the effective moment delay functions and Bm are “smooth,” then all the frequency-dependent terms in (15) vanish if ?1 - ?2 tends to infinity (while keeping ?1,?2 > 0), leaving only   -
  . This implies that the autocovariance between two infinitely distant frequency values is greater than zero, which, at first sight, may seem counterintuitive. Nevertheless, although this seems to be the first time that such behavior is proven to exist for SV-like channel models, it is consistent with the saturation of the channel diversity order as the bandwidth increases, as reported in [13] and [15]. (Further treatment of this phenomenon is presented in Sections III-B3 and IV.) As will be discussed in the following, this nonzero asymptotic autocovariance is due to that the arrival rate of clusters is finite.	
 We now apply these results to some specific cases.
 1) Infinitely Many Clusters: Suppose the arrival densities of rays and clusters are scaled by ? > 0 and µ > 0, respectively, to obtain arrival rates
  
 where
  ,	if T1 = 0 deterministically otherwise.
 Let us also suppose that this is done while preserving the original PDPs (so that the total impulse response power is maintained), which requires one to scale g2 and b2 by ?-1 and µ-1, respectively. This implies that functions g4 and b4 are scaled by ?-2 and µ-2, respectively. Denote the resulting scaled moment profile densities by , and ?.
 Substituting them into (14), the behavior of the terms involved in the frequency-independent part of (15) as cluster and ray densities going to infinity is given by the following limits:
  (16d)
 Therefore, for any given and smooth PDP, lim ?1-?2|?8
 |
 c(?1,?2) = 0 if the cluster arrival rate tends to infinity and the right-hand side (RHS) of (16b) is zero. With this, as µ ? 8, in the case in which clusters have Poisson arrivals (which implies ?(y, z) = B2(y)B2(z)) and if the first cluster arrives randomly (i.e., if q = 0), then the autocovariance of |H(j?)|2 takes the simpler form
 
 2
 	 	.
 squared magnitude Fourier transforms of the intercluster PDP and the intracluster PDP evaluated at ?1 - ?2 and ?1 + ?2.
 Only One Cluster/No Clusters: Consider the case in which rays do not exhibit the presence of clusters. This situation can be analyzed under the framework defined by Assumption 1 by supposing that there is only one cluster with deterministic arrival time T1 = 0 and amplitude A1 = 1, and such that its first ray arrives deterministically at time t1,1 = 0. To ensure that there are no other clusters, we may take E[Ak|Tk = T] =  . With these choices, we obtain  1, and	 0,
 ?? ? R; thus, the autocovariance becomes
  .
 Nonzero Asymptotic Autocovariance: As previously mentioned in Remark 3, Theorem 1 reveals that, for finite cluster arrival rates, the autocovariance of |H(j?)|, i.e., c(?1,?2), tends to a positive value as ?2 ? 8 (i.e., if O and T tend to 8), and not to zero. To the best of our knowledge, this is the first time that this result is derived or revealed, at least for an SV-like channel model. (A simple and intuitive explanation of this phenomenon is provided at the beginning of Section IV-B, in terms of the variance of the aggregate channel power gain over a given band). It is worth noting that such asymptotic behavior cannot be observed if one analyzes the autocovariance of the complex frequency response H(j?), i.e., by looking at the second-order statistics of H(j?) instead of those of |H(j?)|2. Indeed, by applying the independence relationships (7a) and (7d) and then Lemma 1, it becomes easy to show that
  .
 This simple expression (which we believe to be novel), implies that, if the moment delay profiles of clusters and rays within them are smooth, then lim|?1-?2|?8 E[H(j?1)H*(j?2)] = 0.
 C. Correlation Coefficient of |H(j?)|2
 The correlation coefficient of |H(j?)|2, which is denoted as ?(?1,?2), is given by
 	 .	(17)
 From (13), it readily follows that (17) can be written as
  
 It was already shown that, for any given smooth moment delay profile functions, the autocovariance between two frequencies ?1 and ?1 + O tends to zero as O ? 8 if the cluster arrival density goes to infinity. We shall see in Section V-A that even when this arrival density is finite, limO?8 ?(?1,?2 + O) = 0 if the moment delay profiles change so that the number of clusters with a significant amplitude tends to infinity.
 IV. SECOND-ORDER STATISTICS OVER A FREQUENCY BAND
 Here, we will use Theorem 1 to derive closed-form expressions for the mean and the variance of the total channel power over any given frequency band, i.e.,
 ?
 B
 	= [?l,?r] ? [0,8),	0 = ?l = ?r
 where ?l and ?r are in radians per second.
 A. Expected Value of the Channel Power Over a Frequency Band
 ?
 	Denote the channel power over frequency band|	|2] is constant for allB by PB =
 . Given that E[ H(j?)
 [see (11)], it follows from (12) that
  
 where
 ? W = |B| = ?r - ?l.
 B. Variance of the Channel Power Over a Frequency Band
 The	variance	of	the	channel	power	over	band	B can be obtained directly from the autocovariance of |H(j?)|2 as  	(19)
 Before proceeding, it is worth noting that (19) readily implies that, if c(?, u) is bounded, then
  
 since E[PB] grows linearly with |B|. This allows one to provide a simple and intuitive explanation for the fact that c(?, u) (i.e., the autocovariance of |H(j?)|2) does not vanish when the two frequencies ?, u = 0 are infinitely distant from one another. Recall from Parseval’s theorem that, when |B| ? 8, PB equals the total impulse-response power. Then, if the impulse response is amplitude-modulated by a finite number of randomamplitude cluster envelopes, it is clear that the ratio variance of impulse-response power over the square of the expected impulse-response power will not be zero. In view of (20), this will imply that  0. That is, the autocovariance of |H(j?)|2 between a pair of infinitely distant frequencies will not be zero whenever the total number of clusters with random amplitude remains finite. As anticipated, this provides an alternative and more intuitive explanation to what was already discussed in Section III-B3.
 We now return to characterizing the variance of PB. From (15), c(?1,?2) can be written as
 	 	(21)
 where
  
  .	(22b) Substituting (21) into (19) yields
  
 Recalling the change of variables O = ? - u, T = ? + u and integrating along diagonal strips over the square [?l,?r] × [?l,?r], we obtain
  
 We note that var(PB) is a measure of the fluctuation of the received power over a wireless channel and that the latter defines its fade statistics. The fact that fade depth decreases with channel bandwidth has been reported in the literature, both from empirical data (see, e.g., [13]) and from using simulations [12], [27]. However, a closed-form formula relating fade depth and the classical channel model parameters used here have, to the best of our knowledge, not been reported to date. The results derived earlier allow us to directly deal with this issue, provided that the fading distribution can be fully determined from its second moment (an assumption that was successfully applied in [12]). The explicit dependence between var(PB) and W is illustrated in the following example.
 V. EXAMPLE
 Here, we will illustrate the application of the results obtained in Sections III and IV to the “classical” SV model [1]. In particular, we will show that our analytical expressions found in Sections III and IV accurately predict the autocovariance of |H(j?)|2 and the variance of the channel power gain over any given band. In addition, we will show how the presence of clusters can be confirmed or discarded by using the secondorder statistics of |H(j?)|2 and the expressions derived in this paper.
 A. Model Parameters
 In the SV model, the amplitudes |ai,m| are Rayleigh distributed (conditioned to ti,m), and the Ai’s are deterministic exponentially decaying. The PDPs of cluster and ray amplitudes are given by
  .
 For the fourth-order moments, we have
  
 where the last equation follows since the fourth moment E[x4] of a Rayleigh-distributed random variable x is related to its second-order moment as E[x4] = 2E[x2]2. Both cluster and relative ray interarrival times are exponentially i.i.d. with exponents ? and ?, respectively. In addition, the arrival times of the first cluster and the relative arrival time of the first ray in every cluster are, by definition, zero. Therefore, the effective moment profile functions defined in (14) take the following form:
  
 B. Autocovariance
 Recall from (21) that c(?1,?2) can be written as
 	 .	(25)
 In this case, from (22), we have
  .
 If , the decomposition into partial fractions of the term
  
 allows one to write
  
 Substituting this result into (25) yields
  
 Fig. 1 shows (with dashed-line curves) the empirical estimates of c(·,·) of the SV channel model with four different sets of parameters, which are specified in Table I. Each of these curves was obtained after 300000 simulated random realizations of a channel impulse response. On the same plot, the theoretical values of c(·,·) predicted by (28) are traced using solid lines, for each set of parameters. It is shown that, in all cases, (28) matches the simulated data very tightly.
 The set A parameters correspond to those proposed in the IEEE802.15.3a CM3 [2]. Notice that, for the curves of this set, as O is increased, the spectral autocovariance drops by approximately 3 dB when O = 1/G = 7.14 × 107 rad/s, and then decays at about 10 dB/dec, which is precisely the behavior determined by (28) (provided K2 is much larger than K1 and K). From (28), the next corner separation frequency, beyond which c(·,·) almost ceases to diminish, takes place when  , which in this case corresponds to   rad/s, in agreement with what is shown in Fig. 1. The set B curves differ from those of the previous one in that the arrival rates are halved, whereas the decay exponents are doubled. This yields the same products ?G and ?? and the same ratio G/? as the set A. Thus, the constants K, K1?2, and K2G2 are also the same as those yielded by the set A parameters. Accordingly, the only difference with respect with the set A curves is that the corner frequencies are reduced by a factor of
 2. In addition, as predicted by (28), reducing ? to one fourth of
  
 Fig. 1. Spectral autocovariance of |H(j??)c|2=as a function of the frequency109 rad/s) for the SV model separation O (around a central frequency with the four sets of parameters shown in Table I. Simulated curves (in dashed line) are averages over 300000 channel realizations. Each theoretical curve (in solid line) was obtained using (28).
 TABLE I
 SETS OF PARAMETERS FOR THE SV MODEL USED FOR THE SIMULATIONS IN Section V
  
 its value in set A increased the corner frequency 1/? associated with K1 by 4, which is sufficiently higher than 1/G to have a small but noticeable effect (see the slight “bump” in the set C curves between 2 × 108 and 7 × 108 rad/s. With the parameters of set D, these two corner frequencies are two decades away from one another, with their presence becoming clearly visible in the corresponding plot in Fig. 1. Intuitively, the existence of two corner separation frequencies beyond which the spectral autocovariance starts (or restarts) to decrease can be associated with the temporal resolution associated with O. More precisely, when O is too small, all rays in the impulse response are added with roughly the same phase in the sum (10) for ?c + O/2 and for ?c - O/2, yielding a high correlation. As O is increased, a point is reached   at which some clusters within the impulse response contribute with different random phases in (10), which begins to decorrelate the channel power gains. This reduction ceases when O is large enough so that all clusters within the impulse response are added with different phases, restarting only when O begins to make also the rays within each cluster add with different random phases  .
 C. Correlation Coefficient
 The variance of |H(j?)|2 is directly obtained by evaluating (28) for an arbitrary frequency ?/2, which yields
  .
 By substituting this and (28) into (18), the correlation coefficient takes the form in (29), shown at the bottom of the page. By substituting the definitions of K, K1, and K2 into this expression and after some algebra, one obtains that the correlation coefficient satisfies
  .
 Thus, extending what was found for the autocovariance of |H(j?)|2 in Section III-B1, we see here that the lower asymptote of the spectral power correlation as O ? 8 vanishes not only when ? ? 8 but also when the product ?G ? 8.
 Moreover, for any ?1 ? R, we have that limO?8 ?(?1,?1 + O) = 0 if and only if ?G ? 8.
 D. Clusters or no Clusters?
 When analyzing a sufficiently large number of realizations of the impulse response of a wireless channel, the only SV model parameters that can be directly estimated are the ray arrival rate ? and the intercluster decay exponent G. In contrast, the intercluster arrival rate ? and the intracluster decay exponent ? are not readily observable. Indeed, the very existence of clusters is not unquestionably evident from the impulse-response realizations.
 Here, based upon the results earlier, we propose a quantitative method for estimating ? and ?. By doing this, it is possible to assert the presence of clusters (if the value 1/? obtained is comparable to or smaller than G) or the absence of them (if
  
 As previously mentioned, the intercluster decay profile G and the intracluster arrival rate ? can be estimated directly from measurements (as done in, e.g., [1] and [19]). Additional equations for finding the remaining two parameters (? and ?) can be generated by evaluating c(O,O) at two or more different frequencies.5 For the SV channel model considered in this
 4If there were no clusters, the impulse response would be as if there were only a single cluster, beginning at t = 0, with decay exponent G.
 5In practice, it will be also necessary to identify the power scaling factor of the impulse response, namely, the value of b2(T) at T = 0. This can be easily done by taking the empirical mean of the squared magnitude of the first ray in the impulse response.
 example, the two simplest equations can be obtained from (28),
 i.e.,
  
 The left-hand sides (LHSs) of these expressions (L1 and L2) can be directly estimated from the measurements. Denote these estimates as Lˆ1 and Lˆ2, respectively. In principle, one could express ?G and ?? in closed form as functions of L1 and L2, and then evaluate for L1 = Lˆ1 and L2 = Lˆ2. However, ?? is in this case the solution to the following quadratic equation:
 	(?? + 1)2 - (L1 + 1)(?? + 1) + 2L2 - L1 = 0	(32)
 resulting from substituting (30) into (31). Therefore, the approximate nature of Lˆ1 and Lˆ2 can turn a real positive root of (32) into a complex-valued (or negative) one. Moreover, even if one could measure L1 and L2 with infinite precision, there can be still two real-valued roots of (32) greater than 1. To overcome these difficulties, the following algorithm is proposed.
 Find each of the p local minimizers of
 for L1 and L2). Denote these minimizers as y˜1,...,y˜p, respectively, where p ? {1,2}. From these, calculate
 ? the nonnegative pseudominimizers yi = max{0,y˜i}, i = 1,...,p. These will be the preliminary candidates for ??.
 Let xi = Lˆ1/(yi + 1) - 1, i = 1,...,p, be the corresponding preliminary candidates for ?G.
 For each i = 1,...,p, let ?i = xi/G and ?i = yi/?.
 Then, apply a least squares curve-fitting optimization algorithm, with ? and ? as unknown values, to match (28) against the empirical estimates of c(·,·) at several frequencies. Use ?i and ?i as initial values. Define the obtained residual matching errors as i
 Pick the values of ?i and ?i associated with the minimum i.
 This technique for estimating ?G and ?? was applied to simulated impulse responses for the SV channel model, with
 Fig. 2. (Left) Typical impulse response for the SV channel model with the parameters of (top) set E (? = 1.1 ns-1,? = 1.1 ns-1,G = 14 ns, and ? = 7.9 ns) and (bottom) set F (? = 0,? = 16.94 ns-1,? = 14 ns). Multipath components from the same cluster are shown with the same color. (Top right) Twelve estimates of ?G and ??, each from the 4000 realizations of the impulse responses for each set of parameters, which are obtained by applying the method proposed in Section V. (Bottom right) Autocovariance of |H(j?)|2 from (28) and ±5s range along them, where s is the standard deviation of the empirical estimate of
 c(·,·) from 4000 channel realizations.
 two sets of parameters. For the first of these sets (set E), ? = 1.1 ns-1, ? = 1.1 ns-1, G = 14 ns, and ? = 7.9 ns. Notice that this makes ? = ?, i.e., the clusters arrive at the same rate as the multipath components within them, making it very difficult to distinguish one cluster from another in the time domain.
 It may be noticed that the chosen intercluster and intracluster decay exponents (G and ?) are the same as those from the IEEE802.15.3a CM3 [2] used in Section V-B. In turn, the product ?? has been chosen about ten times larger than that used in Section V-B, so that the resulting dense train of rays makes it even harder to recognize the presence of clusters. A typical channel impulse response obtained for these parameters is shown in Fig. 2 (top left). As anticipated, with the chosen parameters, clusters arrive so densely that it is virtually impossible to tell one from the other. After simulating 12 sets of 4000 independent realizations of these impulse responses, estimates of L1 and L2 were obtained by averaging. From the corresponding 12 empirical estimates of c(0,?), which are obtained for 500 values of ? evenly distributed from 0 to 5 × 109 rad/s, the parameters ?G and ?? were estimated using the method described earlier, yielding the results shown in the scatter plot of Fig. 2 (top right). The average error magnitude of these estimates is less than 30% of their true values.  More importantly, in all cases, the existence of clusters is unambiguously revealed since all estimates of ?G are greater than 13. This seems more remarkable after recalling that, in this case, the impulse responses are such that clusters are almost totally overlapped, making it virtually impossible to distinguish one from the other.
 The results for a scenario in which there are no clusters is shown in Fig. 2 (bottom). In this case, the set F parameters is used for the SV model: ? = 0, ? = 16.94 ns-1, and ? = 14 ns. A typical channel impulse response from these parameters, as the one shown on Fig. 2 (bottom left), has the same overall decay exponent and a similar net density of rays as in the previous case. The corresponding estimates of ?G and ??, which are obtained by applying the algorithm proposed here, are shown in the scatter plot in Fig. 2 (top right). Here, for 12 sets of 4000 realizations of the channel impulse response, the corresponding 12 empirical estimates of c(0,?) for 500 values of ? evenly distributed over [0,5 × 109] rad/s were obtained. Notice how, in all 100 realizations, the estimates of ?G are well below 3 × 10-2, unambiguously revealing the absence of clusters.
 The possibility of correctly estimating the true parameters of a channel (?, G, ?, and ? in the SV case) from the empirical estimate of c(·,·), e.g., c˜(·,·), averaged from N independent channel realizations, will ultimately depend upon the accuracy of the latter estimate. To illustrate this dependence, the theoretical autocovariance c(?c + (O/2),?c - (O/2)) is plotted in Fig. 2 (right bottom), for the parameter sets E and F. The regions within five standard deviations of the estimate c˜ above and below each of the c(?c + (O/2),?c - (O/2)) curves is shown as shaded areas. The standard deviation shown corresponds to what is obtained when c˜(·,·) is calculated from N = 4000 channel realizations, which is the same number utilized to obtain the estimates shown in Fig. 2 (top right). Given the significant overlap of these regions and since the variance of c˜(·,·) is proportional to N-1, it is clear that, in this case, a reliable decision about which of the parameter sets better fits the data cannot be achieved from much less than 4000 channel realizations.
 We end by noting that the parameter estimation algorithm described earlier has been proposed for its simplicity and with the purpose of illustrating the potential applicability of the results derived in Sections III and IV. Therefore, the search for better estimation algorithms, which must certainly exist, goes beyond the scope of this paper.
 E. Second-Order Statistics of Total Power Over a Band
 ?
 The variance of the channel power over a band B = [?l,?r] rad/s is obtained by substituting (27) into (23), with the change of variables v = ?l - u and v = ?r + u as follows:
  
 ?
 Evaluating these integrals, using the change of variables ?c = (?l+?r)/2, and applying the identity arctan(x)+arctan(y) = arctan((x + y)/(1 - xy)), we obtain, after some manipulation, that var(PB)
 The relative variability of PB with respect to the average total power over B is better captured by the normalized variance, i.e.,
 	 	(34)
 where the equality is obtained by substituting (24) and (14) into (12).
 Equations (33) and (34) reveal several interesting aspects about the normalized channel power variance, which are discussed in the following.
 Except for the first term on the RHS of (33), all the other terms grow subquadratically when W is above some frequency. It is a simple (although long) exercise of algebra to show that the frequency above which the growth rate with W of all these terms is significantly slower than
 W2 is given by Wsat =? ?-1 max{1,0.5(1 + (2?c?)2)}. If , then the normalized variance of the total power over B can be well approximated as
  
 where we have used (26). Thus, in agreement with what was shown in Sections III-B1 and IV-B, the normalized variance of the power over an asymptotically infinite bandwidth vanishes only when the number of significant clusters tends to infinity.
 The narrow-band normalized channel power variance var(P0)/E[P0]2 is obtained from (34) by letting O ?
 	0. If W	is small enough, then all the terms in
 (33) grow proportional to W2. It is straightforward (but lengthy) to show that the maximum value for
 ? this to be a reasonable approximation is Wflat =
 G-1 min{1, . When W is
 smaller than the latter threshold, it holds that
  
 and therefore
  
 Thus, unless the bandwidth W is comparable to or larger than Wflat, no reduction in normalized channel power gain variance is to be expected.
 Notice also that, if there is a single cluster with deterministic amplitude and infinite duration (i.e., if we let G = 0 and ? ? 8), the narrow-band normalized channel power variance should coincide with that of a Rayleigh channel (in which case PB should distribute exponentially). Indeed, if we substitute the expressions for K1 and K2 in (35), fix G = 0, and let ? ? 8, then it is easy to verify that var(PB)/E[PB]2 ? 1, which is precisely the variance-to-squared-expectation ratio of an exponentially distributed random variable.
 The curves in the dashed line in Fig. 3 display the value of var[PB]/E[PB]2 for a band of width W centered at 109 rad/s obtained from 300000 simulated realizations of the SV model for each set of parameters given in Table I. The corresponding values of var[PB]/E[PB]2 predicted by (34) are plotted in the same figure with solid lines. It is shown that, in all cases, the theoretical and simulation curves are almost indistinguishable, confirming the accuracy of our results. It is also shown that, in all sets, as expected from (34), when W is increased from zero, the normalized channel power variance remains almost constant until a certain value, near the threshold Wflat defined previously. Beyond that threshold, all the curves decay with W until a saturation point is reached, in a way that is consistent with what was found in [13].
 VI. CONCLUSION
 We have derived the general expressions that characterize the second-order statistics of the frequency response power gain in wireless indoor channels. Our results are applicable to several well-established channel models discussed in the literature, all based upon the SV model [1]. In particular, the closed-form formula obtained here for the autocovariance of the squared frequency response magnitude of the channel allows one to predict the variance of the aggregate channel power gain over any frequency band. This provides an approximation
  
 Fig. 3. Normalized variance of the channel power gain over a band B of width W centered at 109 rad/s, for the SV model with the four sets of parameters (A, B, C, D) given in Table I. Simulated values are averages over 3000000 channel realizations. The theoretical curve was obtained using (33) and (34).
 to the diversity order of wideband over narrow-band systems in such channels. Our results also allow one to obtain an upper bound for this measure of spectral diversity, and they show how (and why) this limit arises from having a finite number of clusters with a significant amplitude in the channel impulse response. In addition, the derived formulas explicitly reveal how the statistical properties of the clusters in the channel impulse response affect the spectral autocovariance of the channel power gain. This allows one to use these spectral statistics to identify the presence or absence of clusters. A simple procedure to accomplish this task has been proposed and its effectiveness has been verified by applying it to simulated realizations of the SV channel model.
 APPENDIX
 Proof of Theorem 1: For notational simplicity, we will temporarily adopt a single indexing nomenclature for the path arrival times ti,m. More precisely and with a slight abuse of notation, we define the infinite random set  
  . For our purposes, it will not be necessary to define a mapping between the indexes i and m and the index . Indeed, it will be sufficient to note that the double-index matching condition (i, m) = (j, n) (which means i = j and m = n) is equivalent to the single-index condition .
 Using the single-index notation, the first term on the RHS of (13) is obtained from (9), which yields
  
 Consider the cases in which r is different from all other
 coefficients. Then
  
  
 where (a) follows from (7), and the last equality stems from In (38), (a) is a consequence of the independence relationships the fact that all {pi} have zero mean [see (3)]. Proceeding established in (7), whereas (b) follows directly from Lemma 1. similarly, it is easy to show that each term in the summation Proceeding similarly with the frequency-dependent function of (36) in which one or more indexes is not matched to any T2(·), we obtain other index evaluates to zero. Thus, one must only consider the cases
 and ), which yield, respectively, each of the following sums:
 notation, we have	 
 Substituting these expressions for T1 and T2(·) into (37), followed by inserting the result and (12) into (13), yields
 th moment delay profile of , and the single and joint effective arrival densities ?¯x(·) and ?¯¯x(·,·) are as in
 Proof of Lemma 1: Let f(x) =? E[?km|xk = x], k ? S, with as in Definition 2. Recall the definitions of ?¯(i)(x) and from (5). If x1 = 0 deterministically, then
  
 where Proposition 1 in the Appendix has been used. The last term in the latter expression is the difference between the rightmost term in (38) and the squared RHS of (12). Using the effective moment delay profile functions defined in (14), the autocovariance of |H(j?)|2 can be written as
  
 With the change of variables
 ?	? O = ?1 - ?2,	T = ?1 + ?2
 one can write (39) as (15), completing the proof.	
 Lemma 1: Let  be an incrementally Markov random process, with x1 possibly being deterministically equal to zero, and let  be a random sequence i.i.d. under  . Then, for any given m ? N
  
 provided that the integrals exist, where
 	E [?1m],	if x1 = 0 deterministically
 	???	x
 f¯m(x) =?
 	?E [?km|xk = x],	k?{1,2,...}, if x1 is random
 where (a) follows from Lemma 3 and (5) and (4). Else
  
 For the double sum in the lemma statement, if x1 = 0 deterministically, then
  
  
 where (a) follows from Lemmas 3 and 4 (in the Appendix) and from (5) and (4). In turn, (b) was obtained using the fact that ?¯(2)(y) and f(y) are bounded for y = 0.
 Finally, if x1 is random, it readily follows by applying Lemma 4 in the Appendix that
  
 which completes the proof.	
 Lemma 2: Let c > 0 be such that ln(1/c) > 2. Then
   .
 Proof: We have that
  .
 Then, since ln(1/c) > 2
  
 Similarly
  .
 
 Lemma 3: Let the arrival times  be an incrementally Markov sequence with x1 randomly distributed. Define function  as in (4). Then
 	 	(40)
 Proof: We have that
  
  (41)
 where the discrete random variable n?(x) corresponds to the number of arrivals in [x, x + ?). Its expectation satisfies
 Pr{n?(x) = 1} = E [n?(x)]
  
  (42)
 where the last inequality follows from Lemma 2. Thus
  
 Hence
  .
 On the other hand
 Pr{n?(x) = 1} = Pr{n?(x) = 1} = E [n?(x)].
 Therefore
  	(43) Substituting this result into (41) yields (40), thus completing the proof.	 Lemma 4: Let the arrival times  distribute as in
 Lemma 3, and define ?¯¯(y, z) as in (4). Assume that ?¯¯(y, z) is bounded for all . Then
  
 Proof: We have that
  
 For the expectation within the first integral, we have
  
 By dividing by ? and taking the limit as ? ? 0, and then substituting (43) and applying Lemma 2, it follows that
  .
 On the other hand, for z > y + ?, we have
 E [n?(y)n?(z)]
 = Pr{n?(y) = 1,n?(z) = 1}
 + Pr{n?(y) = 2,n?(z) = 1}
  
 + Pr{n?(z) = 1|n?(y) = 2}Pr{n?(y) = 2}
  
  
 + Pr{n?(z) = 1|n?(y) = 1}Pr{n?(y) = 1}??
  
 	 .	(45)
 Noting that Pr{n?(y) = 1,n?(y) = 1} = E[n?(y)n?(z)] and that the integrand in (44) is symmetric in y and z, and proceeding with (45) as in (42), we conclude that
  
 Additionally, since
 Pr{n?(y) = 1,n?(z) = 1} = Pr{n?(y) = 1,n?(z) = 1} =E [n?(y)n?(z)]
 it follows from (46) that
  
 The proof is completed upon substituting this result into (44).	 Proposition 1: Let ?¯¯x(y, z) be as in Definition 3, and let f : R ? R be a bounded function. Then
  
 Proof: If x1 is random, then ?¯¯(y, z) is bounded, and the result follows trivially. Otherwise, x1 = 0 deterministically, in which case
  
 completing the proof.	
 
 ﻿ In this paper we propose a new approach to robust optimal experiment design . The key departure from work is that we specifically account for the fact that , prior to the experiment , we possess only partial knowledge of the system . We also give a detailed analysis of the solution for a simple case and propose a concave optimization algorithm that can be applied more generally . 
 : Experiment Design , Optimal Input Design . 
  
 . INTRODUCTION 
 It is well known that the choice of experimental a strong influence on the accuracy of from system identification . This substantial research on experiment design over almost a century . Early appear in the statistics literature , ; Cox , ; , ; and , ; and , ; , ; Whittle , ; Wynn , . This work was later to the problem of identification of dynamic , ; , ; and , ;.. and , ;.. and , ; and , ; , . Some of the later work is in and , ; , . 
 Our focus here will be on experiment design for dynamic . Early work on this problem predominately on frequency domain . More recently , there been substantial effort devoted to the inter relationship between identification and control together with the associated issue of input signal design and , ; , . However , a major drawback of all the above work is that , generically , the optimal test signal for dynamic system identification is a function of the unknown system i . e . it on the very thing that the experiment is at finding . 
 Indeed , , : It should be noted that , as usual in experiment design , in order to compute the optimal design the true system to be known . that are robust with respect to uncertainty about the system is a wide open research 
 field . 
 The goal of the current paper is to propose a methodology for this problem . In particular , we formulate a robust optimal experiment design criterion . We also demonstrate the use of this criterion for a simple case . 
 The layout of the remainder of the paper is as : In Section we give a general formulation of the robust optimal experiment design problem . In Section , we focus on a simple one parameter problem so as to give insight into the problem . In Section we convert the problem to an approximate matrix formulation and discuss asymptotic behavior when the prior knowledge is diffuse . In Section we describe an algorithm for the robust optimal experiment and give a numerical example . In Section we describe an extension to parameter . Finally , in Section we draw . 
 . GENERAL FORMULATION 
 Our focus in the current paper is on how to design the experimental so that the information from a particular experiment is in some specific sense . 
 To motivate our approach we consider a single input single output linear discrete time system of the form : 
 G ut G where G , G are rational transfer in the forward shift operator , G and where 
 is white noise of variance . We let where the 
 in G and the in G . 
 We recall that the log likelihood function for , is given by 
  
 where G G ut . 
 Fisher information matrix is by taking the following expectation and , 
  
 and EY the expectation over the distribution of the data given . 
 We assume an open loop experiment so that and ut are uncorrelated . We also assume that G , G no common . Taking , as in , we have be partitioned as 
  
 where M is the part of the information matrix related to and M is independent of the input . Then 
 , 
 where 
 Notice that M on the full parameter vector . large , it is more convenient to work with the scaled average information matrix for the parameter , i . e . 
  
 Theorem , we finally have that 
  
 where fu is the discrete input spectral density . 
 It is also possible to do a parallel development and , for continuous time . In the latter case , is by 
  
 where G and G are continuous time transfer assumed independently and fu is the continuous time input spectral density . 
 a matrix , we will need a scalar measure the purpose of experiment design . In the nominal case in the literature i . e . when is 
  
 assumed known , several of the size been . are 
 i and , 
  
 , fu , fu 
 Experiment design for robust control 
 , ; and , 
  
 where , is a frequency dependent vector related to the gap and , . 
 Thus nominal experiment design is at choosing fu to maximize a function of the type shown in , . Note , however , that the optimal input spectrum , inter on the unknown parameter vector . To address this paradox , we propose an alternative robust optimal experiment design procedure . 
 We assume that we do not have complete a prior knowledge of the true parameter value . Instead , we assume that the can take any value in a compact set . We propose that fu be chosen as : 
  
  
 any suitable scalar measure of . Note that we also depend explicitly on . This is standard in nominal experiment design , see for example . Also , we can use the explicit dependence on to 
  
 associate different weighting to , fu depending on the nature of the prior knowledge regarding . 
 We also need to constrain the allowable set of input . A typical constraint used in experiment design is that the input energy is constrained i . e . we define 
 S 
 We note that the are independent it only as a scaling factor in . For clarity in the following discussion we assume white noise and hence only refer to the parameter in the sequel . 
 . A SIMPLE EXAMPLE 
 To give insight into the robust optimal experiment design problem , we consider a simple continuous time problem where G and 
  
 For the model , it that 
  
 . Nominal Optimal Experiment Design . 
 Nominal experiment design that an initial estimate , is available . Based on this information , the function fu is chosen so as to maximize some scalar function of subject to a constraint on the output or the input power . 
 One interesting observation is that and define a convex combination of the set of all single frequency matrices . This to several use 
  
 , e .. any value of , fu for fu can be by a finite number of . Also , in the scalar parameter case of and it can actually be shown that we need only use a single frequency input for optimal experiment design and , , namely , fu . Moreover , by differentiation it is readily seen that the optimal input frequency is 
  
 This is an intuitively pleasing result , i . e . one the test signal at the nominal break point . However , equation the fundamental difficulty in nominal experiment design , namely the optimal experiment on the very thing that the experiment is at . 
  
  
 Fig . . M , fu as a function of for nominal input , noise solid . 
 To gauge how important the dependence on is , we note our example at the rate of per decade as a function of both and . Hence , given the prior estimate of the . Also , say that the true parameter in say we choose for the input signal 
 the range is 
 proximately th of the nominal value This to suggest that nominal experiment design is limited to those where a good prior estimate is available . A 
  
 plot of M , fu versus is given in Figure . 
 The reason for multiplying by is that is a variance measure and thus relative 
 mean square . 
 . Robust Optimal Experiment Design 
 We next turn to the robust experiment design in Section . For the scalar parameter problem 
  
 all equivalent and we thus use 
  
 Thus , our robust optimal experiment design can be 
 stated as 
  
 u 
 where 
 . 
 In subsequent , we will give further into the above design problem . 
 We first observe the following property of : 
 Lemma . Consider the problem stated in equation , the optimal input all its energy inside . Namely , 
  
 PROOF . Let be the optimal solution of . Then , 
  
 for all fu S 
 Let us now define 
  
 where is the indicator function for the set . 
 Then we readily observe that and 
  
 then 
  
 so that by we must have which the proof . 
 . APPROXIMATE ANALYSIS 
 Here we develop an algorithm for the robust experiment design problem , as in the last section , where we approximate the integral in equation by a sum . Specifically , Lemma , we 
 choose a grid of N 
 so that . 
 Then 
  
 where 
  
 and 
 . 
 Note that the matrix A Am , is symmetric and positive . 
 We can now state the following discrete alternative to the optimization problem in equation : 
 E 
 E N 
 where , 
 E is the column 
 of identity matrix and is vector of . 
 We next show that if there a that all equal , then E is optimal . 
 Lemma . Let E be defined by 
 . 
 Then , if we have 
 E E . 
 PROOF . Let E be the optimal solution for and E . Then we have 
  
 and , since e for all , 
 e 
 all . 
 On the other hand , the symmetry of A we obtain 
  
 negative we must conclude from and that Since by definition , all the of E are nonE which the proof . 
 The discussion above for any choice of grid for 
 . Let us now consider a special 
 choice of a logarithmic grid : 
  
 where 
 . 
 With this specific choice we have the following result for our original problem , 
 Lemma . The robust optimal test signal for diffuse prior information spectrum approximately given by 
 . 
 PROOF . Note first that since . 
 Furthermore , clearly so we have 
 S . Let us now choose a large integer N 
 and the corresponding grid as defined in and . 
 Then , by definition and 
 by 
 Clearly , as 
 . On the other hand , we note 
 that for the matrix A in , apart from the first and last few , the row sum is very nearly constant . 
 Hence , for some . 
 Namely , the in Lemma are approximately satisfied which the proof . 
 Remark . The immediate consequence of Lemma is that band limited noise is an approximation to the robust optimal input for our example see Figure . 
 . A NUMERICAL EXAMPLE 
 We show that the optimization problem can be converted into a standard linear problem . Let us denote 
 F 
 then we can readily show that is equivalent to the following optimization problem : 
 subject to : 
  
 where 
  
 and 
 This problem is readily as a problem . 
 Numerical Example 
  
 We consider the scalar parameter problem in Section where we assume , 
 N and compare 
  
 Fig . . of E for robust optimal input . 
 i A nominal input of frequency Note that this is the optimal input if the initial estimate of the 
 Band limited white noise input , . , rad sec . parameter is . 
 Band limited ‘ f noise input , . , rad sec . The robust optimal input by . 
 Relative for the different experimental are shown in Table . 
 Table . of Cost for Different Input 
  
 We see from Table that noise is approximately an order of magnitude better than a white noise input in of the cost function . Furthermore , going to the true optimum a further improvement . The optimal input energy is shown in Figure and Fig 
  
 ure the corresponding of M , fu as a function of . It is interesting to note from Figure 
  
 that M , fu is an almost constant function of . This should be with the result in Lemma . The latter Lemma that if the input that independent of is feasible , then 
 it is optimal . Here , the input which M , fu constant is not feasible but we see from Figure that the optimal of are almost constant . 
 . GENERALIZATION TO 
 For the parameter case we return to the general 
  
 expression for , given in and . We can again convert this into an approximate discrete form as was done in Section . We write 
  
  
 Fig . . Variation of cost function with for optimal input . 
 as an approximation to the integral in . The the element of the parameter set , the frequency and Em the input energy at the frequency . 
 In this case , we see that is a matrix for each discrete parameter value . Hence , as in Section , we need a measure of the size of . Say we choose ,, then the robust optimal design becomes 
 E E , E k 
 Obviously , there are many for the scalar function . One possible choice is 
 J E , min E 
 This cost function is by the following observation : the scalar function E 
 min min E we note that , by we have k 
 f E 
 min min E k 
 min min E E k 
 min min E min E k 
 min E min min E k 
 E E 
 which that E is a concave function of E . Hence , becomes a standard concave maximization . This latter aspect is the subject of the journal version of this paper . 
 . CONCLUSION 
 This paper a robust optimal experiment design procedure . We have for a simple case , that a near optimal input is band limited noise . We have also an algorithm to design robust in more general and have showing the gains from the use of the algorithm with either the nominal optimal experiment , band limited white noise or band limited noise . 
  
  
 ﻿We present several novel identities and inequalities relating the mutual information and the directed information in systems with feedback. The internal blocks within such systems are restricted only to be causal mappings, but are allowed to be non-linear, stochastic and time varying. Moreover, the involved signals can be arbitrarily distributed. We bound the directed information between signals inside the feedback loop by the mutual information between signals inside and outside the feedback loop. This fundamental result has an interesting interpretation as a law of conservation of information flow. Building upon it, we derive several novel identities and inequalities, which allow us to prove some existing
 information inequalities under less restrictive assumptions. Finally, we establish new relationships between nested directed informations inside a feedback loop. This yields a new and general data-processing
 inequality for systems with feedback.
 INTRODUCTION
 The notion of directed information introduced by Massey in [1] assesses the amount of information that causally “flows” from a given random and ordered sequence to another. For this reason, it has increasingly found use in diverse applications, from characterizing the capacity of channels with feedback [1]–[4], the rate distortion function under causality constraints [5], establishing some of the fundamental limitations in
 networked control [6]–[11], determining causal relationships in neural networks [12], to portfolio theory and hypothesis testing [13], to name a few.
 The directed information from a random  sequence xk to a random sequence yk is defined as
 k
 	I(xk ? yk) ,XI(y(i);xi |yi-1),	(1)
 i=1
 where the notation xi represents the sequence x(1),x(2),...,x(i). The causality inherent in this definition becomes evident when comparing it with the mutual information between xk and yk, given by
 . In the latter sum, what matters is the amount of information about the entire
 sequence xk present in y(i), given the past values yi-1. By contrast, in the conditional mutual informations in the sum of (1), only the past and current values of xk are considered, that is, xi. Thus, I(xk ? yk) represents the amount of information causally conveyed from xk to yk.
 There exist several results characterizing the relationship between I(xk ? yk) and I(xk;yk). First, it is well known that I(xk ? yk) = I(xk;yk), with equality if and only if yk is causally related to xk [1]. A conservation law of mutual and directed information has been found in [14], which asserts that I(xk ? yk)+I(0*yk-1 ? xk) = I(xk;yk), where 0*yk-1 denotes the concatenation 0,y(1),... ,yk-1.
 Given its prominence in settings involving feedback, it is perhaps in these scenarios where the directed information becomes most important. For instance, the directed information has been instrumental in characterizing the capacity of channels with feedback (see, e.g., [3], [4], [15] and the references therein), as well as the rate-distortion function in setups involving feedback [5], [9]–[11], [16].
 In this paper, our focus is on the relationships (inequalities and identities) involving directed and mutual informations within feedback systems, as well as between directed informations involving different signals within the corresponding feedback loop. In order to discuss some of the existing results related to this problem, it is convenient to consider the general feedback system shown in Fig. 1-(a). In this diagram, the blocks S1,...,S4 represent possibly non-linear and time-varying causal systems such that the total delay of the loop is at least one sample. In the same figure, r,p,s,q are exogenous random signals (scalars, vectors or sequences), which could represent, for example, any combination of disturbances, noises, random initial states or side informations. We note that any of these exogenous signals, in combination with its corresponding deterministic mapping Si, can also yield any desired stochastic causal mapping.
 For the simple case in which all the systems {Si}4i=1 are linear time invariant (LTI) and stable, and assuming p,x,q = 0 (deterministically), it was shown in [17] that I(rk ? ek) does not depend on whether there is feedback from e to u or not. Inequalities between mutual and directed informations in a less restricted setup, shown in Fig. 1-(b), have been found in [7], [8]. In that setting (a networkedcontrol system), G is a strictly causal LTI dynamic system having (vector) state sequence, with x0 ,x(0) being the random initial state in its state-space representation. The external signal r (which could correspond to a disturbance) is statistically independent of s, the latter corresponding to, for example, side information or channel noise. Both are also statistically independent of x0. The blocks labeled E,
 
 	(a)	(b)
 Figure 1. (a): The general system considered in this work. (b): A special case, corresponding to the closed-loop system studied in [7].
 D and f correspond to an encoder, a decoder and a channel, respectively, all of which are causal. The channel f maps sk and xk to y(k) in a possibly time-varying manner, i.e., y(k) = f(k,xk,sk). Similarly, the concatenation of the encoder, the channel and the decoder, maps sk and wk to u(k) as a possibly time-dependent function u(k) = ?(k,wk,sk). Under these assumptions, the following fundamental result was shown in [8, Lemma 5.1]:
 	I(x0,rk ; uk) - I(rk;uk) = I(x0;ek).	(2)
 By further assuming in [8] that the decoder D in Fig. 1-(b) is deterministic, the following Markov chain naturally holds,
 	(x0,rk) ?? yk ?? uk,	(3)
 leading directly to
 	I(x0,rk ; yk) - I(rk;uk) = I(x0;ek),	(4)
 which is found in the proof of [8, Corollary 5.3]. The deterministic nature of the decoder D played a crucial role in the proof of this result, since otherwise the Markov chain (3) does not hold, in general, due to the feedback from u to y.
 Notice that both (2) and (4) provide lower bounds to the difference between two mutual informations, each of them relating a signal external to the loop (such as x0,rk) to a signal internal to the loop (such
 as uk or yk). Instead, the inequality
 	I(xk ? yk) = I(rk;yk),	(5)
 which holds for the system in Fig. 1-(a) and appears in [1, Theorem 3] (and rediscovered later in [6, Lemma 4.8.1]), involves the directed information between two internal signals and the mutual information between the second of these and an external sequence. A related bound, similar to (4) but involving information rates and with the leftmost mutual information replaced by the directed information from xk to yk (which are two signals internal to the loop), has been obtained in [7, Lemma 4.1]:
 	,	(6)
 with I¯(x ? y) , limk?8 k1I(xk ? yk) and I¯(r;u) , limk?8 k1I(rk;uk), provided 
 8. This result relies on three assumptions: a) that the channel f is memory-less and satisfies a “conditional invertibility” property, b) a finite-memory condition, and c) a fading-memory condition, these two related to the decoder D (see Fig. 1). It is worth noting that, as defined in [7], these assumptions upon D exclude the use of side information by the decoder and/or the possibility of D being affected by random noise or having a random internal state which is non-observable (please see [7] for a detailed description of these assumptions).
 The inequality (5) has recently been extended in [4, Theorem 1], for the case of discrete-valued random variables and assuming s ?? (r,p,q), as the following identity (written in terms of the signals and setup shown in Fig. 1-(a)):
 	.	(7)
 Letting q = s in Fig. 1-(a) and with the additional assumption that (p,s) ?? q, it was also shown in [4, Theorem 1] that
 	I(xk ? yk) = I(pk;yk) + I(qk-1;yk) + I(pk;qk-1 |yk),	(8)
 for the cases in which u(i) = y(i) + q(i) (i.e., when the concatenation of S4 and S1 corresponds to a summing node). In [4], (7) and (8) play important roles in characterizing the capacity of channels with noisy feedback.
 To the best of our knowledge, (2), (4), (5) (6), (7) and (8) are the only results available in the literature which lower bound the difference between an internal-to-internal directed information and an external-tointernal mutual information. There exist even fewer published results in relation to inequalities between two directed informations involving only signals internal to the loop. To the best of our knowledge, the only inequality of this type in the literature is the one found in the proof of Theorem 4.1 of [9]. The latter takes the form of a (quasi) data-processing inequality for directed informations in closed-loop systems, and states that
 	I(xk ? yk k qk) = I(xk ? uk),	(9)
 provided  q ?? (r,p) and if S4 is such that yi is a function of  (i.e., if S4 is conditionally invertible) ?i. In (9),
 k
 	I(xk ? yk k qk) ,XI(y(i);xi |yi-1,qi)	(10)
 i=1
 corresponds to the causally conditioned directed information defined in [2]. Inequality (9) plays a crucial role [9], since it allowed lower bounding the average data rate across a digital error-free channel by a directed information. (In [9], q corresponded to a random dither signal in an entropy-coded dithered
 quantizer.)
 In this paper, we derive a set of information identities and inequalities involving pairs of sequences (internal or external to the loop) in feedback systems. The first of these is an identity which, under an independence condition, can be interpreted as a law of conservation of information flows. The latter identity is the starting point for most of the results which follow it. Among other things, we extend (4) and (6) to the general setup depicted in Fig. 1-(a), where none of the assumptions made in [7]–[9] (except causality) needs to hold. Moreover, we will prove the validity of (9) without assuming the conditional invertibility of S4 nor that q ?? (r,p). The latter result is one of four novel data-processing inequalities derived in Section III-B, each involving two nested directed informations valid for the system depicted in Fig. 1-(a). The last of these is a complete closed-loop counterpart of the traditional open-loop dataprocessing inequality.
 The remainder of this paper begins with a description of the systems under study and the extension of Massey’s directed information to the case in which each of the blocks in the loop may introduce an arbitrary, non-negative delay (i.e., we do not allow for anticipation). The information identities and inequalities are presented in Section III. For clarity of the exposition, all the proofs are deferred to Section IV. A brief discussion of potential applications of our results is presented in Section V, which is followed by the conclusions in Section VI.
 II. PRELIMINARIES
 A. System Description
 We begin by providing a formal description of the systems labeled S1 ...S4 in Fig. 1-(a). Their inputoutput relationships are given by the possibly-varying deterministic mappings 
 e(i) = S1(ui-d1(i),ri),	(11a) x(i) = S2(ei-d2(i),pi),	(11b) y(i) = S3(xi-d3(i),si),	(11c) u(i) = S4(yi-d4(i),qi),	(11d)
 where r,p,s,q are exogenous random signals and the (possibly time-varying) delays d1,d2,d3,d4 ? {0,1,...} are such that
 	d1(k) + d2(k) + d3(k) + d4(k) = 1,	?k ? N.
 That is, the concatenation of S1,...,S4 has a delay of at least one sample. For every i ? {1,...,k}, r(i) ? Rnr(i), i.e., r(i) is a real random vector whose dimension is given by some function nr : {1,... ,k} ? N.
 The other sequences (q,p,s,x,y,u) are defined likewise.
 B. A Necessary Modification of the Definition of Directed Information
 As stated in [1], the directed information (as defined in (1)) is a more meaningful measure of
 the flow of information between xk and yk than the conventional mutual information I(xk;yk) =
  when there exists causal feedback from y to x. In particular, if xk and yk are
 discrete-valued sequences, input and output, respectively, of a forward channel, and if there exists strictly causal, perfect feedback, so that x(i) = y(i - 1) (a scenario utilized in [1] as part of an argument in favor of the directed information), then the mutual information becomes
 I(xk;yk) = H(yk) - H(yk |xk) = H(yk) - H(yk |yk-1) = H(yk) - H(y(k)|yk-1) = H(yk-1).
 Thus, when strictly causal feedback is present, I(xk;yk) fails to account for how much information about xk has been conveyed to yk through the forward channel that lies between them.
 It is important to note that, in [1] (as well as in many works concerned with communications), the forward channel is instantaneous, i.e., it has no delay. Therefore, if a feedback channel is utilized, then this feedback channel must have a delay of at least one sample, as in the example above. However, when studying the system in Fig. 1-(a), we may need to evaluate the directed information between signals xk and yk which are, respectively, input and output of a strictly casual forward channel (i.e., with a delay of at least one sample), whose output is instantaneously fed back to its input. In such case, if one further assumes perfect feedback and sets x(i) = y(i), then, in the same spirit as before,
 .
 As one can see, Massey’s definition of directed information ceases to be meaningful if instantaneous feedback is utilized.
 It is natural to solve this problem by recalling that, in the latter example, the forward channel had a delay, say d, greater than one sample. Therefore, if we are interested in measuring how much of the information in y(k), not present in yi-1, was conveyed from xi through the forward channel, we should look at the mutual information I(y(i);xi-d |yi-1), because only the input samples xi-d can have an influence on y(i). For this reason, we introduce the following, modified notion of directed information
 Definition 1 (Directed Information with Forward Delay): In this paper, the directed information from
 xk to yk through a forward channel with a non-negative time varying delay of d(i) samples is defined as
 k
 	I(xk ? yk) ,XI(y(i);xi-d(i) |yi-1).	(12)
 i=1
 For a zero-delay forward channel, the latter definition coincides with Massey’s.
 Likewise, we adapt the definition of causally-conditioned directed information to the definition
 k
 I(xk ? yk k ek) ,XI(y(i);xi-d3(i) |yi-1,ei-d2(i)).
 i=1
 when the signals e, x and y are related according to (11).
 Before finishing this section, it is convenient to recall the following identity (a particular case of the chain rule of conditional mutual information [18]), which will be extensively utilized in the proofs of our results:
 	I(a,b;c|d) = I(b;c|d) + I(a;c|b,d).	(13)
 III. INFORMATION IDENTITIES AND INEQUALITIES
 A. Relationships Between Mutual and Directed Informations
 We begin by stating a fundamental result, which relates the directed information between two signals within a feedback loop, say x and y, to the mutual information between an external set of signals and y:
 	Theorem 1:	In the system shown in Fig. 1-(a), it holds that
 	,	(14)
 with equality achieved if s is independent of (p,q,r).	N
 This fundamental result, which for the cases in which s ?? (p,q,r) can be understood as a law of conservation of information flow, is illustrated in Fig. 2. For such cases, the information causally conveyed from x to y equals the information flow from (q,r,p) to y. When (p,q,r) are not independent of s, part of the mutual information between (p,q,r) and y (corresponding to the term can be thought of as being “leaked” through s, thus bypassing the forward link from x to y. This provides an intuitive interpretation for (14).
 
 Figure 2. The flow of information between exogenous signals (p,q,r) and the internal signal y equals the directed information from xk to yk when s ?? (p,q,r).
 Remark 1: Theorem 1 implies that I(xk ? yk) is only a part of (or at most equal to) the information “flow” between all the exogenous signals entering the loop outside the link x ? y (namely (q,r,p)), and
 y. In particular, if (p,q,r) were deterministic, then I(xk ? yk) = 0, regardless of the blocks S1,...,S4
 and irrespective of the nature of s.	N
 Remark 2: By using (13),. Then, applying Theorem 1, we recover (5), whenever s ?? (q,r,p). Thus, [1, Theorem 3] and [6, Lemma 4.8.1]) can be obtained as
 a corollary of Theorem 1.	N
 The following result provides an inequality relating I(xk ? yk) with the separate flows of information
 I(rk;yk) and.
 Theorem 2: For the system shown in Fig. 1-(a), if s ?? (p,q,r) and r, then
 	.	(15)
 with equality if and only if the Markov chain  holds.
 Theorem 2 shows that, provided (p,q,r) ?? s, I(xk ? yk) is lower bounded by the sum of the individual flows from all the subsets in any given partition of , to yk, provided these subsets are mutually independent. Indeed, both theorems 1 and 2 can be generalized for any appropriate choice of external and internal signals. More precisely, let T be the set of all external signals in a feedback system. Let a and ß be two internal signals in the loop. Define Ta,ß ? T as the set of exogenous signals which are introduced to the loop at every subsystem Si that lies in the path going from a to ß. Thus, for any ? ? T \ Ta,ß, if Ta,ß ?? T \ Ta,ß, we have that (14) and (15) become
 I(a ? ß) = I(T \ {Ta,ß};ß),	(16)
 I(a ? ß) - I(?;ß) = I(T \ {? ? Ta,ß};ß),
 respectively.
 To finish this section, we present a stronger, non-asymptotic version of inequality (6):
 Theorem 3: In the system shown in Fig. 1-(a), if (r,p,q,s) are mutually independent, then	(17)
 	.	(18)
 N
 As anticipated, Theorem 3 can be seen as an extension of (6) to the more general setup shown in Fig. 1-
 (a), where the assumptions made in [7, Lemma 4.1] do not need to hold. In particular, letting the decoder D and x0 in Fig. 1-(b) correspond to S4 and pk in Fig. 1-(a), respectively, we see that inequality (15) holds even if D and E have dependent initial states, or if the internal state of D is not observable [19].
 Theorem 3 also admits an interpretation in terms of information flows. This can be appreciated in the diagram shown in Fig. 3, which depicts the individual full-turn flows (around the entire feedback loop) stemming from q, r and p. Theorem 3 states that the sum of these individual flows is a lower bound for the directed information from x to y, provided q,r,p,s are independent.
 
 Figure 3.	A representation of the three first information flows on the right-hand-side of (18).
 B. Relationships Between Nested Directed Informations
 This section presents three closed-loop versions of the data processing inequality relating two directed informations, both between pairs of signals internal to the loop. As already mentioned in Section I, to the best of our knowledge, the first inequality of this type to appear in the literature is the one in Theorem 4.1 in [9] (see (9)). Recall that the latter result stated that I(xk ? yk k qk) = I(xk ? uk), requiring S4 to be such that yi is a deterministic function of  and that q ?? (r,p). The following result presents another inequality which also relates two nested directed informations, namely, I(xk ? yk) and I(ek ? yk), but requiring only that s ?? (q,r,p).
 Theorem 4: For the closed-loop system in Fig. 1-(b), if (q,r,p) ?? s, then
 	I(xk ? yk) = I(ek ? yk).	(19)
 N
 Notice that Theorem 4 does not require p to be independent of r or q. This may seem counter-intuitive upon noting that p enters the loop between the link from e to x.
 The following theorem is an identity between two directed informations involving only internal signals. It can also be seen as a complement to Theorem 4, since it can be directly applied to establish the relationship between I(ek ? yk) and I(ek ? uk).
 Theorem 5: For the system shown in Fig. 1-(a), if (q,s) ?? (r,p), then
 	.	(20)
 with equality if, in addition, q ?? s. In the latter case, it holds that
 	.	(21)
 N
 Notice that, by requiring additional independence conditions upon the exogenous signals (specifically, q ?? s), Theorem 5 (and, in particular, (21)) yields
 	I(xk ? yk) = I(xk ? uk),	(22)
 which strengthens the inequality in [9, Theorem 4.1] (stated above in (9)). More precisely, (22) does not require conditioning one of the directed informations and holds irrespective of the invertibility of the mappings in the loop.
 A closer counterpart of (9) (i.e., of [9, Theorem 4.1]), involving I(xk ? yk k qk), is presented next.
 Theorem 6: For the system shown in Fig. 1-(a), if (q,s) ?? (r,p), then
 	.	(23)
 where the equality labeled (†) hods if, in addition, the Markov chain
 		(24)
 is satisfied for all i ? {1,...,k}.	N
 Thus, provided (q,s) ?? (r,p), (23) yields that (9) holds regardless of the invertibility of S4, requiring instead that, for all i ? {1,...,k}, any statistical dependence between qk and si resides only in qi (i.e., that Markov chain (24) holds).
 The results derived so far relate directed informations having either the same “starting” sequence or the same “destination” sequence. We finish this section with the following corollary, which follows directly by combining theorems 4 and 5 and relates directed informations involving four different sequences internal to the loop.
 Corollary 1 (Full Closed-Loop Directed Data Processing Inequality): For the system shown in Fig. 1-
 (a), if (q,s) ?? (r,p) and q ?? s, then
 	.	(25)
 Equality holds in (a) if, in addition, r ?? p (i.e., if (q,r,p,s) are mutually independent).	N
 To the best of our knowledge, Corollary 1 is the first result available in the literature providing a lower bound to the gap between two nested directed informations, involving four different signals inside the feedback loop. This result can be seen as the first full extension of the open-loop (traditional) dataprocessing inequality, to arbitrary closed-loop scenarios. (Notice that there is no need to consider systems with more than four mappings, since all external signals entering the loop between a given pair of internal signals can be regarded as exogenous inputs to a single equivalent deterministic mapping.)
 IV. PROOFS
 We start with the proof of Theorem 1.
 Proof of Theorem 1: It is clear from Fig. 1-(a) and from (11) that the relationship between r, p, q, s, x and y can be represented by the diagram shown in Fig. 4. From this diagram and Lemma 1 (in the appendix) it follows that if s is independent of (r,p,q), then the following Markov chain holds:
 	y(.	(26)
 	xii-+11+-dd13+d2+d4	si
 ?i  y(i)
 Figure 4. Representation of the system of Fig. 1-(b) highlighting the dependency between p, q, r, s, x and y. The dependency on i of the delays d1(i),...,d4(i) is omitted for clarity.
 Denoting the triad of exogenous signals pk,qk,rk by
 	,	(27)
 we have the following
 k
 I(xk ? yi) = XI(y(i);xi-d3(i) |yi-1)
 i=1
 k
 (13)= XhI(?i,xi-d3(i);y(i)|yi-1) - I(?i;y(i)|xi-d3(i),yi-1)i i=1
 = XhI(?i;y(i)|yi-1) - I(?i;y(i)|xi-d3(i),yi-1)i i=1	(28a)
 (b) k	(c) k
 = XI(?i;y(i)|yi-1) = XI(?k;y(i)|yi-1)
 	i=1	i=1	(28b)
 = I(?k;yk).	(28c)
 k (a)
 In the above, (a) follows from the fact that, if yi-1 is known, then xi-d3(i) is a deterministic function of ?i. The resulting sums on the right-hand side of (28a) correspond to I(qk,rk,pk ? yk)-I(qk,rk,pk ? yk k xk), and thereby proving the first part of the theorem, i.e., the equality in (14). In turn, (b) stems from the non-negativity of mutual informations, turning into equality if s ?? (r,p,q), as a direct consequence of the Markov chain in (26). Finally, equality holds in (c) if s ?? (q,r,p), since y depends causally upon
 ?. This shows that equality in (14) is achieved if s ?? (q,r,p), completing the proof.	
 Proof of Theorem 2: Apply the chain-rule identity (13) to the RHS of (14) to obtain
 	.	(29)
 Now, applying (13) twice, one can express the term  as follows:
 I(pk,qk;yk |rk) = I(pk,qk ; yk,rk) - I(pk,qk;rk) = I(pk,qk ; yk,rk)
 (30)
 ,
 where the second equality follows since . The result then follows directly by combining (30) with (29) and (14).	
 Proof of Theorem 3: Since q ?? (r,p,s),
 		(31)
 		(32)
 	,	(33)
 where (a) is due to Theorem 5, (b) follows from Theorem 1 and the fact that (s,q) ?? (r,p) and (c) from the chain rule of mutual information. For the second term on the RHS of the last equation, we have
 		(34)
 		(35)
 		(36)
 		(37)
 		(38)
 	(f)	k	k	k	k	k
 	= I(p ;e ) + I(p ;u |e ),	(39)
 where (a) holds since r ?? p, (b), (d) and (e) stem from the chain rule of mutual information (13),
 and (c) is a consequence of the Markov chain e which is due to the fact that ek = S1(uk-d1(k),rk). Finally, (f) is due to the Markov chain r, which holds because r ?? (p,s,q) as a consequence of Lemma 1 in the appendix (see also Fig. 1-(a)). Substitution of (39) into (33) yields (18), thereby completing the proof. 
 Proof of Theorem 4: Since (p,q,r) ?? s, we can apply (5) (where now (q,r) plays the role of r), and obtain
 	.	(40)
 Now, we apply Theorem 1, which gives
 	,	(41)
 completing the proof.	
 Proof of Theorem 5: Applying Theorem 1, since (r,p) ?? (s,q),
 I(xk ? uk) = I(rk,pk ; uk).
 For the other directed information, we have that	(42)
 
 		(43)
 
 
 
 
 
 		(44)
 	,	(45)
 where (a) follows from Theorem 1, which also states that equality is reached if and only if (r,p,q) ?? s. In turn, (b) is due to the fact that uk is a deterministic function of q. Equality (c) holds if and only if (r,p) ?? q. Finally, from Lemma 1 (in the appendix), (d) turns into equality if q ?? (r,p,s). Substitution of (42) into (45) yields (21), completing the proof. 
 Proof of Theorem 6: We begin with the second part of the theorem, proving the validity of the equality (†) in (23). We have the following:
 k
 I(xk ? yk k qk) = XI(y(i);xi-d3(i) |yi-1,qi)
 i=1	(46)
 k
 	(13) Xh	i	i	i-d3(i);y(i)|yi-1,qi) - I(ri,pi;y(i)|xi-d3(i),yi-1,qi)i
 	=	I(r ,p ,x
 i=1	(47)
 (a) k
 = XI(ri,pi,xi-d3(i);y(i)|yi-1,qi) i=1	(48)
 	(49)
 i=1
 k
 = XhI(ri,pi,qki+1;y(i)|yi-1,qi) - I(qik+1;y(i)|yi-1,qi,ri,pi)i i=1	(50)
 k
 (c) Xh i i ki+1 i-1,qi)i = I(r ,p ,q ;y(i)|y
 i=1	(51)
 k
 (13) Xh	i	i	i-1,qk) + I(qik+1;y(i)|yi-1,qi)i =	I(r ,p ;y(i)|y
 i=1	(52)
 k
 (d) X	i	i	i-1,qk)
 =	I(r ,p ;y(i)|y
 i=1	(53)
 	(54)
 (13)
 i=1
 where equality holds in (a) if and only if the Markov chain si ? qi ? (ri,pi) holds for all i ? {1,... ,k} (as a straightforward extension of Lemma 1). In our case, the latter Markov chain holds since we are assuming (qk,sk) ?? (rk,pk). In turn, (b) stems from the fact that, for all i ? {1,... ,k}, xi-d3(i) is a function of yi-1,qi,ri,pi. To prove (c), we resort to (13) and write
 		(55)
 From the definitions of the blocks (in (11)), it can be seen that, given qi, the triad of random sequences
  is a deterministic function of (at most) . Recalling that  and that q (see (24)), it readily follows that , and thus each of the mutual
 informations on the right-hand-side of (55) is zero. To verify the validity of (d), we use (13) and obtain
 	,	(56)
 where (d) now follows since , where the
 last term in this chain of inequalities was shown to be zero in the proof of (d). Equality holds in (e) if
 and only if , a Markov chain which is satisfied in our case from the fact that (q,s) ?? (r,p) and from Lemma 1.
 Finally, since (rk,pk) ?? (qk,sk), we have that the chain of equalities from (43) to (44) holds, from which we conclude that
 	.	(57)
 Inserting this result into (54) and invoking Theorem 1 we arrive at equality (†) in (23).
 To prove the first equality the (23), it suffices to notice that I(xk ? yk |qk) corresponds to the sum on the right-hand-side of (53), from where we proceed as with the first part. This completes the proof of the theorem.	
 POTENTIAL APPLICATIONS
 Information inequalities and, in particular, the data-processing inequality, have played a fundamental role in Information Theory and its applications [20]–[27]. It is perhaps the lack of a similar body of results associated with the directed information (and with non-asymptotic, causal information transmission) which has limited the extension of many important information-theoretic ideas and insights to situations involving feedback or causality constraints [5], [28]. Two such areas, already mentioned in this paper, are the understanding of the fundamental limitations arising in networked control systems over noiseless digital channels, and causal rate distortion problems. In those contexts, causality is of paramount relevance an thus the directed information appears, naturally, as the appropriate measure of information flow (see, for example, [5], [9], [11], [29], [30] and [7]). We believe that our results might help gaining insights into the fundamental trade-offs underpinning those problems, and might also allow for the solution of open problems such as, for instance, characterizing the minimal average data-rate that guarantees a given performance level [10] (an improved version of the latter paper, which extensively uses the results derived here, is currently under preparation by the authors). On a different vein, directed mutual information plays a role akin to that of (standard) mutual information when characterizing channel feedback capacity (see, e.g., [3], [4] and the references therein). Our results may also play a role in expanding the understanding of communication problems over channels used with feedback, particularly when including in the analysis additional exogenous signals such as a random channel state, interference and, in general, any form of side information. Thus, we hope that the inequalities and identities presented in Section III may help in extending results such as dirty-paper coding [31], watermarking [32], distributed source coding [25], [26], [33], [34], multi-terminal coding [35], [36], and data encryption [37], to scenarios involving causal feedback.
 CONCLUSIONS
 In this paper, we have derived fundamental relations between mutual and directed informations in general discrete-time systems with feedback. The first of these is an inequality between the directed information between to signals inside the feedback loop and the mutual information involving a subset of all the exogenous incoming signals. The latter result can be interpreted as a law of conservation of information flows for closed-loop systems. Crucial to establishing these bounds was the repeated use of chain rules for conditional mutual information as well as the development of new Markov chains. The proof techniques do not rely upon properties of entropies or distributions, and the results hold in very general cases including non-linear, time-varying and stochastic systems with arbitrarily distributed signals. Indeed, the only restriction is that all blocks within the system must be causal mappings, and that their combined delay must be at least one sample. A new generalized data processing inequality was also proved, which is valid for nested directed informations within the loop. A key insight to be gained from this inequality was that the further apart the signals are in the loop, the lower is the directed information between them. This closely resembles the behavior of mutual information in open loop systems, where it is well known that any independent processing of the signals can only reduce their mutual information.
 APPENDIX
 Lemma 1: In the system shown in Fig. 5, the exogenous signals r,q are mutually independent and
 S1,S2 are deterministic (possibly time-varying) causal maps characterized by 
 , for some k ? N. For this system, the following Markov chain holds
 y q u
 Figure 5.	Two arbitrary causal systems S1,S2 interconnected in a feedback loop. The exogenous signals r,q are mutually
 independent.
 	r.	(58)
 Proof: Since  and u are deterministic functions, it follows that
 for every possible pair of sequences yk,uk, the sets ?yk,uk , {rk : yk = S1(rk,uk)} and fyk,uk ,
 {qk : uk = S2(qk,yk)} are also deterministic. Thus,  and
 . This means that for every pair of Borel sets (R,Q) of appropriate
 dimensions,
 Pr
 = Pr{rk ? R,qk ? Q|rk ? ?yk,uk , qk ? fyk,uk}
 = Pr{rk ? R|rk ? ?yk,uk , qk ? fyk,uk}Pr{qk ? Q|rk ? (?yk,uk n R), qk ? fyk,uk}
 	(a)	k	k	k	k
 = Pr{r ? R|r ? ?yk,uk}Pr{q ? Q|q ? fyk,uk}
 ,
 where (a) follows from the fact that rk ?? qk. This completes the proof.	
 ﻿ This work demonstrates a formal connection between density estimation with a data-rate constraint and the joint objective of fixed-rate universal lossy source coding and model identification introduced by Raginsky in 2008 (IEEE TIT, 2008, 54, 3059–3077). Using an equivalent learning formulation,
 we derive a necessary and sufficient condition over the class of densities for the achievability of the joint objective. The learning framework used here is the skeleton estimator, a rate-constrained learning scheme that offers achievable results for the joint coding and modeling problem by optimally adapting its learning parameters to the specific conditions of the problem. The results obtained with the skeleton estimator significantly extend the context where universal lossy source coding and model identification can be achieved, allowing for applications that move from the known case of parametric collection of densities with some smoothness and learnability conditions to the rich family of non-parametric L1-totally bounded densities. In addition, in the parametric case we are able to remove one of the assumptions that constrain the applicability of the original result obtaining similar performances in terms of the distortion redundancy and per-letter rate overhead.
 Keywords: fixed-rate lossy source coding; joint coding and modeling; universal source coding; learning with rate constraints; the skeleton estimator; L1-totally bounded classes
  
 Introduction
 Universal source coding (USC) has a long history in information theory and statistics [1–5]. Davisson’s seminal work [4] formalized the variable-length lossless coding problem and introduced important information quantities for performance analysis [1,2]. In this lossless setting, it is well-understood that the Shannon entropy provides the minimum achievable rate (in bits per sample) [2] to code a stationary and memoryless source when the probability (model) of the source is available. When the probability of the source is not known but belongs to a family of distributions F (the so called universal source coding problem), the focus of the problem is to characterize the penalty (or redundancy in bits per sample) that an encoder and decoder pair will experience due to the lack of knowledge about the samples’ probability [1]. In the lossless case, a seminal result states that the least worst-case redundancy over F (or the minimax solution of the USC problem for F) is determined by the information radius of F [1].
 Building on this connection between least worse-case redundancy and information radius of F, there are numerous important results developed for lossless USC [1,6–9]. In particular, it is known that the information radius grows sub-linearly (with the block-length) for the family of finite alphabet stationary and memoryless sources [1], which implies the existence of a universal source code that achieves Shannon entropy as the block length goes to a large value for every distribution in F.
 Entropy 2018, 20, 640; doi:10.3390/e20090640	www.mdpi.com/journal/entropy
 However universality is not possible for the family of alphabet stationary and memoryless sources because the information radius of this family is unbounded [3,5,7]. More recent results on lossless USC over countable infinite alphabets have looked at restricting the analysis to specific collections of distributions (with some tail bounded conditions) to achieve minimax universality [7–9] and also looked at weak variations of the lossless source coding setting [10–12].
 In the fixed-rate lossy source coding problem, assuming first that the probability µ of a memoryless source is known, the performance limit of the coding problem is given by the Shannon distortion-rate function Dµ(R) [2,13]. Consequently, the universal lossy source coding problem reduces to compare the distortion of a coding scheme (satisfying a fixed-rate constraint) with the Shannon distortion-rate function assuming that the designer only knows that µ ? F. The literature on this problem is rich [3,5,14–18] with a first result dating back to Ziv [17] who showed the existence of weakly minimax fixed-rate universal lossy source code for the class of stationary sources under certain assumptions about the source, the alphabet, and the distortion measure. More refined results were presented in [5,16] one of which established necessary and sufficient conditions to achieve weakly minimax universality for the class of stationary and ergodic sources. To provide a more specific analysis of universal lossy source coding, Linder et al. [14] presented a lossy USC scheme with a distortion redundancy that goes to zero as O  for the case of independent and identically distributed (i.i.d.) bound sources. Later Linder et al. [15] improved previous results showing a fixed-rate lossy
 construction with a distortion redundancy that vanishes as O(n-1log n) and O(pn-1 log n) with n for finite alphabet i.i.d. sources and bounded infinite alphabet i.i.d. sources, respectively. Similar convergence results were obtained using a nearest-neighbor vector quantization approach in [19].
 It is also understood that universal variable length lossless-source coding is connected with the problem of distribution estimation [3,6,20] as there is a one-to-one correspondence between prefix-free codes and finite-entropy discrete distributions in the finite and countable alphabet case [1,2,21]. Building on this one-to-one correspondence in the lossless case, Györfi et al. ([3], Theorem 1) showed that the redundancy (in bits per sample) of a given code upper bounds the expected divergence between the true distribution of the source µ and the estimated distribution derived from the code. Therefore, the existence of a universal (lossless) source code for F implies the existence of a universal (distribution-free in F) estimator of the distribution in expected (direct) information divergence [22]. This means that achieving lossless USC not only provides a lossless representation of the data, but it offers a consistent (error-free) estimator of the distribution at the receiver.
 The connection between coding and distribution estimation that is evident in the lossless case is not, however, present in the (fixed-rate) lossy source coding problem. As argued in [18], a fixed-rate lossy source code does not offer a direct map with a probability distribution (model) for the source. In light of this gap between lossy codes and distributions (models) and motivated by some problems in adaptive control, where it is relevant to both compress data in a lossy way and identify the distribution of the source at the receiver [18,23], Raginsky explored the joint objective of fixed-rate universal lossy source coding and model (i.e., distribution) identification in [18].
 Inspired by Rissanen’s achievability construction in [6,20], Raginsky [18] proposed a new setting for the problem of fixed-rate universal lossy compression of continuous memoryless sources based on the idea of a two-stage joint coding and model or distribution identification framework. In this context, he proposed a two-stage scheme to consider two objectives: fixed-rate universal lossy source coding and source distribution (model) identification. The first objective of the scheme is to transmit the data (optimally) in the classical distortion-rate sense [24], while the second objective is to learn and transmit a description (quantized version) of the source distribution (model) [25,26]. Taking ideas from statistical learning, Raginsky proposed [18] splitting the data into training and testing samples. The training data is used in the first-stage of the encoding process to construct a quantized estimation of the source distribution and encode it (the first stage bits). Then in a second stage of the encoding process, the first-stage bits are used to pick a matched (with the estimated distribution) fixed-rate lossy source code to encode the test data (the second stage bits). In this joint coding and modeling setting, the existence of a zero-rate consistent estimator of the density (in expected total variation) is sufficient to show the existence of a weakly minimax universal fixed-rate source coding scheme [18] (Theorem 3.2), achieving the Shannon distortion-rate function [2,24,27,28], for any given rate. This result is obtained for a wide class of single-letter bounded distortion functions and for a family of source densities F = {µ? : ? ? T} indexed over a bounded finite dimensional space  L, L] ? Rk (i.e., a parametric collection) with some needed smoothness and learnability conditions [18] (Theorem 3.2).
 It is important to highlight that the joint coding and modeling achievability results in [18] did not degrade the performance of the source coding objective. In fact by restricting the analysis to the source coding objective alone, the joint coding and modeling framework in [18] showed the same state-of-the-art performance results as conventional two-stage universal source coding schemes (or universal vector quantizers) [14,15,19] in terms of distortion redundancy and per-letter rate
  
 overhead (O(plog(n)/n) and O(log(n)/n), respectively) as the block length n tends to a large number. Importantly, the first-stage bits of this joint coding and modeling scheme are used to achieve model identification at the receiver with arbitrary precision in total variation (with a rate of convergence
  
 of O(plog(n)/n) as n goes to infinity), with no extra cost in bits per-letter compared with conventional fixed-rate lossy source coding methods.
 Contributions of This Work
 This work formally studies the interplay between density estimation under a data-rate constraint and the joint fixed-rate universal lossy source coding and modeling problem with training data or memory introduced in [18]. The first main result (Theorem 1) establishes a connection between zero-rate density-estimation and a universal joint coding and modeling scheme that achieves optimal lossy source coding (in a distortion-rate sense) and lossless model identification. This result is obtained for the general family of bounded single-letter distortions [13]. Remarkably, this connection implies that the construction of a joint coding and modeling scheme reduces to the construction of a zero-rate density estimator. From this result, the second main result (Theorem 2) stipulates a necessary and sufficient condition for the existence of a weakly minimax universal joint coding and modeling scheme. For the achievability part of this result, we used the skeleton estimator as our learning framework [29]. Using this learning framework we extend the parametric context explored in [18] to the rich non-parametric scenario of L1-totally bounded densities [30].
 Furthermore, revisiting the parametric case studied in [18], by using the skeleton estimator we are able to remove some of the assumptions that limit the applicability of the original result. We show that the skeleton estimator matches the best performance reported in [18] in terms of the distortion redundancy and (per-letter) rate overhead, in particular obtaining rates of convergence to
  
 zero of O(plog(n)/n) and O(log(n)/n), respectively, as the block-length tends to infinity. To obtain this, our result relaxes the finite Vapnik and Chervonenkis (VC) dimension assumption considered in [18]. On the other hand, when the finite VC dimension assumption is added in the analysis,
  
 the skeleton learning scheme offers a convergence rate of O(1/vn) for the distortion redundancy as the sample-length goes to infinity. Finally, the skeleton framework is implementable in the parametric case as its minimum-distance decision is carried out on a finite number of candidates and the oracle e-skeleton (or the e-covering in total variation of F) [30] (Chapter 7) can be replaced by a practical uniform covering of the compact index set T ? Rk (Theorem 4). Finally, it is worth noting that a preliminary version of this work (in the context of density estimation under a data-rate constraint) was presented in [31].
 The rest of the paper is organized as follows: Section 2 introduces the setting of the joint coding and modeling with training data. Section 3 elaborates the connections with zero-rate density estimation. Section 4 presents the main joint coding and modeling result (Theorem 2) and introduces the skeleton estimator. Finally, Section 5 revisits a special case where the distributions are indexed by finite dimensional bounded space (the parametric context). A summary of the results is presented in Sections 6 and 7. Finally, the proofs are presented in Section 8.
 Preliminaries
 The fixed-rate coding and modeling problem introduced in [18] is presented in this section. This joint coding and modeling problem will be the main focus of this work. In addition, notations and definitions used in the rest of the paper will be presented.
 2.1. Basic Definitions
 Let X ? B(Rd) be a separable and complete subset of Rd where B(Rd) is the Borel sigma field. Let P(X) be the collection of probability measures on (X, B(X)), with B(X) denoting the Borel sigma field restricted to X, and let AC(X) ? P(X) denote the set of probability measures absolutely continuous with respect to the Lebesgue measure ? [32]. For any µ  denotes its probability density function. The total variational distance [30] of v and µ in P(X) is given by (to avoid any confusion, if S is a set then |S| denotes its cardinality).
 	V  v(A)| .	(1)
 For µ and v belonging to AC(X), if we define the Scheffé set for the pair (µ, v) by
 	Aµ,v  ,	(2)
 then V(µ, v) = µ(Aµ,v) - v(Aµ,v) [30,33].
 2.2. Fixed-Rate Universal Lossy Source Coding with Memory or Training Data
 Let {Xn : n = 1} be an i.i.d. stochastic process (or stationary and memoryless source), where Xi takes values in X ? Rd and has a distribution µ in F = {µ? : ? ? T} ? AC(X). T is in general an index set for F. The problem of lossy source-coding of a finite block of the process Xn = (X1, ..., Xn) reduces to find a mapping (or code) Cn(·) from Xn to Sn, where Sn is a finite set. Given a cardinality constraint on Sn, the design objetive is to make Cn(Xn) as close as possible to Xn (in average) using for that a distortion function. The standard coding problem assumes the knowledge of µ for finding the optimal code (for any finite block n) [1,2,13], as well as for characterizing the fundamental performance limits of this task as n goes to infinity [2,24,28,34–36].
 A more realistic scenario is the universal source coding (USC) problem [2], where the source distribution µ ? F is unknown and a coding scheme needs to be designed optimally for the family F. Here we focus on a specific learning variation of this task introduced by Raginsky in [18], where in addition to the data that needs to be compressed and recovered (with respect to a fidelity criterion),
 we have a finite number of i.i.d samples following the same distribution µ and that can be used to estimate µ in the encoding process (more details of this approach in Section 2.3). This additional data can be interpreted as memory, training data, or side information about µ available at the encoder because it is data that is not required to be compressed and recovered. The existence of this memory departs from the standard zero-memory setting considered in universal source coding [1]. However, this information can be seen as a realistic assumption in the context of a sequential block by block coding of an infinite sequence, where the data is partitioned into blocks of the same finite length and compressed sequentially block by block. Then in a given stage of this sequential process, the data from previous blocks are available at the encoder (lossless) for the process compressing the current block [18].
 More specifically following the fixed-rate block coding and modeling setting introduced by Raginsky in [18], we consider an n-block coding scheme with finite memory m, where there is a distinction between the data Zm = (Z1, ..., Zm) that is available (as side information) to estimate the source distribution (training data) and the data Xn that needs to be encoded and recovered (source or test data), under the important assumption that both data sets are i.i.d. samples of the same unknown probability µ ? F. A systematic exposition of this coding setting and its connection with the classical setting of zero-memory block coding is presented in [18] (Section II). Formally, let us define an (m, n)-block code by the pair
 	  Sn,	  .	(3)
 Then given a set of training samples zm ? Xm and a finite block of the source xn ? Xn, Cm,n is the composition of: a encoding function f(zm, xn) that maps xn to an element in a finite set Sn conditioned on the training data (or memory) represented by zm, and a decoding function f(·) that maps a
 symbolof Cm,n. In this context,s ? Sn into the reproduction pointsXˆ denotes the reproduction space. As a short-hand, we denote byGCm,n = {f(s) : s ? Sn} that we called the codebookxˆn =
 Cm,n(xn) = f(f(zm, xn)) the reconstruction of xn obtained by Cm,n and its memory zm (for simplicity, the dependency of xˆn or Cm,n(xn) on the memory zm will be implicit in the rest of the exposition.).
 The rate of Cm,n in bits-per-letter is given by R . In general, it is not possible to recover xn from xˆn given the cardinality constraint on Sn, and thus a single-letter distortion measure ? : X × Xˆ ? R+ is used to quantify the n-block discrepancy by [24]
 n
 	?n(xn, xˆn) = ??(xi, xˆi).	(4)
 i=1
 Finally considering Xn ~ µn and Zm ~ µm, the average distortion per-letter of Cm,n given Zm is
 	Dµ (Xn, Xˆ n) ,	(5)
 which is a function of Zm and hence the average distortion per-letter of Cm,n is
 	Dµ(Cm,n) = EZm~µm	Dµ  .	(6)
 In universal source coding the performance of a code Dµ(Cm,n) is evaluated over a collection of distributions µ ? F and is compared (point-wise) with the best code that can be obtained assuming that µ is known. For this analysis, we need the following definitions:
 Definition 1 ([18]). For a finite block length n and distribution µ ? F, the n-order operational distortion-rate function of µ at rate R is
 	Dµn(R) = inf	inf	Dµ(Cm,n).	(7)
 m=0 with RC(mC,mn,n)=R
 In this context, the operational distortion-rate function (DRF) [2,28] is given by
 Dµ(R) = lim Dµn(R) = inf Dµn(R).	(8) n?8	n=1
 The celebrated Shannon lossy source-coding theorem [27] provides a single letter theoretical characterization for Dµ(R) in (8) (also known as the Shannon DRF). A nice exposition of this celebrated result can be found in [2,24,28].
 It is worth noting that the operational distortion-rate function in (7) is equivalent to the classical zeromemory n-order operational distortion-rate function given by inf  : such that R 
 (Lemma 2.1). Then, allowing a nonzero memory (side information at the encoder) does not help in the minimization of the distortion when µ is known.
 For the rest of the exposition, we will concentrate on the simple case studied in [18] where n = m (i.e., the block-length is equal to the memory of the code). To be precise about the meaning of universality in this context, we resort to some standard definitions:
 Definition 2 ([16]). A coding scheme {Cn,n : n = 1} is weakly minimax universal for the class F at rate R,
 if ?µ ? F
 	lim Dµ(Cn,n) = Dµ(R)	(9)
 n?8
 and   R. Alternatively, the scheme is said to be strongly
 minimax universal for the class F at rate R if
 	Dµ	 0	(10)
 n?8µ?F
 and lim supn?8 R(Cn,n) = R.
 Decomposing the distortion redundancy in two terms,
 	Dµ ,	(11)
 the first term  is the n-order distortion redundancy, which is the discrepancy that can be attributed exclusively to the goodness of the coding scheme. The second term in (11), i.e.,
  , has to do with how fast Dµn(R) converges to the Shannon DRF as the block length
 tends to infinity (see further details in [14] (Section III) and references therein). From this observation,
 we introduce the following definition:
 Definition 3. A coding scheme {Cn,n : n = 1} is strongly finite-block universal for the class F at rate R if
 	lim sup hDµ(Cn,n) - Dµn(R)i = 0	(12)
 n?8µ?F
 and lim supn?8 R(Cn,n) = R.
 Note that if {Cn,n : n = 1} is strongly minimax universal then it is strongly finite-block universal, but the converse result is not true in general. The missing condition to make these two criteria equivalent is the uniform convergence of Dµn(R) to Dµ(R) in the class F. More discussion about this point in Section 6.
 2.3. Raginsky’s Two-Stage Joint Universal Coding and Modeling
 Motivated by the work of Rissanen [6], Raginsky [18] proposed a two-stage block code with finite memory (training data), with the objective of doing both fixed-rate lossy source coding, and identification of the source distribution at the receiver. More precisely, given Zn ~ µn? and Xn ~ µn? (the training and the source-data samples, respectively), an (n, n)-joint coding and modeling rule is given by
 Cn,n = fn : Xn ? S˜n, fn : S˜n ? T,
 	 ,	(13)
 where Sn and S˜n are finite-set functions of n. Cn,n processes (Zn, Xn) in two stages. In the first stage, the pair (fn, fn) in (13) uses Zn to do density estimation and finite-rate encoding (quantization) by fn(Zn), and fn(·) decodes an estimated density in . At the end, the first stage
 provides a quantized estimation of µ? ? F given by	
 ?ˆn(Zn) = fn(fn(Zn)) ? T.	(14)
 Using the index s˜ = fn(Zn) ? S˜n, the second stage of Cn,n, represented by  ), encodes and decodes the source data Xn by
 	Cn,n(Xn) = fn,s˜(fn,s˜(Xn)).	(15)
 In summary, the outcome of the whole encoding process is the concatenation of the bits that represent fn(Zn) (first-stage bits), and the bits that represent fn,fn(Zn)(Xn) (second-stage bits). The decoding process, on the other hand, reads the first-stage bits to recover ?ˆn(Zn) and then reads the second-stage bits to recover  . (see Figure 1 in which this process is illustrated). The rate (in bits per letter) of Cn,n is
 	R .	(16)
  
 (source reconstruction)
 Figure 1. Illustration of Raginsky’s two-stage joint source coding and modeling scheme. Top figure illustrates the coding process and the bottom figure shows the respective decoding process.
 Based on this two-stage scheme, we could simultaneously achieve source coding and density estimation (modeling) at the decoder. This new joint coding and modeling objective motivates the introduction of the following definition:
 Definition 4. A joint coding and modeling scheme {Cn,n : n = 1} in (13) is strongly minimax universal for a class of distribution F = {µ? : ? ? T} ? AC(X) at the rate R > 0, if
  ,
 ••	limlim supn?8nsup?8µR?F(CEn,nZ)n~=µnR.(V(µ?ˆn(Zn), µ)) = 0, and
 Consequently, if {Cn,n : n = 1} is strongly minimax universal for F, it follows that as n tends to infinity, density estimation is achieved at the decoder (in expected total variations) and, from the source coding perspective, {Cn,n : n = 1} is strongly finite-block universal for F in the sense of Definition 3. For the rest of the paper, the strongly minimax universality of Definition 4 will be the main coding and modeling objective.
 Connections with Zero-Rate Density Estimation
 This section formalizes a connection between the objective of joint coding and modeling (declared in Definition 4) and a problem of zero-rate density estimation.
 3.1. Density Estimation with a Rate Constraint Let us first introduce the problem of rate constrained density estimation. Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities as introduced in Section 2.2.
 Definition 5. An (n, 2nR) learning rule of length n and rate R for F is a pair of functions (f, f), with f :
 Xn ? S and f : S ? T, where S is a finite set and
 1
 	 	.	(17)
 n
 The composition of these two functions p = f ? f : Xn ? T defines the rate-constrained learning rule for F taking values in the codebook {f(s) : s ? S} ? T, where R(p) = log2(|S|)/n denotes its description complexity in bits per training sample.
 Definition 6. The rate R = 0 is achievable for F, if a learning scheme ? = {(fn, fn) : n = 1} exists such that
 	and lim sup R(pn)	R,	(18)
 	nn?8	=
 where Z1, Z2 . . . in the left hand side (LHS) of (18) corresponds to i.i.d. realizations driven by µ ? F. In this case, we say that ? is an R-rate uniformly consistent scheme (or estimator) for the class F.
 3.2. Main Results
 Proposition 1. If for a given R > 0, {Cn,n : n = 1} is strongly minimax universal for the class F at the rate R (Definition 4), then its induced finite-description learning scheme obtained from the first stage in (13), i.e., ? = {(fn, fn) : n = 1}, is a zero-rate uniformly consistent estimator for F (Definition 6).
 The proof is presented in Section 8.1.
 Interestingly, the existence of a zero-rate uniformly consistent scheme for F is also sufficient to achieve the joint coding and modeling objetive (Definition 4) if some mild conditions are adopted from the work in [18]. This is stated in the following result:
 Theorem 1. Let us assume that
 ? : X×Xˆ ? R+ can be expressed by ?(x, xˆ) = d(x, xˆ)p where d(, ) is a bounded metric in X?Xˆ ×X?Xˆ with p > 0 and
 for all µ ? F, for all n = 1, and for all R > 0, there exists a (0, n)-block code, say Cµ*n, that achieves the n-order operational DRF Dµn(R) in (7).
 Then the existence of a learning scheme ? = {(fn, fn) : n = 1} that is zero-rate uniformly consistent for F implies that ?R > 0 there exists a joint coding and modeling scheme {Cn,n : n = 1} that is strongly minimax universal for F at rate R (Definition 4).
 The proof is presented in Section 8.2.
 Remark 1. The construction proposed for {Cn,n : n = 1} at any rate R > 0 (in Section 8.2) using the zero-rate density estimation scheme ? = {pn = fn ? fn : n = 1} satisfies that:
 	 µn (V(µpn(Zn), µ)) and	(19)
 	R(Cn,n) - R = R(pn),	(20)
 ?n = 1, where C > 0 is a constant. It is worth noting that these two inequalities summarize the result in Theorem 1 and, importantly, these two bounds are independent of R.
 Remark 2. An important consequence of the bounds in (19) and (20) is the fact that constructing a learning scheme ? = {pn : n = 1} with specific rates of convergence for supµ?F E(V(µpn(Zn), µ)) and R(pn) (as n goes to infinity) produces a joint coding and modeling scheme that achieves a uniform rate of convergence to zero (over F) of the overhead in distortion by (19) and a uniform rate of convergence to zero of the overhead in rate by (20). This observation will be used in all the achievable results presented in Sections 4 and 5, where, consequently, the problem reduces to determine ? and expressions for supµ?F E(V(µpn(Zn), µ)) and R(pn).
 Joint Source Coding and Modeling Achievability Results
 From the connection with zero-rate density estimation in Section 3, here we present a set of new results for the joint coding and modeling problem of Section 2.3. In these results, the general conditions (i) and (ii) stated in Theorem 1 are assumed.
 4.1. Main Result: The Skeleton Density Estimator
 Let us first introduce some notions from approximation theory [37].
 Definition 7. Let F ? AC(X) be a class of densities. We say that F is L1-totally bounded if for every e> 0, there is a finite set of elements {µi : i = 1, ..., N} in F such that,
 N
 	F ? [ BeV(µi),	(21)
 i=1
 where BeV(µ) = {v ? AC(X) : V(µ, v) <e}.
 Definition 8. For F L1-totally bounded, let Ne denote the smallest positive integer that achieves the condition in (21). Ne is called the e-covering number of F and K(e) = log2(Ne) is called the Kolmogorov’s e-entropy of F [30].
 Definition 9. An e-covering Ge of F such that |Ge| = Ne is called an e-skeleton of F [29].
 Theorem 2. There is a strongly minimax universal joint coding and modeling scheme for F at rate R for any rate R > 0 if, and only if, F is L1-totally bounded.
 The proof is presented in Section 8.3.
 The achievability part of the proof of Theorem 2 relies on the adoption of the skeleton estimator [29] (with its minimum distance learning principle in (42)), which is a zero-rate uniformly consistent density estimator for F (Definition 6). Furthermore, Theorem 2 can be complemented saying that the proposed construction {Cn,n : n = 1} derived from the skeleton estimator satisfies that (Pµ is a short-hand for the process distribution of (Zn)n=1 characterized by µ ? F under the i.i.d. assumption.)
 lim D ( n,n Zn) = D (R), P almost surely,
 n?8 µ C	|	µ	µ -	(22)
 lim V(µpn(Zn), µ) = 0, Pµ - almost surely,	(23)
 n?8
 ?µ ? F. The argument is presented in Appendix A.
 4.2. Examples of L1-Totally Bounded Clases
 Knowing specific expressions for K(e) = log2 Ne < 8, the skeleton estimator can be optimized selecting its design parameter appropriately. In particular, the sequence (en)n=1 (see details in Section 8.3) is selected as the solution of the optimal balance between estimation and approximation errors (see (45)
 in Section 8.3), which is given by e  ] (Chapter 7.2). The details of this analysis are presented in Section 8.3 and [30] (Chapter 7). By doing so, an optimized zero-rate skeleton scheme  , with concrete rate of convergence for supµ?F EZn~µn(V(µpen*(Zn),µ)) and
 R(pen*), can be obtained. From Remarks 1 and 2, these results imply specific performance results for the induced joint coding and modeling scheme. To illustrate, we present three interesting examples below.
 4.2.1. Finite Mixture Classes
 Let F = {µ? : ? ? T} with  be the class of measures which are a convex combination of {µ1, ..., µd} ? AC(X), i.e., k · µk(A). F is
 L1-totally bounded with K(e) being O(d log(1/e)) [30] (Chapter 7.4). From (45) the optimal sequence
   is O(vd/n) [30], which implies the following finite-rate performance bound [30] (Chapter 7.4):
 r
 	sup?? EnV(µpe*(Zn), µ?)o =	Cd logn	n,
 	T	n
 with C a universal non-negative constant. The rate in bits per-sample R /n is O(log n/n).
 4.2.2. Monotone Densities in [0, 1]d
 Let F be the collection of densities with support on [0, 1]d, monotonically decreasing per coordinate and bounded by a constant L > 0. This class is known to be L1-totally bounded, and furthermore K(e) = CLd [30] (Lemma 7.1), with the constant C depending only on d. From (  being O(Ld/d+2/n1/d+2) ed
 is optimal (please see details in [26,30]) with the following performance bound,
 2
  .
 In this case, the rate in bits per sample R /n is O(1/n2/d+2).
 4.2.3. r-Moment Smooth Class in [0, 1]
 Let F be the class of densities defined on the bounded support [0, 1], with r absolutely continuous derivatives (with r an integer greater than zero) and satisfying that:  C for a constant C > 0. This class is L1-totally bounded with K(e) being O(1/er+1) [30] (Chapter 7.6). From (45), the optimal sequence  , where supµ?F EnV(µpen*(Zn), µ)o is O(1/n1/3+r) and the rate in bits per sample R /n is O(1/n2/3+r).
 Notably, the last two examples are fully non-parametric, where K(e) is a polynomial function of
 1/e. Richer non-parametric examples of L1-totally bounded clases of densities, where K(e) is even exponentially in 1/e, are presented in [30] (Chapters 7.6 and 7.8) and its references.
 4.3. Yatracos Classes with Finite VC Dimension
 Looking at the distortion redundancy bound in (19), when F is totally bounded the fastest rate ofv
  
 convergence that could be achieved with the skeleton estimator proposed in Theorem 2 is O( 1/n) (see Section 8.3 and the estimation error bound in (45)). In this section, more specific density collections
  
 are studied to achieve this best rate O(v1/n) for density estimation and distortion redundancy from (19). We follow the path proposed by Yatracos in [38], who explored families of distributions with a finite Vapnik and Chervonenkis (VC) dimension the so-called VC classes [39,40]. Let us first introduce some definitions:
 Definition 10 ([38]). Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities. The Yatracos class for such a collection is given by
 	 ,	(24)
 where A  is the Scheffé set of µ? with respect to µ?¯, as defined in (2).
 Theorem 3. Let us assume that
 F is L1-totally bounded,
 the Yatracos class AT has a finite VC dimension (DefinitionF	A1 in Appendixv	B), and
  
 the Kolmogorov’s entropy of	associated with the sequence en = 1/	n grows strictly sub-linearly, i.e., log2(N1/v n) is o(n), then there is a zero-rate density estimator scheme ? = {(fn, fn) : n = 1} for F such that
  
 µsup EZn~µn nV(µpn(Zn), µ)o is O(1/vn),
 ?F
  
 where pn(Zn) = fn(fn(Zn)) is the skeleton estimator in (42) with en = 1/vn. Furthermore, ? is also a zero-rate strongly consistent density estimator where ?µ ? F
  
 V(µpn(Zn), µ) is O(plog n/n), Pµ - almost surely.
 The proof is presented in Section 8.4.
 From Definition 7, log2(Ne) is inversely proportional to e. In fact, depending of how riche F is, log2(Ne) can go from being O(log 1/e), passing from being polynomial in 1/e, to being O(e1/ ) (see a number of examples in [30] (Chapter 7) and its references). Then the role of (iii) in the statement of Theorem 3 is to bound how fast Ne should tend to infinity as e goes to zero, to guarantee a zero-rate in the skeleton learning scheme. It is simple to show that Ne being O(e(1/e)q ) with q ? [0, 2) is sufficient to achieve that log2(N1/v n) is o(n). This is a condition satisfied by a rich collection of L1-totally bounded classes in AC(X). Concrete examples are presented in [30] (Chapter 7).
 The Parametric Scenario
 The results presented so far are of theoretical interest because they rely on the skeleton estimator that is constructed from the skeleton covering of F (see Definition 9), which is unknown in practice. Moving towards making the zero-rate skeleton learning scheme of practical interest, we revisit the important parametric scenario in which T, the index set of F, is a compact set contained in a finite-dimensional Euclidean space Rk. Interestingly, in this context we can consider a practical covering of F induced by the uniform partition of the parameter space T, as used in [18]. Unlike [18],
 where a minimum-distance estimate is first found and then quantized, here we first quantize the space T and then find the minimum-distance estimate among a finite collection of candidates (i.e., over a finte number of prototypes in T). Some assumptions will be needed.
 Definition 11 ([18]). Let F = {µ? : ? ? T} with T ? Rk. Let IF : T ? F be the index function of F that maps ? to µ?. IF is said to be locally uniformly Lipschitz, if there exists r > 0 and m > 0, such ?? ? T,
 ?f ? Br(?),
 	V(µ?, µf) = m ||? - f|| ,	(25)
 where Br(?) ? T denotes the ball of radius r (with respect to the Euclidean norm in Rk) centered at ?.
 The following lemma shows that F is L1-totally bounded under some parametric assumptions.
 Lemma 1. Let F = {µ? : ? ? T} ? P(X) with T ? Rk. If T is bounded (?L > 0 such that T ?
  bounded. Furthermore, NL, L]) and the mappinge is O(I1F/: kT) for this family.? F is locally uniformly Lipschitz (Definition 11), then F is L1-totally
 e
 The proof is presented in Section 8.5.
 It is important to note that the e-covering of F used in the proof of Lemma 1 to derive an upper bound for Ne is practical (see Appendix C). This offers the possibility of implementing a practical skeleton estimator, which is the focus of the following result.
 The Practical Skeleton Estimator
 Under the assumptions of Lemma 1, let (f˜n,e, f˜n,e) denote the learning rule of length n associated with the minimum-distance principle in (42) with parameter e (see details in Section 8.3), where instead of using the e-skeleton Ge of F (in Definition 9), the implementable (see Appendix C) e-covering of T presented in the proof of Lemma 1 is used. This practical e-covering is denoted by G˜e (by definition, Ne   N˜e ~ O(1/ek), this last part from Lemma 1.). With this, let  (f˜n,en,   denote our practical learning scheme indexed by the precision numbers (en)n=1 ? (R+)N. We are in a position to integrate Theorem 3 and Lemma 1 to state the following:
  
 Theorem 4. Under the assumptions of Lemma 1, the practical skeleton estimator ?˜ ((en)n=1) with en* = 1/vn satisfies that
 	µsup? EZn~µn nV(µp˜n,en*(Zn), µ?)o is O(plog n/n), and R ,	(26)
 ?F
 where p˜n,e(Zn) = f˜n,e(f˜n,e(Zn)).
 In addition, if the Yatracos collection  has a finite VC dimension equal to
 J, then
 	µsup? EZn~µn nV(µp˜n,en*(Zn), µ?)o is O(1/vn), and R .	(27)
 ?F
 The proof is presented in Section 8.6.
 When X ? Rd, Raginsky [18] showed that the finite VC dimension assumption of Theorem 4 is satisfied by the class of mixture families presented in Section 4.2.1 and a rich collection of exponential families of the form F = {µ? : ? ? T} ? P(X) with  dµ  , where f(x) is a reference density, {hi(·) : i = 1, ..., k} is a set of arbitrary real-valued functions, g(?) is a normalization constant (g dx see details in [18] (Section V)), and T is a compact subset of Rk (see details in [18] (Section V)).
 Summary of the Results
 We summarize the results of the proposed zero-rate density estimation approach adopted for the problem of joint fixed-rate lossy source coding and modeling of continuous memoryless sources.
 Proposition 1 and Theorem 1 formalize the interplay between the two-stage joint fixed-rate coding and modeling objective and the problem of zero-rate uniformly consistent (in expected total variation) density estimation.
 Theorem 2 establishes a necessary and sufficient condition on a family of densities for the existence of a strongly minimax joint coding and modeling scheme achieving both source coding and model identification objectives (Definition 4). The result is obtained for the rich non-parametric collection of L1-totally bounded densities.
 For the modeling stage, we propose using the skeleton estimator, which first quantizes the data and then finds the minimum-distance decision on this finite set of density candidates (42). This is a practical solution in the sense that the inference (minimization) is carried out over a finite set.
 By introducing combinatorial regularity conditions on the family of distributionsv	F = {µ? : ? ? T},
  
 the skeleton scheme achieves O(1/ n) rate of convergence in the n-order distortion redundancy, and the same rate in the expected total variational distance for the modeling part (Theorem 3). • Finally, for a relevant parametric setting, a practical skeleton-based joint coding and modelingO v scheme is proposed that achieves a rate of (1/ n) for the n-order distortion redundancy (Theorem 4). This rate is slightly better than the O(plog n/n) achieved in [18] under the same rate overhead of O(log(n)/n). Furthermore, Theorem 4 removes the finite-VC-dimension assumption over the Yatracos class AT considered in [18] (Theorem 3.2), while achieving the same performance
  
 rates in terms of n-order distortion redundancy O(plog n/n), uniform expected risk to learn the
  
 density O(plog n/n), and rate overhead O(log n/n).
 Concerning the last parametric result, we note that the result in [18] can be improved by the adoption of Dudley’s entropy bound [41], which would yield the same asymptotic rate reported in this work for the n-order distortion redundancy.
 A final remark is that under the bounded distortion metric assumption of Theorem 1 condition (i), Linder et al. [14] (Theorem 2) showed that ?? ? T, and for every R > 0 such that Dµ?(R) > 0, there is a constant K?(R) > 0 such that
 r
 Dµn?(R) - Dµ?(R) = (K?(R) + rn)	log n,	(28) n
 where (rn) is a sequence that converges to zero (o(1)) uniformly in T. This result offers a rate of convergence of the n-order operational distortion-rate function to the Shannon DRF as the block length tends to infinity. In view of (11), we can adopt this result in Theorems 3 and 4, to say that the average distortion of the respective joint coding and modeling schemes at rate R, i.e., Dµ(Cn,n), convergences to the Shannon DRF Dµ(R) as O  point-wise ?µv? F. Therefore in the process of comparing
 Dµ(Cn,n) with the Shannon DR function, we lose the O(	1/n) rate of convergence.
 Conclusions
 This work revisits the problem of fixed-rate universal lossy source coding and model identification with training data proposed in [18] from a learning perspective. Remarkably, we found that the problem is equivalent to the problem of density estimation of the source distribution with some concrete but non-conventional operational data-rate constraints in bits per sample. This learning problem can be seen as the task of estimating and encoding the distribution of samples with a zero-rate in bits per sample, while achieving a consistent estimation in expected total variations of the distribution after the decoding process. From our perspective, the rate-constraint density estimation problem is interesting in itself and can have relevant applications in other contexts such as distributed learning scenarios and sensor network problems.
 Importantly for the joint coding and modeling problem, the connection with density estimation provides a context for the use of the skeleton estimator proposed by Yatracos in [29]. We highlight two important implications from its use. First, we extend results about minimax universality from the parametric context explored in [30] to the rich non-parametric family of L1-totally bounded densities [26,30]. This result significantly expands the contexts where the joint model and coding objective can be achieved. We illustrated this with some examples in Section 4.2 and many more can be found in the literature of density estimation [26,30].
 Second, in the parametric case studied in [18], we were able to remove some of the assumptions and obtain not only the same performance result in terms of rate of convergence of the n-order distortion redundancy but also slightly better convergence results. Therefore, the Skeleton estimator, though essentially a non-parametric learning scheme, is shown to be instrumental in enriching the applicability of the joint coding and modeling framework.
 Proofs of Results
 8.1. Proposition 1
 Proof. The fact that ? is uniformly consistent for F is directly from Definition 4. On the other hand, the rate of pn = fn ? fn is R . From the definition of Dµn(R), it is simple to show from the strict monotonicity of Dµ(R) that in order for limn?8 supµ?F hDµ(Cn,n) - Dµn(R)i = 0, it is required that  e for any e > 0. Then, from (16), and since log |S˜n|/n = R(pn), lim supn?8 R(Cn,n) = R implies that limn?8 R(pn) = 0.  
 8.2. Theorem 1
 Proof. The proof builds upon the ideas elaborated in [18] (Theorem 3.2, p. 3065). Let us consider an arbitrary R > 0 and let ? = {(fn, fn) : n = 1} be the zero-rate learning scheme of the assumption. Using ?, let us construct the joint coding and modeling rule of length n by:
 Cn,n = fn : Xn ? S˜n, fn : S˜n ? T,
 	 .	(29)
 Concerning the first stage of {Cn,n : n = 1}, it is induced directly from the coding-decoding rules of ?.
 For the second stage, ?n = 1, ?s˜ ? S˜n the pair (fn,s˜, fn,s˜) is picked such that 	?n s	fn,s˜, which
 ,˜
 is the optimal n-block code that achieves Dµn?n s (R) (from the hypothesis in (ii)), with ?n,s˜ = fn(fn(s˜))
 ,˜
 short-hand for the reproduction codeword induced from the first stage-pair (fn, fn), and Sn satisfying the R-rate constraint, i.e., |Sn| = 2nR. From construction and the fact that ? has zero-rate,
 R,
 n?8
 then {Cn,n : n = 1} satisfies the rate condition. On the other hand, based on the assumption that ? is zero-rate uniformly consistent, it follows that
 0,	(30) n
 where ?ˆn(Zn) = fn(fn(Zn)). Then {Cn,n : n = 1} achieves the modeling objective. Concerning the coding objective, we use the following key result:
 Lemma 2 ([18] (Lemma C.1)). Let P and Q be two probability measures in (X, B(X)). Let Cn = (f, f) be a zero-memory n-block coder with the nearest neighbor property (i.e., Cn is nearest neighbor if, n,   arg minxˆ  with GCn the reproduction codebook of Cn.). If we denote the performance
 of Cn (Cn = f ? f) with respect to P by
 	DP(Cn) =  n1EXn~Pn (?(Cn(Xn), Xn)) ,	(31)
 where Pn denotes the product measure with marginal P in (Xn, B(Xn)), and ? satisfies the condition i) of Theorem 1 and is bounded by dmax, then
 	 (P, Q).	(32)
 Furthermore, the inequality can be extended for the n-order operational distortions in (7), i.e.,
 	 (P, Q),	(33)
 ?R > 0.
 Let us work with the following distortion redundancy,
 Dµ
 (34)
 	= Dµ	 ?ˆn(Zn)	µ?ˆn(Zn)	?n(Z ), µ)	(35)
 = Dµ(Cµ 
 + 21/pdmax · V(µ?ˆn(Zn), µ)	(36)
 = 21/p+1dmax · V(µ?ˆ (Zn), µ).	(37)
 n
 For the first equality we use (5). The inequality in (35) is from the definition in (31) and (33), and the equality in (36) is from the construction of ?nˆn(Zn) which is n-operational optimal for the distribution µ?ˆn(Zn) at rate R. Finally, (37) is from (32).
 Concluding, Dµ(Cn,n|Zn) - Dµn(R) is random (a measurable function of Zn) and dominated by
 V(µ?ˆn(Zn)µ). Hence taking the expected value (with respect to Zn) on both sides of this inequality
 (see (6)), we have the uniform convergence in (30) implying that
 	lim sup hDµ(Cn,n) - Dµn(R)i = 0,	(38)
 n?8µ?F
 and then the coding objective is achieved.	 
 8.3. Theorem 2
 Proof. Let us first assume that F is L1-totally bounded and prove the direct part of the statement.
 We adopt the skeleton estimate proposed by Yatracos [29] and extended by Devroye et al. [42,43]
 (a complete presentation can be found in [30] (Chapter 7)). For any arbitrary e> 0, let us consider the
 o	dµ?e e-skeleton Ge	1, ..., Ne	of F. We use g?ie(x) = d?i (x) as short-hand for the i-th pdf in
 , and we define
 Ge
 1, ..., Ne} ? T
 to represent the index set of Ge. Let us consider the Yatracos class of Ge given by [30]
 	n	o
 A
 	e =	Aie,j, Aej,i : 1 = i < j = Ne ,	(39)
 where Aie,j   is the Scheffé set of µ?ie with respect to µ?je in (2) [30,33].
 Hence, given i.i.d. realizations X1, ..., Xn with Xi ~ µ? (µ? ? F), let us propose the encoder-decoder pair (fn,e, fn,e) associated with Ae by,
 	fn,e(Xn) = arg	 min	µˆn ,	(40)
 	 ,	(41)
 where µˆn  is the standard empirical distribution. In this context,
 	?ˆe(Xn) = fn,e((fn,e(Xn))) = arg min	µˆn ,	(42)
 	?ie?Te B	e
 is the well-known skeleton estimate [29]. ?ˆe(X1n) is the minimum-distance approximation of µˆn with elements of Ge [29,30], adopting the measure in the right-hand-side of (42) that is reminiscent of the total variational distance in (1). In order to choose a sequence (en)n=1, we consider the following performance bound.
 Lemma 3 ([30] (Theorem 6.3)). For any µ ? F,
 	V(µ  |µˆn(B) - µ(B)| .	(43)
 e
 Equation (43) is valid for any e > 0 and, consequently, it provides a trade-off between an approximation error term and an estimation error term. The approximation error is minv?Ge V(v, µ),
 which is bounded by the definition of Ge. For the estimation error, on the other hand, Yatracos proposed the use of Hoeffding’s inequality [44] to obtain that ?µ ? P(X) [30] (Theorem 7.1),
 	EXn~µn  .	(44)
 Using (44) in (43), it follows that, supµ . This last expression is distribution-free and it is valid if the approximation fidelity e is a chosen function of n [30]. Consequently, for any sequence (en)n=1,
 s
 	n	o	8 log(2Ne2n )
 	µsup??F E V(µ?ˆen (Xn), µ?)	= 3en +	n	,	(45)
 for all n	=	1. Hence,	we consider en*	  proposed in [30]
 (Chapter 7.2), which is well-defined and converges to zero as n tends to infinity. Consequently from (45), limn?8 supµ  0. Then the learning scheme
   satisfies the learning requirement in Definition 6, where in
 particular R   is O(1/vn) by construction. To conclude the argument of this part (i.e., presenting the construction of the second stage of a joint coding & modeling scheme), we adopt the result and the construction presented in the proof of Theorem 1 (see Remark 1 for details). This result implies that ?R > 0 there is a strongly minimax universal joint coding and modeling scheme for F at rate R.
 For the other implication (the converse part of the statement), let us fix R > 0 and assume that we have a joint coding & modeling scheme that is strongly minimax universal (Definition 4) for F at rate R. Then from Proposition 1, we have a learning scheme ? = {(fn, fn) : n = 1} such that limn?8 R(pn = fn ? fn) = 0 and
 	n 	E	= 0.	(46)
 µ
 For the learning rule of length n, we have its reproduction codebook that we denote by Tn =  . Let us define the minimum-distance oracle solution in Tn by
 ?˜n(µ) = arg inf V(µ?, µ).	(47) ??Tn
 From (46), we have that limn?8 supµ?F V(µ?˜n(µ), µ) = 0. In other words, ?e > 0, there exists
 N(e) <8, such that for all n = N(e), V(µ?˜n(µ), µ) <e uniformly for every elementS µ ? F. This means that ?e> 0 there exists N(e) <8, such that for any arbitrary n¯ > N(e), F ? ? Tn¯ Be(µ?), where by
 | n¯ | <8. Then F is totally bounded, which concludes the proof. ? construction T
 8.4. Theorem 3
 Proof. From Lemma 3, for any arbitrary sequence (en)n=1
 	V(µ?ˆen (Xn), µ?) = 3en + 4 sup |µˆn(B) - µ?(B)| .	(48)
 B?Aen
 with Aen the Yatracos class of the skeleton Gen. It is clear that ?e> 0, Ae ? AT. Then by monotonicity
  , for all e > 0 and for any distribution
 µ ? P(X). Here is where we use the assumption that AT has finite VC dimension J, which implies from [30] (Theorem 3.1) that
 	 	! r
 E	ˆ (B)	(B)	= c	J	(49) n
 for some constant c > 0. Substituting this result in (48), the argument concludes by replacing (en) = (1/vn), a solution which achieves the intended rate of convergence for supµ? EnV(µ?ˆ vn(Xn), µ?)o. Finally, the rate of the learning rule is dlog2(Nn1/vn)e, which tends to
 	?F	1/
 zero by the last hypothesis.
 	For the almost-sure convergence part if e 	vn, it is sufficient to show that the second term in
  
 the right hand side (RHS) of (48) is O(plog n/n) Pµ-almost surely. From the fact that AT has finite VC dimension (Definition A1), and from the classical VC inequality [30] (Corollary 4.1 and Theorem 3.1) and [45] (Chapter 12.4), it follows that
   e  ,
  
 ?n = 0 and ?e> 0. Then considering an = plog n/n and M2/32 > J + 2,
 	?	?
 	(n + 1)J	K
 P? sup |µˆn(B) - µ?(B)| > M · an? = 8 nM2/32 = n2
 B?Aen*
 for some K > 0, hence .	Then from the Borel
 Cantelli Lemma,	  	e |µˆn(B) - µ?(B)| = M Pµ-almost surely, which concludes
 n
 the proof. As (an) is o(1), this result implies the almost-sure convergences to zero of V(µ?ˆe*(Xn), µ?) as
 n n goes to infinity.
 Finally, using similar arguments, it is possible to show that V(µ?ˆe*(Xn), µ?) is o(1/nt) Pµ-almost
 n
 surely for any t ? (0, 1/2).	 
 8.5. Lemma 1
 Proof. First note that T is contained in a compact set k, consequently, T inherits the finite covering property of a compact set, i.e., ?e> 0, there exists a finite covering  such that, K(e)
  [ [ .	(50) ??T	i=1
 On the other hand, from the locally uniformly Lipschitz assumption on IF : T ? F, there exists r > 0 and m > 0 such that V(µ?, µf) = m ||? - f||, ?? ? T, ?f ? Br(?). Then, by considering eo < r, it follows by construction of Teo that
 	K(eo)	K(eo)
 	 ,	(51)
 i
 	i=1	i=1
 where BdV(µ) = {v ? P(X) : V(v, µ) <d} is the ball centered at µ ? P(X), induced from the total variational distance, and the last inequality stems from the Lipschitz condition. Hence, from (51), ?e> 0
 e o	SM there exists M(e) = K(min {e/m,r},..., µM(e)	? P(X), such that F ?	i, which proves the result.
 For the final part, let (m, r) be the uniform parameters that characterize the Lipschitz condition of
 from (IF(·) 51(Definition) Ne is upper bounded by11). Without loss of generality, let us assume the critical regime whereK(e/m), which is the covering number of	me <L, Lr, hence] ?	k,
 R
 we will work with a uniform partition of   to find a bound for K(e/m). Let e¯ = me , then inducing a product-type partition, where in each coordinate we have   uniform length cells, we have the required e¯-covering. The number of prototypes is O , which is O(1/ek) as a function of e (e = e¯ · m).
 To clarify the constructive nature of the e-covering used to prove this result, an algorithm with the basic steps of the construction of this practical covering is sketched in Appendix C.	 
 8.6. Theorem 4
 Proof. Let G˜e ? F be the e-covering induced from the uniform partition of T presented in Lemma 1.
 From this we can construct the minimum-distance estimate in (42) adopting the Yatracos class of G˜e (with index set T˜ e), i.e., A˜e, which, from (39), yields
 	?˜e(Xn) = arg min.	(52)
 ?ie?T˜ e B?A˜e
  
 Considering en = 1/vn, from (45) it follows that
 	n	o	t
 .
 	µ??F	n	vn	n
 The latter upper bound is asymptotically dominated by (plog n/n) from the fact that  is O(k log(n)) (Lemma 1), which proves the assertions made in (26).
 Concerning part (ii), using the arguments presented in the proof of Theorem 3, we can obtain that ?e> 0,
 	.	(53)
 µ
 From this point, the proof follows from the arguments of Theorem 3 and the fact that  is O(k/2 · log2 n).  
 Author Contributions: Conceptualization, J.F. Silva and M.S. Derpich; Methodology, J.F. Silva and M.S. Derpich;
 Formal Analysis, J.F. Silva and M.S. Derpich;	Investigation and Results, J.F. Silva and M.S. Derpich;
 Writing—Original Draft Preparation, J.F. Silva and M.S. Derpich; Writing—J.F. Silva & Editing, M.S. Derpich; Project Administration, J.F. Silva; Funding Acquisition, J.F. Silva.
 Funding: The work is supported by funding from FONDECYT Grants 1170854 and 1171059, CONICYT-Chile and the Advanced Center for Electrical and Electronic Engineering (AC3E), Basal Project FB0008. In addition, J.F. Silva acknowledges support from Project Anillos ACTI 1405, CONICYT-Chile.
 Acknowledgments: We want to thank the anonymous reviewers for their constructive comments that were instrumental to improve the technical content and organization of this work. We thank Diane Greenstein for editing and proofreading all this material and Sebastian Espinosa for preparing Figure 1.
 Conflicts of Interest: The authors declare no conflict of interest.
 Appendix A. Proof of (22) and (23)
 First, we show that the zero-rate skeleton estimate ?((en)) = {(fn,en, fn,en ) : n = 1} proposed in (40) and (41) is also strongly consistent.
 Proposition A1.   is strongly consistent, i.e., for any µ ? F,
 nlim?8 V(µ?ˆen*(Xn), µ) = 0, Pµ-almost surely.
 Proof. Let us consider the skeleton estimate µ?ˆe*(Xn), where the sequence was chosen by the rule
 n
 e  . Then  n for all n. From Lemma 3,
 V(µ . As by construction  , we just need to concentrate on the estimation error term. Applying Hoeffding’s inequality [44] ?d> 0,
 	  e-2nd2 = 2e(vn/ log e-2nd2),	(A1)
 where from the Borel-Cantelli lemma [46,47], the estimation error convergences to zero almost-surely.	 
 Finally considering the inequality in (37), we have that Dµ  V(µ?ˆn(Zn), µ), ?µ ? F, which concludes the argument.
 Appendix B. Basic Definitions of Vapnik and Chervonenkis Theory
 Let C ? B(X) be a collection of measurable events, and xn = (x1, ..., xn) be a sequence of n points in Xn. Then we define by S(C, xn) the number of different sets in
 {{x1, x2, ..., xn} n B : B ? C} ,
 and the shatter coefficient of C by [40,45]
 	Sn(C) = sup S(C, xn).	(A2)
 xn?Xn
 The shatter coefficient is an indicator of the richness of C to dichotomize a finite sequence of points in the space, where by definition Sn(C) = 2n.
 Definition A1. The first time (in the index n) where Sn(C) is strictly less than 2n is called the Vapnik and
 Chervonenkis (VC) dimension of C [45]. If C has a finite VC dimension then it is called a VC class; otherwise if Sn(C) = 2n ?n = 1, then the class is said to have an infinite VC-dimension.
 Appendix C. Pseudo Algorithm to Implement the Practical e-Covering Presented in Lemma 1
 Under the parametric assumptions of Lemma 1, we recognize four structural parameters that characterize F: k the dimension of the Euclidean space that contains T, L > 0 associated with the assumption that  L, L], and (r, m) the parameters associated with the locally Lipschitz assumption of IF. Given these four parameters (k, L, m, r) and e> 0, there is a constructive e-covering presented in the proof of Lemma 1 that can be implemented in the following steps:
 In each of the k dimensions ofv	T, the interval [-L, L] is partitioned uniformly with sub-intervals-	d v	e
 of length 2e/(m	k). This produces a scalar quantization of [	L, L] with	m	kL/e prototypes per coordinate.
 A product partition of L, L] is made with the scalar quantizations of the previous step.
 From the proof of Lemma 1, this is a e/m-covering of T with K = dmvkL/eek prototypes. Let us denote this set by {?i, i = 1, ..., K} ? T.
 From the proof of Lemma 1, the covering of T constructed in the previous step induces an e-covering of F by applying the indexing function IF, i.e., by {IF(?i) : i = 1, ..., K} .
 ﻿ This paper novel on the optimal design of Noise Shaping Differential Pulse Modulation . The main contribution in the derivation of explicit analytic for the optimal and the minimum achievable frequency weighted reconstruction error . A novel aspect in the analysis is the fact that we account for fed back quantization noise and that we make no on the order of the . 
 I . INTRODUCTION 
 to Digital which utilize a scalar and linear , time invariant in a feedback loop have been extensively employed as a source method since the concept was first in the . The generalized form of this architecture , which we denote Noise Shaping Differential Pulse Code Modulation , can be as in Fig The in a system allow one to account for the correlation between consecutive input , and to spectrally shape the quantization noise in the output , so as to minimize the frequency weighted mean square reconstruction error . Special of the architecture include and noise shaping , such as one and bit Sigma Delta . are extensively used in the context of audio compression , digital image half and A conversion . 
 Provided that the input power spectral density , frequency weighting error criterion , and scalar are known , the design of an converter that minimum to finding the corresponding optimal . This been an intense area of research for at least . However , available to date on optimal filter design for have been assuming either fixed , finite order , negligible fed back quantization noise or have upon heuristic design Since optimal performance can , in general , only be by arbitrary order designed accounting for fed back quantization noise , an exact characterization of the optimal performance and for an open problem . In this paper we derive an explicit analytic expression for the optimal performance and filter frequency for . We characterize the scalar via its signal to noise ratio , and adopt a white quantization noise model . The performance bound to the minimum that can be by an with any linear , time invariant . A key departure from which , to the best of our knowledge , the only currently available explicit analytic to the problem , is that we account for fed back quantization noise . This us to derive exact . 
 Our show that an optimal converter several interesting . These include a spectrally flat frequency weighted error spectrum , and a white signal at the input of the scalar . We also show that , for AR , the rate distortion efficiency with the optimal only on how efficient the scalar is at nearly . 
 Notation and 
 We use standard vector space notation for . For example , is used to denote . We also the argument of the transform . Given two square integrable complex valued and defined over ,, we adopt the inner product where complex conjugation . We denote the usual 
 norm as is a transfer function , then we use the short hand refer to the associated frequency response , ,. If I is a set , then we will write a . e . on I almost everywhere on I as a short hand notation for everywhere on I except at most on a zero measure set of . 
 We use to denote the variance of a given wide sense stationary ... random process Note that , where 
 v 
 is a frequency response satisfying 
 ,. For a given function : , , we define , 
 provided this integral . This 
 one to describe the minimal prediction error variance of a ... , . The spectral flatness measure of a ... . It is easy to show that , and that if and only if is constant a . e . on ,. 
 . MODEL 
 As in the introduction , we consider the general form of an architecture shown in Figure . In our model , the input assumed to be a zero mean ,... random process , with known satisfying , a . e . on ,. The element scalar , with given and known . For each input ,, it and the quantization error , . The three discrete time , and in Fig . are design . 
 To performance , we introduce the delay frequency weighted error 
 , , where . The error weighting filter the impact that reconstruction have on each frequency . Thus , it is application dependent . 
 In this paper , we restrict attention to the in which , ,, i . e ., no on the unit circle . Additionally , we require : 
 Constraint : and are stable . In addition , is strictly causal i . e ., . 
 The first part in the above constraint is in order to avoid unbounded in the converter . The additional requirement on is for the feedback loop in Fig . to be well defined see , e .. Chap 
 Since the architecture a nonlinear element a scalar within a feedback loop , exact analysis of quantization is , in general , a formidable task . This the widespread use of an additive noise model for quantization This model one to study the converter via linear analysis . It is usually as : 
 Assumption : The quantization are i . i .. random , uncorrelated with the input signal . 
 In order not to limit our subsequent analysis to a specific type of scalar , the following is also assumed : 
 Assumption : The probability density function not affected by the in the converter other than via its second moment . 
 Under Assumption , any given type of scalar with a fixed number of quantization to quantization whose variance is proportional to the variance of its input . This can be stated as 
 , 
 where is the signal to noise ratio of the scalar not to be confused with that of the system . on the number of quantization , the of the signal being and the of the scalar itself . 
 . FORMULATION OF THE OPTIMIZATION PROBLEM 
 Our ultimate goal is to find the frequency of the A ,, minimize the variance of under and , and for given and known , and . The will constitute the achievable lower bound on the for the converter . 
 Towards the above goal , we first derive an expression that the decision to the error measure that we wish to minimize . From equation , Assumption , and we have where is the variance of the quantization error , and is a delay version of , the frequency response . The first term on the right hand side of to the variance of the frequency weighted quantization error in . The second term in for the frequency weighted linear distortion by the in the pair . 
 The variance is related to via . From Assumption , the latter is given by . Combining this result with 
 When substituted into , this 
 The above expression the , and the signal to noise ratio , to the . Minimization of this cost functional will yield for the optimal and performance . 
 For comparison , we note that the cost functional , together with and , is also part of the analysis in , and , wherein equivalent optimization are . 
 Finally , we note that , since must be positive , 
 Constraint : OPTIMAL 
 In this section we derive explicit analytic for the optimal and the associated optimal performance for the scheme , subject to a mild restriction . The analysis is based on a set of that the optimal must necessarily satisfy . To facilitate the flow of , all are given in the Appendix . 
 Minimization of is simplified by that , for stable and strictly causal , it that . Substitution of this equality into 
 We then have the following result : 
 Lemma : For given , the 
 optimal 
 Notice that the cost functional in only two unknown , . This it simpler to work with than the functional in . 
 The optimization problem can be further simplified by writing the of . Unfortunately , the relationship the optimal , for the general case , can only be stated implicitly , as shown next . 
 Lemma : For a given frequency response , the optimal 
 W 
  
 a . e . on ,. 
 Remark : Notice that , from , is a positive , symmetric and real valued function of . It then from that the product of the optimal , must exhibit linearly decreasing phase . 
 In general , the presence of in the inner product on the right hand side of it difficult , if not impossible , to express the in of . However , under specific on , and , an analytical explicit solution to can be , as : 
 Lemma : Provided , a . e . on ,, then , for a given frequency response , the optimal , a . e . on ,. 
 To summarize our so far , we have shown that , provided , the . 
 These two , in turn , determine the optimal A and , respectively . We can now state the main result of this paper : 
 Theorem : If a . e . on ,, then the minimum achievable frequency weighted reconstruction of an converter is 
 This minimum is when the , A : 
 is an arbitrary constant . 
 V . DISCUSSION 
 The stated in Theorem have very interesting . Some of these are below . 
 of Scalar Quantization Without Feedback : It is easy to verify from the in Theorem that scalar quantization without feedback is optimal if and only if is constant . In particular , it from that if , a . e . on ,, then the optimal converter to a converter with a fully whitening filter and a post filter satisfying A and A . 
 Comparison with : The minimum for an system derived by Noll in , fed back quantization noise , is . Perhaps surprisingly , Theorem that the optimal performance is slightly better compare with . Moreover , the corresponding optimal AN , and derived in 
 respectively . Substituting these into actually an , where is the spectral flatness 
 measure of . It then that for any finite , and that . 
 Total Frequency Weighted Distortion is White : It from and that , in an system , the of frequency weighted quantization noise and linear distortion are , respectively 
 From the above , one can see that when the condition of 
 Theorem , the noise shaping by an optimal system is not complete , i . e ., frequency weighted quantization noise is not white . However , the of the total frequency weighted error is white , since 
 Relation with the Reverse Water Filling Paradigm : The parametric Rate Distortion formula for a ... process and as the distortion measure is given by the well known reverse water filling paradigm see , e .. 
 For , it total frequency weighted distortion to be equally distributed over frequency . It also the input signal to appear at the output with 
 , i . e ., less significant spectral higher attenuation . Interestingly , is equivalent to , a . e . on ,. Furthermore , is flat , as , and , in full agreement with the above prediction . 
 Output of the Scalar is White : It can be seen from a that , unless is constant , the optimal A is not a full whitening filter for . Interestingly , however , it is straightforward to verify that the optimal in render a Fig . with flat . More precisely A , where is the same arbitrary constant that in a . A remarkable implication is that the output of the converter can be efficiently into by of a first order entropy coder . 
 Rate Distortion Analysis : The rate distortion efficiency of any source scheme with quadratic error as distortion measure can be established by its 
 against the upper bound derived by O Neal in . For the case min , and to , this bound can be written as 
 , log log , bit rate in per sample , is the spectral flatness measure is the minimum variance associated Section I . Notice upper bound for the in of a memoryless source . 
 On the other hand , under the of Theorem and , the best achievable of an system is given by : 
 In , this ratio is 
 By and , we see that the of the converter via Theorem from the information theoretic upper bound as : 
 log log . 
 The difference , log for of long been known for a variety of scalar memoryless see , e .. and the therein . be in the converter , can be by . and . for a uniform with entropy E .., non uniform with E .., non uniform for without E .., and uniform quantization without E .. and a loading factor of , respectively see . 
 . This paper derived explicit analytic for the best achievable performance and optimal for . These , which we believe to be novel , were found by accounting for fed back quantization noise in the optimization . The in this paper simplify the analysis and design of , and provide valuable insight into the trade inherent in linear feedback . 
 . APPENDIX 
 A . Preliminary Result 
 Lemma : a given function such that 
 , , and e is finite . Then min , 
 is the set of non negative log integral . 
 Proof : is a monotonically increasing function ,¡ minimization of is equivalent to . From inequality and the constraint , we obtain 
 Equality is in a if and only if , a . e . on ,, for some . equality if and only if . This the proof . 
 This last equation directly to , the proof . 
 Thus , 
 Substituting this into . Notice that that the denominator on the right hand side of is strictly positive . The proof is by that substitution of into the inequality , thus our initial supposition . 
 Proof : Theorem Suppose the such that in Lemma . Then , one can substitute into to obtain 
 . After some algebra , this becomes 
 to be stable and strictly causal from Con straint is equivalent to the function to belong to the set of non negative log integral defined in , see , e . Theorem .. and . Then , it from Lemma that the optimal is as in a . Substitution of the latter into . It also from a that the inequality in Lemma is equivalent to the condition by the theorem . This our initial supposition . Notice also that the latter inequality also that , as by Condition . Finally , substituting a into , and the of . This the proof . 
  
 ﻿In this work we obtain capacity gaps for a class of N-pair bidirectional Gaussian relay networks, where one relay can help communications between the corresponding user pairs. For the uplink, we apply the generalization of successive compute-andforward strategy (SCAF) for decoding the linear combinations of the messages of each user pair at the relay. The downlink channel is considered as a broadcast network with N receiver groups. It is shown that for all channel gains, the achievable rate boundary lies within gaps of (N- 1 + log2N)/2N and (N + log2N)/2N bits/sec/Hz below the cut-set upper bound for restricted and nonrestricted models, respectively. These gaps tend to 1/2 bits/sec/Hz per user as N goes to infinity. We first derive a comprehensive formulation for the N-step asymmetric SCAF and use it to derive the capacity result for our problem.
 I. INTRODUCTION
 In [1], a compute-and-forward strategy (CAF) has been proposed for Gaussian relay networks with equal power constraints and asymmetric channel gains based on nested lattice codes [2]. The receiver can recover the associated linear equations which are closer to the channel fading coefficients. This strategy simultaneously provides protection against noise and the opportunity to exploit interference for cooperative gains [1]. In [3], a scheme has been proposed for Gaussian relay networks with unequal power constraints and symmetric channel gains based on nested lattice codes where it computes an achievable multi-cast rate within a constant gap below the capacity. In [4], a successive CAF (SCAF) scheme has been presented for the same network as defined in [1]. In this method after decoding a linear combination of transmitted codewords, the receiver can combine it with its channel observation to obtain a new effective channel that is better for decoding the next targeted linear combination. In [5] and [6] an asymmetric CAF has been proposed for MIMO networks and Gaussian relay networks, respectively, with unequal power constraints and asymmetric channel gains based on nested lattice codes.
 In [7] and [8], a modified asymmetric CAF strategy has been presented. The method utilizes a set of scaling factors to decode integer linear combinations of transmitted codewords. The use of scaling factors in fact is equivalent to decoding non-integer linear combinations of lattice codewords. It allows different users to have different coding rates. Thus, by appropriately adjusting those factors, different points on the boundary of the rate region can be achieved. Also, in [9], the idea of decoding non-integer linear combinations of lattice codewords has been presented which can be considered as a special case of the methods of [7] and [8].
 In this paper we generalize the method of [6] to Nstep SCAF. We also consider scaling factors as defined in [8]. Then we use these results to address Multi-Pair TwoWay Relay Networks (MPTRN), where one relay can help communications between the user pairs of each of the twoway relay channels (TRC). We present an achievable rate region for the MPTRN based on the generalization of the SCAF strategy. It is shown that this method achieves rates within (N -1+log2 N)/2N and (N +log2 N)/2N bpcu of the cut-set upper bound for the restricted and non-restricted MPTRN, respectively. As N goes to infinity, these gaps tend asymptotically to 1/2 bpcu. Some of the previous works about these topics are as follows:
 In [10], an achievable rate region was proposed for twopair two-way relay networks (TPTRN) based on using the combinations of lattice and Gaussian codewords. In that network, a relay node facilitates the communication between two pairs of users. They assume a complex AWGN channel model. It was shown that for all channel gains, rates to within 3 bits/sec/Hz per user of the cut-set upper-bound are achievable. In [11], we studied the same network model as in [10] but with the assumption of real AWGN channel model, obtaining a larger achievable rate region for that network by exploiting nested lattice codes and the successive compute-and-forward approach of [4]. It was shown that this method achieves within 1/2 and 3/4 bits/sec/Hz per user of the cut-set upper bound for restricted and non-restricted network models, respectively. The paper is organized as follows: In Section II the system model for the general Gaussian relay network is defined. In Section III we present our generalization of the SCAF approach. The system model, achievable rate regions and capacity gaps for the multi-pair two-way relay networks are presented in Section IV. Some concluding remarks are provided in Section V.
 II. THE GENERAL GAUSSIAN RELAY NETWORK
 In this section we consider a Gaussian relay network with L transmitters and M relays. We use the same model as defined in [1], but with the assumption of unequal power constraints and asymmetric channel gains. Each relay (indexed by m = 1,2,··· ,M) observes a noisy linear combination of the transmitted signals through the channel,
 L
 	ym = Xhm`x` + zm,	(1)
 `=1
 where hml ? R are the channel gains and zm is i.i.d. Gaussian noise, zm ~ N(0,In×n). Let hm = [hm1,··· ,hmL]T denote the vector of channel gains from the L users to relay m. Each channel input x` is a n-length sequence subject to the average power constraint P`, for all `, i.e.,  . The channel gains are assumed to be known and constant.
 Let  ,
 [1;n] denote the set of integers {1,··· ,n}, and dxe denote x rounded to the nearest integer value. Throughout the paper, vectors are shown with boldface letters.
 III. ASYMMETRIC SUCCESSIVE COMPUTE-AND-FORWARD
 In this section we propose a straightforward and comprehensive upper bound to the rate of the multi-step asymmetric successive compute-and-forward strategy (SCAF). We consider the system model as defined in Section II, which assumes unequal power constraints and asymmetric channel gains. We also consider real-valued scaling factors ß for decoding the non-integer linear combination of lattice codewords.
 In N-step SCAF, at each step, the receiver can combine its estimation from the previous step with its channel observation to obtain a new effective channel that is better for decoding the next targeted linear combination. This continues for N steps. The general formulation N-step SCAF is expressed in the following theorem.
 Theorem 1. Consider the Gaussian relay network defined in Sec. II, with unequal power constraints P1,··· ,PL and equation coefficient vectors ami ? ZL, where ami = [ami,1,··· ,ami,L]T and i ? [1;N]. Let ß1,··· ,ßL be L nonzero real numbers. By N-step SCAF strategy, if the computation rate tuple (R1,··· ,RL) is achievable, then
 R` =	 
 	,i	mi,`6	N
 for all ` ? [1;L], where  is expressed as,
 	 ,	(3)
 for all i, with
 	gpmi   ˜amj,	(4)
 h , (5) ˜a  (6)
 ˜a 
 Proof: The proof can be done by combining the proofs used in [4], [6], and [8].	 
 Remark 1. For N = 1, and P` = P, for all `, (2)-(3) reduces to the results presented in [7]-[8].
 Remark 2. (Equal power constraints and asymmetric channel coefficients) For N = 2, P` = P, and ß` = 1, for all `, (2)-(3) reduces to the results presented in [1] and [4].
 Remark 3. (Unequal power constraints and symmetric channel coefficients) For M = 1, N = 1, ß` = 1 for all `, and h = a = I1×L, (2)-(3) reduces to the results presented in [3].
 As it is seen from the above remarks, the proposed formulation in (2)-(3) is capable of manipulating both unequal power constraints and asymmetric channel coefficients conditions even if we omit the scaling factors from the equations.
 IV. MULTI-PAIR GAUSSIAN TWO-WAY RELAY CHANNEL
 In this section we present the system model for a class of multi-pair Gaussian two-way relay channel (MPTRN) consisting of N TRC pairs. We express the cut-set upper bound, achievable rate regions based on the proposed asymmetric SCAF strategy and calculate the capacity gaps. We also show that allocating the users powers according to the channel gains results the larger achievable rate region.
 A. System Model
 A MPTRN with N pairs is shown in Fig. 1, where nodes (2i - 1) and (2i) denote the ith pair of users which can communicate to each other via node r (a relay). There is no direct link between users. The relay is assumed to be able to listen and transmit at the same time. Uplink and downlink channels related to the ith pair are denoted as   and
 (hir1,hir2), respectively.
 Let nij denote the jth node of the ith pair, where j ? {1,2}.
 For each S ? [1;N], let TS = {Tk}k?[1;K] denote the set of all the sets of nij, where i ? S, and j ? {1,2} such that if nij,nil ? Tk then j = l, i.e., Tk doesn’t include two nodes of a pair. For example for S = {1,3},   denote
 the set of (i,j) such that nij ? Tk, for k ? [1;K], where K denotes the number of the members of TS. For j ? {1,2}, let ?j ? {1,2} denote the other node in a pair which communicates with j, i.e., if j = 1 then ?j = 2. Let mij?j ? [1;M] denote the message that node j of pair i communicates to
  
 Fig. 1: The multi-pair Gaussian two-way relay channel. node ?j of pair i. This message is encoded into a codeword xij using an encoding function xijq = fjti (mij?j,yji,q-1), for q ? [1;Q], where, Q denotes the number of channel uses, xiqt is a realization of a real random variable Xjqi , satisfying the power constraint  , and yji,q-1 denotes the previously received symbols at node j ? {1,2} of pair i ? [1;N]. This mode, called non-restricted encoding, can introduce some dependency between the signals transmitted by different users. In contrast, restricted encoding uses the encoding function xij = fji(mij?j). The messages are assumed to be independent. The rate of transmission is defined as R = 1/Llog2 M.
 The received signals at node nij, for all i,j, and at the relay node, r, at time instant q, are given by
  ,	(8a)
 	N	2
 yrq = XXhijrxijq + zrq,	(8b)
 i=1 j=1
 respectively, where xrq denotes the transmitted signal by the relay at instant q. zjqi and zrq are independent and realizations of i.i.d. real-valued Gaussian noise N(0,1). hirj and hijr denote the downlink and uplink channel gains, respectively, for all i,j. Node nij uses a decoding function gji to decode  as , where y .
 B. Cut-Set Upper Bound
 In this section the cut-set upper bound for restricted and non-restricted MPTRN are given. These are extensions of the cut-set upper bound presented in [10] and [11] for two-pair two-way Gaussian relay networks.
 Theorem 2. The capacity region of the non-restricted Gaussian MPTRN is upper bounded by,
  ,
  
 (9b)
 where k ? [1;K].
 Proof: Consider the cut S ? [1;N], the first and second terms in the right-hand side of (9b) denote the upper bounds for the multiple access and the broadcast cuts of the Gaussian relay network, respectively.	 
 Corollary 1. For the capacity region of the restricted Gaussian MPTRN, we have the outer bound (9a) but (9b) is changed as follows,
  
 where k ? [1;K]. The proof can be realized considering the fact that the inputs are mutually independent in the restricted network.
 Lemma 1. The non-restricted cut-set upper bound is within 1/2 bit/ sec/Hz of the restricted cut-set upper bound.
 Proof: The proof can be done by the same way as done for two-pair two-way relay network in [10].  
 C. The Achievable Rate Region
 In this section we present an achievable rate region for the described MPTRN. In our scheme, we use a restricted encoding function, so it will be an achievable rate region for both restricted and non-restricted network models. The idea is based on the fact that in the uplink communication, MPTRN can be considered as an AWGN network with 2N transmitters and one receiver, and so we can apply the proposed asymmetric successive compute-and forward method to decode the linear combinations of the codewords of each pair successively.
 In the downlink, MPTRN is considered as a broadcast channel with one transmitter and N receivers and decoding at the users is done by successive interference cancellation. Note that we consider N receivers instead of 2N receivers, because of the fact that each pair is a two-way relay channel and the downlink channels of each pair is not considered as a broadcast channel since each receiver knows about its own message and can subtract it from the its observed signal.
 1)	Encoding at the Users:   2N nested lattices are constructed such that ?1 ? ?2 ? ···?2N ? ?. For each ` ? [1 : 2N], ?` has second moment P` = aji?jP, for some distinct i ? [1 : N] and j ? {1,2}, where P1 = P2 = ··· = P2N. All lattices are assumed to be simultaneously good. For each message of transmitters, the codebook C` = V` T? is constructed, where V` denotes the Voronoi region of ?`, and ` ? [1 : 2N]. The transmitter i sends
  
 where t` ? C` and d` is a random vector uniformly distributed in V`. So,   and xij?j belongs to an ensemble
 of dithered lattice codes with sizes  , with xij?j independent of t` and uniformly distributed in V`.
 2)	Decoding at the Relay: Decoding at the Relay is done according to the successive compute-and-forward method described in Theorem 1. Here we have one relay, so M = 1. Without loss of generality, we assume,
  ,	(12) where , for all i.
 At the ith stage of successive compute-and-forward, the relay decodes the ith linear combinations of lattice code ensembles
  , with the coefficient vector,
  ,
 in which only the (2i-1)th and (2i)th elements are non-zero. So, we have
  
 Theorem 3. The coefficient vectors ai can be successfully decoded at the ith stage successive compute-and-forward, so long as,
 !
 (14)
 for i ? [1;N].
 Proof: The combination of the uplink channel gains and node transmitted powers yields the following vector of incoming signal amplitudes at the relay: (hp)T is defined as follows
  .
 More similarity between vectors ˜api and (hp)T results in a larger achievable rate region (see Theorem 1). So we select   and  . Also in order to attain simpler equations and also a larger achievable rate, we consider power allocation of the users as follows,
 	 ,	(15)
 where atri = 1, e.g., if  , we can set  and
  .
 Substituting the above quantities in (2) and (3) results in (14).
  
 3)	Encoding at the relay: The relay maps N lattice codes ti to Gaussian codewords   from codebooks with sizes {2nRrti } for i ? [1;N], respectively, where  
  . The signal transmitted by the relay is constructed as
 N
 q
 where		xr = X arti xirt,	(16)
 i=1
 N
 	0 = Xarti = 1,	(17)
 i=1
 i	nRi
 and xrt is a Gaussian codeword with size d2	rte, for all i.
 4)	Decoding at Users: Decoding at users is done by using the successive interference cancellation method. In this way, the user with the largest channel gain first decodes the messages related to the users with worse channel gain conditions. Then, it decodes the message related to itself, i.e., the message of the other user which belongs to its pair. Thus, at each user with a specific channel gain, only the users of other TRC pairs with higher channel gains can cause interference. For each nij, let Sji ? [1;N] \ {i} denote the pairs such that for each k ? Sji, |hkrj0| = |hirj| for some j0 ? {1,2}. By the described strategy, it can be seen that the following message rates are obtained:
 (18)
 For example, let N = 3, and assume that |h1r1| = |h2r2| = |h3r1| = |h1r2| = |h2r1| = |h3r2|, so we have S11 = f, S21 = {2,3}, S12 = {1,3}, S22 = {1}, S13 = {1,2}, S23 = {1,2}, and decoding at the users is done as follows:
 Decoding at the users of the first pair, i.e., users 1, 2, with channel gains |h1r1| and |h1r2| can be done with an arbitrary small probability of error if the message rates associated to
 the part part1 x1rt of xr satisfy the following conditions:
 (19a)
 	.	(19b)
 User 1 of the first pair has the highest channel gain, thus it first decodes the other messages and then cancels them from its received signal. Then it decodes the message of user 2 of the first pair. For user 2 of the first pair, , so the
 parts part2 x2rt and part3 x3rt of xr make interference in the decoding of part1 x1rt.
 By the same reasoning, the following message rates are obtained for decoding at the other users,
 	,	(20a)
 (20b)
 (20c)
 (20d)
 D. Capacity Gap Calculation
 In this section we state the capacity gap for the uplink and downlink channels of the restricted MPTRN in the following Lemma:
 Lemma 2. By using the described strategy, for any rate tuple r = {rji?j}i?[1;N],j?{1,2} satisfying
 (21a)
 	 	!
 Xri =C X|hijr|2P -|S|-1+log2 |S|, (21b) j?
 ,
 (21d)
 where k ? [1;K], there exists a choice of power assignments such that decoding of codewords at the users in the downlink with rates r can be done with arbitrary small error probability for all channel gain coefficients.
 Proof. We must show that if (14), (18) and (21a)-(21d) are satisfied, all power scaling factors between zero and 1 are obtained. Next we prove the theorem for uplink (21a) and (21b), the proof for the downlink can be done similarly. We need the following inequality: if x = 0 then
 	-2x = -1	(22)
 From (14) we have
 	 .	(23)
 By induction, for i = N -n, where n ? [1;N -1], we have,
  (24)
 For , we have .
 
 From the above Lemma, it is seen that the maximum gap between inner and outer bounds is  , and from Lemma 1 for the non-restricted Gaussian MPTRN, this gap is  bits/sec/Hz per user.
 Remarks:
 1) For N = 1, i.e., the two way relay channel, the gap for the non-restricted channel model is equal to  which is compatible with the result of TRC in the literature. 2) For N = 2, i.e. the two pair two way relay networks, the
 gaps are equal to  and 
 for restricted and non-restricted channel models, respectively which are the same results as obtained [11].
 3) As N ? 8, the gaps for restricted and non-restricted channel models, i.e., GR and GNR asymptotically goes to 1/2.
 V. CONCLUSION
 In this work, we presented a comprehensive asymmetric multi-step successive compute-and-forward scheme which includes the previously proposed schemes. Based on the proposed approach, we obtained the capacity gaps for a class of multi-pair two-way Gaussian relay networks (MPTRN), where one relay can carries communications between N asymmetric TRCs. It was shown that for all channel gain conditions, the proposed compute-and-forward scheme achieves rates to within constant gaps (N - 1 + log2 N)/2N and (N + log2 N)/2N bits/sec/Hz per user of the cut-set upper bound for the restricted and the non-restricted network models, respectively. These gaps tend asymptotically to 1/2 bits/sec/Hz per user as N goes to infinity.
 
 ﻿We present an empirically based model for the gain of an indoor antenna array. This corresponds to the reparametrization of the Greenstein-Erceg model, as applied to 5 m–50 m narrowband outdoor-indoor links. Our model is applicable to a few adjacent tones in an OFDM femtocell system such as LTE. We find that as much as 14 out of 16 dB of antenna-gain are attainable even in rich indoor scattering conditions. At the same time, no larger fade margin is required when using the array vs. the omnidirectional antenna. To provide a reference for the observed results we consider a simple propagation model, which is analyzed theoretically and via simulation. This model is found to match our empirical results very well. 
 Index Terms—Channel models, directive antennas, gain reduction factor, outdoor-indoor wireless links. 
 IRECTIVE antennas have been used to improve received signal power since the beginning of radiocommunications. However the specification of an antenna’s effectiveness in aspects such as improvement of signal strength and reduction of interference is typically specified for propagation settings devoid of obstructing or reflecting elements, i.e., under “free-space” conditions. The more recent appearance of cellular wireless service, results in propagation channels where the presence of obstacles is the rule rather than the exception. 
 Relatively little has been published on the interaction between multipath propagation and the antenna gain characteristics. The decrement in achievable antenna gain due to local scattering elements was originally defined in [1] as the “Gain Reduction Factor” (GRF). This factor was characterized as a location dependent random variable and its statistical parameters were computed for a specific environment, namely suburban outdoor-to-outdoor links. The very fast growth in wireless traffic in urban environments has forced base-coverage ranges to those of femtocells [2], [3], for which the aforementioned results may no longer hold. 
 While short-range outdoor-to-indoor links for static or nomadic users have been discussed [4]–[8], the effect of directional antennas on those links has received very little attention. In fact, most reported studies have been based on the use of omnidirectional antennas [4]–[6]. Moreover as stated in [9], path-loss prediction errors are much greater when using directional instead of omnidirectional antennas. 
 Recent developments in antenna design including the use of metamaterials have made it practical to consider the application of steered arrays in relatively small devices, such as those used in wireless local area networks (WLANs) [10]–[13]. Such arrays will allow taking advantage of propagation conditions where the dominant arriving rays lie in an angular range that is much narrower than the array mainlobe [1]. To the best of our knowledge, there is no general model available to describe angular spread when indoor wireless service is provided from outdoor bases. The actual gains achievable with directional antennas in this setting therefore requires an empirical study similar to what was reported in [1] for suburban links. This is the main focus of the work we describe here. We provide the first statistical description of the GRF for this type of environment. 
 The “Mean Effective Gain” (MEG), was proposed in [14] as a useful single parameter to describe the impact of the antenna characteristics on the link budget for multipath environments, primarily for mobile links. This is defined as the ratio between the (spatial) average power received by the antenna and the sum of the average powers that would have been received in the same environment by two isotropic antennas, vertically and horizontally polarized. A theoretical method is described in [14] to analyze the MEG of a mobile antenna. A general expression based on a statistical model of incident waves is proposed and empirical data was obtained at 900 MHz to validate the model. A statistical model for incident waves is also discussed [15], where model-derived and measured values of MEG are compared for LOS street-links at 2.6 GHz. An analysis of some fundamental properties of the MEG and the corresponding physical interpretations is presented in [16] for theoretical Rice/Rayleigh channel models. In contrast to the GRF, described as a random variable, the MEG is an average value. 
 1536-1276 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 
 An approach based on modeling deviations from the freespace gain pattern of an antenna as a result of multipath propagation is discussed in [17]–[19]. This is defined as the Effective Directivity Antenna Model (EDAM), which provides a bin-fitted stochastic model to correct the nominal antenna pattern. The empirically based model described in the above references was obtained for specific directive antennas in shortrange outdoor-outdoor and indoor-indoor links, using 802.11 transmitters and receivers. Simulations are carried out to show that the model provides more accurate path-loss prediction than the standards models when directive antennas are used. The results however do not focus on the reduction of available gain for beam-steered indoor antennas when users are served from outdoor bases, as we discuss here. 
 The GRF originally defined in [1], was subsequently discussed in [19]–[21] but the corresponding statistical model is based on wideband empirical data collected only in suburban links [21], [22]. The approach is based on estimating the angleof-arrival-distribution of power density which differs from our methodology, as will be discussed in the next section. 
 Short-range outdoor-indoor wireless channels are multipathrich links for which there is still a lack of empirical results [4], [6]. Several papers such as [4]–[8] and the references quoted therein have addressed the subject of path-loss characterization for settings that in some cases are similar to ours. However, links with directional antennas are only considered in [7] and in [8]. A comparison of maximum received power using directive and dipole antennas is presented in [7], but this work does not include a statistical characterization of the gain-difference between both antenna options. In [8], simulations and measurements of outdoor-indoor links that include directive antennas are presented, but this work does not include a comparison of received power for different antenna types. 
 The main objective of this paper is to empirically evaluate the effectiveness of using steerable directional antennas at the indoor user terminal in typical short-range, fixed-wireless, outdoor-indoor service. We propose a statistical model for the GRF of narrowband links, applicable for instance to a group of tones within the coherence bandwidth in OFDM transmission. These results are important since antenna gain may compensate for the propagation loss disadvantage of indoor-outdoor (as compared to indoor-indoor) links, opening up a much wider application space for cellular service providers. The measurements were carried out at 3.5 GHz using a narrowband transmitter and a carefully calibrated power-measuring receiver. The frequency used is the same as in the WiMAX fixed wireless service [25]. A total of 179 different links were tested in the urban settings described in Section III. The links included changes in the relative position of outdoor and indoor terminals and variation in path lengths, ranging from 5 to 50 m. 
 The remainder of this paper is organized as follows: Section II presents the theoretical background. Section III describes the measurement hardware and methodology. Section IV presents statistical models based on our empirical data. Finally, Section V provides the conclusions. 
 For convenience we assume the outdoor base to be the transmitter and the indoor user the receiver. The receiver is connected to a directional antenna, capable of searching in azimuth for maximum power. The received power at the antenna terminals in a wireless link with properly aimed antennas can be expressed as [26] 
 where Pt is the transmit power, Gt and Gr are the transmit and receive boresight antenna-gains in free space, Lf accounts for feeder cable losses between the antennas, the transmitter and the receiver and PL is the path-loss associated with the propagation environment. Expression (1) holds if the angular spread of the arriving wavefronts is much smaller than the antenna mainlobe [1], as may be the case for a base antenna placed above clutter. For an indoor antenna surrounded by clutter this directivity gain will in general no longer hold. While the antenna can be aimed in the direction of the strongest arrival, its radiation pattern will attenuate wavefronts arriving from other directions, which may be captured by a wider-beam antenna, in particular an omnidirectional element. Further, as in [1], we define the GRF as the decrease in gain advantage of a directive over an omnidirectional antenna, due to the presence of clutter. Let G0 dir and G0 omni be the nominal (free-space) antenna gains of a directive and an omnidirectional antenna respectively. We denote as Gc the actual in-clutter gain advantage of the directive antenna when aimed for maximum received power. Then, 
 To estimate GRF for a given link, we measure the received power using an omnidirectional (in azimuth) antenna. Let this power be Pr omni. For exactly the same antenna positions and transmit power, we repeat the measurement with the directive antenna, rotating it in search of the maximum power Pr dir. The dB difference in received power Pr dir[dBm]-Pr omni[dBm] is then the in-clutter gain advantage of the directive antenna, Gc[dB], at that spatial position. 
 Since Pr dir[dBm] and Pr omni[dBm] are the position dependent random powers received by the antennas, the GRF is described by the difference between the spatial-fades of a directive and an omnidirectional antenna, both placed at the same position, adjusted by the difference in free-space gains. 
 To establish a basis that can serve as a reference for comparison with our empirical results, we use a very simple model for our propagation environment, which nevertheless proved to be quite accurate, as we will see in Section IV. This model, described in [27] assumes that at each indoor measurement location a very large number of equal-power multipath wavefronts arrives at the receive antenna. These wavefronts are distributed uniformly over the complete azimuth angular range with phases distributed uniformly from 0 to 2p. In addition to these scattered paths, a single dominant wavefront arrives from one direction, as illustrated in Fig. 1. 
 The average power ratio of the dominant wave over the total of the scattered components is the Ricean K-factor. We now consider a directional antenna rotated in azimuth. For simplicity we assume that the directive and the omnidirectional antennas have the same vertical gain and that the horizontal gain pattern for the directive antenna is constant within the mainlobe and zero elsewhere. If this antenna is pointed in a direction that does not include the dominant path, then the received voltage will be Rayleigh-distributed, provided that one can still assume that a large number of multipath components fall within the beamwidth. On the other hand, when the directive antenna’s mainlobe receives the dominant wavefront, then the received voltage will be Rice-distributed. However the K-factor for this 
 distribution will be GG00omnidir times that characterizing the voltage received by the omnidirectional antenna. This is because the dominant wave is subject to the increased horizontal antenna gain of the directive antenna, while the scattered power is (on average) unchanged, as the gain increase is exactly offset by the reduction in number of the received diffuse components. 
 With these considerations we can statistically describe the ratio of the power received using both types of antennas. For the omnidirectional antenna placed at random in the room the output voltage envelope vomni is a Rice-distributed random variable characterized by the local K-factor. We now consider that at the same position the directive antenna is rotated over N nonoverlapping positions in search of the maximum power, where N = 360/antenna beamwidth, with the antenna beamwidth expressed in degrees. Then the directive’s antenna output voltage vdir can be written as vdir = maxi=1···N[vi], where the collection of random variables {vi}i=1···N consists of N - 1 Rayleighdistributed random variables and one Rice-distributed random variable with a K-factor equal to K · GG00omnidir . It is reasonable to assume that these random variables are independent, since they arrive from non-overlapping angular sectors. From this we can obtain the statistical characterization of the ratio of powers between the directive and the omnidirectional antennas. 
 As an example we will consider the limiting situation where K = 0. We note that in this case the directive antenna will only be able to choose among N Rayleigh-distributed random variables and will thus act as a N-branch selection combiner. This allows us to use well-known results. We characterize the statistics of the ratio PPr omnir dir , as it follows from (3) that this will determine the statistics of the GRF. Using the results derived for an N-branch selection combiner [30], we see that the Probability Density Function (PDF) of the output power Pr dir, 
 where we have assumed that the average power for each direction is unity. This can be done without loss of generality since we will only consider power ratios. For the omnidirectional antenna the PDF of output power will then have an exponential distribution with unit average. We will also assume that this random variable is independent of the preceding ones. 
 Given the statistics of Pr omni and Pr dir we now obtain the ratio distribution for ? = PPr omnir dir . Using the MATHEMATICAsoftware (version 9), we find that the PDF of ? for the Rayleigh case can be calculated as 
 The above integral can in this case be obtained in closed form. Using again MATHEMATICA we obtained the PDF?(u) and from it the corresponding Cumulative Distribution Function (CDF) 
 In the above example the directive antenna can only offer a diversity gain. The case where K is nonzero can be treated using the same procedure, using Ricean instead of Rayleigh distributions in (4) and (5). However the resulting integral corresponding to (6) does not yield a closed form expression and is thus best evaluated numerically. We note that when using the Ricean distributions these must not be normalized, since the model must consider that the average received powers are not equal as the K-factors vary with angular position. We will in general use ?K when referring to the (random) power ratio, given that the Ricean K-factor is equal to K. As the GRF is 
 equal to PPr omnir dir multiplied by the ratio of nominal antenna gains, the statistics of the GRF are readily obtained from those of 
 We plot in Fig. 2 the CDF of the GRF for the cases K = 0,1,3,100 under the assumption N = 24. We have here again assumed that both type of antennas have the same gain in the 
 vertical plane so that GG00omnidir = 24. The curves were calculated using (7) for the Rayleigh case, and numerical integration when K is nonzero. In addition we simulated the previously described model generating the corresponding random powers. The results were identical. 
 We see that for small values of K, such as observed in our measurements, the model predicts that the GRF will be negative with non-zero probability i.e., the array gain advantage can appear to be higher than in free space. This corresponds to the condition where the omnidirectional antenna is subject to a large fade while the directive array placed at the same position can combat such a fade using angular selection diversity. Under the conditions we simulated, even for K-factors as low as 1, which implies a significant fade probability for omnidirectional 
 Fig. 2. CDF of the simulated GRF for the cases K = 0, 1, 3, 100 under the assumption N = 24. 
 reception, the directive antenna would almost in every case select the dominant wavefront direction and as a result only exhibit very shallow fades. Thus, we have a combination of classic (deterministic) beamforming gain as well as statistical reduction of fade margin, which can result in an overall gain higher than in free space. 
 Throughout this work we consider that the beam-scanning of the directive antenna is only performed in azimuth and that the elevation beamwidth of the antennas is larger than the corresponding angular spread and hence the vertical gain is not reduced. 
 Our approach to calculating the GRF is different from the one originally presented in [1] (see (1) in [1]), which is based on a transmission bandwidth much larger than the typical coherence bandwidth of the links considered. The use of the (real valued) angle-of-arrival distribution of power and the antenna power gains used in [1] is justified under such conditions due to the averaging effect over frequency. Instead, for short range links the coherence bandwidth is much larger, in the range of 10 MHz [28]. We thus here consider the case of a narrowband transmission, for example a range of OFDM carriers within the channel coherence bandwidth. Although the definition of the GRF is not dependent on the relation between the transmission and channel coherence bandwidths, in the narrowband case the approach used in [1] would not be practical. In fact, calculating the received power from the angle-of-arrival distribution of the electric field would require phase information, and the complex antenna voltage-gain. Instead we found it much simpler to directly measure received power with various antennas including an omnidirectional dipole used as reference and to apply (3). 
 The measurement system consisted of a 3.5 GHz continuous wave (CW) transmitter and a purpose built narrowband receiver coupled to a power meter. The outdoor base station (BS) transmitted 19 dBm into the antenna terminal of a 
 2.4 dBi gain dipole. The receiver bandwidth is 200 kHz. This is wide enough to capture any frequency dispersion that affects the CW transmission, induced for example by vegetation movement [29]. 
 The automated measurement procedure is essentially the same already described in [4], involving a laptop computer to control antenna position and the synchronized acquisition of power samples. For each antenna position 40 consecutive samples were obtained. These samples were then averaged to remove residual temporal fades, which were typically less than 0.5 dB. Before each measurement we performed calibration procedures to assure that the transmitter and the receiver were operating with their nominal powers and gains. In all field measurements the received power was at least 20 dB above the noise floor. 
 Measurements were carried out at 4 urban type settings in Santiago and Valparaiso, Chile. We selected placements where the base terminal could be positioned at a range of 5 m to 50 m from the indoor antenna. This space was in some cases occupied by gardens with shrubs and trees no higher than 5 m. Three locations were chosen in Santiago, one of them on the campus of Universidad Diego Portales and two in private houses. The fourth location was the campus of Universidad Santa Maria in Valparaiso, which offers a wide range of indoor options that includes offices and relatively large laboratories. A schematic of one of them is shown in Fig. 3 where we also specify the outdoor base and indoor subscriber unit (SU) locations. Fig. 4 shows the setup of the indoor measurement system for the analysis of both small-scale and GRF statistics. 
 Fig. 4. Measurement configuration for the LOS case showing the on-axis rotating system and the 0.4 m rotating arm. 
 At these scenarios we measured a total of 179 links that corresponded to two types of conditions, line-of-sight (LOS) through a window or non-line-of-sight (NLOS) when the direct path was blocked. This blockage included foliage of shrubs, small trees or a wall. The thickness of the foliated obstruction was typically in the range of 0.5 m to 1 m. The construction was of the brick and mortar type, with non-metalized windows. Window sizes varied in width in the range of 1.2 m to 3.2 m and in height from 1.8 m to 2.5 m. The minimum window area was 2 m2. All measurements were made by placing the outdoor antenna that simulates the wireless BS at 2.1 m height. 
 We firstly performed measurements aimed at fully characterizing our test locations. This involved connecting the receiver to the dipole antenna placed on a 0.4 m length rotating-arm moved stepwise in 6? azimuth increments, which implies a displacement of approximately   between successive antenna positions. The 60 power measurements obtained in a rotation at each placement were used to calculate the spatially averaged path-loss at that range and to generate the statistics of smallscale fades with respect to that average. The size of the region for the spatial average is well within the shadow-fade correlation distance reported in previous work [26], [31], [32]. In fact the smallest values mentioned in the literature, corresponding to indoor scenarios, are in the range of 1 to 2m [31]. This was repeated over a variety of positions in our test scenarios. The results allowed contrasting our statistical data with that of typical path-loss models. We note that these measurements were only aimed at characterizing the propagation environment and that spatial averaging was not used in obtaining the GRF, as will be described below. 
 To directly measure the values of the GRF as discussed in the previous section, we used a platform capable of rotating the antennas about their axes in 6? steps with no displacement other than rotation. Power samples were acquired as before at each angular position. Four types of antennas were used indoor, including an omnidirectional 2.4 dBi gain dipole used as reference and three different directional antennas. The first was a 10.2 dBi gain patch antenna with elevation and azimuth half power beamwidths of 50? and 60? respectively. The second was a 15.2 dBi gain horizontal linear array of four vertically polar- 
 ized patches with elevation and azimuth half power beamwidths of 50? and 15? respectively. The third was a 16.5 dBi gain 2 × 2 array of four vertically polarized patches with elevation and azimuth half power beamwidths of 25?. The corresponding normalized horizontal gain patterns for the directive antennas, measured in an anechoic chamber, are illustrated in Fig. 5. At each measurement location, the nominally omnidirectional antenna was rotated exactly as the directive ones to average out minor gain variations (less than +/-1 dB). Care was taken to place the 4 types of antennas in exactly the same position at the beginning of each rotation and this was repeated for a wide range of placements as will be described later. We recorded power samples for the full rotation, which in the case of the directional antennas allowed us to obtain the maximum signal strength as well as the direction of arrival of the diverse resolvable wavefronts. The computer-controlled automated measurement procedure is accurate to fractions of one degree and allowed us to verify that results were repeatable, i.e., that successive rotations at any given location would yield the same result. 
 This section shows the results of the measurements performed and the statistical data derived from them. 
 We start by characterizing our measurement environment from a propagation point of view. To this effect we processed the data collected with the rotating arm in our scenarios to model average path-loss and small-scale fade statistics. The extensively used log-normal model [33] assumes that the spatially averaged path-loss (in dB) may be described as: 
 where d denotes distance between the BS and SU, PLdB(d0) is the free-space path-loss at a distance d0 in dB units, n is the 
 SUMMARY OF PATH-LOSS EXPONENT n AND STANDARD DEVIATION s FOR THE CASES LOS AND NLOS 
 path-loss exponent and Xs is a zero-mean Gaussian distributed random variable (in dB) with standard deviation s. 
 We applied this model to our data after averaging out smallscale fades over one rotation of the arm that holds the antenna. As conventionally done, we chose d0 = 1 m. Variations of the model in (8), using multiple slopes or choosing the intercept point PLdB(d0) different from the free-space path-loss did not provide a measurably better fit. In addition, we computed the CDF of the small-scale fades with respect to the local averages. As expected, we found that the fit to Ricean/Rayleigh distributions was very good. We show in Table I the bestfit model parameters. The observed K-factors were below 3.0 in 95% of the cases for both LOS and NLOS links attesting to a very rich multipath environment. It follows that our test environment is not characterized by large shadow-fade related power variations, particularly for the LOS case, but that it is rich in multipath-propagation, as expected for an indoor setting. 
 Consistent with the above results, we found the indoor propagation environment to be rich in multipath over a broad angular range. The dominant energy arrives in general from the direction of the nearest window. Moreover, when pointing the antenna in the general direction of the dominant path, angular displacements in the range of a beamwidth typically resulted in a received power pattern that closely resembles that of the specific antenna used. 
 This may be seen in Fig. 6, where we plotted received power vs. angle for the case of the 4 × 1 patch array in NLOS scenarios. To avoid a very cluttered figure we only show 10 realizations chosen at random among the total of 96 measurements. These were normalized to their maximum power and the angle at which this power was observed was set to zero. We also plot the antenna gain-pattern and the average values per angle obtained from all realizations. It can be seen that the dominant path is not resolvable into separate wavefronts by the antennas we used and that the angular spread of these dominant wavefronts is narrower than the beamwidths of the directive antennas. Other wavefronts were typically observed to arrive as wall reflections from angles larger than the beamwidths used, as seen in the above figure. This can be compared with simulations of the propagation model described in Section II, assuming the rotation of a directional antenna with the same gain-pattern as that used in our measurements. Since in this case our results are exclusively based on simulation, we chose to also include the randomness of the K-factor, which for our NLOS links was approximately uniformly distributed in the range 0 to 1.5. Thus in Fig. 7 we show 10 realizations for K ranging from 0.1 to 1.5 and the average of 100 simulations. As seen, the model results match the empirically observed behavior quite well, although in contrast with the simulations, the average power for the 
 measurements is not constant at angular positions outside the dominant direction. It may be concluded that in our scenarios, the angular distribution of the multipath components favors the direction of the dominant path. 
 To further describe the angular characteristics of the arriving wavefronts, we compared the angle of arrival of the strongest signal, with the angle of the straight line connecting BS with SU. The CDF of this difference is shown in Fig. 8 for both the LOS and NLOS cases. As seen, for LOS links the dominant wavefront arrives on a close-to-direct path from the base. We found that for 90% of LOS cases the angular deviation was no larger than 7.5?, i.e., about half a beamwidth. Instead, for NLOS links the angular range for 90% of cases was 46.2?. The latter value is explained by the angular difference of between the direct path and that of the nearest window in our scenarios, which for 90% of placements was less than 40?. Alternatively, we found that when considering 90% of cases, aiming along a direct path resulted in a power reduction reaching up to of 3.5 dB for LOS and 12 dB for NLOS links. 
 Fig. 8. CDF of the difference between the angle of arrival of the strongest signal and the angle of the straight path between BS and SU. 
 Our results show that, as previously observed in suburban settings, the GRF may be quite accurately modeled as a Gaussian random variable. Fig. 9 shows the cumulative distribution functions for the case of the 2 × 2 array in LOS and NLOS settings. In the same figure we also show the results obtained when using the model described in Section II, considering K-factors of 1.0 and 2.5 for NLOS and LOS respectively. 
 Here, the model-based results need to take into account that the vertical gains of the omni and the directive antennas are not equal. As a result the increase in the K-factor that characterizes the directive antenna’s voltage only corresponds to the horizontal gain advantage over the omnidirectional antenna (or equivalently its horizontal beamwidth reduction), a factor of 14 for this case. On the other hand the total gains of each antenna were considered when calculating the powers they receive. As seen, despite the simplicity of the model, it predicts measurement results accurately, the error being negligible at the 
 90% and 50% probability levels, while at 10% probability it is 2.6 dB for LOS and to 3.4 dB for NLOS. We have omitted the corresponding graphs for the remaining antennas, as they are quite similar. In contrast with [1], the observed GRFs exhibit a considerable likelihood of negative values i.e., the array gain advantage would appear to be higher than in free space. As already discussed, this happens at placements where the omniantenna is subject to a large fade, which the angular selection diversity gain of the directive antenna can significantly reduce. In [1] negative GRF values are not possible since the omniantenna small-scale fades have been averaged out in frequency. 
 The CDF of the best-fit Gaussian random variables describing the GRFs for all cases is characterized by their mean µGRF and standard deviation sGRF in Table II. 
 Although our propagation conditions are different, we fitted the model proposed in [1] to these average GRF values. The expression used is 
 where ß is the half-power azimuth antenna beamwidth in degrees. The best-fit values for A and B are summarized in Table III. As observed in Fig. 10, the fit is quite good, although the actual average GRF values observed in our case are considerably lower than those measured in suburban settings. 
 Furthermore, we measured an increasing average GRF with diminishing beamwidth, but this is much less than reported for the outdoor case. This may be due to the scalar integration of powers for the broadband case vs. vector addition of wavefronts in our narrowband case. Therefore in the wideband case narrowing the beamwidth monotonically decreases the total received power, whereas in our case excluding secondary reflected signals can in some cases be of benefit. Another manifestation of this phenomenon is the fact that we measured a significant fraction of negative GRF values, which contribute to lowering the average. 
 We also found a relatively large standard deviation of the GRF with no statistically significant variation with beamwidth as compared to [1]. This is consistent with our observed angular spread, which is typically dominated by a strong component that is narrow compared with the directive-antenna beamwidths. Therefore when aiming these antennas towards 
 the strongest signal, there will be little difference among them resulting from the effect of secondary wavefronts. Equivalently, all directive antennas see the same strongly dominant planewavefront, from which they extract power in accordance with their respective effective areas. The variation of the GRF, which is similar for all directive antennas, may instead reflect the fact that in our narrowband measurement the omnidirectional antenna used as the common reference is subject to considerable Rice/Rayleigh spatial fading. 
 We also study the possible correlation between the GRF and the shadow-plus-small-scale fade at the location, as measured by the dipole. It cannot be assumed a-priori that the GRF is a random variable independent of these fades. We calculated the total fades of the dipole with respect to the average path-loss regression (8) as well as the GRF for the same measurement point. The correlation coefficient between these random variables is then easily obtained. We found that for all antennas used and both LOS and NLOS cases, the correlations ranged between -0.4 and -0.5. The negative values observed in all cases reflects the fact that deeply faded locations for the dipole antenna will be those where the directive antennas can offer the greatest benefit (lowest GRF), due to angular diversity gains. The negative correlation between the spatial fades and the GRF, suggests that a directive array is capable of partially offsetting fades that would affect a dipole at the same position. We discuss this further in what follows. 
 To illustrate the actual gains that were achieved with directive antennas we plot in Figs. 11 and 12 the received power for our measurements with the 2 × 2 array and the dipole, considering the LOS and NLOS separately. We recall that for the dipole these measurements do not include spatial averaging, while for the directive antennas they represent the peak-received power over a rotation at the same measurement position of the dipole. This is consistent with a scenario where the user places the omni antenna at a random position, without searching to optimize 
 Fig. 11. Received power measurements, LOS, transmitted power of 19 dBm. Straight and dashed lines represent the best-fit linear regression for a dipole and for a 2 × 2 array respectively. 
 Fig. 12. Received power measurements, NLOS, transmitted power of 19 dBm. Straight and dashed lines represent the best-fit linear regression for a dipole and for a 2 × 2 array respectively. 
 location and hence, small scale Rice/Rayleigh fade-margins must be included in the link budget. 
 In these figures we also include the linear regressions for both types of antennas, considering an intercept at 1 m based on free-space propagation with the respective antennas. The least squares fit parameters are described in Table IV. As seen, the slopes are consistent with those obtained for pathloss when averaging out small-scale fades. In the LOS case, which is dominated by small-scale fades, the (negative) slope is marginally larger, as the lack of spatial averaging emphasizes deep fades when measuring power in dBm. The CDFs of the dB difference of received power, with respect to the regression lines, were in both cases very close to Gaussian. The difference between the two regression lines in the distance range where our measurements were concentrated is as expected almost identical to the nominal gain difference of the antennas after subtracting the average GRF. 
 SUMMARY OF RECEIVED POWER EXPONENT n AND STANDARD DEVIATION s FOR THE CASES LOS AND NLOS 
 We observe that the standard deviation of received power is about the same for the directive antennas and the dipole. While the antenna array may benefit from fade reduction due to diversity effects, this is more or less cancelled by the variability of the GRF. As a result, similar link fade-margins can be used for both antenna types. 
 In summary, we found that indoor users able to search for the strongest signal with a directive antenna instead of an omnidirectional one, can achieve very significant receivepower increases. For example a nominally 16.5 dB gain antenna will in a NLOS indoor setting be able to provide an average 14.4 dB power gain. At the same time, the choice of a directive over an omni-antenna over does not increase the required fademargin. As already noted, part of the gain is attributable to classic beamforming and part to the angular diversity advantage of the directive antenna. As a reference we note that for a purely Rayleigh-fading environment, the average diversity gain of the 2 × 2 antenna, would have been only 5.1 dB, corresponding to a 14 branch selection combiner. 
 The results of an extensive measurement campaign, covering a variety of short-range outdoor-indoor links, show that significant power gains are available to indoor users using a steerable directive antenna. For the narrowband case that we considered, the antenna gains that should be used for link budget calculations are within 2.3 dB of the nominal, free-space gains (for example 14.4 instead of 16.5 dB for the 2 × 2 array). Moreover, while the directive antennas exhibit considerable randomness in the GRF, they also benefit from angular diversity and the combination of both effects resulted in fade statistics that are very similar to those of the omnidirectional antenna. Thus, the use of a directive antenna indoors implies an increase in average received power, without the need for increasing the fade margin. 
 The authors wish to acknowledge the help provided by Dr. Hector Carrasco in the design and calibration of the antennas used in this work. 
 ﻿In this work, we deal with zero-delay source coding of an unstable vector Gaussian Auto-Regressive (AR) source under average mean square error fidelity criterion. To begin with, we consider zero-delay coding of a vector Gaussian AR(1) source modeled in state space form. It turns out to be convenient to use an asymptotically stationary time-domain scheme with feedback, which was recently proposed in [1] in the context of filtering theory applications via the Information Nonanticipative Rate Distortion Function (INRDF). In this scheme, the feedback path comprises a Kalman filter, which produces an estimate of the source. Our idea is then to encode the vector innovations due to Kalman filtering via lattice quantization with subtractive dither and memoryless entropy coding. We show that the resulting system is stable and furthermore provide new bounds to the zerodelay RDF. We then generalize our results to vector Gaussian AR sources of any order. With this, we are able to establish a timedomain approach with feedback, which by use of a single filter gets very close to the zero-delay RDF for vector Gaussian AR sources of any order. Interestingly, for infinite dimensional vector sources, the INRDF coincides with the Optimal Performance Theoretically Attainable (OPTA) by zero-delay codes. 
 Zero-delay processing of information in source coding is the reproduction of each source sample at the same time instant that the source sample is encoded. Zero-delay source coding is desirable in various applications, like for instance, signal processing [2] and network control systems [3], [4]. The class of zero-delay codes is a subclass of the so-called causal source codes [5] where the reproduction of the current source sample depends only on the present and past source samples but not on the future source samples. A preference for zero-delay source coding over causal source coding stems from the fact that the latter does not exclude the possibility of long blocks of quantized samples, which allows for arbitrary end-to-end delays. Of course, every zero-delay code needs to be causal, but the opposite is not true. 
 It is well known that zero-delay codes (and causal codes) in constrast with non-causal codes cannot achieve the classical rate distortion function (RDF) [6], i.e., the optimal performance theoretically attainable (OPTA) by the class of noncausal codes. Indeed, an open problem in information theory is quantifying the gap between the OPTA by non-causal, denoted hereinafter by R(D), and the OPTA by causal and zerodelay codes, hereinafter denoted by rcop(D) and RZDop (D), respectively. Notable exceptions where this gap is explicitly found are memoryless sources [5], stationary sources in high rates [7], and zero mean stationary scalar Gaussian sources with average mean square error (MSE) distortion [8]. 
 Throughout the years, the interest in studying information theoretic rate distortion functions that performs as tight as possible to the OPTA by causal or zero-delay codes has been very high. For example, the authors in [9] introduced the so-called nonanticipatory -entropy, hereinafter denoted by  , to demonstrate its utility in real-time (zero-delay) applications. Perhaps the most striking result inspired by  , is the work of [10] where the author introduced the so-called sequential RDF, hereinafter denoted by RSRD(D), to investigate control related applications. A decade later, the authors in [1], introduced the so-called information nonanticipative RDF (INRDF), denoted by Rna(D), in the context of filtering theory applications . The work of [1] complemented with the work of [12] implies that under squared error distortion constraints, for any AR(1) source Xt, the optimal reproduction distribution of the output Yt, conditioned upon Xt , {X0,...,Xt} and Y t-1 , {Y0,...,Yt-1}, simplifies to P(Yt|Y t-1,Xt), i.e., P(Yt|Y t-1,Xt) = P(Yt|Y t-1,Xt) . Using the latter structural result, the authors in [12] used an unstable Gaussian AR(1) source and showed a lower bound to RZDop (D). This bound can be achieved by a feedback realization that was first proposed in [1], where a Kalman filter provides a prediction Xˆt|t-1 of Xt to the encoder, which then “encodes” via scalings that act as amplifiers the prediction error (innovations) Xt - Xˆt|t-1. Interestingly, it turns out that the Kalman filter in [12] is obtained from the noisy observations and thereby parallel a result of Zamir et al. [13], on achieving the noncausal RDF of a stable, stationary, and scalar Gaussian process by noisy prediction. Note that for stable stationary Gaussian sources, the use of Kalman filter is not mandatory. In this paper, we show the following: 
 For the case of zero mean unstable  vector fully observed Gaussian AR(1) sources with average MSE fidelity, the feedback realization scheme of [1] complemented by the work in [12], implies that an upper bound to the RZDop (D) can be obtained by quantizing the innovations due to using a Kalman filter in a feedback loop, which predicts the source Xt based on its noisy past Y t-1. We provide a simple quantization strategy and assess the rate loss as compared to the achievable lower bound (see Theorem 1). In addition, we show how to generalize our scheme to vector Gaussian AR sources of any order (see Theorem 2). 
 When the vector source is unstable, it is necessary to guarantee that the feedback realization is stable under quantization. We provide an upper bound to the RZDop (D) due to finite-dimensional quantization of the innovations. We show that the rate loss is finite and directly linked to the space-filling loss of the quantizer as well as the loss of the entropy coder due to zero-delay constraints. If the vector dimension of the Markov source tends to infinity, it is possible to completely eliminate the rate loss and thereby show that in this limit, the OPTA coincides with the INRDF (see Section IV-B). 
 This paper is structured as follows. In Section II we cast our problem. In Sections III and IV we derive upper and lower bounds to the OPTA by zero-delay codes and we discuss possible extensions and related results. In Section V we draw conclusions. 
 Notation: We let R = (-8,8), N0 = {0,1,...}. For t ? N0, we denote random variables and p-dimensional vectors with boldface letters, i.e., xt ? R and xt ? Rp, respectively. 
 The transpose of a matrix or vector Z is denoted by Z . We denote by {·}G any term that is Gaussian. For a square matrix Z ? Rn×n with entries ?ij on the ith row and jth column, we denote by diag{Z} the matrix having ?ii, i = 1,...,n, on its diagonal and zero elsewhere. We denote the time index with “t” and the dimension index with “i”. 
 In this paper we consider the zero-delay source coding setting illustrated in Fig. 1. In this setting, the p-dimensional (vector) Gaussian source is governed by the following discretetime linear time-invariant state-space model 
 N(0;Sx0) is the initial state, and the noise process wt ? Rq is an i.i.d. Gaussian N(0;Iq×q) sequence, independent of x0. 
 The system operates as follows. At every time step t = 1,2,..., the encoder observes the source xt and produces a single binary codeword zt from a predefined alphabet set Zt of at most a countable number of codewords. Since the source is random, zt and its length lt are random variables. Upon receiving zt, the decoder produces an estimate yt of the source sample. We assume that both the encoder and decoder process information without delay and they are allowed to have infinite memory of the past. 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable-length binary codes zt. The countable set of all codewords (codebook) Zt is time-varying 
 Fig. 1: A zero-delay source coding scenario using variablelength binary codewords. 
 to allow the binary representation zt to be an arbitrarily long sequence. There is no loss by restricting to uniquely decodable codes [3]. The encoding and decoding policies are described by sequences of probability density functions as {P(zt|zt-1,xt) : t = 0,1,...} and {P(yt|yt-1,zt) : t = 
 D, where D > 0 is the pre-specified distortion level, d(xn,yn) , . The objective is to minimize the expected average codeword length denoted by 
 decoding policies. These design requirements are formally cast by the following optimization problem: 
 that (A, BBT) is stabilizable (see, e.g., [14]) to ensure that the limit exists in (2) (i.e., RZDop (D) < 8). 
 In this section, we present a lower bound on the OPTA by zero-delay codes using the INRDF, Rna(D). In general, the expression of the OPTA by zero-delay codes given by (2) is very hard to find and often bounds are obtained (see, for example, [7], [8]). 
 First, recall that since the OPTA by zero-delay codes is a subset to the OPTA by causal codes (see Section I) then 
 Moreover, it is also known that for general sources the following bounds hold (see, e.g., [8, equation (11)]). 
 Note that, inequality (a) is strict, in general, and becomes equality when the source is i.i.d. or when the rate tends to infinity. In contrary, inequality (b) is strict at high rates (high resolution) due to space-filling loss and becomes equality at zero rate. Also, for unstable sources like the source model of (1), Rna(D) has to be greater or equal to the sum of the unstable eigenvalues of matrix A, i.e., whose magnitudes are greater than one (see e.g., [15]). 
 Since, the INRDF is a tighter lower bound on the OPTA by zero-delay codes compared to the classical RDF, we can use this information measure to obtain tighter bounds on (2). 
 In nonanticipative rate distortion theory (see e.g., [1], [8], [9]) the mutual information [16] can be written as 
 where E{·} is the expectation with respect to the joint probability distribution 
 with P(xn) , ?P(xt|xt-1), P(yn||xn) , ?P(yt|yt-1,xt), while P(yt|yt-1) is the marginal distributions induced by the joint probability distribution P(xn,yn). We now state the definition of INRDF as given in [11, 
 where the infimum in (6) is taken with respect to the sequence of conditional probability density functions {P(yt|yt-1,xt) : t = 0,1,...,n}. 
 If one replaces liminf by inf lim in (7), then an upper bound to Rna(D) is obtained, defined as follows. 
 It is shown in [11] that provided the limit in (8) exists, and the source is stationary then Rna(D) = R¯na(D). The optimization problem of (7), in contrast to the one given in (2) is convex (see [17]). In addition, Rna(D) < 8 due to the assumption of stabilizability on the pair (A,B) of (1). 
 {encoder, channel, decoder} via an asymptotically stationary feedback realization scheme illustrated in Fig. 2 to find the optimal causal and zero-delay information based filter. A detailed analysis on this scheme is provided in [1, Section VI]. Next, we briefly explain the methodology presented in Fig. 2. 
 {kt ? Rp : t ? N0} of {xt : t ? N0} based on the previous estimates {y0,...,yt-1} defined by 
 The covariance matrix ?t is diagonalized by introducing a unitary transformation E (invertible matrix) such that 
 Preprocessing at Decoder: Analogously, we introduce the estimated error process {k˜t : t ? N0} defined by k˜t , yt - xˆt|t-1 and the scaling process {?˜t : t ? N0} defined by ˜?t,Tßt, with ßt , (FEkt + vt), vt ~ N(0;V ), and 
 The fidelity criterion  at each t is not affected by the above processing of {(xt,yt) : t ? N0}, in the sense that the preprocessing at both the encoder and decoder do not affect the form of the distortion function, that is, 
 Using basic properties of conditional entropy (see, e.g., [1, Eq. (IV.35)]), it can be shown that 
 Fig. 2: Realization of the optimal minimizing distribution P*(yt|yt-1,xt) (see Section III-A). 
 In this section, we derive upper bounds to the OPTA by zero-delay codes using a universal quantization scheme based on a subtractive dither with uniform scalar quantization (SDUSQ) [18] on the feedback realization scheme illustrated in Fig. 2. Toward this end, we consider unstable vector Gaussian AR(1) source modeled as in (1), and we quantize each time step t over p independently operating SDUSQ, with their outputs being jointly entropy coded conditioned to the dither. Our approach is far from new in the literature. In fact, it is adopted in a variety of papers, e.g., [3], [4], [8]. Compared to these works, our scheme deals with a purely information theoretic setup while at the same time it gives more general and tighter bounds. 
 Before we proceed, we state the definition of a scalar uniform quantizer with subtractive dither [18]. A scalar quantizer function is defined as 
 where ? > 0 is the quantization step which is freely designed by the designer. A scalar universal uniform quantizer with subtractive dither is defined as 
 where r is the realization of a uniformly distributed random variable R over the interval . 
 The execution of  requires a common randomness both at the encoder’s and the decoder’s ends. In practice, the dither r acts as a synchronized pseudo-random noise generator that can be used at both encoder and decoder’s end. 
 Next, we use use the asymptotically stationary feedback realization scheme illustrated in Fig. 2 to design an efficient 
 We select the quantizer step size ? so that the covariance of the resulting quantization error meets V . The encoder does not quantize the observed state xt directly. Instead, it quantizes the deviation of xt from the linear estimate xˆt|t-1 of xt. This method is known in least squares estimation theory as innovations approach [19]. As a result, we name the encoder innovations encoder. 
 We consider the zero-delay source coding setup illustrated in Fig. 2 with the additional change of the p-parallel AGWN channel with p independently operating SDUSQ. This change is illustrated in Fig. 3. Note that, all matrices and scalings adopted in Fig. 2 still hold when the aforementioned change is applied. 
 For each time step t, the input to the quantizer, is a scaled estimation error defined as follows 
 Fig. 3: Perfoming componentwise scalar uniform quantization by replacing a p-dimensional AWGN channel with p independently operating SDUSQ. 
 Moreover, at is an Rp-valued random process. The parallel p-dimensional AWGN channel is replaced by p independently operating SDUSQ, hence we can design the covariance matrix V of the AWGN corresponding to the p-parallel AWGN channels in such a way, that for each t, each diagonal entry of Vi,i = 1,...,p, i.e., V , diag{V1,...,Vp} to correspond to a quantization step size ?i,i = 1,...,p, such that 
 This results into creating a multi-input multi-output (MIMO) transmission of parallel and independent SDUSQ. We apply SDUSQ to each component of at, i.e., 
 and we let rt be the Rp-valued random process of dither signals whose individual components {rt,1,...,rt,p} are mutually independent and uniformly distributed random variables independent of the corresponding 
 source input components at,i, ?t,i. The output of the quantizer is given by ß˜t,i = Q?i(at,i + rt,i), i = 1,...,p. (19) 
 Note that ß˜t = {ß˜t,1,...,ß˜t,p} can take a countable number of possible values. In addition, by construction (see Fig. 2), the sequences {at : t = 0,1,...} and {ß˜t : t = 0,1,...} are not Gaussian any more since by applying the change illustrated in Fig. 3, {at : t = 0,1,...} and {ß˜t : t = 0,1,...} contain samples of the uniformly distributed process {rt : t = 0,1,...}. As a result, the Kalman filter in Fig. 2 is no longer the least mean square estimator since the obtained quantized signals may no longer be Gaussian. 
 For completeness, we illustrate in Fig. 4 the relation between a SDUSQ and a scalar uniform additive noise channel, a result that was first pointed out in [18]. 
 Entropy coding: In what follows, we apply joint entropy coding across the vector dimension p and memoryless coding across the time, that is, at each time step t the output of the quantizer ß˜t is conditioned to the dither to generate a codeword zt. The decoder reproduces ßt by subtracting the dithered signal rt from ß˜t. 
 Specifically, at every time step t, we require that a message ß˜t is mapped into a codeword zt ? {0,1}lt designed using Shannon codes [16, Chapter 5.4]. For a random variable x, the codes constructed based on Shannon coding scheme give an instantaneous (prefix-free) code with expected code length that satisfies the following bound 
 Fig. 4: An equivalent model to Fig. 3b based on scalar uniform additive noise channel. 
 In view of the assumption that the scalar uniform quantizer with subtractive dither operates using memoryless entropy coding over time, the following theorem holds. 
 Consider the realization of the zero-delay source-coding scheme illustrated in Fig. 2 with the change of AWGN channel with p-parallel independently operating SDUSQ illustrated in Fig. 3. If the vector process {ß˜t : t = 0,1,...} of the quantized output is jointly entropy coded conditioned to the dither signal values in a memoryless fashion for each t, then the operational zero-delay rate, RZDop (D), satisfies 
 where p is the dimension of the state-space representation given in (1), while the average MSE distortion achieves the end-to-end average distortion D of the system. 
 Evidently, by combining the lower and upper bounds to the OPTA by zero-delay codes, we obtain 
 The bounds derived in (23) based on the scheme of Fig. 2 hold for vector Gaussian sources of any order. 
 In the next remark, we comment on (23) and draw connections to existing results in the literature. 
 Remark 1. (1) For scalar Gaussian AR sources, i.e., p = 1, then (22) degenerates to 
 This result was first obtained in [8, Theorem 7] but only for stable scalar Gaussian AR sources. 
 (2) Note that the zero-delay rate distortion performance per dimension is given by the following expression 
 It is interesting to observe that if instead of scalar uniform quantization we quantize over a vector lattice quantizer followed by memoryless entropy coded conditioned to the dither, then the upper bound in (22) becomes 
 where Gp is the normalized second moment of the lattice [20]. If we take the average rate per dimension then (26) becomes 
 Additionally, by assuming an infinite dimensional vector Gaussian source, then by [20, Lemma 1], , and we obtain 
 As expected, using vector quantization for infinite dimensional vector Gaussian source, the term due to space-filling loss and the loss due to entropy coding asymptotically goes to zero. 
 In this work, we considered zero-delay source coding of an unstable vector Gaussian AR source under average MSE fidelity criterion. Based on a simple feedback realization scheme that quantizes the innovations of a Kalman filter with a SDUSQ, we derived new bounds to the OPTA by zero-delay codes. We discussed the performance of this scheme when using lattice quantization. For infinite dimensions we observed that the INRDF is in fact the OPTA by zero-delay codes. 
 In the realization scheme proposed in Fig. 2, with the change of AWGN channel with p-parallel independently operating SDUSQ, the operational rate for each t is equal to the conditional entropy H(ß˜t|rt) where ß˜t = {ß˜t,1,...,ß˜t,p}, ß˜t,i = Q?i(at,i + rt,i), i = 1,...,p, i.e., the entropy of the quantized output ß˜t conditioned on the t-value of the dither signal rt. This leads to the following analysis. 
 where (a) follows from [18, Theorem 1]; (b) follows from the fact that the quantization noise is ?t = ßt - at (see Fig. 4); (c) follows from the fact that the relative entropy D(x||x0) = h(x0)-h(x), see, e.g., [16, Theorem 8.6.5]; (d) follows from the fact that D(at+?t||aGt +vt) = 0, with equality if and only if {?t : t = 0,1,...} becomes a Gaussian distribution; (e) from the fact that the differential entropy h(vt) of a Gaussian random vector with covariance V , diag{V1,...,Vp} is 
 where (a) follows from the structural properties of specific extremum problem resulting in the realization of Fig. 2 (see, e.g., the analysis in Section II and [1, Remark IV.5]); (b) follows from the analysis in [1, Equation (35)]; (c) follows from the fact that E,F,T are invertible matrices and as a result the information from kt to k˜t given k˜t-1 is the same as from at to ßt (information lossless operation). 
 Since we are assuming joint memoryless entropy coding of p independently operating scalar uniform quantizers with subtractive dither, then by (20), for t = 0,1,...,n, we obtain 
 Then, by first taking the per unit time limiting expression in (32) and then the infimum, we obtain 
 where (a) follows by (2) and (8) respectively, and R¯na(D) is the upper bound expression of Rna(D) for the unstable vector Gaussian AR(1) source model given by (1). 
 Finally, by assumption, (i.e., stabilizability) the Gaussian source model given by (1) is asymptotically stationary, hence as n -? 8 the source becomes stationary. Utilizing this, we have R¯na(D) = Rna(D) and the result follows. This completes the proof. 
 This can shown by augmenting the state of the Gaussian source model given in (1) as follows. Let x˜t ? Rs denote a new state vector of the additional dynamics. Then the model of (1) is modified as follows. 
 where the matrices (A˜1,A˜2) are the coefficient matrices of augmented dynamics. At this point, (34), (35) can be written as 
 The state space representation of (37) with the same assumptions on the augmented coefficient matrices (A,˜ B˜) can be used precisely as the feedback realization scheme of Fig. 2 via a Kalman filter where the structure of the filter is the one derived in [12, Section IV]. This completes the proof. 
 ﻿ We consider a system subject to an average data rate constraint in the feedback path . We provide upper to the minimal source rate to achieve mean square stability and a desired level of performance . In the quadratic case , an almost complete rate distortion characterization is . 
 This paper on the interplay between average in per sample and stationary performance for a control system a noisy plant and an average data rate constraint in the feedback path . In such a setup , the of guarantee that it is possible to find causal and such that the resulting closed loop system is mean square stable , if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant . This result been extended in several see , e .., , . However , when performance subject to average data rate are sought , there are relatively available . Indeed , to our knowledge , there are no computable of the optimal in control , , , , . 
 In this note , we present upper and lower on the minimal average data rate that one to attain a given performance level as measured by the stationary variance of the plant output . From a source perspective , we are aiming at the rate distortion function in closed loop . This beyond causal rate distortion theory due to being subject to a stability constraint . Our exploit a framework for control system design subject to average data in , . 
 Consider the of Figure , an plant with state and initial state , u is the control input , is a sensor output , e is a signal related to closed loop performance , and is a disturbance . We assume that , are jointly second order and with finite . The feedback path in Figure a noiseless digital channel , a causal whose output is a sequence of binary , and a causal . 
 Fig . . control system . average data rate across the channel is defined as 
 We do not restrict the complexity of the or the a , and only assume them to be causal , and to have access to independent side information SE and . Our aim is 
 where se trace , is the stationary variance matrix of e , is a desired level of performance , and the optimization is carried out with respect to all causal E render the resulting asymptotically mean square stable , i . e ., that render , u , jointly second order and asymptotically wide sense stationary . 
 R I u I uG , where I a the mutual information rate between a and , and , uG are jointly with the same second order statistics as , u . 
 Thus , in order to bound from below , it to minimize the directed mutual information rate that would appear across the source scheme , when all in the loop are jointly . 
 Lemma . : Suppose that , in Fig . are second order and jointly random . Then can be from as u i Li , i , i ,..., where , for each i ,...,, i is a zero mean random variable such that i , , si , and 
 Fig . . that when , in Figure , the E a linear source scheme . 
 where Li : i is a linear operator such that Li , is the minimum mean square error estimator of u i given , . 
 We conclude from the above that , for a given performance level , the minimum of I uG over all causal and is achievable by an pair which as a linear system plus additive white noise such that i , , i . 
 We next define the class of linear source , which are capable of yielding a relationship u of the form given by . 
 Definition . : A source scheme is said to be linear if and only if , when used around a noiseless digital channel , is such that its output u are related via 
 auxiliary , is a second order i . i .. sequence , proper , independent of ,. 
 When a linear source scheme is used in the of Figure , the feedback system of Figure . 
 Lemma . : Consider the of Figure and assume that the E and a linear source scheme . Under suitable , I u I and 
 where is the stationary power spectral density is the variance of the auxiliary noise . 
 Linear source have sufficient of freedom to allow one to compromising . Thus , our lead to : 
 Theorem . : Consider the of Figure under suitable . Define , with reference to the feedback scheme of Figure , the signal to noise ratio function 
 where sa , a ,, e , is the stationary variance of a in Figure , and the optimization is carried out with respect to all and all render 
 Theorem . the minimal average data rate that a given stationary performance level , in of , i . e ., in of the minimal that the desired performance level in a related architecture . Interestingly , the upper bound in is valid even if one the assumption of , being 
 To find , one can resort to the in . A case where an explicit solution is available is when , i . e ., when only stabilization is sought . In that case , it from Theorem . and that 
 where p ,..., are the unstable of . If one in and , then one , within a modest gap , the absolute minimal average data rate compatible with stability derived in . 
  
 ﻿ This paper an empirical study of the achievable data of network multiple input multiple output zero forcing , zero forcing dirty paper and dirty paper actual by indoor wireless channel at . . Their are with those of conventional , in which either the base are not , or their interference is frequency division . The were taken in aisle to office and large unobstructed hall . The study of these that , at high signal to noise , and can yield more than a three fold increase in attainable data when to and . The gains are smaller , but still significant . At low the system is noise rather than interference limited , and only gains . The in this paper also show that collaborative such as can benefit from interference prone to yield transmission capacity . With regard to the propagation channel , the classical log normal plus fading model , with fitted to the scenario type , was found to be good at the statistics of the achievable data of all the considered . 
 Index Wireless communication channel characterization and modeling , performance analysis . 
 I . INTRODUCTION 
 T 
 HERE is an ever increasing demand for high data in and with all the mobility provided by wireless technology . Since growing of are to be within confined , the performance of practical indoor wireless is to be limited , increasingly often , by interference . Therefore , in order to improve or even maintain high data , it will become necessary to make use of communication capable of the coupling between the various propagation links within a given service scenario so as to reduce interference and increase received signal power . 
 This management of the interference produced by access and can be by of . all operating in a fashion turns the set of and into a network multiple input multiple output system , and the communication medium into a broadcast channel . Network hold the potential of interference in a , greatly increasing efficiency in wireless , . 
 Several have been in the literature to take advantage of the high between several and a group of that is often in . The technique dirty paper , been shown to be capacity in this setting , and thus it is optimal , . is a nonlinear technique that full knowledge of channel state information at the transmitter . 
 The high implementation complexity of the need to consider other simpler , near optimal transmission . One such technique , commonly to as zero forcing , , sum close to those of with smaller computational complexity . In , part of the interference is removed by linearly combining the intended for all so as to effectively obtain , from end to end , a lower triangular channel matrix . The of the channel matrix , which perfect at the transmitter side , it possible to avoid part of the interference . The effect of all interference is by in a sequential fashion . 
 Another suboptimal technique , simpler than and , is zero forcing as in . Zero forcing is an entirely linear strategy . In this case , have perfect and to eliminate interference for all , yielding an effectively diagonal channel matrix between data and . 
  
 Simpler to attain communication in a are if each a single user and its is limited to the radio link to that user only . In our work , we consider two possible . The first one is frequency division , that is , splitting the available spectrum into disjoint frequency , each one to a single user pair , with each at full power . The second approach , which will be to as the strategy , is to let each transmit at full power all the spectrum available , thus that interference from other will limit data as transmission power . The simpler and in general have inferior than those of and . On the other hand , the former impose the minimum possible load , since each needs to know only the data intended for and the related to the user it is serving . There exist other for communication in a that do not require among and whose performance is under certain better than that of and , such as fractional frequency reuse . Although it is known that in some specific see , e .., , our objective here is only to provide a few simple as a reference for comparison . It is understood that whichever is the choice , different propagation may favor different . 
 The performance evaluation of diverse network for been in various works , . This included the assumption of log normal plus fading , and the use of the model . The effect of limited capacity between in overall performance is assessed in . 
 Regarding the channel model , it may be reasonable to assume that small scale are independent for each channel when are several apart , but this may not hold for shadow . Several report on the correlation of in different of transmit and receive . To what extent such correlation may affect the network channel , based on widely base station , is not self evident . To the best of our knowledge , no based in support of the assumption that a network channel is equivalent to a collection of independent have been . 
 The fact that in network all can act in a fashion taking advantage of , two related of capacity improvement . Interference between user can be reduced and , at the same time , the total received signal power at each user terminal can be . Since these effects depend on the degree of connectivity or , conversely , isolation , this the question of how much performance improvement can be for a given type of deployment scenario . 
 In this paper , we evaluate and compare the maximum data achievable by the , and , for measured as well as by indoor , under a per antenna power constraint . These were all calculated under the assumption of perfect at the transmitter . Although this would be hard to attain with currently available technology , the serve as a comparison basis , being the best performance achievable under equally for all . Channel were for two representative : with alongside them , and large . A single slope path loss model with log normal shadow fading and Rice small scale , was found to be adequate in all the for the at which were taken . With regard to achievable data , we found that at high in the range of , and achieve about a three fold gain when to or . The gains of are somewhat smaller , depending on the scenario , but still quite significant . A particularly interesting finding is that both not only are able to effectively mitigate the effect of interference but also in some can exploit low isolation between user , to yield higher data than in with greater natural isolation . As would be , at low , where interference is not the dominant limitation on capacity , the gain over the non . Our study also that the log normal plus Rice fading model is accurate at the statistics of the maximum achievable by each of the network considered , when the model are selected according to the type of scenario . Thus , an indoor channel as a collection of independent , would yield similar to those of our based study , provided that the proper model are used . However , rather than justifying what could have been as somewhat arbitrary of to illustrate the achievable of a system , we decided to use actual measured channel data from typical indoor to validate our comparison . In the following we describe our measurement and the empirical from them . 
 . MEASUREMENT SETUP AND 
 A . Measurement Setup 
 In all , the custom built channel sounder used of a single channel transmitter and a channel receiver operating at . for . The single transmit antenna was at that would in practice correspond to possible of a user . Four receive , one per , were at the typical of . The used for transmission and reception were coaxial , vertically . The transmit antenna was mounted on a long rotating arm and stepwise under computer control in degree . 
 In all , were taken at night in the absence of pedestrian movement . For each of the aisle to office and , more than a complex by channel were collected , each vector the complex gains from a user location to the four . 
 The single conversion receiver channel at . The receiver output were converted into a vector of complex channel gains the Fast Transform . Phase coherence was by locking the transmitter and receiver to . Exact synchronization of sampling data corresponding to an integer number of of the , which without the need for . For each position of the transmit antenna , the channel were simultaneously for second , and the total interval was partitioned into non of each . This us to verify the consistency of our which should only differ as a consequence of receiver noise . Since the capacity on the 
  
 Fig . . Diagram of scenario aisle to office . 
 relative rather than the absolute phases of the , we calculated phase with respect to one of the , arbitrarily chosen as a reference . In all our , the ratio between the average gain magnitude and its fluctuation , calculated over the sequence of , . Thus by our , we further reduced the effect of the measurement noise . 
 B . Measured 
 We in two of which we considered relevant and where , as our data subsequently confirmed , different propagation are to be . These scenario are as : 
 Aisle to office : Channel were carried out in three different aisle to office . In all the building was a steel concrete structure with interior made of wood and . For these , the four receive were wall mounted at a . height along straight , with a separation from the interior . Adjacent were by approximately . The transmit antenna was at height inside various and adjacent to the aisle . These are non line of sight . The between the transmit antenna and the receive ranged from to . The of transmit and receive for scenario aisle to office is schematically in Fig . . In this figure , the and represent and user , respectively . The other two of this type had similar . In each office , were the rotating arm on three or four different user , more than apart from one another , as space would allow for simplicity , only one of these user per office is shown in Fig . . 
 Large Hall : We from two large . The first one , hall , was a , glass central hall , about high , with concrete floor and several lateral leading to and on two . This hall is built with steel with concrete separating the hall from the adjacent . The four receive were at the of an imaginary rectangle , with each antenna at approximately from a wall on the hall , at of about . . The transmitter rotating arm was in different over a grid of within the rectangle formed by the receive . This is a line of sight scenario , in which link ranged from to . 
 The second hall scenario , hall , was a gymnasium with wooden floor and concrete . The four receive were as in hall , while the rotating arm was in different over a grid inside the gymnasium . Transmitter receiver varied from to nearly . 
 . STATISTICS OF MEASURED DATA AND MODEL FITTING 
 Before the channel to calculate the data achievable with network , we describe the statistical of the in each scenario . 
 The measured channel gains proved to be consistent with what been in the literature for similar . We found that log normal shadow fading combined with or small scale in an excellent model fit to our , provided the model are to the measured data . The statistics of each type of fading are below . 
 A . Small scale fading 
 For each wireless link , small scale fading statistics were by the channel gain corresponding to a single turn of the rotating arm . As , in all aisle to office , these channel gains fading statistics that were well by a distribution . This is consistent with a situation . 
 In the hall and hall , these gains fit a Rice distribution with factor between and . It is worth that despite the fact that these are , they are , as the previous , by strong propagation , although to a lesser extent . In all , the channel phases associated with each angular position of the rotating arm were uniformly distributed and uncorrelated . 
 B . Large scale fading 
 The large scale fading statistics were by calculating , at different , the of the channel gains over an arm rotation . The statistics of these average gains were well by a log normal distribution . More precisely , for each link distance , the path loss , in , corresponding to each rotation channel gain the behaviour of a random variable of the form 
 log , where is a path loss exponent , is a zero mean random variable with variance and is the path loss in at , which we chose as . For each measured scenario , the model by was fitted to measured large scale path , finding and by linear regression , and then setting as the empirical variance of log . The of and so are listed in Table I . For each scenario type , a 
 TABLE I 
 OF FITTED TO MEASURED . 
 Scenario 
 Aisle to office . . . 
 Aisle to office . . . 
 Aisle to office . . . 
 All aisle to office . . . 
 Hall . . . 
 Hall . . . 
 All hall . . . 
 row in the model resulting from combining all data of its corresponding . 
 It is worth that no evidence of a break point in the path loss exponent was found from the . More precisely , no slope were found to provide a better match in a least sense to our empirical data . This is consistent with what been before for short range indoor , . 
 It was also found that the correlation between the shadow in the links between a transmitter antenna and any two was , in all , below . in modulus . This was true even when the two in all several apart were seen by the transmitter at an angle narrower than . 
 . BRIEF REVIEW OF , AND 
 In this section we present a brief review of the transmission to be in their relation to the channel model . 
 A . Channel Model 
 A general wireless channel between single antenna and single antenna can be by a complex valued matrix . the vector of by the access by , the vector of received can be written as 
 y , 
 where the noise . i .. circularly symmetric complex with variance . Notice that ,, the element in the th row and th column of , is the narrow band channel gain between the th and the th user . Notice also that no joint of the output of the channel is , since it is assumed that between is not practical . 
 For the three network considered in this work , can be as 
 x , 
 where the th element in the vector u is the information bearing signal intended for the th user , is a linear matrix , and : is either a non linear transformation , in the case of , or the identity matrix , in the case of . We note that an element be a function of two or more user only if the are able to operate . Following standard practice , we will assume that the of u are independent zero mean complex random with 
 E 
 for , ,...,. Thus , can be written as 
 y . 
 We will assume that the are subject to a of the form 
  
 where x is the variance of the th element of , for some maximum power . All channel matrices considered in the sequel are by , that is , , our analysis to four simultaneously by four . 
 B . Non System 
 We include and as , against which to compare the other three network . In the case of , each a single user , and all operate without , at maximum power over the same frequency band . In of the model , this toto be a diagonal matrix , or any row or column permutation of it . Since the user signal can be chosen freely , there is no loss in generality in assuming for this case that I , where I is the identity matrix . With this , the sum rate achievable with an system is readily found to be 
 . 
 In this expression , , is the power of the signal as received by its intended user . On the other hand , 
 user , produced by the serving all the other . , the interference affecting the th 
 C . Frequency Division Scheme 
 In this case , each at full power a fraction of the available spectrum . We will assume that this spectrum is partitioned into adjacent non equal width . Therefore , assuming the same total as for , the maximum achievable rate of is trivially given by 
 . 
 D . Zero Forcing 
 The idea in is to eliminate , in the , all interference produced by the . To do this , the is chosen as . This all to have and each of them to process the intended to all . 
 In this case , the highest sum rate per second per achievable under a is readily found to be 
 , 
 where the signal are subject to 
 a 
 , , , ,..., b 
 and where the entry on the th row and th column of . The optimization problem defined by and been shown to be convex in . Therefore , a global maximum for the right hand side of can be numerically standard convex optimization . Notice that the solution to subject to will , in general , yield antenna transmit for one or more , ,...,. is known to perform poorly at low but it is easy to improve its achievable rate under such by a inverse of its true inverse . We denote this variant of as zero forcing . 
 E . Zero Forcing Dirty Paper 
 This technique was first in and then further studied in . The idea behind is to utilize the linear assure that the th user no interference from the , for all , and then use to avoid the effects of the interference . More precisely , is chosen so as to obtain 
  
 , ,, 
  
 where , are the of some lower triangular is the th element of the vector 
 v Tu , 
 see . In and , this is by choosing 
 W , 
 the unitary matrix in a decomposition of , i . e ., , an upper triangular matrix and the conjugate transpose operator . Choosing 
 W as in , we have that 
 y 
 which the interference avoidance outcome by with . invertible as is the case in all the channel matrices considered in this work , then all this factorization differ only by a sign inversion in any subset of their . Therefore , in our case , the absolute value of each element of and thus well is fixed known . 
 In , the effect of the interference by the sum at the right end of is by dirty paper . The latter , at the end , perfect as well as full knowledge of all user . After and the result through the channel , each is as if there had been no interference at all . Therefore , the achievable sum rate of is given by 
 , 
  
 where , are the in the diagonal where the maximization is over all row power that satisfy the for each permutation . Notice that in this case the , in are the of in for the row matrix . 
 It is easy to show that the optimization problem defined by and is also convex for each permutation . To see this , it to notice that the change of , and , ,, the optimization problem defined by , equivalent to , . 
 The performance of been channel data in , under a sum power constraint , and in , under a . In both , was shown to be superior to and various non . 
 F . Dirty Paper 
 Although the practical implementation of still specific code , we use this technique as an ultimate upper bound for the achievable data in each scenario . 
 In , no a restriction is on the matrix . Here , the signal intended for user , , is by possibly all as if there were no other being , i . e ., . In contrast , the data at user is into , the fact that all have full , which together with knowledge of , its interfering effect . The resulting signal is then sent possibly all as well . Notice that , by doing this , is added to the signal received by user as interference . A similar process is employed to successively encode the data for the , previously as known interference . In this setting , the signal with power all through the th column of , thus at the corresponding user with power as before , , the entry on the th row , th column of . Therefore , the maximum rate achievable with this technique is 
 , 
  
  
  
 Rate per user 
 Fig . . of rate per user , when , and achieve their maximum sum , under the power constraint , from empirical channel taken in scenario aisle . 
 where the maximum is taken over all of the over all matrices and 
 satisfying for each permutation . An efficient method to numerically solve the optimization problem by been in . This is the method we used to evaluate for the measured and channel data . 
 In the next section we use the above to compare the performance of the various in realistic indoor . We use both actual measured channel matrices as well as channel matrices by the model from our . 
 V . RATE EVALUATION AND 
 In this section we evaluate the maximum sum of , and as respectively by , , , and , subject to the , for the measured channel matrices in all . We then repeat the sum rate evaluation channel matrices by the model , the appropriate as derived from our . For the transmission , we calculate the per user at specific , i . e ., the maximum per user rate that is to be met or with some given probability . 
 We further assume that the in this system is perfectly unbiased in that , on average , all are an equal amount of time . At the same time , our procedure of to user is designed to avoid that will yield high interference when better exist . To achieve both we proceed , on each time slot , as : 
 Assign each user the access point providing the signal and group the with the same . 
 For each group , draw randomly one user . 
 It is assumed that the number of in each list is comparable . Over a long period of time this fairness as all have an equal chance of being , while the possibility of choosing in the same time slot more than one user the same preferred . More precisely , the channel matrix , in every time slot is such that 
 , , , . 
  
 This that interference prone such as are not unfairly in our comparison by particularly poor . In addition , and are ordered so that the th is the path loss for the th user . This a channel matrix in which the magnitude element in each row on the main diagonal of the matrix . 
 The fact that the above procedure in equal likelihood of service for all was confirmed in our by that when the algorithm was repeated many times , any specific choice of position received service the same number of times . While it may be possible to find that further benefit the performance of the scheme , it should first be that these gains are not at the expense of fairness . 
 As is to be , unobstructed hall provide less natural isolation between user than aisle to office . This in the fact that the hall were by a significantly smaller path loss exponent , see Table I . As a consequence , for any given distribution of between and , the channel matrices associated with hall will tend to be less diagonally dominant , i . e ., the off diagonal be on average . The effect of this on the rate achievable by each transmission technique will be later in this section . 
 In all the , the noise variance in the receiver , for all full band i . e ., excluding , was chosen to be , which roughly to the thermal noise in a receiver with a noise figure of , operating at room temperature , over a of . 
 A . Aisle to office 
 For each aisle to office scenario , by channel matrices satisfying were selected by randomly choosing from the set of measured channel data for the corresponding site . 
 Fig . . User at different for aisle to office first column and hall second column . The of the per user , are chosen so as to maximize the sum rate in each scheme , is shown in Fig . for scenario aisle to office , under the b with . The in the other aisle to office were quite similar . It can be seen directly from the graph that at this transmit power limit , the three network outperform and for all . We also observe that the performance of and are very close , which is consistent with before for i . i .. . It should also be noted that the optimization used only assure that the sum capacity of is the best of all . While this was in all our , it does not necessarily imply that at all availability , the per user rate of will exceed that of the other . In fact we found that for some , the would exceed those of , as can be from the at low . At high however , invariably proved to be best . As the transmit power limit is reduced , e .. to below , we found that an increasing number of have a non zero probability of being assigned zero rate when , or . This is a consequence of the fact that we are considering the per user when each of the network is for maximum sum rate , not fairness . As a consequence , the best strategy may include not serving some at all in some channel . This behavior will be in more detail below . 
 The corresponding per user of each scheme in office scenario , as a function of , are shown in the first column of Fig . , for and . On the upper edge of each of these we have included an additional horizontal axis to provide an algorithm independent measure of the received signal to noise ratio for each site . This to the average received in the setting excluding interference i . e ., supposing interfering are turned off , that is 
 , 
 where , is the th diagonal entry of the th . This would also be the true for all the full band considered here if the off diagonal of the channel matrix were zero . Since the latter condition never , this notion of is not the actual per user signal to noise ratio , which is algorithm dependent . Instead , it is the average ratio of the power received by a user from its , to the receiver noise . This ratio only on the environment , specifically on the average path loss between user , as can be seen from . 
 Figure that , in scenario aisle to office and for a power limit , and attain a data rate gain in excess of three times , for both and , when with or . Similar gains are at higher power . In relation to this , we note that , loosely speaking , for large of , a gain over of at most four times would be , since for large of and assuming perfect interference cancellation , the improvement in capacity will be dominated by the ratio of transmission used , which is in our case . 
 For small of , the interference 
 variance , which that the performance of is , in are small to the noise 
 noise limited . In the specific case of , the power of some will be reduced below to meet the while the channel by choosing . The cost of such interference avoidance effort is than the benefit stemming from zero interference , yielding a sum rate smaller than that with at full power without . The poor performance in low is a well known shortcoming of pure , which can be upon by zero forcing , as in . We illustrate this in Fig . which in greater detail the behavior of all at low power for the case of availability . Under such the performance improvement of over becomes evident . As previously , it can also be seen in this figure that at low power , the objective of sum rate may be by 
  
  
  
  
 P 
 Fig . . User at availability for aisle to office . 
  
 Rate per user 
 Fig . . of rate per user , when , and achieve their maximum sum , under the power constraint , from empirical channel taken in scenario hall . 
 not serving some at all , and as a consequence the rate that can be to of may drop to zero . 
 B . Hall 
 A procedure similar to the one before was to generate by channel matrices in the two hall where were taken . The of the peruser with each network technique when for sum rate under power constraint are shown in Fig . , for scenario hall . In this figure we see again that the three network considered here provide higher per user data than and . 
 The corresponding per user for several as a function of for this scenario type are shown in the right column of Fig . , where scenario hall been chosen . The for the other hall scenario are very similar and are for the sake of brevity . 
 In this scenario , in the high range , and attain roughly the same gain , in per user over , as that in the aisle to office . On the other hand , in this more interference prone environment , the maximum are significantly reduced when to those of the aisle to office , by higher natural isolation between user . 
 The gain of the over is also when to the aisle to office . Loosely speaking , this can be to the fact that the aisle to office channel matrices are more diagonally dominant than the hall channel matrices . As a consequence , and in view of the disc theorem , the determinant , at times , be smaller in hall . Since for the matrix the inverse of , it will be possible have in the hall . In view of the power constraint , this smaller signal and thus smaller sum for see . In contrast , is less sensitive to the smaller determinant matrices that arise in the hall . The reason for this behaviour can be found in the fact that the determinant equal to the product of all the diagonal of . Thus , a given decrease in the determinant , in general , entail a smaller reduction of rate for see . 
 As can be from Fig . , high interference do not always result in per user . While this certainly for a non system as clearly seen from the and to a lesser degree may yield an performance in the hall in comparison to that in the better isolated aisle to office . The improvement is particularly significant a factor of around at low power , as seen at the left extreme of the . We note that the performance improvement is still evident when the comparison is carried out at equal , i . e ., when the in average path between user for the been . This behaviour can be by that , at low power , noise rather than interference is the relevant factor in limiting transmission . Since the channel matrices of the office are more strongly diagonal dominant , less power a user from access other than the one that is dominant . In contrast , in an environment with less isolation such as the unobstructed , the can take advantage of the fact that significant power from all bases will reach each user , while to turn most of this power into signal , not interference . Indeed , it can be seen from that for sufficiently large noise power , increasing the magnitude of the off diagonal produce a relatively increase in the numerator of the fraction within the log than on its denominator . Thus the signal plus interference ratio is . As transmit power however , interference becomes dominant , and the hall suffer from the need to compensate for this impairment . 
 C . Accuracy of the channel model at achievable network 
 In order to assess the accuracy of the log normal plus Rice model at the maximum achievable by the in Section , we those from channel matrices by the general model for each scenario type aisle to office or hall , that is , the in the of Table I . The data by the model were , in general , in good agreement with those from empirical data , for each scenario . Figure of per user rate with respect 
  
  
  
  
  
 P 
 Fig . . availability empirical and per user for scenario aisle to office . were from the model the scenario type in row All aisle to office of Table I . 
 to maximum transmit power constraint for availability for the scenario aisle to office . It can be seen that , for this scenario , the from are smaller than those from empirical data . This may be to the fact that the scenario specific parameter for aisle in the third row of Table I is smaller than that of the corresponding general model used for the scenario type in the fourth row of Table I . Thus , in this case the actual received will be on average than those by the model . 
 . 
 In this paper we have the data achievable in an indoor wireless by system by network such as zero forcing , zero forcing dirty paper and dirty paper . We these with those achievable with frequency diversity and no , measured and indoor channel matrices at . . Our show that a single slope log normal plus fading model with properly chosen is good at the performance statistics of these . By numerical evaluation based on empirical data we have shown that in the tested , at high , the network are able to achieve about a three fold increase in per user data over or , when considering and . It was also found that dirty paper can attain higher per user in hall , where interference is greater than in aisle to office . This difference was greater for high availability data and at small . Our also revealed that while in general higher data than and , this advantage is lost at low , particularly when high availability is considered . In contrast , based provide significant under virtually all practical . 
  
 The would like to thank Silva , and for their valuable help . 
  
  
 ﻿ This paper novel on perfect reconstruction feedback , i . e ., noise shaping , predictive and sigma delta A whose signal transfer function is unity . Our analysis of this class of is based upon an additive white noise model of quantization . Our key result is a formula that the minimum achievable of such to the signal to noise ratio of the scalar in the feedback loop . This result us to obtain analytical that characterize the corresponding optimal . We also show that , for a fixed of the scalar , the end to end of an optimal which the optimal which for this case turn out to be increasing ratio . Key from work include the fact that fed back quantization noise is explicitly taken into account and that the order of the converter is not apriori restricted . 
 Index Differential pulse code modulation , optimization , quantization , sigma delta modulation , source . 
 I . INTRODUCTION 
 T 
 HE term feedback to a class of to digital converter wherein a scalar is within a linear feedback loop . Well known of include , and sigma delta . The latter have been very successfully applied in a number of , audio compression , , A conversion , , subband , digital image half , power conversion , and control over . 
 Fig . a general configuration . In this scheme , may take the form of a nonuniform or a uniform , the latter being either or . 
 The in an system allow one to exploit the predictability of the input signal so as to reduce the variance of . When with simple 
  
 Fig . . Feedback quantization system and frequency weighting filter . 
 smaller quantization step . The error feedback filter the possibility of spectrally shaping the effect of quantization noise in the frequency where it is less harmful from a user point of view . Accordingly , it is convenient to use a frequency weighted error criterion , via an error frequency weighting filter , and to focus on the frequency weighted see discussion in and . 
 For the sake of generality , we consider the possible use of a clipper before . This device the value of the input signal so that if , and if 
 , where is the saturation threshold of the clipper which is helpful in reducing limit cycle idle in an with high order , as in . On the other hand , if we chose to be sufficiently large , then , and the clipper no effect on the system . 
 If the of and the spectral of the input signal are known , then the design of an converter that the variance of to choosing the 
 . 
 It is often desirable that a converter is transparent to the system in which it is inserted . This to the widespread paradigm in which the scheme to the application that it , without need to modify the latter . A transparent converter is one whose signal transfer function i . e ., the transfer function from input to output is unity at the of interest . The design of such perfect reconstruction feedback the main topic of the present work . are by the property that , in the absence of quantization effects , there is no frequency weighted reconstruction error , i . e .,. If we denote the power spectral density of , then it can be seen from Fig . that the latter if and only 
 if 
 X . 
  
 Thus , in the design of an optimal converter , only two of freedom are available : the or , alternatively 
 To the best of our knowledge , on optimal filter design for either consider finite order , , , assume or require that the variance of the signal is much smaller than that of , , 
 , , or have a heuristic component in the optimization , , , . available for the optimal performance and corresponding filter frequency of a converter are those given in . However , the assumption of negligible fed back quantization in these suboptimal . Indeed , as we will show in the sequel , there exist where the in yield large fed back quantization error , even when a fine step scalar is used . In these , not only is the main assumption in , but also an much than can result due to excessive overload see , e .., and . 
 In the present paper , we will show how to design optimal . For this purpose , as in , , and , we model the scalar as a linear device that additive white noise whose variance is proportional to that of the signal being . A key departure from , however , is that we explicitly take into account fed back quantization noise in the feedback loop . Our main are : 
 i We derive one parameter that relate the minimum achievable frequency weighted to the signal to noise ratio of ; We show , within our model , that the frequency weighted in an optimal where the of is fixed exponentially with ratio ; and We derive that characterize the optimal for a . Our can be applied to any given number of quantization , and to almost arbitrary input spectra and frequency weighting criteria . 
 The remainder of this paper is organized as : In Section , we present our analysis model for . In Section , we formulate the associated optimization problem . Section a one parameter characterization of the solution . In discuss the main of an . The case of is in Section . Section the relationship to previous and the importance of taking account of fed back quantization noise . Section simulation . Section . For ease of exposition , all of our are included in the Appendix . 
 A . and Notation 
 We write as a short hand expression for if and only if . The of all complex valued square integrable and absolutely integrable on are by and , respectively . Given we adopt the standard inner product , where complex conjugation . We denote the corresponding 
 norm as . We use as the argument of the 
 transform . If is a transfer function , then we use the short hand notation to refer to the associated frequency response 
 . If is a set , then we write a . e . on almost everywhere on for everywhere on , except on a zero measure subset of . We use to denote the variance of a given wide sense stationary ... random process We recall that if zero mean , then 
 , where 
 is a frequency response satisfying , 
 . For any or we write 
 and to denote the and 
 , respectively . 
 To simplify notation , we introduce the operator , defined as : 
  
 where is any given function and any arbitrary and positive bounded value . For later use , we also recall the following definition . 
 Definition Almost Constant Function : A function 
  
  
 . CONVERTER MODEL 
 In this section , we discuss some of the main of feedback quantization . We also describe the analysis model and the to be considered later in the search for the optimal . 
 A . Feedback 
 We begin by the that describe the behavior of the shown in Fig . . 
 Quantization and Clipping : From Fig . , the quantization error is given by 
  
 Every practical scalar an associated constant 
 such that , if , then is said to be . When the is not , then is only granular quantization error , namely , which can be bounded as for some 
 see , e .., . For example , if is a symmetric , uniform , with and quantization interval , then one needs in order to obtain . 
 In general , we can write 
  
 where 
  
 is the overload error . Clearly overload are bounded as 
 , but they cannot be bounded by a 
 constant unless is bounded . 
 As outlined in the introduction , the clipper in Fig . can be used to keep from . For simplicity , we will only consider here two , namely , that , or else 
 . The former choice that does not overload , since clipping error , defined as 
  
 place instead . More precisely , if we have that 
  
 clipping is that , unlike overload , clipping are not fed back into through . This to avoid large limit cycle from the overload of , see . Since such are not part of the analysis model we will use , their occurrence could increase the significantly above the value by the model . 
 the above , and from Fig . , we can write 
  
 which that from by the sum of the quantization and clipping . 
 Transfer : From Fig . and we have that 
 a b 
 c 
 Notice are require no on the involved . From b one can see that to the signal transfer function , from to , of the converter . Similarly , the product is the transfer function for quantization , usually to as the noise transfer function of the converter . The term will play a crucial role in the derivation of the optimal 
 in Section . 
 Stability : We say that a is Bounded Input Bounded 
 Output stable for any input sequence satisfying . 
 infinitely many quantization , then 
 , and , thus , all the other in the converter are bounded . On the other hand , if , then can be written as 
  
 If the a finite number of quantization , then is bounded . If is stable and is minimum phase , then it from that is bounded . This , in turn , that and all the other in the converter are bounded see and if all the in Fig . are stable , and if no on or outside the unit circle , then the resulting is stable . 
 In addition , if and are stable , then the norm of their impulse , namely and , are bounded . 
 Thus , if there a bounded such that 
  
 is that , where 
  
 Therefore , for a uniform with quantization interval , it to have or more quantization in order to avoid clipping or overload . 
 B . 
 The associated with our model are next . 
 Input Spectrum and Frequency Weighting : The error weighting filter in Fig . the impact that reconstruction have at each frequency . This performance assessment filter is application dependent , and is assumed to be stable and given . The input signal is a zero mean ... stochastic process with known and finite power , i . e . In 
 order to simplify our subsequent analysis , we shall further restrict and to satisfy the following : 
 Assumption : The product is a piece wise differentiable function at most a finite number of and satisfying In addition , is such that one of the following . 
 There a constant such that 
 , for all , or 
 such that . Furthermore , if the set of noncontiguous and nonoverlapping in such that 
 , then , for every , such that is as 
 . 
 We note that the above is a rather weak constraint , since i and include almost any product of practical or theoretical interest . In particular , condition i all the where the product no on the unit circle . In turn , condition is satisfied if is zero over any interval on nonzero measure , or if is rational and on the unit circle . 
 The : We shall focus our analysis on the effect that granular quantization have on the . For this effect to closely represent the actual , we need to assume the following : 
 Assumption : The of overload and clipping are negligible , i . e . 
 a 
 or 
 b 
 In addition , and as stated in the introduction , we will adopt an additive white noise model for . This model is widely used for the analysis and design of data see , e .., , , , and , being usually as . 
 Assumption : The sequence of quantization noise is a zero mean ... random process , 
 related with the input of the , and constant 
  
 where is the variance of . 
 The above additive white noise model , although not exact , is , in general a good approximation when a signal with a smooth probability density function is with many and negligible overload in the sense of Assumption , see , e .., . The model can be made exact , even for few quantization , by a uniform scalar with either subtractive or dither , provided overload does not occur , see . As before , one way to achieve this is to use a with a sufficiently large number of quantization , so as to satisfy . In this case , if the quantization interval is and the dither sequence 
 , uncorrelated to when is not and is bounded as , then any number of greater than or equal to will make Assumption hold exactly . If a smaller number of quantization are employed so that , then the use of dither with the same as before , together with clipping i . e ., setting , will also make satisfy Assumption exactly . 
 Assumption one to write the variance of as 
  
 . . on through the feedback path . However , if the scalar a finite and fixed number of quantization , then another link between these two needs to be considered . In order to model this relationship , we will use the fixed model employed in , e .., , , , , and . 
 Assumption : For a fixed number of quantization , the variance of quantization is proportional to the variance of the signal being , i . e ., there such that 
  
  
 If no clipping is used i . e ., if , then exactly to the of . If , then is a good a approximation of the of when b in Assumption . In our model , is assumed fixed and given . Strictly speaking , on the of , on the number of quantization of , and on how quantization and are distributed along the dynamic range of . In practice , for a given number of quantization , should be chosen such that the dynamic range of is used efficiently , whilst en 
 Here and in the sequel , we assume the dither is such white and uncorrelated not . 
 a low probability of overload or clipping . For example , for the often uniform with loading factor equal to we obtain assuming that a uniform and overload . We note that for large , and provided overload are negligible , a quadratic relationship between and for most of scalar see , e .., . This is indeed the well known rule of reduction of quantization noise variance per additional bit of resolution . 
 In the sequel , we refer to the model of determined by , , and as The Linear Model the Linear Model is exact if the a enough quantization to avoid overload . If not enough quantization are available and dither is used jointly with clipping , then the model is exact in the effects of granular quantization , and is a good approximation in the total if Assumption also . If the scalar is , a small quantization interval relative to and enough quantization to avoid overload , then the Linear Model can be to yield a good approximation of the total . Perhaps surprisingly , the Linear Model turns out to predict with remarkable accuracy the of an optimal when few quantization and clipping are used with a loading factor big enough to satisfy Assumption , even without dither , and even for a bit . This can be from the simulation in Section . 
 C . Optimization 
 The in Fig . are design . 
 We shall restrict the search for the optimal to those satisfying the following constraint . 
 Constraint : 
 and satisfy . 
 and are stable . 
 is stable and strictly causal i . e ., 
 As in Section I , the first constraint perfect reconstruction . As in Section A , the stability on are a necessary condition for the converter to be stable . The additional requirement on , namely strict causality , is for the feedback loop in Fig . to be well defined see , e .., . Notice that we will not a require to have only inside the open unit disk . Instead , we will show that the latter property naturally from the solution of the design optimization problem . 
 An additional constraint on from the value of , as next . The ratio between the of and 
 by the feedback can be by dividing by , yielding 
  
 One can see from the above that if , then any filter or scaling of the quantization of will yield , thus , making large overload or clipping inevitable . This would and , if no clipping is used , may lead to large limit cycle . We , thus , conclude that the use of feedback the following constraint . Constraint : 
 If the above constraint is met , then can be found by substituting into . This 
  
 . OPTIMAL DESIGN 
 Given the model in the previous section , we can now evaluate the quantity that we aim to minimize , namely , the frequency weighted mean squared error . From c , and and , it that the is given by 
 . Thus , in view of , the minimization 
 of the in the Linear Model can be stated as . Optimization Problem : For given , and for given and satisfying Assumption , find the frequency , and satisfying and that minimize 
  
 The following proposition us to further reduce the number of in by the optimal 
 to 
 the following change of : 
 , 
 proof of Proposition in the Appendix , Constraint is satisfied . In addition , a stable and strictly causal 
 i . e ., one satisfying Constraint always to a function , see , which 
  
 This result directly from formula see also the Bode Integral Theorem in , e .., . 
 On the other hand , as we shall see in Section , if Assumption , then the optimal within the set of by and the requirement turns out to be piece wise differentiable on , at most a finite number of discontinuity , and 
 a 
 b 
 Under these , it is always possible to find a stable and strictly causal filter such that arbitrarily well on , as stated in the following lemma . 
 Lemma : Suppose that is piece wise differentiable on 
 , that it at most a finite number of discontinuity and that it . Then , for every , there 
 a finite order rational , strictly proper and stable such that . 
 the above , Optimization Problem can be as . 
 Optimization Problem : For given and known and for satisfying Assumption , find 
 the optimal feedback filter , say , via see also Lemma . In the following section , we will show how to solve this optimization problem . 
 . SOLUTION OF THE OPTIMIZATION PROBLEM 
 It would be desirable to provide an explicit analytical solution to Optimization Problem . Unfortunately , and as will become apparent in the discussion later , a closed form solution , for arbitrary , infeasible . Nevertheless , we can provide a one parameter characterization of the optimal function in as . 
 Theorem : For any given satisfying Assumption , and for any , the function in to the one parameter family of , where 
 a 
 and 
 b 
 Here is the lower bound of feasible , and , if it , is the unique scalar such that 
 . If such a scalar does not exist , then we choose . 
 Note that the above result an explicit analytic expression for , once the optimal , defined as 
  
 been found , i . e .,. Expression a also 
 insight into the structure of 
 Theorem can be used to develop an efficient algorithm to solve Optimization Problem . The key point is that substitution of a into the search space from the infinite dimensional set to the real interval . More precisely , Optimization Problem is turned into the simpler problem of finding the minimizer of the single variable scalar function 
  
 We will show next that the global minimizer of , i . e ., 
 , and hence the solution of Optimization Problem is unique . Furthermore , can be by finding the root of a scalar , convex , and monotonically decreasing function . 
  
 Theorem : hand side of is and 
  
 Moreover , it from and that , for any satisfying Assumption , and for any , the global minimizer of and is unique . In addition , these guarantee that can be easily found by , via , for example , the bisection algorithm , or any other convex optimization method . 
 We can now express and the minimum achievable , namely , in of , and . Indeed , combining and a with after some algebraic simplification that 
  
 the of Lemma . 
 It can be seen from a that is a monotonically increasing function of . In view of Theorem , this that , as , is monotonically decreasing with increasing . As a consequence , the converse of Optimization Problem , namely , finding the optimal and minimum of for a given target distortion , can be by and . Moreover , since the of a is a concave , monotonically increasing function of , this parameter can be easily found by standard iterative , as in the original optimization problem . 
  
 It is also interesting to note that and a , which relate and via the parameter , have a structure akin to the well known reverse water filling see , e .., , and . The latter characterize the rate distortion function for . 
 To summarize , we have given an explicit analytic expression for the optimal and , once been determined . Furthermore , we have shown that the parameter always , is unique , and can be easily found simple numerical . 
 In the following , we will provide additional insight into the of , as well as into some of optimal , 
 V . OF OPTIMAL 
 In the sequel , we say that a is optimal or if its , satisfy for negligibly small of and , and is such that , a . e . on , with as defined by . 
 A . The Effect of the of 
 It from and that , for any given satisfying Assumption , in a the family of all noise shaping that are optimal for some . 
 As we will show , from to equivalently , from to one to undergo a smooth progression from full noise shaping to no noise shaping , in an optimal manner . An example of this progression is shown in Fig . . Note in this figure how solid a unit transfer function as the for which in the figure , becomes smaller and . It can also be that the inverse of is . 
 Such asymptotic convergence does indeed take place in general , as the following theorem : 
 Theorem : For any satisfying Assumption , the defined in a converge uniformly to 
  
 as . Similarly , for any function satisfying condition i in Assumption , the defined in a converge uniformly to 
  
 as in to the choice of no feedback 
 , which the to a converter . In view of , this no noise shaping scenario is asymptotically optimal as . In turn , defined in to the full whitening feedback in , , . From and , is optimal . See also the discussion in Section . 
 B . Signal Spectra 
 The Output of the : By looking at Fig . and Assumption , we find that the of in an is given by 
 to the latter result 
  
  
  
 and , it is easy to see that 
 . If , then we have 
 . With the choice , and 
 and , we conclude that the variance of the quantization noise in an is given by 
  
 a been used . Substitution of a and into this expression to 
  
 , the output of the in an is white . This that near optimal of the output can be with a memory less entropy coder . 
 The Frequency Weighted Reconstruction Error : The of the frequency weighted reconstruction error is given by of into the above . 
 to the latter , we obtain 
  
 Thus , we conclude that the frequency weighted quantization error in an is not white . This fact in stark contrast to the when the are without the perfect reconstruction constraint , see , e .., . It also from the result when the feedback filter is fed back quantization error , as in and . Note that , as is made , not only becomes smaller , but its asymptotically a constant function over the 
 . 
 . FEEDBACK QUANTIZATION 
 It is well known that i . e ., sampling a continuous time signal at a frequency above its rate one to smaller error for a given , fixed number of quantization . For instance , the of simple scalar quantization without feedback is known to decrease as , see , where is the ratio , given by the order of the feedback filter see also recent work in . From a rate distortion viewpoint , the inversely polynomial error decay of this error estimate is too slow to compensate for the increase in the overall bit rate due to which is proportional to . To be more precise , let us consider a scalar with quantization , where the quantization resolution in per sample . If the additional by was instead to increase , then the would decay as , i . e ., exponentially . 
 A faster decay of the of with can be by a different feedback filter with possibly different order for each ratio . An example of such a family of bit was given in . Here , the continuous time reconstruction error can be uniformly bounded by is independent of . This bound an that with as , which is faster than any inverse polynomial , but still far from 
 Substitution of a into 
 g 
  
 S g : 
 Thus , for all ;, and as 
 , such that . 
 Strictly speaking , this only for whose have finite support . Indeed , it been shown that for several infinite support , the of uniform quantization asymptotically faster than , where a is a constant independent of , see . 
 exponential . Based on this result , the family of bit in achieve an that is , i . e ., increasing . Notably , the in and were an exact , deterministic model of quantization . 
 We will next show that , within the Linear Model , if the optimal infinite order in Section are used for each value of , then one can achieve an exponential decay of with the ratio , provided is kept constant . If the input sequence is from sampling a band limited signal , would cause defined in to vary with . To capture this effect , we replace 
 by the family of , defined as 
 if 
 if . 
 In , the square root of the of the frequency weighted input without , and . Notice that , that is , the total power of in of variance per sample , remains constant for all . This a uniform comparison basis for the distortion . 
 We can now make explicit the dependence of 
  
 to the output of . Interestingly , it is possible to establish a precise exchange formula for and . Indeed , in of minimal achievable distortion , the effect of increasing equivalent to an exponential increase in the output of . This is shown in the next theorem : 
 Theorem : Under the Linear Model in Section , for any function , and for any , the minimum achievable : 
  
  
 If we assume that exponentially on the number of per sample , then Theorem an that exponentially with , provided the Linear Model and that optimal , and by , a and are employed for each . The following simple example this idea : 
 Example : Flat Weighted Input Spectrum Consider an input 
 signal and a weighting filter such that 
 is constant , without . For this setup , the optimal for our model of is 
 , i . e ., a converter . From , the minimum without i . e ., with becomes 
 where . To analyze behavior of in this case , we apply Theorem to the above expression . 
 This that , and , thus 
  
 for all . Note that , to achieve , needs to be according to b and . Therefore , for this example , the of an with fixed an exponential decay with the ratio since , by definition 
 If we further assume to depend on the number of per 
 sample as which would correspond to 
 being a uniform with many and operating with a loading factor of , then becomes 
  
 . This 
 posing that and hold , we obtain from that is lower and upper bounded by proportional to 
 . For loading factor of , , and , the exponent in the latter expression to , respectively . 
 The next theorem that the exponential decay of the in the example above can be extended to arbitrary band limited input and frequency weighting criteria . 
 Theorem : For any and function satisfying 
 Assumption , the following : 
  
 where the optimal for . 
 Thus , under the Linear Model , we have that the of an exponentially with . 
 Remark : Model in Section . Here it is convenient to present some further regarding the validity of that model when the ratio to infinity , for different of a . 
 As already in Section , if is bounded and a sufficiently large number of quantization to avoid overload is used together with dither , then the Linear Model is exact . Nevertheless , there is no guarantee that the number of necessary quantization to avoid overload remains constant as . If such number with , then can only be kept constant by increasing the number of quantization in the . 
 If the number of quantization is insufficient to avoid clipping overload , and if dither and clipping are used with a fixed loading factor , then there a certain finite value of beyond which Assumption is . This from the fact that , for any fixed loading factor , the effect of clipping in the output does not decay with , thus , becoming the dominant component in the for sufficiently high . Further reduction of the would then require one to balance clipping and granular quantization by increasing the loading factor . If the number of quantization is fixed , this would necessarily reduce the value of , clearly increasing the component of the due to granular quantization . Nevertheless , if clipping and dither are used with , then the Linear Model and Theorem is exact in the due to granular quantization . 
 . THE IMPORTANCE OF TAKING ACCOUNT OF FED BACK QUANTIZATION NOISE 
 If one tried to optimize the of a fed back quantization noise , i . e ., by trying to minimize 
 compare to , then one would 
 obtain a sub optimal feedback filter , namely , which 
  
  
 a 
 where 
 b 
 provided see in the proof of 
 Theorem . This to the result in 
 , the noise transfer function magnitude is also equivalent to that derived in . The latter is optimal in the sense of the ratio , but not in the sense of for a fixed . 
 As shown in Theorem in general , does approach as . One can then expect to be near optimal in where , see . The latter is often satisfied at high bit i . e ., when many quantization are available . However , for any given number of quantization , it is easy to find practical where is such that is comparable to or greater than . More precisely , from , and that see Appendix , one can show that , if over a set of in with measure , where is some positive 
 scalar , then 
  
 This that a large is for any product 
 whose magnitude becomes significantly small in relative over certain frequency . An example is included in Section . A direct consequence is that , for these , and in view of , trying to match to will yield a performance far from optimal , also increasing the risk of large limit cycle if no clipping is employed see , e .., and . 
 The possibly unbounded increase of 
 was already in . Several heuristic have been since then see , e .., , , , , , and . In contrast to these , the method derived in the present paper one to characterize the true optimal , by explicitly taking into account in the cost functional to be see . Our method not only that , but also the actual optimal . Our proposal also the advantage of being applicable to arbitrary input spectra and frequency weighting , regardless of how small the 
 may be , within the scope of validity of the Linear Model . 
 . SIMULATION STUDY 
 To illustrate our , we have designed the of a at digitally audio in a as well as the of both the and the numerical are given later . 
 A . Simulation Setup 
 The of audio was as unit variance zero mean white noise through 
 . The mag 
 of the frequency response of is in Fig . solid line . The frequency weighting filter considered had a frequency response magnitude which the curve derived in , Table , thus , modeling the sensitivity of human hearing to noise . The corresponding frequency response is plotted with dotted line in Fig . the sampling frequency is . . The resulting 
 for these is also shown in the same figure dashed line . For this choice of , and in view of , one could expect the norm of a full whitening feedback filter to be very large . This is indeed the case : . Thus , the suboptimal feedback filter by the use of a scalar with at least in order to become feasible see Constraint . 
 In the , was chosen to be a uniform mid rise with quantization interval . Several were considered for the , calculated as 
 , where and where 
 the loading factor . Two different loading were considered : and . The latter choice a slightly lower than the usual loading factor of . However , this regime the benefit of making overload smaller and more infrequent . As the simulation will show , for our of and , this more conservative loading factor lower 
 overall distortion when above per sample . 
  
 Fig . . Frequency response for solid line , dotted line and e e dashed line . 
 For each and corresponding two for , one for each loading factor , the of the converter were designed according to the following : 
 The parameter was calculated by numerically 
 . 
 The optimal , and were via b and . 
 These were then with rational transfer , of order and 
 of order . 
 An appropriate value for the parameter in was chosen via , see , assuming 
 recall that for all the . 
 This that . 
 For each combination of and , the resulting converter was two different . 
 : This scheme is as in Fig . 
 , with virtually infinitely many . Thus , for all neither clipping nor overload er 
 occur . 
 and Clipped : Here which a scalar with a finite input dynamic range . As a consequence , any value would overload if or produce clipping error if . To avoid large limit cycle , this variant was clipping i . e ., 
 . 
 Each simulation with the comprised , . For the converter , five , were for each combination of 
 and . 
 B . 
 The of the numerical and the are next . 
  
 Fig . . Frequency weighted for f ;...; g . 
 Comparison Between and the Rate Distortion Function : The information theoretic lower bound see for the associated with the given source and filter is plotted in Fig . solid line . This to quadratic frequency weighted Distortion Rate function when . As the bit rate is , the gap between and this absolute lower bound to approximately . for and for 
 , at . This difference can be to the rate distortion inefficiency of the uniform scalar . On the other hand , the performance gap at lower bit can be to the perfect reconstruction constraint . Recall that , at low bit , the achievement of rate distortion function the suppression of relatively less significant of the of the input signal see , e .., and . This linear distortion , which a cannot achieve , is more severe at lower bit . Thus , the performance gap as is reduced . 
 : The of this converter variant is in four of the in Fig . , with beginning with opt . These differ in the loading factor , and in the meaning of in each case . For the whose do not have the ending E .. entropy , is simply the number to generate the value for which the were . The whose end in E .. correspond to the same , but for each point the value of is the scalar entropy of the output of the converter . It can be seen in Fig . that the for the without entropy is remarkably close to the theoretical value by a . More importantly , even for bit as small as , each ratio from its nominal value of by less than . For the extreme situation , the was slightly lower than , while was higher than due to the highly nonuniform of the resulting sequence . It can also be seen that the scalar entropy of the output of the in these is very close to function for a given distortion . This with the observation that the output of in an is white , see the comment at the end of Section . The difference between these is bigger for lower of , for the same reason in Section . 
 : For the an of , the along with the corresponding 
 for . However , the measured varied 
 , several higher than over 
 that range of bit . This performance degradation can be to clipping . The fact that overload become noticeable only for high bit many quantization might seem , at first , surprising . However , this phenomenon can be easily by that the size of the of the of that fall outside the dynamic range of remains approximately constant in relation to for all . This is a direct consequence of the loading factor rule . In contrast , granular quantization error is proportional to 
 which is constant in the . Therefore , the ratio between clipping and granular quantization approximately as and clipping become dominant for sufficiently high bit . 
 Because of the reduced occurrence and magnitude of clipping , the with 
 and an smaller than that of its counterpart with . Furthermore , this more conservative loading factor the converter to perform almost exactly as by our analytical expression for 
 . 
 Comparison With : The theoretical of a 
 A converter , by , can be found from 
 by making and , which 
 . For the chosen input and frequency weighting filter , and calculating as , the value of with as shown in Fig . dotted line . As seen in this figure , the gap between and , for each value of , smaller as the bit rate . This with the fact that the optimal a converter as , see Section A . It can also be seen in Fig . that the with and an improvement of over at . Equivalently , in order to obtain the same as that of at , the converter with less than . At lower bit , the improvement of the optimal is also significant . For example , the with a lower than the converter with , thus , a data rate compression of see Fig . . 
 . CONCLUSION 
 This paper studied perfect reconstruction feedback based on an additive white noise model for quantization . We have derived that relate the minimum frequency weighted and the of the scalar in the converter . We have also provided closed form for the optimal frequency of the in the converter and have derived several of optimal . In particular , we have shown that the optimal frequency response of the are unique , that the frequency weighted of an optimal are nonwhite , and that consecutive of the output sequence of the scalar are uncorrelated . We have also shown that , within our model , exponentially with ratio . 
 APPENDIX A PRELIMINARY 
 The following preliminary are necessary to prove the stated in the previous . We begin by the following definition . 
 Definition Similarly Oppositely Functionally Related : 
 We say that two are similarly functionally related there a monotonically increasing function such that , for all , and write . Similarly , if there a monotonically decreasing function such that , for all 
 , we say that and are oppositely functionally related , and write . 
 Theorem : If are similarly function 
 ally related , then 
  
 If and are oppositely functionally related , then the inequality in is reversed . In either case , equality is and therefore is almost constant . 
 Proof : We will examine the difference between the and in . We obtain 
  
 manipulation . In order to show d , we define the 
  
 Each of the above can be upper bounded as : 
  
 b 
 Inequality is due to and to the fact that . 
 Inequality from . Inequality from 
 , while from the fact that , 
 . Inequality from , while 
  
 from the fact that 
  
 , see , and from . Inequality 
 from the fact that 
 a 
 b 
 which is readily from and . Inequality from 
  
 is clearly , stable , minimum phase and such that 
  
 We now proceed to upper bound the last term in the above inequality . From and , we have that 
  
 stable and minimum phase , we obtain 
  
 It then from that 
  
 Substituting the latter into , we obtain 
  
 where the last inequality from and . Substitution of into 
  
 Since is bounded , and from b , it from that for any , one can always choose sufficiently large 
 bounded for see and see so that 
 and are small enough to yield . This the proof . 
 C . Proof of Theorem 
 Denote the squared norm of see via , and define the set of all the the same norm as 
 by . Define 
  
 It is easy to show that must belong to . From this , and since , it that 
 . Minimization of sub 
 to can be stated as the following problem : 
  
  
  
 The problem by within the category of isoperimetrical , well known in variational calculus see , e .., and . The standard solution of these is based upon the fact that any that see needs to satisfy 
 , 
 of . 
 a . e . on , where the 
  
 are such that the in are met . 
 We note that for the trivial case in which is almost constant see Definition , is also almost constant . this to constraint i in that , for this case , is such that 
 . Thus , the remainder of the proof only the in which is not almost constant . 
 In order to find , we will next discard the possible of which do not correspond to global of in . The unique function , which is with and in , will characterize the solution of Optimization Problem . 
 The Case : Fore this case , substitution of into that needs to satisfy 
  
 so that can be explicitly from . Note that cannot be zero in the above expression , otherwise would be undefined . From this , the feasible sign for , the sign before the square root , and in are 
  
 We will next show that only option the optimum . 
 Option a : We show next that any solution by option a in , say , a greater 
  
 by Theorem to the numerator of , together with and the fact that . Both are strict since is not almost constant see Theorem and Definition . 
 On the other hand . From the above , it that for all non 
 A . E . flat , the global of the associated to Option a . 
 Option : The candidate are now by and only . a to and , these take the form 
  
 Combining this result with , and considering to be not almost constant , we obtain 
 , i . e ., the optimal noise shaping 
 response magnitude in the absence of fed back quantization noise recall . This is not surprising , since taking to removing constraint i which the 
 power gain of fed back quantization noise , see Fig . . 
 We will discard this option and its associated solution showing that 
  
  
  
  
  
 stated in can not be the 
  
  
  
 directly from . This the proof . 
 D . Proof of Theorem 
 Since the , are continuously differentiable , so is . We , therefore , have that if 
  
 Or else , if we extend the support of the function by 
 , then we obtain . This would imply for all such that . Thus , belong to , the integral of over the needs to be infinite . Since ; x , this that infeasible and . 
 then , the minimizer of , needs to satisfy 
  
 We will first elaborate upon to derive . Then we will prove that . 
  
 where and are as defined in . Application of the identity , which from and a to the numerator on the of 
 . 
 Now we will prove . 
 The Sign of : Since , this limit needs to be for two possible , depending on whether or not is positive . 
 The Case For this case so we need to prove that . From Proposition 
 it that the first condition in Assumption 
 must necessarily hold in order to obtain . Thus , and its first are continuous . Therefore , in view of , we get 
 . 
 The Case . For this case , we need to prove that 
 . Rewrite as 
  
 From , it easy to see that 
 On the other hand , from d , and given that and 
 , we conclude that is bounded . Thus , is 
 bounded . From this , and that 
 , it is clear from that there a value for greater than under which is small enough to render negative . Therefore 
 The Sign of : Substitution of and 
 into 
  
  
 with 
 a 
 b 
 c 
 d 
 Direct application of Theorem to one to conclude 
 that 
  
 are continuous , we have from that 
  
 It directly from the last equation that the sign of 
  
 see . From d , the of is lower bounded as 
  
 E . Proof of Theorem 
  
 wherein a been used . This the second claim in Theorem . 
 Convexity : Differentiation of 
  
 of is a convex function , proving the first claim of the theorem . 
 : In order to show that the and in Theorem hold , we write as 
  
 We will first prove the validity of . Clearly , if for all condition i of Assumption , then the of the above equation to as 
 . If this the case , then the second condition of Assumption must be satisfied , and therefore the of 
 Proposition are met . Proposition and the fact that 
 see , it that to as . This the validity of . 
 In order to show that i . e ., , we first note from that for all . On the 
 other hand , it from d that 
  
 where . Since as by 
 Assumption , the of clearly to as . 
 Therefore 
 F . Proof of Theorem 
 In view of Theorem , it to proof the for 
 , respectively . The uniform convergence of 
 From and we have 
 H . Proof of Theorem c to one can write Since is monotonically decreasing see Theorem 
 , it that with increasing . Since , this directly to 
 , where is 
 independent of . Theorem to both sides of 
 the latter inequality , we obtain 
 . Since 
 to the minimum for a constant , by virtue of we have that . Substitution of this into the last inequality . 
 This the proof . 
  
  
 ﻿In this work we present an empirical study of the added propagation losses that may be associated with providing fixed wireless service from near-ground base-stations to homes in a suburban environment. We present results for various types of environments, classified according to the existence of obstructions in the propagation path and the choice of outdoor-outdoor or outdoor-indoor service. Our results indicate that while on average the additional path-losses associated with lowering the base antenna are relatively small, the variance of these losses will increase at near-ground level, particularly in obstructed links. This has as a result that the power margin required for high availability of a near-ground base antenna may be quite significant. 
 Index Terms—Channel modeling, fading channels, fixed wireless service, near ground propagation, path-loss measurements. 
 HE explosive growth in wireless broadband demand creates an intense interest in diverse deployment options. New players, eager to participate in this very attractive market are considering various scenarios, including suburban residential areas where services (electricity, telephone, cable TV, etc.) are being supplied from underground, without the use of lampposts. This creates the need to also evaluate the effectiveness of providing wireless service from outdoor near-ground bases. We validate the applicability of well known path-loss and small-scale fading models, which will be useful when designing such systems. To the best of our knowledge, such studies have not been reported in the open literature for this type of environment. Most published results with near-ground bases (less than 1 m height) deal with urban environments or correspond to sensor networks in open or forested spaces, as will be discussed in detail below. Suburban areas have received less attention. Furthermore, as stated in [1], [2] there is a lack of results on the characterization of outdoor-indoor wireless links, which are important for voice/data transmission and wireless local area networks. The statistical description of the variation in the channel model parameters, associated with lowering the base-station (BS) will be useful for a system designer, considering that most currently available data corresponds to service provided by bases that are at least at lamp-post height, where well established propagation models apply. The scenario that is the objective of our study differs quite significantly from those that have been reported in the literature before. Therefore the characterization of this new propagation scenario cannot be based on currently known results. 
 A thorough bibliographical search of published work related to the subject led us to classify the relevant papers into 4 categories. Each category deals with one specific aspect covered in our study: outdoor-indoor propagation, near-ground propagation, the effect of vegetation barriers, and path-loss models. We discuss this classification in what follows, stressing only those aspects that specifically relate them to our work. 
 Outdoor-Indoor Propagation: Various papers such as [3][5] and references quoted therein have addressed this subject for settings that in some cases are similar to ours. However in contrast to our study, in all of them the BS was located at heights of at least 2.75 m, while in some ([3], [4]) the altitude of the subscriber unit (SU) was varied, to altitudes not lower than 1.5 m. We note that due to the considerable difference between the scattering elements surrounding an outdoor BS and those in proximity of an indoor SU, lowering the altitude of the latter cannot be expected to have the same effect as that of lowering the BS. However some reported observations are similar to those in our work, such as a wider range of fade values for the lower altitude antennas. 
 Near-Ground Propagation: This topic has been addressed in various papers, many of which are centered on modeling electromagnetic field propagation, such as [6]-[8]. Here the scope is sensor networks in indoor scenarios with both terminals at less than a 10 cm height and link lengths below 10 m. The analytical and empirical results address very specific propagation settings, which can be treated analytically from electromagnetic principles and do not lend themselves to the derivation of statistical models. Conversely, our study encompasses a wide range of scenarios of such complexity that they cannot be treated analytically and instead require statistical models such as those described in [9], [10]. Propagation losses for sensor networks in an office environment are treated in [11], again a setting too far removed from ours to extrapolate valid conclusions. Models for outdoor sensor network propagation losses with both terminals at low altitude over diverse types of open terrains have been presented in [12] and [13]. In [14] and [15] path-losses for urban environments using low-altitude bases are reported in the context of military applications. The effect of lowering the subscriber terminal from an altitude of 28 to 3 cm is quantized in [15], however this scenario is very different from our application. Nevertheless, among the reported results is the reduction of the Ricean K-factor with antenna altitude, a result to be compared with that observed in our work. 
 Effect of Vegetation Barriers: Another element that influences the signal attenuation between base and user is vegetation. Theoretical models describing the effect of these barriers have been discussed in [16]-[21]. The effect of vegetation barriers on frequency dispersion was treated in [22] based on various simulation models. Only some of the available theoretical models for the attenuation caused by vegetation have been validated through empirical data [16]. None of them have been developed for the residential scenarios studied here, but some results relate to certain conditions present in our study. In [23] military applications are considered with bases at heights exceeding 0.75 m and links that can involve forested areas. In this work a decrease in antenna height is also reported to reduce the Ricean K-factor. Empirical studies reported in [3], [23]-[26], which involve various types of vegetation including rainforests, show excess path-losses between 10 dB and values exceeding 20 dB. 
 Path-Loss Models: With regard to the statistical description of path-loss, single and multi-slope log-distance models that consider shadow and small-scale fading have typically been used. This is also the approach chosen for this work. In the case of outdoor propagation spaces, the two-ray model is often considered as a reference [14], [27]-[32]. The presence of a break point leading to the dual-slope path-loss model has been discussed extensively [27], [28], [32]-[34]. The use of these models and the effect of lowering the BS antenna in a microcellular setting is described in [27], however the altitudes considered range between 3.2 and 13.4 m and measurements were carried out on streets rather than in outdoor-indoor settings. 
 The aforementioned papers provide a background with regard to methodology and previous theoretical and empirical results on modeling propagation links that may involve a terminal at low altitude. As discussed, they do not specifically address the problem that is the objective of our study, namely to draw conclusions regarding the effect of providing suburban outdoor-outdoor or outdoor-indoor residential wireless service from low-altitude bases, in contrast to doing so from a conventional height. We here consider both outdoor-outdoor and outdoor-indoor narrowband links, with and without the presence of vegetation. These results will be useful when planning fixed wireless service to suburban residences using narrowband channels, which may also be viewed as a few adjacent tones in an OFDM system. In such cases of course, the relation between coherence bandwidth for the specific environment and the transmission bandwidth will also need to be considered in order to take into account possible fade reduction through frequency diversity [35]. 
 We found that the additional propagation losses resulting from lowering the base antenna from 2.1 m to 0.3 m was not larger than 4 dB at the median level, but may grow to 17.5 dB when 90% availability is required. We also quantify penetration losses when serving indoor locations from the outdoor antenna, and power losses caused by vegetation barriers within the link. The latter are relevant when the lowering of the base antenna adds such an obstruction to the link. The measurement campaign was carried at 3.5 GHz using a narrowband transmitter and a power-measuring receiver. The choice of frequency is based on the fact that it has been used for fixed wireless service such as WIMAX [36]. A total of 23 different links were tested in 5 residential urban settings. For each link, the distance between base and subscriber units ranged from 5 to 40 m. Two large-scale propagation models were used as a comparison basis to our empirical model: Friis equation and the two-ray model [9]. 
 The remainder of this paper is organized as follows: Section II describes the measurement hardware and methodology. Section III presents the statistical models based on our empirical data. Finally, Section IV provides the conclusions. 
 The channel sounding system employed consisted of a synthesized continuous wave (CW) transmitter and a purpose built narrowband receiver coupled to a power meter. The receiver bandwidth is 200 kHz. This is wide enough to capture any frequency dispersion that affects the CW transmission, induced for example by movement of vegetation [37]. This receiver was connected to a rotating arm of 0.4 m length and moved stepwise under computer control in 6? increments (a displacement of approximately ?/2). At each of the 60 angular positions, 100 consecutive power measurements were made. This allowed verifying consistency and averaging to remove residual temporal fades. These were found to be very small (typically less than 0.5 dB). The resulting timeaverage was used as the power sample at that range and angular position. At each placement of the rotating arm, the 60 power measurements obtained in a rotation were used to calculate the spatially averaged path-loss at that range and to generate the statistics of small-scale fades with respect to that average. The size of the region for the spatial average is well within the shadow-fade correlation distance reported in previous work [38]-[40]. In fact the smallest values mentioned in the literature, corresponding to indoor scenarios, are in the range of 1 to 2 m [39], the values for outdoor suburban settings being much larger [38], [40]. 
 The operating frequency was 3.5 GHz. The BS and SU used dipole and patch antennas depending on the type of scenario tested. When the angular spread of arriving wavefronts in a multipath environment is comparable to the antenna beamwidth, the received power and thus the calculated pathlosses will be affected by the antenna gain pattern. For this reason, in selecting the antennas we chose elements that may be considered representative for the type of service at the specific scenario, as will be detailed later. In the case of the nominally omnidirectional antennas, we took into account in our calculations the minor gain variations (less than +/1 dB) over the 360? azimuth range through averaging of the measured antenna pattern. This is in correspondence with the fact that our measurements involve a 360? rotation. In the case where we used patch antennas, in our path-loss calculations we considered the measured gain of the antenna in the direction of the direct arrival path. All antennas were measured in an anechoic chamber, and automated measurement procedures were used to ensure repeatability. 
 Before initiating measurements at each environment, two calibration procedures were performed. First, the transmitter and receiver system were connected back to back with a short, calibrated cable and a step attenuator, bypassing the antennas, to verify the expected received power. Next, the transmitter and receiver antennas were connected to the system and a free-space calibration was performed in an open area with the antennas extended 2.1 m above ground and separated by 2 m. The results were compared to the design specifications of the equipment to confirm its proper operation and to verify the dynamic range for reliable measurements. In all field measurements the received power was at least 20 dB above the noise floor. Measurements were carried out at 5 residential type settings, representative of a suburban environment where the base-station could be placed at the street curb, at a range of 5 m to 40 m from a one or two story house. This space was occupied by gardens with shrubs and trees no higher than 5 m. We tested suburban locations in Santiago and Vin˜a del Mar, Chile, and on the campus of Universidad Santa 
 Mar´ia in Valpara´iso, which provides settings with garden-like vegetation outside of ground level laboratories. At each of these scenarios, multiple measurements were carried out for similar conditions (LOS: line-of-sight / NLOS: obstructedLOS; outdoor-outdoor / outdoor-indoor). The construction is of the brick and mortar type, with non-metalized windows of varying size as specified below. 
 All measurements were made by first placing the antenna that simulates the wireless service base-station at 2.1 m and then at 0.3 m at exactly the same spot. This allows quantifying the effect of reducing the antenna altitude. The choice of 2.1 m as reference height was based on the fact that at such altitude, non-obstructed links exhibit practically freespace path-losses at the measured distances, thus providing a convenient reference value. Further increases in height would have added complexity without providing novel information. 
 1)	LOS outdoor-outdoor: In this case, we emulate the BS terminal by placing the rotating arm (receiving the CW signal) outside the home at varying distances from the residence, with link lengths in the range 5-30 m. The SU in this case is designed to represent a customer premises terminal mounted on the wall of the residence, being serviced by the BS. The SU was placed in close proximity to an outside wall at an altitude of about 2.1 m. It was placed close to a window that would subsequently allow outdoor-indoor measurements through that same window without otherwise changing the setting. In this case, the SU used a patch antenna with a maximum gain of 8 dBi, typical for a wall-mounted element. This was placed at three different positions along the wall (for each link length tested) to measure at nominally similar 
 locations. The BS on the rotating arm, whose height was varied from 0.3 m to 2.1 m employed a 2 dBi dipole. The setup is illustrated in Fig. 1. 
 2)	LOS outdoor-indoor: The setup is shown in Fig. 2. In this case, the SU terminal was emulated by placing the rotating arm inside the construction across a window with respect to the outside BS (transmitting the CW signal). Link lengths ranged from 8 to 33 m. The height of the SU antenna was 1.5 m, at locations that would in practice correspond to the possible positions of indoor pedestrians or of repositionable user terminals. The indoor floor level X shown in Fig. 2 was between 0.15 m and 0.4 m above ground. The BS antenna was placed outside of the residence at positions adequate for base-station locations, again with altitudes of 2.1 and 0.3 m. In this case, both antennas used were dipoles. Window sizes varied in width in the range of 1.2 m to 3.2 m and in height from 1.8 m to 2.5 m. The minimum window area was 2 m2. 
 3)	NLOS outdoor-outdoor and outdoor-indoor: These measurements were performed under the same conditions as in the scenarios LOS outdoor-outdoor and LOS outdoor-indoor respectively, with the only difference that the direct path was obstructed by the foliage of one or two rows of shrubs and small trees. These were usually low, densely foliated hedges of the genus “ligustrum” (privet), while trees were typically “Aesculus hippocastanum” (horse chestnut). The thickness of the foliated obstruction was typically in the range of 0.5 m to 1 m. The link lengths tested ranged from 6 to 40 m. 
 This section presents the results of the measurements performed. We describe models for the path-loss (PL) in each of the previously described settings and we include a thorough statistical description of the effect on the channel model that is a consequence of lowering the base-station antenna. 
 1)	LOS outdoor-outdoor: We here present the average pathloss (over one receive-antenna rotation) vs. distance for all measured links of this type, at the two base-station heights considered. This is shown in Fig. 3. In each case the plot includes as a reference the free-space path-loss (Friis equation) and the path-loss obtained using the two-ray model [9]. In the latter case, two conditions for permittivity () and conductivity (?), that cover the range of values mentioned in the literature [41]-[44] have been considered. Although these dielectric parameters are frequency dependent, their change -even over bands of 100 MHz or more- will not place them outside the range of values considered for our models [44]. As seen in Fig. 3 the theoretical two-ray path-loss model is not very sensitive to the actual dielectric parameter values and thus frequency dependent effects will be negligible. The fit of empirical data to the theoretical models (free-space and tworay) is remarkably good. The dispersion of the data points with respect to the free-space model is comparable to the difference between the free-space and the two-ray model losses. It should also be mentioned that the path-losses plotted here exclude small-scale fades, which as mentioned before were averaged out. However as will subsequently be shown, for these scenarios the small-scale spatial fades were typically quite shallow (high spatial Ricean K-factor). Consequently, the total path-loss at any specific antenna position within the rotation is quite close to the average plotted here. It is also interesting to note that the measurements follow the contour of the two-ray model particularly at the ground level, even 
 though measurements were made in a variety of terrains, some of which included height irregularities of the order of one wavelength. In this setting, lowering the base antenna caused no significant change in path-loss, as will be discussed in greater detail in subsection C. It is worth noting that for both heights, the first Fresnel zone was not blocked. 
 2)	LOS outdoor-indoor: As explained in Section II, for measurements in the LOS outdoor-indoor scenario, the movable arm antenna was located inside the residence at a height of 1.5 m, emulating user positions. Several indoor placements were used, all of them possessing LOS to the outside basestation antenna through a single window. The transmitting antenna, outside of the residence, was again located at 2.1 m and at 0.3 m height. The measured path-losses at both heights are shown in Fig. 4, where we also include free-space and two-ray model path-losses as a reference. As expected, outdoor-indoor path-losses are somewhat larger than those obtained for the outdoor-outdoor case and the dispersion of values is also larger, which may be attributable to the variety of window sizes considered. There is however no evidence that lowering the base antenna will alter the path-loss model in a significant way. 
 3)	NLOS outdoor-outdoor and outdoor-indoor: Both NLOS cases considered here correspond to situations where the direct path was obstructed by one or two barriers of typical garden vegetation, basically bushes and small trees as previously mentioned. Fig. 5 shows the results for the NLOS outdoor-indoor scenario. The scarcity of data points between 11 m and 13 m is produced by the existence of a row of bushes 
 at this distance from the indoor antenna in some of the places where measurements were made. Additional vegetation existed at closer ranges from the window. Therefore, measurements at ranges in excess of 11 m were in many cases obstructed by two vegetation barriers. The results show added attenuation and larger dispersion of values than in the previous cases, a logical consequence of the greater variability of the environment. In this case, lowering the base antenna results in somewhat greater losses, as well as greater uncertainty in their values. 
 The extensively used log-normal model [10] assumes that the spatially averaged path-loss (in dB) may be described as: 
 where d denotes distance between the base-station and subscriber unit, PLdB(d0) is the free-space path-loss at a distance d0 in dB units, n is the path-loss exponent and Xs is a zero-mean Gaussian distributed random variable (in dB) with standard deviation s. We applied this model to our data after averaging out small-scale fades over one rotation of the arm that holds the antenna. As conventionally done, we chose d0 = 1 m. Variations of the model (1), using multiple slopes or choosing the intercept point PLdB(d0) different from the free-space path-loss did not provide a measurably better fit for the set of link lengths tested, therefore we only used this very simple model. 
 SUMMARY OF PATH-LOSS EXPONENT n AND STANDARD DEVIATION s FOR VARIOUS TYPES OF ENVIRONMENTS AND ANTENNA HEIGHTS 
 We generated the empirical Cumulative Distribution Function (CDF) of the differences ?P between the best-fit linear regression (1) and the measured average path-losses. The CDF of ?P would be that of a zero mean Gaussian random variable (i. e. Xs), under the assumption that the log-normal model holds. We found that in fact the fit was very good, the corresponding graphs of the CDFs not being included here to meet space constraints. The results indicate that in general lowering the base antenna will increase the standard deviation of ?P, i.e., the model path-loss prediction becomes more uncertain. Having established that the log normal model is indeed accurate for our scenarios, we summarize in Table I the relevant parameters of the model (1) for all the studied cases. 
 Our results so far have excluded small-scale spatial fades, which as stated were averaged out over the rotation of the arm with the receive antenna. Our analysis of the differences between the average path-loss and the path-losses measured at the various arm positions revealed that the corresponding histograms are very well matched by Rice/Rayleigh probability distributions. We thus use the Ricean K-factor as a metric to determine the small-scale statistics, estimated from the empirical data according to the method described in [45]. Fig. 6 shows the CDF of the observed Ricean K-factors for both heights and for the scenarios LOS outdoor-outdoor and NLOS outdoor-indoor, which provided respectively the conditions for the smallest and largest (small-scale) fade values among all scenarios tested. As can be seen, no significant differences in the small-scale fade statistics are associated with the changes in base-station altitude. As expected, small-scale spatial fades are much shallower for the LOS outdoor-outdoor case than for the far more complex NLOS outdoor-indoor environment. 
 We here present the statistics of the difference in path-loss associated with lowering the base antenna from the 2.1 m position to 0.3 m. We define this difference ?PL as 
 where PL0.3 and PL2.1 are respectively the path-losses in dB for base antenna altitudes of 0.3 and 2.1 m. These are obtained at exactly the same azimuth position of the rotating arm for both antenna heights, i. e., without any small-scale path-loss averaging. 
 Fig. 7a shows the CDF of ?PL for the cases LOS outdoor-outdoor and NLOS outdoor-indoor. As before, these correspond to the two extreme conditions observed, the remaining curves falling between the two shown. The ensemble 
 Fig. 6. CDF of Ricean K-factors at heights 0.3 m and 2.1 m for LOS outdoor-outdoor and NLOS outdoor-indoor scenarios. 
 size for our calculated CDFs ranges between 2400 and 4300 points depending on the amount of measurements in each category. As can be seen, for the LOS outdoor-outdoor case, lowering the base antenna will result in a slight decrease in path-loss when considering the median ensemble value. This is consistent with what can be observed in Fig. 3b, which shows that at some ranges the path-loss is decreased with respect to the free-space value by the constructive interference of the ground ray. There is a clear increase in the median path-loss for the more complex NLOS outdoor-indoor scenario. For the latter case, as already discussed, there is a larger variability of path-losses with respect to the mean, particularly at low altitude. The result of this can also be observed in Fig. 7a. This figure reveals that a considerable fade margin may need to be added to the link budget if adequate availability is to be preserved when lowering the base antenna. This added margin grows from about 3 dB for the LOS outdoor-outdoor case to approximately 17.5 dB for the NLOS outdoor-indoor case at the 90% probability level. 
 In many cases it may be more informative to have statistical knowledge on the increase in path-losses with respect to a well-known and deterministic reference, rather than with respect to losses at 2.1 m, which are random. Choosing as reference the free-space losses at the same distance (d), we thus obtained the statistics of the excess path-loss (EPL). This is defined as the difference in decibels between the path-loss PLH measured at height H at any given azimuth position of the receive antenna and the corresponding pathobtained from the Friis equation according to 
 Fig. 7b shows the CDF of the EPL for the LOS outdoor-outdoor and NLOS outdoor-indoor scenarios, for the two antenna heights considered. Consistent with our previous results, for the LOS outdoor-outdoor case the values of the EPL are quite low for both antenna heights. At 0.3 m the EPL is actually somewhat lower than at 2.1 m. This is consistent with the previously discussed result on the stronger constructive interference from the near-ground antenna ob- 
 (a)	CDF of the difference in path-loss ?PL for antenna heights of 0.3 m and 2.1 m. 
 served in Fig. 3b at some ranges. In contrast, for the NLOS outdoor-indoor scenario, the EPL is always larger than at 2.1 m. As can be seen, to cover 90% of users, a fade margin of 19 dB needs to be added for a base placed at 2.1 m, while for a base at 0.3 m this margin grows to 26 dB, i. e. an extra 7 dB is required to achieve the same degree of coverage. This value may appear at first sight to be inconsistent with the 17.5 dB increase observed in Fig. 7a. However we note that the statistics of path-loss increase described in Fig. 7a are the result of the difference between two random variables. The increase may occur under conditions where the EPL is high or is low. When evaluating percentage of wireless coverage or link budgets, an increase in EPL due to the lowering of the antenna may be of little consequence when the initial value of the EPL is low. In contrast, Fig. 7b shows the difference in path-losses at both antenna altitudes with respect to an absolute reference. This allows us to draw conclusions on the fade margin with respect to free-space propagation at a specific coverage level for both antenna heights. 
 We have obtained a large amount of statistical data for narrowband path-loss statistics in suburban environments, representative of fixed wireless services using near-ground base-station antennas. We observed that for unobstructed outdoor-outdoor locations, with base-antenna heights of 0.3 and 2.1 m, the propagation channel is well approximated by the two-ray or even the free-space models. Our results 
 also indicate that the increase in excess path-loss (EPL) over free-space propagation, associated with lowering the base antenna is quite modest. Typical values are 3 to 4 dB at the median level and 7 dB for 90% availability, for all scenarios tested, provided all LOS (NLOS) links remain of the same type when lowering the base antenna. However, when we consider all links of all types, the EPL will vary over a considerable range. For 90% availability, 26 dB additional losses may have to be countered for an NLOS, near-ground, outdoor-indoor link, while only 4 dB are needed if the link is of the LOS outdoor-outdoor type. For a system designer this means evaluating the tradeoff between providing services through a wall mounted outdoor antenna, with additional indoor retransmission hardware, vs. considering the need for a significant fade margin to avoid such additional elements. Both solutions also involve taking into account the associated interference problems. Extra transmission power should be managed through link adaptation techniques to limit it to the site-specific requirements, while indoor retransmission may need to be coordinated to reduce interference between neighboring homes. 
 ﻿ We derive closed form for the statistics of the spectral power gain of wide band microwave indoor . We obtain our within a framework general enough to be compatible with several popular channel , such as those by the task group , as well as the channel model . As all these , our channel description is based upon and with and random . Our consist of closed form for the statistics of the channel power frequency response , where statistical involve over ray and arrival times . We first express the auto covariance of this frequency response in closed form . We then use this result to obtain an analytical expression for the variance and moment of the channel power within any given interval of . This us to express the channel spectral diversity as a function of model and . From this function , we determine the range within which diversity scales approximately linearly with and its upper limit . 
 I . INTRODUCTION 
 Stochastic wireless channel allow one to predict the statistics of the radio propagation over an ensemble of with similar . This is specially useful in complex , heterogeneous and time , such as office , residential and industrial indoor . The the . . a task group accepted a channel model for ultra wide band indoor , based on the channel model , and similar have been in that led to the . . c standard . Like , the . . a channel model is a discrete time description of the impulse response of a wireless channel , in which path are grouped into . The arrival times of and within them follow . One exponential ray gain decay profile is defined among and another one for the in each cluster . Unlike , the fading statistics of path in the . . a channel model are log normal , not . 
 Some of the useful temporal statistics of channel like this are the power delay profile , the average delay and the delay , which allow one to determine spectral statistics such as coherence and average power gain under suitable , been extensively in the literature , . In contrast , the second order statistics of the channel power frequency response have received less 
 attention . In , it been shown that the number of significant of the covariance matrix of the random channel impulse response , and hence the diversity order of the channel , scales approximately linearly with . Such an increase in diversity , which to the reduction of the relative channel power variance as the , been to reach a saturation point , . In a recent paper , analytical for the of the channel frequency response squared magnitude , here by , as well as the variance of the power over any frequency band , have been derived for the . . a channel model , conditioned to fixed and given ray and cluster arrival times . The corresponding statistics over random arrival times were via . To the best of the knowledge , no closed form are available in the literature for second order un conditional statistics of the spectral power gain of wireless channel such as the . . a and the . 
 In this paper , we derive closed form for the second order statistics of for greater than some minimum value min rad , whose precise definition will be given later . Statistical randomness of ray an arrival times , for a generalized version of the ultra wide band microwave indoor channel model accepted by the . . a task group . We obtain these within a framework general enough to be compatible with other related channel , such the . The flexibility of our formulation it to include some of the in the . . a standard channel model for high as well . As in all these , our channel description is based upon and with , with different average power decay among and within . We first derive a closed form expression for the of . We then use this result to obtain an analytical expression for the variance and raw second order moment of the channel power gain within any given interval min . This us to estimate fade depth as a function of channel explicit . Our also predict a positive lower bound to the ratio between the variance of and its squared mean . This bound only as the arrival rate of in the model to infinity . 
 . CHANNEL MODEL 
 In this section we formulate the wireless channel model framework which will be throughout this work . As in the . . a channel model , we describe the radio propagation of indoor by a random impulse response of the form 
  
 where 
 ti ,, Ti ti ,, i ,..., ; ,..., 
 is the random arrival time of the th path in the i th cluster , Ti is the random arrival time of the i th cluster , and ti , is the random delay of the th path in the i th cluster relative to Ti . Each real valued random coefficient ai , the amplitude of the th path in cluster i . These random are formed as 
 ai , , 
 where Ai is a random variable the amplitude of the i th cluster , pi , are un binary random taking from , , and each real valued random variable ai , the amplitude or gain of the th path or ray within the i th cluster relative to that cluster amplitude . For simplicity , we do not consider here a large scale fading factor before the sum in , but its effects can be applied to our with ease . As in , , , all the random in the super set Ti ti , Ai pi , ai , are independent , except Ai from Ti , and ai , from ti ,, for all i ,. For future use , we describe the dependency of the second and of Ai and ai , on Ti and ti ,, respectively , by of the following : 
 b , E Ai Ti , b , E Ai Ti a 
 g , E am , g , E am b 
 In our formulation , the number of Ti and the number of ti , that take within any unit length time interval follow with mean and , respectively . This is also part of the channel of and . 
 By choosing the ai as distributed , for fixed and given , and deterministic Ai with the form Ai b Ti , , for some ,, , the above description the widely used model . A similar choice with log normally distributed Ai and ai the model by the . . a task group excluding its large scale fading term . The decay of the by can also be chosen to match those that the . a standard for high in some scenario such as office and industrial . Mixed , another model feature present in , can be handled within the current framework with little additional effort . 
 In , the number of and the number of within each cluster are finite . Such choice to the fact that , in practice , choosing and large enough will be sufficient to include all path component with significant power . This is equivalent to saying that for every e , there exist finite and such that 
  
 with probability close to . Clearly , this equivalence that the decay fast enough with increasing delay . We will make this requirement precise by assuming that the conditional of the satisfy 
 , a 
 , b 
 and work in the sequel with the channel impulse response 
 . 
 We will also assume that the conditional moment , b , and g decay slow enough so that , for some minimum angular frequency min rad , we can approximate 
  
 where the non negative integer ni satisfy n n , n n . Thus , our will be valid for and where the band of interest above min . In general , the for which is a good approximation will be determined by whichever of the within the integral of . Therefore , min will depend mostly on the decay time of the , the reciprocal of which should be several times smaller than min . 
 From , the squared magnitude of the channel frequency response is 
  
 Since and arrival times are random , so is . In the following we will derive analytical for the second order statistics of as well of the channel power over a given frequency interval . 
 . SECOND ORDER STATISTICS OF 
 Here we will obtain closed form for the mean and correlation coefficient of over any interval of angular , min , . To derive these , we will make use of the following technical lemma . 
 Lemma : Suppose the sequence of non negative random a process with finite arrival rate be any two such that exist and are bounded . Then 
  
 E xi a 
  
  
 E xi , , ,... b 
 i , i 
 E xi E xi . c 
 i 
 where E expectation with respect to all the random that determine the expression within the square . N 
 Proof : For each of i . i .. random uniformly distributed over ,. We have that 
 i 
 When becomes the process 
 see , e .. . Thus , 
 E xi lim 
 which a . The validity of b and b is shown in a similar manner . 
 value of From and , it is easy to verify that 
 E . 
 and the independence between stated in the previous section , we obtain 
 E 
 Thus , the value of at any min is the product of the average energy of the cluster Ai and the ray relative ai ,. Notice also that E ai ti the average power profile of in a cluster with amplitude Ai . For this reason , it will be to as the the cluster . Similarly , E Ai Ti the average power density of cluster . Accordingly , we will call the inter cluster power delay profile . 
 of 
 Denote the auto covariance of by , , E 
  
  
  
 E E . 
 For notational simplicity , we will temporarily adopt a single indexing nomenclature for the path arrival times ti ,. More precisely , and with a slight abuse of notation , we define the infinite random set For our , it will not be necessary to define a between the i , and the index . Indeed , it will be sufficient to note that the double index matching condition 
 the single index , , i is equivalent . 
 single index notation , the first term on the right hand side of is from , which 
 E 
 E e 
 l ,,, 
 E al , 
  
 from Lemma , , the fact that 
 , and the independence of the relative 
 ray arrival times ti ,, which imply that the in the sum of are zero for the ,. ,,, ,, and ,, r 
 with double indexing notation we obtain 
 E 
 On the other hand , from , 
  
 E E E ai , m E , n . 
 this from , , and the independence between Ai , ai ,, Ti and ti , in the previous section , we obtain 
  
 c , i E ai , E ai , 
  
 i E Ai E Ai E ai , m E ai , n 
  
  
 E Ai E ai , m e ti , 
 i 
  
 It can already be seen from that the the sum of a frequency independent term and a term which only on the frequency difference . From , the latter term as this frequency difference to infinity , so because of the first term , the between two infinitely distant frequency is greater than zero . At first sight this may seem . Nevertheless , such behaviour also been found in , and is consistent with the saturation of the channel diversity order as in , . As will be in Remark below , such behaviour is due to the fact that the arrival for and its path are finite . Indeed , as the path environment becomes , i . e ., the number of path component , the constant term to zero . 
 Before the last step in our derivation of a simpler closed form expression for , , we define 
 . 
 With these , and Lemma , we obtain that the of is given by 
  
 where 
 K , a 
 b 
 and and denote the of 
 and , respectively . Notice in this expression that only the K and K depend on the conditional fourth order of the . Therefore , for given inter and cluster , the of ray and cluster will modify the frequency independent K and K . Nevertheless , such change would have no effect on the frequency dependency of the spectral power . 
 Remark : Dense impulse If we keep the average energy of the the cluster and impulse constant while increasing the cluster arrival rate to infinity , then the frequency independent in and vanish . To see this , notice that the cluster impulse response energy is given by 
 E 
 where Lemma been used . Thus , if we keep this average energy constant and increase the density by a factor , then the have to be divided by . By uniformly scaling the Ai to yield such reduction , the resulting function b is divided by . Therefore , the integral of b in the form b , which clearly to zero as . 
 From this analysis it that the of when the impulse response becomes infinitely dense , say , the simple form 
 . 
 That is , in this case is the product of the squared magnitude of the inter cluster and the cluster at . 
 C . Correlation Coefficient of 
 The correlation coefficient of , by , , is given by 
  
 From and , becomes 
 . 
 . SECOND ORDER STATISTICS OVER A FREQUENCY 
 BAND 
 In this section we will derive closed form for the mean and variance of the channel power over any band 
 B , , min , , 
 where and are in rad . 
 Value of the Channel Power Over a Frequency Band : Denote the channel power over the frequency . Given the fact that E is constant for all min see , it from that 
 E , 
 where 
 B , . 
 A . Variance of the Channel Power Over a Frequency Band 
 The variance of the channel power over a band , , min , can be directly from the of as : 
  
 With the change of , u and , the latter becomes 
  
 Substituting into , 
  
 The fluctuation of the received power over a wireless where ,, as defined in . 
 channel its fade statistics . The fact that fade depth with channel been in the literature , both from empirical data see , e .., as well as . However , a closed form formula fade depth and the classical channel model used here , to the best of our knowledge , not been to date . The derived above allow us to easily deal with this issue , as seen in the example of the next section . 
 V . EXAMPLE 
 In this section we will illustrate the application of the previous , our framework to the classical model . In this model , the ai , are distributed and the Ai are deterministic exponentially . The decay profile of the ray is given by 
 Ai Ti e G 
 E ai , m ti e a 
 b 
 where is the average power gain of a ray at the beginning of a cluster at Ti . Since the fourth moment E x of a distributed random related to its second order moment as E E , we have that 
 E c 
 E ai , m ti , g e t d 
 With these , 
 a 
 and the squared of the of and are 
  
 In order to find the of for this example , we substitute into , 
  
 The variance of is directly by , which the previous expression for 
  
 From this , and , the correlation coefficient 
 . 
 Thus , again it can be that , as in Remark , the lower asymptote of the spectral power correlation only when the cluster arrival rate as the mean channel power is kept constant . 
 B . Second Order Statistics Over a Band The variance of the channel power over a band 
 min , rad is by substituting into , and then the in . This 
  
 On the other hand , from , E . Thus , the channel power variance for this example is 
  
 B 
 E 
 Equation several interesting about the channel power variance , which are below : 
 The channel power variance is the sum of three . The first one to K , which on all the except the . It is inversely proportional to the product , which is in turn proportional to the average number of with significant in the channel impulse response . This term is the lower asymptote of E when , and the latter limit to total channel power variance divided by its squared mean . Again , in agreement with what was shown in Remark , this term only when the impulse response becomes infinitely dense with . From the viewpoint of , this is by the fact that , with a finite number of with random , the law of large does not apply to the total impulse response energy , and hence its variance to squared mean ratio is not zero . 
 The second term on the of all the other , except , which it does 
 Although also the right hand side of vanish , grow unbounded would violate the under which the above are valid see and . 
  
 Figure . coefficient for with from the . . a . are over . channel . p . The analytical plot was and . 
 not change with the arrival rate of within . This term is monotonically decreasing in and , approaching when , and when . It also when the cluster arrival rate to infinity . 
 The third term only the and inter cluster power decay . In particular , it is independent of cluster and ray arrival . It is monotonically decreasing with the , tending to zero when to infinity , and to when . 
 It can be seen that the channel power variance over a band unbounded as . This is due to the fact that , when , the channel power is close to zero with increasing probability . 
 To see how this tendency can make the ratio E go to infinity , consider a simpler exam 
 B B 
 . Let the discrete random the with with probability . Then E , which clearly goes to infinity as . 
 The narrow band channel power variance , P E P is from by . The ratio between the of power of a and a system , i . e ., P E P E can be as a measure of frequency diversity order . From , such diversity order would range from when to , when . For » , the latter limit is approximately , which is roughly twice the average number of that fall within . It can also be seen from that before reaching this limit , and for » , the diversity order approximately linearly with , consistent with , . 
 The coefficient for with the of the . . a is plotted in Figure , both from and from our . The of the are : . , . , , and . . It can be seen from this figure that from channel agree well with . In particular , w , the . threshold for p between and p , as by , and close to its lower asymptotic value when p . 
 . 
 We derived general that characterize the statistics of the frequency response power gain in microwave indoor . Our are applicable to several well established channel in the literature . In particular , the closed form formula here for the of the squared frequency response magnitude of the channel one to predict the variance of the channel power over any frequency band . This an approximation to the diversity order of wide band over in such . Our also allow one to obtain an upper bound for this measure of diversity , and show how and why this limit from a finite number of in the channel impulse response . 
  
 ﻿Abstract— We prove achievability of the recently characterized quadratic Gaussian rate-distortion function (RDF) subject to the constraint that the distortion is uncorrelated to the source. This result is based on shaped dithered lattice quantization in the limit as the lattice dimension tends to infinity and holds for all positive distortions. It turns out that this uncorrelated distortion RDF can be realized causally. This feature, which stands in contrast to Shannon’s RDF, is illustrated by causal transform coding. Moreover, we prove that by using feedback noise shaping the uncorrelated distortion RDF can be achieved causally and with memoryless entropy coding. Whilst achievability relies upon infinite dimensional quantizers, we prove that the rate loss incurred in the finite dimensional case can be upper-bounded by the space filling loss of the quantizer and, thus, is at most 0.254 bit/dimension. 
 Shannon’s rate-distortion function R(D) for a stationary zero-mean Gaussian source X with memory and under the MSE fidelity criterion can be written in a parametric form (the reverse water-filling solution) [1] 
 (1a)(1b)where SX(?) denotes the power spectral density (PSD) of X and the distortion PSD SZ(?) is given bySZ(?) =?,	if SX(?) > ?	(1c) 
 SX(?),	otherwise. The water level ? is chosen such that the distortion constraint (1b) is satisfied. 
 It is well known that in order to achieve Shannon’s RDF in the quadratic Gaussian case, the distortion must be independent of the output. This clearly implies that the distortion must be correlated to the source. 
 Interestingly, many well known source coding schemes actually lead, by construction, to source-uncorrelated distortions. In particular, this is the case when the source coder satisfies the following two conditions: a) The linear processing stages (if any) achieve perfect reconstruction (PR) in the absence of quantization; b) the quantization error is uncorrelated to the source. The first condition is typically satisfied by PR filterbanks [2], transform coders [3] and feedback quantizers [4]. The second condition is met when subtractive (and often when non-subtractive) dither quantizers are employed [5]. Thus, any PR scheme using, for example, subtractively dithered quantization, leads to source-uncorrelated distortions. 
 An important fundamental question, which was raised by the authors in a recent paper [6], is: “What is the impact on Shannon’s rate-distortion function, when we further impose the constraint that the end-to-end distortion must be uncorrelated to the input?” 
 In [6], we formalized the notion of R?(D), which is the quadratic rate-distortion function subject to the constraint that the distortion is uncorrelated to the input. For a Gaussian source X ? RN, we defined R?(D) as [6] 
 N1 tr(KY-X)=D,N1 |KY-X|N >0 where the notation KX denotes the covariance matrix of X and |·| refers to the determinant. For zero mean Gaussian stationary sources, we showed in [6] that the above minimum (in the limit when N ? 8) satisfies the following equations: 
 is the PSD of the optimal distortion, which needs to be Gaussian. Notice that here the parameter a (akin to ? in (1)) does not represent a “water level”. Indeed, unless X is white, the PSD of the optimal distortion for R?(D) is not white, for all D > 0. 1 
 In the present paper we prove achievability of R?(D) by constructing coding schemes based on dithered lattice quantization, which, in the limit as the quantizer dimension approaches infinity, are able to achieve R?(D) for any positive D. We also show that R?(D) can be realized causally, i.e., that for all Gaussian sources and for all positive distortions one can build forward test channels that realize R?(D) without using non-causal filters. This is contrary to the case of Shannon’s rate distortion function R(D), where at least one of the filters 
 1Other similarities and differences between R?(D) and Shannon’s R(D) are discussed in [6]. 
 of the forward test channel that realizes R(D) needs to be non-causal [1]. To further illustrate the causality of R?(D), we present a causal transform coding architecture that realizes it. We also show that the use of feedback noise-shaping allows one to achieve R?(D) with memoryless entropy coding. This parallels a recent result by Zamir, Kochman and Erez for R(D) [7]. We conclude the paper by showing that, in all the discussed architectures, the rate-loss (with respect to R?(D)) when using a finite-dimensional quantizer can be upper bounded by the space-filling loss of the quantizer. Thus, for any Gaussian source with memory, by using noiseshaping and scalar dithered quantization, the scalar entropy (conditioned to the dither) of the quantized output exceeds R?(D) by at most 0.254 bit/dimension. 
 A randomized lattice quantizer is a lattice quantizer with subtractive dither ?, followed by entropy encoding. The dither ? ~ U(V0) is uniformly distributed over a Voronoi cell V0 of the lattice quantizer.Due to the dither, the quantization error is truly independent of the input. Furthermore, it was shown in [8] that the coding rate of the quantizer, i.e. 
 can be written as the mutual information between the input and the output of an additive noise channel Y ' = X+E', where E' denotes the channel’s additive noise and is distributed as -?. More precisely, RQN = N1 I(X;Y ') = N1 I(X;X + E') and the quadratic distortion per dimension is given by N1 EkY ' - Xk2 = N1 EkE'k2. 
 It has furthermore been shown that when ? is white there exists a sequence of lattice quantizers {QN} where the quantization error (and therefore also the dither) tends to be approximately Gaussian distributed (in the divergence sense) for large N. Specifically, let E' have a probability distribution (PDF) fE', and let be Gaussian distributed with the same mean and covariance as E'. Then limN?8 N1 D(fE'(e)kfEG' (e)) = 
 0 with a convergence rate of log(NN) if the sequence {QN} is chosen appropriately [9]. 
 In the next section we will be interested in the case where the dither is not necessarily white. By shaping the Voronoi cells of a lattice quantizer whose dither ? is white, we also shape ?, obtaining a colored dither ?'. This situation was considered in detail in [9] from where we obtain the following lemma (which was proven in [9] but not put into a lemma). 
 Lemma 1: Let E ~ U(V0) be white, i.e. E is uniformly distributed over the Voronoi cell V0 of the lattice quantizer 
 denotes the shaped Voronoi cell and M is some invertible linear transformation. Denote the covariance of E' by KE' = MMTo. Similarly, let EG ~ N(0,KEG) having covariance matrix KEG = KE and let 
 Proof: The divergence is invariant to invertible transformations since h(E') = h(E) + log2(|M|). 
 The simplest forward channel that realizes R?(D) is shown in Fig. 1. According to (3), all that is needed for the mutual information per dimension between X and Y to equal R?(D) is that Z be Gaussian with PSD equal to the right hand side (RHS) of (3b). 
 In view of the asymptotic properties of randomized lattice quantizers discussed in Section II, the achievability of R?(D) can be shown by replacing the test channel of Fig.1 by an adequately shaped N-dimensional randomized lattice quantizer  and then letting N ? 8. In order to establish this result, 
 Lemma 2: Let X, X', Z and Z' be mutually independent random vectors. Let X' and Z' be arbitrarily distributed, and let X and Z be Gaussian having the same mean and covariance as X' and Z', respectively. Then 
 Theorem 1: For a source X being an infinite length Gaussian random vector with zero mean, R?(D) is achievable. 
 Proof: Let X(N) be the sub-vector containing the first N elements of X. For a fixed distortion D = tr(KZ(N))/N, the average mutual information per dimension N1 I(X(N);X(N)+ Z(N)) is minimized when X(N) and Z(N) are jointly Gaussian and 
 see [6]. Let the N-dimensional shaped randomized lattice quantizer Q'N be such that the dither is distributed as , with KE'(N) = KZ(N). It follows that the coding rate of the quantizer is given by RQN = 
 N1 I(X(N);X(N) + E'(N)). The rate loss due to using QN to quantize X(N) is given by 
 where fEG' (N) is the PDF of the Gaussian random vector EG' (N), independent of E'(N) and X(N), and having the same first and second order statistics as E'(N). In (8), inequality (a) follows directly from Lemma 2, since the use of subtractive dither yields the error E'(N) independent of X(N). 
 To complete the proof, we invoke Lemma 1, which guarantees that the RHS of (8) vanishes as N ? 8. 
 Remark 1: 1) For zero mean stationary Gaussian random sources, R?(D) is achieved by taking X in Theorem 1 to be the complete input process. For this case, as shown in [6], the Fourier transform of the autocorrelation function of Z(N) tends to the RHS of (3b). 
 For vector processes, the achievability of R?(D) follows by building X in Theorem 1 from the concatenation of infinitely many consecutive vectors. 
 Note that if one has an infinite number of parallel scalar random processes, R?(D) can be achieved causally by forming X in Theorem 1 from the k-th sample of each of the processes and using entropy coding after Q. 
 The fact that R?(D) can be realized causally is further illustrated in the following section. 
 We will next show that for a Gaussian random vector X ? RN with positive definite covariance matrix KX, R?(D) can be realized by causal transform coding [11], [12]. A typical transform coding architecture is shown in Fig. 2. In this figure, T is an N×N matrix, and W is a Gaussian vector, independent of X, with covariance matrix KW = sW  I. The system clearly satisfies the perfect reconstruction condition Y = X + T-1W. The reconstruction error is the Gaussian random vector Z , Y - X, and the MSE is D = N1 tr{KZ}, 
 By restricting T to be lower triangular, the transform coder in Fig. 2 becomes causal, in the sense that ?k ? {1,..,N}, the k-th elements of U and Uˆ can be determined using just the first k elements of X and the k-th element of W. 
 Since T-1 is lower triangular, (9) is the Cholesky decomposition of KZ?/sW2 , which always exists.2 Thus, R?(D) can be realized by causal transform coding. 
 In practice, transform coders are implemented by replacing the (vector) AWGN channel Uˆ = V + W by a quantizer 
 (or several quantizers) followed by entropy coding. The latter process is simplified if the quantized outputs are independent. When using quantizers with subtractive dither, this can be shown to be equivalent to having 
 N1 I(U;Uˆ) in the transform coder when using the AWGN channel. Notice that, since T in (9) is invertible, the mutual information per dimension is also equal to R?(D). By the chain rule of mutual information we have 
 with equality iff the elements of Uˆ are mutually independent. If Uˆ is Gaussian, this is equivalent to KUˆ being diagonal. Clearly, this cannot be obtained with the architecture shown in Fig. 2 using causal matrices (while at the same time satisfying (9)). However, it can be achieved by using error feedback, as we show next. 
 Consider the scheme shown in Fig. 3, where A ? RN×N is lower triangular and F ? RN×N is strictly lower triangular. Again, a sufficient and necessary condition to have 
 for some diagonal matrix D with positive elements. If we substitute the Cholesky factorization KZ? = LLT into (12), we obtain (I - F)(I - F)T = ALLTAT/sW2 , and thus 
 Thus, there exist  A and F satisfying (12) and (13). Substitution of (14) into (15) yields D = A(KX + KZ?)AT, and log|D| = 2log|A| + log|Kx + KZ?|. From (12) and the fact that |I - F| = 1 it follows that |A|2 = sW2 /|KZ?|, and therefore 
 We have seen that the use of error feedback allows one to make the average scalar mutual information between the input and output of each AWGN channel in the transform domain equal to R?(D). In the following section we show how this result can be extended to stationary Gaussian processes. 
 In this section we show that, for any colored stationary Gaussian stationary source and for any positive distortion, R?(D) can be realized by noise shaping, and that R?(D) is achievable using memory-less entropy coding. 
 The fact that R?(D) can be realized by the additive colored Gaussian noise test channel of Fig. 1 suggests that R?(D) could also be achieved by an additive white Gaussian noise (AWGN) channel embedded in a noise-shaping feedback loop, see Fig. 4. In this figure, {Xk} is a Gaussian stationary process with PSD Sx(ej?). The filters A(z) and F(z) are LTI. The AWGN channel is situated between V and Uˆ, where white Gaussian noise {Wk}, independent of {Xk}, is added. The reconstructed signal Y is obtained by passing Uˆ through the filter A(z)-1, yielding the reconstruction error Zk = Yk-Xk. 
 Fig. 4: Test channel built by embedding the AWGN channel Uˆk = Vk + Wk in a noise feedback loop. 
 The following theorem states that, for this scheme, the scalar mutual information across the AWGN channel can actually equal R?(D = sZ2 ). 
 Theorem 2: Consider the scheme in Fig. 4. Let {Xk}, {Wk} be independent stationary Gaussian random processes. Suppose that the differential entropy rate of {Xk} is bounded, and that {Wk} is white. Then, for every D > 0, there exist causal and stable filters A(z), A(z)-1 and F(z) such that 
 Proof: Consider all possible choices of the filters A(z) and F(z) such that the obtained sequence {Uˆk} is white, i.e., such that. From Fig. 4, this is 
 On the other hand, since is Gaussian, a necessary and sufficient condition in order to achieve is that 
 are bounded and positive for all ? ? [-p,p], and that a bounded differential entropy rate of {Xk} implies that 
 . From the Paley-Wiener criterion [15] (see also, e.g., [16]), this implies that (1 - F(z)), A(z) and A(z)-1 can be chosen to be stable and causal. Furthermore, recall that for any fixed D > 0, the corresponding value of a is unique (see [6]), and thus fixed. Since the variance sW2 is also fixed, it follows that each frequency response magnitude  that satisfies (22a) can be associated to a unique value of . Since F(z) is strictly causal and stable, the minimum value of the variance  is achieved when 
 i.e., if 1 - F(z) has no zeros outside the unit circle (equivalently, if 1 - F(z) is minimum phase), see, e.g., [17]. If we choose in (22a) a filter F(z) that satisfies (23), and then we take the logarithm and integrate both sides of (22a), we obtain 
 where (a) follows from the Gaussianity of Wk and Uˆk, and (b) from the fact that Wk is independent of Vk (since F is strictly causal). This completes the proof. Alternatively, 
 In (a), equality is achieved iff the right hand side of (19) equals (22a), i.e., if Z has the optimal PSD. Equality (b) holds because  8, which follows from (22b). The fact that is stationary has been used in (c), wherein equality is achieved iff |1 - F| is minimum phase, i.e., if (23) holds. Equality in (d) holds if an only if the elements of {Uˆk} are independent, which, from the Gaussianity of {Uˆk}, is equivalent to (18). Finally, (e) stems from the fact that Wk is independent of Vk. Notice that the key to the proof of Theorem 2 relies on knowing a priori the PSD of the end to end distortion required to realize R?(D). Indeed, one could also use this fact to realize R?(D) by embedding the AWGN in a DPCM feedback loop, and then following a reasoning similar to that in [7]. 
 In order to achieve R?(D) by using a quantizer instead of an AWGN channel, one would require the quantization errors to be Gaussian. This cannot be achieved with scalar quantizers. However, as we have seen in II, dithered lattice quantizers are able to yield quantization errors approximately Gaussian as the lattice dimension tends to infinity. The sequential (causal) nature of the feedback architecture does not immediately allow for the possibility of using vector quantizers. However, if several sources are to be processed simultaneously, we can overcome this difficulty by using an idea suggested in [7] where the sources are processed in parallel by separate feedback quantizers. The feedback quantizers are operating independently of each other except that their scalar quantizers are replaced by a single vector quantizer. If the number of parallel sources is large, then the vector quantizer guarantees that the marginal distributions of the individual components of the quantized vectors becomes approximately Gaussian distributed. Thus, due to the dithering within the vector quantizer, each feedback quantizer observes a sequence of i.i.d. Gaussian quantization noises. Furthermore, the effective coding rate (per source) is that of a high dimensional entropy constrained dithered quantizer (per dimension). 
 ﻿In this paper, we study a 3-node asymmetric fullduplex AWGN two-way relay channel with relay private messages where two nodes can communicate with each other only through a third node called the relay, while the relay can itself exchange private messages with two other nodes. Cut-set upper bound and single sided genie upper bound are presented for this channel. To obtain an achievable rate, we use a superposition of nested lattice codes and random Gaussian codes for encoding at the senders, and successive interference cancelation for decoding at the receivers. It is shown that capacities are achieved within constant gaps 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie for restricted and non-restricted models, respectively. 
 Relaying has the potential to obtain higher coverage extension and throughput enhancement at lower cost. One of the important research topics in this area is two-way relay channel (TRC) where two nodes can exchange information via a relay [1]-[4]. It is commonly used as a building block in network coding problems. It was shown in [1], that by using joint decode-and-forward and network coding, nearly optimal throughput can be accessible for the broadcast phase but it causes loss in multiplexing as expressed in [2]. In [3], it was shown that compress-and-forward scheme achieves rates within half bit of the capacity region in the Gaussian setting. 
 In another powerful method, called compute-and-forward, the relay decodes linear equations of the transmitted messages instead of decoding each of them completely. Users, by knowing their own messages can extract the desired messages by receiving linear equations. This method can exploit interference for cooperative gains as well as noise suppression [5]. Based on this concept, in [4], an achievable scheme based on nested lattice codes was proposed for the asymmetric Gaussian TRC, which was shown to achieve the cut-set bound within 1/2 bit. In [6], by using nested lattice codes, the sum capacity of Y-channel is achieved within additive gap of 2 bits/sec/Hz and multiplicative gap of 4 bits/sec/Hz of single sided genie upper bound. In [7], by using a superposition of lattice codes and random Gaussian codes, the capacity region of two-pair bidirectional Gaussian relay network to within 3 bits/sec/Hz per user is characterized. 
 In this paper, we consider an asymmetric full-duplex AWGN TRC with relay private messages where two user nodes can communicate with each other through a third node called the 
 relay. The relay has its own private messages to be delivered to two other nodes, and also some parts of users messages are to be used only at the relay. For simplicity, we use the term TRPC to refer to this model, in the sequel. There is no direct link between the users. Forward and backward channels between the relay and users are assumed to be asymmetric. We consider both restricted and non-restricted models for this channel. This situation resembles the case where a base station relays messages between users and delivers messages between the backbone system and the users [8]. In [8], the deterministic capacity of TRPC was obtained and was shown to be coincided with single sided genie upper bound. Here we consider Gaussian TRPC and obtain two upper bounds for this channel based on the cut-set upper bound, and the single sided genie upper bound. 
 Also we present an achievable rate for this channel based on the schemes proposed in [4] and [7]. In uplink, nested lattice codes are used for messages that must be exchanged between the users while superimposed Gaussian random codes are used for messages that must be decoded at the relay (private parts). In downlink, structured binning and successive interference cancelation are used at the decoders. By examining the proposed achievable rate region, we show that for all channel gains, it achieves within 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie upper bound for restricted and non-restricted TRPC, respectively. 
 The paper is organized as follows: In section II, system model and some preliminaries are reviewed. The upper bounds and achievable rates for Gaussian TRPC are proposed in sections III and IV, respectively. In section V, capacity gap calculation is characterized. Some concluding remarks are provided in section VI. 
 Consider TRPC as shown in Fig. 1. All nodes are assumed to be full-duplex, and there is a forward and a backward AWGN channel between each node and the relay. Let f+(.) = max(f(.),0), C(x) = 1/2(log2(1 + x)), S = {1,2,r}, and Si = S\{i}, i ? S, (i.e. S excluding i). 
 Node i ? S has messages mij ? Mij = [1,2nRij] to be delivered to the nodes j ? Si, where Rij ? R+ (positive real set). The messages are assumed to be independent. The messages of node i ? S are encoded into a codeword xni using an encoding function. 
 where, xik, the kth symbol of xni , is a realization of a real random variable Xik, satisfying the power constraint 
 received symbols at node i. This is called non-restricted encoding which can result some dependency between the signals transmitted by different users. In contrast, restricted encoding uses the encoding function 
 The received signal at node i ? S at time instant k are given by the following equations. 
 where hij denotes the channel gain between nodes i,j ? S, (i 6= j). zik is a realization of i.i.d. Gaussian noise {Zi}i?S ~ N(0,1) with zero-mean and unit-variance. Each node i uses a decoding function gi to decode {mji}j?Si as, 
 a vector R = ({Rij}i,j?S,i6=j) and the sum-rate is denoted by Rsum = Pi,j?S,i6=j Rij. The messages Mij, encoding functions fik or fi (restricted), and decoding functions gi define a code (R,n) for TRPC. A decoding error happens if {mˆ ij 6= mij}i,j?S,i6=j. A rate tuple R   is said to be achievable if there exists a sequence of (R,n) codes with an average error probability that approaches zero as n increases. The set of all achievable rate tuples denotes the capacity region C of the TRPC. The sum-capacity is the maximum achievable sum-rate given by Csum = maxR?C Rsum. The degree of freedom (DoF) is defined as 
 SNR?8 0.5log(SNR) P?8 0.5log(P) where SNR denotes the ratio of available transmit power P to noise power which is one. 
 Next, we review some preliminaries on lattice coding that will be used in the sequel. For more details see [4] and the references therein. 
 A nested lattice code is defined in terms of two ndimensional lattices   and ?n, which form a lattice partition ?nC/?n, i.e., ?n ? ?nC. The nested lattice code is a lattice code which uses ?nC as codewords and the Voronoi region of ?n as a shaping region. For ?nC/?n, the set of coset leaders is defined as C = {?nC mod ?n}. 
 In [4, Theorem 1], an achievable rate was proposed for TRC, where two nested lattice codes were designed, one for each source node. The theorem is as follows, 
 Theorem 1: For an AWGN TRC, with power constraints P1 = P2 on the transmitted signals of the users, and PR on the transmitted signals of the relay. The following region is achieved, 
 where , and , denote variances of zero-mean AWGN at the relay and users 1 and 2. The achievable rate region in Theorem 1 is within 1/2 bit of the cut-set upper bound, regardless of channel parameters. 
 Remark 1: Note that, in the downlink, although the channel setting is broadcast, nodes 1 and 2 achieve their point-to-point channel capacities, (second terms of the two expressions in Theorem 1), without being affected by each other. This is because of the side information on the transmitted message at each node and the binning of message. For the complete proof see [4, Theorem 1]. 
 In this section, we present cut-set upper bound and single sided genie upper bound for both non-restricted and restricted Gaussian TRPC. 
 The cut-set bounds provide upper bounds from a set T ? S to its complement Tc ? S. The cut-set bounds for the TRPC are provided in the following theorem. 
 Theorem 2: The capacity region of non-restricted Gaussian TRPC is upper bounded by 
 Corollary 1: The sum-capacity of non-restricted Gaussian TRPC, Rsum, is upper bounded by min{C(|hr2|2P) + C(|hr1|2P) + C{(|h1r| + |h2r|)2P}, 
 C(|h1r|2P) + C(|h2r|2P) + C{(|hr1|2 + |hr2|2)P} (12) and DoF constraint satisfy d = 3. 
 Proof: By summing (8)-(10), the first term of (12) is obtained. By summing (6), (7), and (11), the second term of 
 According to the general explanations presented in [8, Sec. V], in the traditional cut set bounds [9, Thm. 15.10.1], the nodes are divided into two sets T and Tc which represent the transmitting and receiving nodes, respectively. Nodes in each of these sets are assumed to fully cooperate by sharing their side information. Applying the traditional cut set bound to relay networks gives loose results. In contrast, in the single sided genie bound, not all the nodes in a set can share their side information. This method was used in the proof of the following theorem. 
 Proof: Single sided genie approach is used to bound the last two sum-rates as follows: To obtain the bound for (Rr1+ Rr2+R21) in (15), we assume that a genie transfers 
 to node   to node 2. To obtain the bound for (Rr1 +Rr2 +R21) in (15), we assume that a genie transfers   to node 1 and   to node 2. To obtain the bound for (Rr1 + Rr2 + R12) in (16), we assume that a genie transfers   to node 1 and   to node 2. To obtain the bound for 
 (Rr1 + Rr2 + R21) in (16), we assume that a genie transfers   to node 1 and   to 
 The Proof of some parts are presented in Appendix A. The whole strategy is similar to what was done in [6] for Y channel. The differences exist in the used side information which is resulted from the differences between the nature of two networks. In Y-channel, the senders are located in the same symmetric situation around the relay and the relay doesn’t have any private message. But in TRPC, the relay also acts as a sender node and is in a different situation in comparison to the other two senders. 
 Rsum = C{(|hr1|2 +|hr2|2)P}+C{(|h1r|+|h2r|)2P} (17) and DoF constraint satisfy d = 2. 
 Proof: By summing (15)-(16), the first term of (12) is obtained. Also by applying (5) to (17), d = 2 is obtained. 
 In this section, we consider restricted Gaussian TRPC which uses the encoding function (2) instead of (1). In this model the transmit signals are independent which results to a tighter upper bound. Corollary 1 and 2, and Theorems 2 and 3 are true for this model except that the term (C{(|h1r|+|h2r|)2P}) should be replaced with C{(|h1r|2 + |h2r|2)P} in (8), (12), (15), (17). 
 In this section, we present an achievable rate for Gaussian TRPC. In our scheme, we use (2), so it is an achievable rate for both restricted and non-restricted Gaussian TRPC. 
 The scheme uses the superposition of nested lattice codes and Gaussian random codes for encoding at the senders, and successive interference cancelation for decoding at the receivers. It can be considered as a mixture of the proposed method for TRC in [4], and the proposed method for twopair bidirectional Gaussian relay network in [7]. In encoding strategy, each user expresses its message into two parts, one for decoding at the relay and one for decoding at the other user. It uses Gaussian random codes for encoding the first part and nested lattice codes for encoding the second part which is not necessary to be decoded by the relay. 
 We should mention that there is a difference between this method and the one used in [7] in which no private messages are assumed by the relay. In fact in [7], if we have e.g., R12 = R21, user 1 splits its message into a Gaussian codeword and a lattice codeword, while user 2 only transmits a lattice codeword. But we believe that by using Theorem 1, it is not necessary to consider Gaussian random codes for encoding the part of the message of user 1 with rate R12 -R21. All can be decoded by nested lattice codes. 
 In the next step, the relay decodes Gaussian random code, and the linear equation of two lattice codes, with successive interference cancellation. Then, the relay maps its own private messages intended for each user and the linear equation of two lattice codes, each into a random Gaussian codeword, and sends the weighted superposition of them to two users. The last step is decoding at the users, where each user first decodes the undesired codewords that have larger weights than the desired codewords. Thus, those codewords are decoded and successively canceled from the received signal one by one. 
 For the limited space of the paper, in the following, we only consider the case where the rates of relay private messages are greater than the rate of the other messages. 
 According to the described strategy, The transmit signals at the nodes are given by, 
 Gaussian codebook of size 2nR1r, 2nR2r, and 2nRri, for i ? [1,3], respectively.   and  are nested lattice coded ensembles with sizes 2nR12 and 2nR21 respectively. 
 The relay first decodes the Gaussian x1r, x2r, simultaneously, then the lattice point . It can be done successfully so long as, 
 from codebooks of size 2nRri, for i ? [1,3], respectively, with Rr3 = max(R12,R21). 
 We consider the case |hr2| = |hr1|. Other case can be solved by the same interpretation. The decoding at the users can be done by successive interference cancelation as follows: Decoding at user 2: It can be done successfully so long as, 
 Decoding at user 1: First user 1 decodes  , then by using Theorem 1, part t can be decoded. It can be done successfully so long as, 
 In next section, we show that by choosing power coefficient appropriately any rate tuple within a constant of the upper bound is achievable. 
 In this section, we establish the capacity gap calculation for Gaussian TRPC from single sided genie upper bound. Since the gap between the corresponding upper bounds of restricted and non-restricted Gaussian TRPC is within 1/2 bits/sec/Hz, we only characterize the capacity gap calculation for restricted Gaussian TRPC in the following lemma. The proof of this conjecture is shown based on the conclusions in section IV, via the following expressions: 
 Lemma 1: By using the described strategy, for any rate tuple (r12,r21,r1r,r2r) satisfying 
 there exists a choice of power assignments {aij}i,j?S,i6=j such that decoding of codewords with rates {Rij = rij}i,j?S,i6=j can be done with arbitrary small error probability for all channel gain coefficients. (See Appendix B for the proof.) 
 Note that if (R > 1/3I), where I donotes the unit vector, satisfies (13)-(16), then the rate-tuple (r = R - 1/3) satisfies the conditions of Lemma 1. Thus, the sum-capacity and the capacity of restricted Gaussian TRPC can be obtained within 2 bits/sec/Hz, and 2/3 bits/sec/Hz per user of single sided genie upper bound, respectively. By respect to (31), these gaps are 2.5 bits/sec/Hz, and 2.5/3 bits/sec/Hz per user of single sided genie upper bound for non-restricted Gaussian TRPC, respectively. (Note that each user i ? S has two corresponding rates, {Rij}j?S\i). 
 In this paper, we studied the capacity region of an AWGN two-way relay channel with relay private messages (TRPC) with separated users (no direct link between users). In this model the relay can exchange private messages with the users in addition to establishing the communication between two users. We consider both restricted (independent users signals) and non-restricted (dependent users signals) cases. We obtained a cut-set upper bound and a tighter single sided genie for upper bound for restricted and non-restricted nonasymmetric Gaussian TRPC. Then we extracted an achievable rate for this channel which was shown to be within constant gaps 2/3 and 2.5/3 bits/sec/Hz per user of single sided genie are achieved for restricted and non-restricted TRPC, respectively. The proposed scheme applied the same strategy proposed in [7] which is based on using superposition of lattice codes and random Gaussian codes for encoding, and successive interference cancelation for decoding. 
 Proof: By applying Fano’s inequality, and by considering that side information   is available at user 1 and 
 where   as n ? 8. (a) follows from the fact that the messages are independent. Adding (38) and (39) results, 
 where (b) follows from the fact that conditioning reduces entropy. (c) follows from the fact that given Xrn, ({mij}i,j?S) and   are independent. 
 Again, we consider genie transfers side information   and   to users 1 and 2, respectively. By applying Fano’s inequality and assuming independent messages, we have, 
 (d) follows from (3) and the fact that Xri is a function of (mr1,mr2,Yri-1). (e) follows from (4) and given   are completely known. (f) follows from (4) and the fact that conditioning reduces entropy. 
 Starting with (21)-(25) for uplink: From (24) and (25), we have, 22r12-2r21 = (|h1r|2a12)/(|h2r|2a21), and, 
 Starting with (26)-(30) for downlink: It is obvious that (26) is redundant since hr2 = hr1 and log(1 + x/(1 + x)) is an increasing function of x. By working on the other equations, we have, 
 ﻿
 We study the increase in per-sample differential entropy rate of random sequences and processes after being passed through a non minimum-phase (NMP) discrete-time, linear time-invariant (LTI) filter G. For LTI discrete-time filters and random processes, it has long been established that this entropy gain, G(G), equals the integral of  . It is also known that, if the first sample of the impulse response of G has unit-magnitude, then the latter integral equals the sum of the logarithm of the magnitudes of the non-minimum phase zeros of G (i.e., its zeros outside the unit circle), say B(G). These existing results have been derived in the frequency domain as well as in the time domain. In this note, we begin by showing that existing time-domain proofs, which consider finite length-n sequences and then let n tend to infinity, have neglected significant mathematical terms and, therefore, are inaccurate. We discuss some of the implications of this oversight when considering random processes. We then present a rigorous time-domain analysis of the entropy gain of LTI filters for random processes. In particular, we show that the entropy gain between equal-length input and output sequences is upper bounded by B(G) and arises if and only if there exists an output additive disturbance with finite differential entropy (no matter how small) or a random initial state. Unlike what happens with linear maps, the entropy gain in this case depends on the distribution of all the signals involved. Instead, when comparing the input differential entropy to that of the entire (longer) output of G, the entropy gain equals B(G) irrespective of the distributions and without the need for additional exogenous random signals. We illustrate some of the consequences of these results by presenting their implications in three different problems. Specifically: a simple derivation of the rate-distortion function for Gaussian non-stationary sources, conditions for
 I. INTRODUCTION
 In his seminal 1948 paper [1], Claude Shannon gave a formula for the increase in differential entropy per degree of freedom that a continuous-time, band-limited random process u(t) experiences after passing through a linear time-invariant (LTI) continuous-time filter. In this formula, if the input process is bandlimited to a frequency range [0,B], has differential entropy rate (per degree of freedom) h¯(u), and the LTI filter has frequency response G(j?), then the resulting differential entropy rate of the output process y(t) is given by [1, Theorem 14]
 		(1)
 The last term on the right-hand side (RHS) of (1) can be understood as the entropy gain (entropy amplification or entropy boost) introduced by the filter G(j?). Shannon proved this result by arguing that an LTI filter can be seen as a linear operator that selectively scales its input signal along infinitely many frequencies, each of them representing an orthogonal component of the source. The result is then obtained by writing down the determinant of the Jacobian of this operator as the product of the frequency response of the filter over n frequency bands, applying logarithm and then taking the limit as the number of frequency components tends to infinity.
 An analogous result can be obtained for discrete-time input {u(k)} and output {y(k)} processes, and an LTI discrete-time filter G(z) by relating them to their continuous-time counterparts, which yields
 (2)
 where
 is the differential entropy rate of the process {u(k)}. Of course the same formula can also be obtained by applying the frequency-domain proof technique that Shannon followed in his derivation of (1).
 The rightmost term in (2), which corresponds to the entropy gain of G(z), can be related to the structure of this filter. It is well known that if G is causal with a rational transfer function G(z) such that limz?8 |G(z)| = 1 (i.e., such that the first sample of its impulse response has unit magnitude), then
 	,	(3)
 where {?i} are the zeros of G(z) and D, {z ? C : |z| < 1} is the open unit disk on the complex
 plane. This provides a straightforward way to evaluate the entropy gain of a given LTI filter with rational transfer function G(z). In addition, (3) shows that, if limz?8 |G(z)| = 1, then such gain is greater than one if and only if G(z) has zeros outside D. A filter with the latter property is said to be non-minimum phase (NMP); conversely, a filter with all its zeros inside D is said to be minimum phase (MP) [2].
 NMP filters appear naturally in various applications. For instance, any unstable LTI system stabilized via linear feedback control will yield transfer functions which are NMP [2], [3]. Additionally, NMP-zeros also appear when a discrete-time with ZOH (zero order hold) equivalent system is obtained from a plant whose number of poles exceeds its number of zeros by at least 2, as the sampling rate increases [4, Lemma 5.2]. On the other hand, all linear-phase filters, which are specially suited for audio and imageprocessing applications, are NMP [5], [6]. The same is true for any all-pass filter, which is an important building block in signal processing applications [5], [7].
 An alternative approach for obtaining the entropy gain of LTI filters is to work in the time do-
 main; obtain y as a function of un1, for every n ? N, and evaluate the limit
 . More precisely, for a filter G with impulse response , we can write
 	? g0	0	···	0 ?
 	y,	(4)
 Gn
 where yn1 , [y1 y1 ··· yn]T and the random vector u1n is defined likewise. From this, it is clear that
 	h(yn1) = h(u1n) + log|det(Gn)|,	(5)
 where det(Gn) (or simply detGn) stands for the determinant of Gn. Thus,
 ,
 (6)
 regardless of whether G(z) (i.e., the polynomial g0 +g1z-1 +···) has zeros with magnitude greater than one, which clearly contradicts (2) and (3). Perhaps surprisingly, the above contradiction not only has been overlooked in previous works (such as [8], [9]), but the time-domain formulation in the form of (4) has been utilized as a means to prove or disprove (2) (see, for example, the reasoning in [10, p. 568]).
 A reason for why the contradiction between (2), (3) and (6) arises can be obtained from the analysis developed in [11] for an LTI system P within a noisy feedback loop, as the one depicted in Fig. 1. In
 
 Figure 1.	Left: LTI system P within a noisy feedback loop. Right: equivalent system when the feedback channel is noiseless and has unit gain.
 this scheme, C represents a causal feedback channel which combines the output of P with an exogenous (noise) random process c81 to generate its output. The process c81 is assumed independent of the initial state of P, represented by the random vector x0, which has finite differential entropy. For this system, it is shown in [11, Theorem 4.2] that
 	,	(7a)
 with equality if w is a deterministic function of v. Furthermore, it is shown in [12, Lemma 3.2] that if
 |h(x0)| < 8 and the steady state variance of system P remains asymptotically bounded as k ? 8, then
 	,	(7b)
 where {pi} are the poles of P. Thus, for the (simplest) case in which w = v, the output y81 is the result of filtering u81 by a filter (as shown in Fig. 1-right), and the resulting entropy rate of {y(k)} will exceed that of {u(k)} only if there is a random initial state with bounded differential entropy (see (7a)). Moreover, under the latter conditions, [11, Lemma 4.3] implies that if G(z) is stable and |h(x0)| < 8, then this entropy gain will be lower bounded by the right-hand side (RHS) of (3), which is greater than zero if and only if G is NMP. However, the result obtained in (7b) does not provide conditions under which the equality in the latter equation holds.
 Additional results and intuition related to this problem can be obtained from in [13]. There it is shown that if {y(k)} is a two-sided Gaussian stationary random process generated by a state-space recursion of the form
 sk+1 = (A - ghH)sk - g uk,	(8a)
 yk = hHsk + un,	(8b)
 for some A ? CM×M, g ? CM×1, h ? CM×1, with unit-variance Gaussian i.i.d. innovations u, then its entropy rate will be exactly  (i.e., the differential entropy rate of u) plus the RHS of (3) (with {?i} now being the eigenvalues of A outside the unit circle). However, as noted in [13], if the same system with zero (or deterministic) initial state is excited by a one-sided infinite Gaussian
 i.i.d. process u81 with unit sample variance, then the (asymptotic) entropy rate of the output process y81 is just  (i.e., there is no entropy gain). Moreover, it is also shown that if vl1 is a Gaussian random sequence with positive-definite covariance matrix and l = M, then the entropy rate of y also exceeds that of u81 by the RHS of (3). This suggests that for an LTI system which admits a statespace representation of the form (8), the entropy gain for a single-sided Gaussian i.i.d. input is zero, and that the entropy gain from the input to the output-plus-disturbance is (3), for any Gaussian disturbance of length M with positive definite covariance matrix (no matter how small this covariance matrix may
 be).
 The previous analysis suggests that it is the absence of a random initial state or a random additive output disturbance that makes the time-domain formulation (4) yield a zero entropy gain. But, how would the addition of such finite-energy exogenous random variables to (4) actually produce an increase in the differential entropy rate which asymptotically equals the RHS of (3)? In a broader sense, it is not clear from the results mentioned above what the necessary and sufficient conditions are under which an entropy gain equal to the RHS of (3) arises (the analysis in [13] provides only a set of sufficient conditions and relies on second-order statistics and Gaussian innovations to derive the results previously described). Another important observation to be made is the following: it is well known that the entropy gain introduced by a linear mapping is independent of the input statistics [1]. However, there is no reason to assume such independence when this entropy gain arises as the result of adding a random signal to the input of the mapping, i.e., when the mapping by itself does not produce the entropy gain. Hence, it remains to characterize the largest set of input statistics which yield an entropy gain, and the magnitude of this gain.
 The first part of this paper provides answers to these questions. In particular, in Section III explain how and when the entropy gain arises (in the situations described above), starting with input and output sequences of finite length, in a time-domain analysis similar to (4), and then taking the limit as the length tends to infinity. In Section IV it is shown that, in the output-plus-disturbance scenario, the entropy gain is at most the RHS of (3). We show that, for a broad class of input processes (not necessarily Gaussian or stationary), this maximum entropy gain is reached only when the disturbance has bounded differential entropy and its length is at least equal to the number of non-minimum phase zeros of the filter. We provide upper and lower bounds on the entropy gain if the latter condition is not met. A similar result is shown to hold when there is a random initial state in the system (with finite differential entropy). In addition, in Section IV we study the entropy gain between the entire output sequence that a filter yields as response to a shorter input sequence (in Section VI). In this case, however, it is necessary to consider a new definition for differential entropy, named effective differential entropy. Here we show that an effective entropy gain equal to the RHS of (3) is obtained provided the input has finite differential entropy rate, even when there is no random initial state or output disturbance.
 In the second part of this paper (SectionVII) we apply the conclusions obtained in the first part to three problems, namely, networked control, the rate-distortion function for non-stationary Gaussian sources, and the Gaussian channel capacity with feedback. In particular, we show that equality holds in (7b) for the feedback system in Fig. 1-left under very general conditions (even when the channel C is noisy). For the problem of finding the quadratic rate-distortion function for non-stationary auto-regressive Gaussian sources, previously solved in [14]–[16], we provide a simpler proof based upon the results we derive in the first part. This proof extends the result stated in [15], [16] to a broader class of non-stationary sources. For the feedback Gaussian capacity problem, we show that capacity results based on using a short random sequence as channel input and relying on a feedback filter which boosts the entropy rate of the end-to-end channel noise (such as the one proposed in [13]), crucially depend upon the complete absence of any additional disturbance anywhere in the system. Specifically, we show that the information rate of such capacity-achieving schemes drops to zero in the presence of any such additional disturbance. As a consequence, the relevance of characterizing the robust (i.e., in the presence of disturbances) feedback capacity of Gaussian channels, which appears to be a fairly unexplored problem, becomes evident.
 Finally, the main conclusions of this work are summarized in Section VIII.
 Except where present, all proofs are presented in the appendix.
 A. Notation
 For any LTI system G, the transfer function G(z) corresponds to the z-transform of the impulse response g0, g1,..., i.e., . For a transfer function G(z), we denote by Gn ? Rn×n
 the lower triangular Toeplitz matrix having [g0 ··· gn-1]T as its first column. We write as a shorthand for the sequence {x1,...,xn} and, when convenient, we write in vector form as x1n , [x1 x2 ··· xn]T , where ()T denotes transposition. Random scalars (vectors) are denoted using non-italic characters, such as x (non-italic and boldface characters, such as x). For matrices we use upper-case boldface symbols, such as A. We write ?i(A) to the note the i-th smallest-magnitude eigenvalue of A. If An ? Cn×n, then
 	x0	z81
 u81  y81
 Figure 2.	Linear, causal, stable and time-invariant system G with input and output processes, initial state and output disturbance.
 Ai,j denotes the entry in the intersection between the i-th row and the j-th column. We write , with i1 = i2 = n, to refer to the matrix formed by selecting the rows i1 to i2 of A. The expression m1[A]m2 corresponds to the square sub-matrix along the main diagonal of A, with its top-left and bottom-right corners on Am1,m1 and Am2,m2, respectively. A diagonal matrix whose entries are the elements in D is denoted as diagD
 II. PROBLEM DEFINITION AND ASSUMPTIONS
 Consider the discrete-time system depicted in Fig. 2. In this setup, the input u81 is a random process and the block G is a causal, linear and time-invariant system with random initial state vector x0 and random output disturbance z81 . In vector notation,
 	yn1 ,Gnu1n + y¯n1 + zn1,	n ? N,	(9)
 where y¯n1 is the natural response of G to the initial state x0. We make the following further assumptions about G and the signals around it:
 Assumption 1. G(z) is a causal, stable and rational transfer function of finite order, whose impulse
 response g0,g1,... satisfies g0 = 1.	N
 It is worth noting that there is no loss of generality in considering g0 = 1, since otherwise one can write G(z) as G'(z) = g0 ·G(z)/g0, and thus the entropy gain introduced by G'(z) would be logg0 plus the entropy gain due to G(z)/g0, which has an impulse response where the first sample equals 1.
 Assumption 2. The random initial state x0 is independent of u81 .
 Assumption 3. The disturbance z81 is independent of u81 and belongs to a ?-dimensional linear subspace, for some finite ? ? N. This subspace is spanned by the ? orthonormal columns of a matrix F ? R|N|×?
 (where |N| stands for the countably infinite size of N), such that |h(FT z1 )| < 8. Equivalently, z1 =
 	8	8
 Fs1?, where the random vector s1? ,FT z18 has finite differential entropy and is independent of u18.
 As anticipated in the Introduction, we are interested in characterizing the entropy gain G of G in the presence (or absence) of the random inputs u, denoted by
 	.	(10)
 In the next section we provide geometrical insight into the behaviour of for the situation where there is a random output disturbance and no random initial state. A formal and precise treatment of this scenario is then presented in Section IV. The other scenarios are considered in the subsequent
 sections.
 III. GEOMETRIC INTERPRETATION
 In this section we provide an intuitive geometric interpretation of how and when the entropy gain defined in (10) arises. This understanding will justify the introduction of the notion of an entropybalanced random process (in Definition 1 below), which will be shown to play a key role in this and in related problems.
 A. An Illustrative Example
 Suppose for the moment that G in Fig. 2 is an FIR filter with impulse response g0 = 1, g1 = 2, gi = 0, ?i = 2. Notice that this choice yields G(z) = (z - 2)/z, and thus G(z) has one non-minimum phase zero, at z = 2. The associated matrix Gn for n = 3 is
 	?1	0	0?
 	G3 = ????2	1	0????,
 	0	2	1
 whose determinant is clearly one (indeed, all its eigenvalues are 1). Hence, as discussed in the introduction, h(G3u13) = h(u13), and thus G3 (and Gn in general) does not introduce an entropy gain by itself. However, an interesting phenomenon becomes evident by looking at the singular-value decomposition (SVD)
 of G3, given by G, where Q3 and R3 are unitary matrices and D3 , diag{d1,d2,d3}. In this case, D3 = diag{0.19394, 1.90321, 2.70928}, and thus one of the singular values of G3 is much smaller than the others (although the product of all singular values yields 1, as expected). As will be shown in Section IV, for a stable G(z) such uneven distribution of singular values arises only when G(z) has non-minimum phase zeros. The effect of this can be visualized by looking at the image of the cube
 [0,1]3 through G3 shown in Fig. 3. If the input u13 were uniformly distributed over this cube (of unit
 
 Figure 3.	Image of the cube [0,1]3 through the square matrix with columns [1 2 0]T, [0 1 2]T and [0 0 1]T.
 volume), then G3u13 would distribute uniformly over the unit-volume parallelepiped depicted in Fig. 3, and hence h(G3u13) = h(u13).
 Now, if we add to G3u13 a disturbance z13 = Fs, with scalar s uniformly distributed over [-0.5, 0.5] independent of u13, and with F ? R3×1, the effect would be to “thicken” the support over which the resulting random vector y31 = G3u13 +z13 is distributed, along the direction pointed by F. If F is aligned with the direction along which the support of G3u13 is thinnest (given by q3,1, the first row of Q3), then the resulting support would have its volume significantly increased, which can be associated with a large increase in the differential entropy of y31 with respect to u13. Indeed, a relatively small variance of s and an approximately aligned F would still produce a significant entropy gain.
 The above example suggests that the entropy gain from u1n to yn1 appears as a combination of two factors. The first of these is the uneven way in which the random vector Gnu1n is distributed over Rn. The second factor is the alignment of the disturbance vector z1n with respect to the span of the subset {qn,i}i??n of columns of Qn, associated with smallest singular values of Gn, indexed by the elements in the set ?n. As we shall discuss in the next section, if G has m non-minimum phase zeros, then, as n increases, there will be m singular values of Gn going to zero exponentially. Since the product of the singular values of Gn equals 1 for all n, it follows that Qi/??n dn,i must grow exponentially with n, where dn,i is the i-th diagonal entry of Dn. This implies that Gnu1n expands with n along the span of {qn,i}i/??n, compensating its shrinkage along the span of {qn,i}i??n, thus keeping h(Gnun1) = h(u1n) for all n. Thus, as n grows, any small disturbance distributed over the span of {qn,i}i??n, added to Gnu1n, will keep the support of the resulting distribution from shrinking along this subspace. Consequently, the expansion of Gnu1n with n along the span of {qn,i}i/??n is no longer compensated, yielding an entropy increase proportional to log(Qi/??n dn,i).
 The above analysis allows one to anticipate a situation in which no entropy gain would take place even when some singular values of Gn tend to zero as n ? 8. Since the increase in entropy is made possible by the fact that, as n grows, the support of the distribution of Gnu1n shrinks along the span of {qn,i}i??n, no such entropy gain should arise if the support of the distribution of the input u1n expands accordingly along the directions pointed by the rows {rn,i}i??n of Rn.
 An example of such situation can be easily constructed as follows: Let G(z) in Fig. 2 have nonminimum phase zeros and suppose that u81 is generated as, where u˜81 is an i.i.d. random process with bounded entropy rate. Since the determinant of G-n1 equals 1 for all n, we have that h(u1n) = h(u˜1n), for all n. On the other hand, yn1 = GnG-n1u˜1n + z1n = u˜1n + z1n. Since z1n = [F]1ns1? for some finite ?
 (recall Assumption 3), it is easy to show that ,
 and thus no entropy gain appears.
 The preceding discussion reveals that the entropy gain produced by G in the situation shown in Fig. 2 depends on the distribution of the input and on the support and distribution of the disturbance. This stands in stark contrast with the well known fact that the increase in differential entropy produced by an invertible linear operator depends only on its Jacobian, and not on the statistics of the input [1]. We have also seen that the distribution of a random process along the different directions within the Euclidean space which contains it plays a key role as well. This motivates the need to specify a class of random processes which distribute more or less evenly over all directions. The following section introduces a rigorous definition of this class and characterizes a large family of processes belonging to it.
 B. Entropy-Balanced Processes
 We begin by formally introducing the notion of an “entropy-balanced” process u81 , being one in which, for every finite ? ? N, the differential entropy rate of the orthogonal projection of un1 into any subspace of dimension n - ? equals the entropy rate of u. This idea is precisely in the following
 definition.
 Definition 1. A random process is said to be entropy balanced if, for every ? ? N,
 	,	(11a)
 for every sequence of matrices, with orthonormal rows.	N Equivalently, a random process {v(k)} is entropy balanced if every unitary transformation on vn1 yields
 a random sequence yn1 such that . This property of the resulting random
 sequence yn1 means that one cannot predict its last ? samples with arbitrary accuracy by using its previous n - ? samples, even if n goes to infinity.
 We now characterize a large family of entropy-balanced random processes and establish some of their properties. Although intuition may suggest that most random processes (such as i.i.d. or stationary processes) should be entropy balanced, that statement seems rather difficult to prove. In the following, we show that the entropy-balanced condition is met by i.i.d. processes with per-sample probability density function (PDF) being uniform, piece-wise constant or Gaussian. It is also shown that adding to an entropy-balanced process an independent random processes independent of the former yields another entropy-balanced process, and that filtering an entropy-balanced process by a stable and minimum phase filter yields an entropy-balanced process as well.
 Proposition 1. Let u81	be a Gaussian i.i.d. random process with positive and bounded per-sample
 variance. Then u81	is entropy balanced.	N
 Lemma 1. Let u81 be an i.i.d. process with finite differential entropy rate, in which each ui is distributed according to a piece-wise constant PDF in which each interval where this PDF is constant has measure greater than o, for some bounded-away-from-zero constant o. Then u81 is entropy balanced. N
 Lemma 2. Let u81	and v81	be mutually independent random processes. If u81	is entropy balanced, then
 w81 , u is also entropy balanced.	N
 The working behind this lemma can be interpreted intuitively by noting that adding to a random process another independent random process can only increase the “spread” of the distribution of the former, which tends to balance the entropy of the resulting process along all dimensions in Euclidean space. In addition, it follows from Lemma 2 that all i.i.d. processes having a per-sample PDF which can be constructed by convolving uniform, piece-wise constant or Gaussian PDFs as many times as required are entropy balanced. It also implies that one can have non-stationary processes which are entropy balanced, since Lemma 2 imposes no requirements for the process v81 .
 Our last lemma related to the properties of entropy-balanced processes shows that filtering by a stable and minimum phase LTI filter preserves the entropy balanced condition of its input.
 Lemma 3. Let u81 be an entropy-balanced process and G an LTI stable and minimum-phase filter. Then the output w is also an entropy-balanced process.	N
 This result implies that any stable moving-average auto-regressive process constructed from entropybalanced innovations is also entropy balanced, provided the coefficients of the averaging and regression correspond to a stable MP filter.
 We finish this section by pointing out two examples of processes which are non-entropy-balanced, namely, the output of a NMP-filter to an entropy-balanced input and the output of an unstable filter to an entropy-balanced input. The first of these cases plays a central role in the next section.
 IV. ENTROPY GAIN DUE TO EXTERNAL DISTURBANCES
 In this section we formalize the ideas which were qualitatively outlined in the previous section. Specifically, for the system shown in Fig. 2 we will characterize the entropy gain defined in (10) for the case in which the initial state x0 is zero (or deterministic) and there exists an output random disturbance of (possibly infinite length) z81 which satisfies Assumption 3. The following lemmas will be instrumental for that purpose.
 Lemma 4. Let A(z) be a causal, finite-order, stable and minimum-phase rational transfer function with
 impulse response a0,a1,... such that a0 = 1. Then  and 
 8.	N
 Lemma 5. Consider the system in Fig. 2, and suppose z81 satisfies Assumption 3, and that the input process u81 is entropy balanced. Let Gbe the SVD of Gn, where Dn = diag{dn,1,...,dn,n} are the singular values of Gn, with dn,1 = dn,2 = ··· = dn,n, such that |detGn| = 1 ?n. Let m be the number of these singular values which tend to zero exponentially as n ? 8. Then
  .	(12) N
 (The proof of this Lemma can be found in the Appendix, page 34).
 The previous lemma precisely formulates the geometric idea outlined in Section III. To see this, notice that no entropy gain is obtained if the output disturbance vector z1n is orthogonal to the space spanned by the first m columns of Qn. If this were the case, then the disturbance would not be able fill the subspace along which Gnu1n is shrinking exponentially. Indeed, if [Qn]1nz1n = 0 for all n, then
 , and the latter sum cancels out the one on the RHS of (12), while limn?8 n1 h([Rn]1nu1n) = 0 since u81 is entropy balanced. On the contrary (and loosely speaking), if the projection of the support of z1n onto the subspace spanned by the first m rows of Qn is of dimension m, then h([Dn]1mRnu1n +[Qn]1mz1n) remains bounded for all
 n, and the entropy limit of the sum  on the RHS of (12) yields the largest possible entropy gain. Notice that  (because det(Gn) = 1), and thus
 this entropy gain stems from the uncompensated expansion of Gnu1n along the space spanned by the
 rows of .
 Lemma 5 also yields the following corollary, which states that only a filter G(z) with zeros outside the unit circle (i.e., an NMP transfer function) can introduce entropy gain.
 Corollary 1 (Minimum Phase Filters do not Introduce Entropy Gain). Consider the system shown in Fig. 2 and let u81 be an entropy-balanced random process with bounded entropy rate. Besides Assumption 1, suppose that G(z) is minimum phase. Then
 	.	(13)
 N
 Proof: Since G(z) is minimum phase and stable, it follows from Lemma 4 that the number of singular values of Gn which go to zero exponentially, as n ? 8, is zero. Indeed, all the singular values vary polynomially with n. Thus m = 0 and Lemma 5 yields directly that the entropy gain is zero (since the RHS of (12) is zero).	
 A. Input Disturbances Do Not Produce Entropy Gain
 In this section we show that random disturbances satisfying Assumption 3, when added to the input u81 (i.e., before G), do not introduce entropy gain. This result can be obtained from Lemma 5, as stated in the following theorem:
 Theorem 1 (Input Disturbances do not Introduce Entropy Gain). Let G satisfy Assumption 1. Suppose that u81 is entropy balanced and consider the output
 	y.	(14)
 where b18 = ?a1?, with a1? being a random vector satisfying h(a1?) < 8, and where ? ? R|N|×? has orthonormal columns. Then,
 		(15)
 Proof: In this case, the effect of the input disturbance in the output is the forced response of G to it. This response can be regarded as an output disturbance z. Thus, the argument of the differential entropy on the RHS of (12) is
 		(16)
 = [Dn]1mRnu1n + [Dn]1mRnb1n	(17)
 	.	(18)
 Therefore,
 )	(19) u.	(20)
 The proof is completed by substituting this result into the RHS of (12) and noticing that
 .
 
 Remark 1. An alternative proof for this result can be given based upon the properties of an entropybalanced sequence, as follows. Since det(Gn) = 1, ?n, we have that h(Gn(u1n + bn1)) = h(un1 + b1n).
 Let Tn ? R?×n and  be a matrices with orthonormal rows which satisfy and such that  is a unitary matrix. Then
 	,	(21)
 where we have applied the chain rule of differential entropy. But
 		(22)
 which is upper bounded for all n because h(a1n) < 8 and h(Tnu1n) < 8, the latter due to u81 being entropy balanced. On the other hand, since b1n is independent of u1n, it follows that h(u1n +bn1) = h(u1n),
 for all n. Thus , where the last equality
 stems from the fact that u81	is entropy balanced.	N
 B. The Entropy Gain Introduced by Output Disturbances when G(z) has NMP Zeros
 We show here that the entropy gain of a transfer function with zeros outside the unit circle is at most the sum of the logarithm of the magnitude of these zeros. To be more precise, the following assumption is required.
 Assumption 4. The filter G satisfies Assumption 1 and its transfer function G(z) has p poles and p zeros, m of which are NMP-zeros. Let M be the number of distinct NMP zeros, given by, i.e., such that |?1| > |?2| > ··· > |?M| > 1, with li being the multiplicity of the i-th distinct zero. We denote by ?(i), where ? : {1,... ,m} ? {1,...,M}, the distinct zero of G(z) associated with the i-th non-distinct zero of G(z), i.e.,
 	.	(23)
 N
 As can be anticipated from the previous results in this section, we will need to characterize the asymptotic behaviour of the singular values of Gn. This is accomplished in the following lemma, which relates these singular values to the zeros of G(z). This result is a generalization of the unnumbered lemma in the proof of [15, Theorem 1] (restated in the appendix as Lemma 8), which holds for FIR
 transfer functions, to the case of infinite-impulse response (IIR) transfer functions (i.e., transfer functions having poles).
 Lemma 6. For a transfer function G satisfying Assumption 4, it holds that
 		(24)
 	{	} ??a2n,l	,otherwise ,
 where the elements in the sequence	an,l	are positive and increase or decrease at most polynomially
 with n.	N
 (The proof of this lemma can be found in the appendix, page 36).
 We can now state the first main result of this section.
 Theorem 2. In the system of Fig. 2, suppose that u81 is entropy balanced and that G(z) and z81 satisfy assumptions 4 and 3, respectively. Then
 	,	(25)
 where ?¯ , min{?,m} and ? is as defined in Assumption 3. Both bounds are tight. The upper bound is achieved if limn?8 det([Qn]1?¯[F]1n([Qn]1?¯[F]1n)T ) > 0, where the unitary matrices Qhold the
 left singular vectors of Gn.	N
 	Proof: See Appendix, page 37.	
 The second main theorem of this section is the following:
 Theorem 3. In the system of Fig. 2, suppose that u81	is entropy balanced and that G(z) satisfies
 Assumption 4. Let z81 be a random output disturbance, such that z(i) = 0, ?i > m, and that. Then
 	.	(26)
 N
 	Proof: See Appendix, page 39.	
 V. ENTROPY GAIN DUE TO A RANDOM INITIAL SATE
 Here we analyze the case in which there exists a random initial state x0 independent of the input u81 , and zero (or deterministic) output disturbance.
 The effect of a random initial state appears in the output as the natural response of G to it, namely the sequence y¯n1. Thus, yn1 can be written in vector form as
 	yn1 = Gnu1n + y¯n1.	(27)
 This reveals that the effect of a random initial state can be treated as a random output disturbance, which allows us to apply the results from the previous sections.
 Recall from Assumption 4 that G(z) is a stable and biproper rational transfer function with m NMP zeros. As such, it can be factored as
 	G(z) = P(z)N(z),	(28)
 where P(z) is a biproper filter containing only all the poles of G(z), and N(z) is a FIR biproper filter, containing all the zeros of G(z).
 We have already established (recall Theorem 1) that the entropy gain introduced by the minimum phase system P(z) is zero. It then follows that the entropy gain can be introduced only by the NMP-zeros of N(z) and an appropriate output disturbance y¯81 . Notice that, in this case, the input process w81 to N (i.e., the output sequence of P due to a random input u81 ) is independent of y¯81 (since we have placed the natural response y¯81 after the filters P and N, hose initial state is now zero). This condition allows us to directly use Lemma 5 in order to analyze the entropy gain that u81 experiences after being filtered
 by G, which coincides with . This is achieved by the next theorem.
 Theorem 4. Consider a stable p-th order biproper filter G(z) having m NMP-zeros, and with a random initial state x0, such that |h(x0)| < 8. Then, the entropy gain due to the existence of a random initial
 state is
 	.	(29)
 Proof: Being a biproper and stable rational transfer function, G(z) can be factorized as
 	G(z) = P(z)N(z),	(30)
 where P(z) is a stable biproper transfer function containing only all the poles of G(z) and with all its zeros at the origin, while N(z) is stable and biproper FIR filter, having all the zeros of G(z). Let C˜ nx0 and Cnx0 be the natural responses of the systems P and N to their common random initial state x0, respectively, where C˜ n,Cn ? Rn×p. Then we can write
 	y.	(31)
 ,{wzn1
 Since P(z) is stable and MP, it follows from Corollary 1 that h(wn1) = h(u1n) for all n, and therefore
 	h(yn1) - h(u1n) = h(yn1) - h(wn1).	(32)
 Therefore, we only need to consider the entropy gain introduced by the (possibly) non-minimum filter N due to a random output disturbance z1n = y¯n1 = NnC˜ nx0 +Cnx0, which is independent of the input wn1. Thus, the conditions of Lemma 5 are met considering Gn = Nn, where now N is
 the SVD for Nn, and dn,1 = dn,2 = ··· = dn,n. Consequently, it suffices to consider the differential entropy on the RHS of (12), whose argument is
 (33)
 (34)
 	= [Dn]1mRnvn1 + [Qn]1mCnx0,	(35)
 where vn1 , u1n + C˜ nx0 has bounded entropy rate and is entropy balanced (since C˜ nx0 is the natural response of a stable LTI system and because of Lemma 2). We remark that, in (35), vn1 is not independent of x0, which precludes one from using the proof of Theorem 2 directly.
 On the other hand, since N(z) is FIR of order (at most) p, we have that C, where
 Ep ? Rp×p is a non-singular upper-triangular matrix independent of n. Hence, Cnx0 can be written as
 [F]1ns1p, where  and s1p , Epx0. According to (35), the entropy gain in (25) arises as long as h([Qn]1mCnx0) is lower bounded by a finite constant (or if it decreases sub-linearly as n grows). Then, we need [Qn]1m[F]1n to be a full row-ranked matrix in the limit as n ? 8. However,
 	 ,	(36)
 where [Qn(p)]1m denotes the first p columns of the first m rows in Qn. We will now show that these
 
 determinants do not go to zero as n ? 8. Define the matrix Qn ? Rm×(p-m) such that [Qn(p)]1m = . Then, it holds that ?x ? Rn,
 		(37)
 = k(1[Qn]m)T xk2	(38)
 	 .	(39)
 Hence, the minimum singular value of [Q(np)]1m is lower bounded by the smallest singular value of 1[Qn]m, for all n = m. But it was shown in the proof of Theorem 3 (see page 39) that limn?8 ?min(1[Qn]m(1[Qn]m)T ) > 0. Using this result in (37) and taking the limit, we arrive to
 	.	(40)
 Thus
 		(41)
 is upper and lower bounded by a constant independent of n because v81 is entropy balanced, [Dn]1m has decaying entries, and h(sp1) < 8, which means that the entropy rate in the RHS of (12) decays to zero. The proof is finished by invoking Lemma 6.	
 Theorem 4 allows us to formalize the effect that the presence or absence of a random initial state has on the entropy gain using arguments similar to those utilized in Section IV. Indeed, if the random initial state x0 ? Rp has finite differential entropy, then the entropy gain achieves (3), since the alignment between x0 and the first m rows of Qn is guaranteed. This motivates us to characterize the behavior of the entropy gain (due only to a random initial state), when the initial state x0 can be written as [F]1ps1t, with t = p, which means that x0 has an undefined (or -8) differential entropy.
 Corollary 2. Consider an FIR, p-order filter F(z) having m NMP-zeros, such that its random initial state can be written as x0 = Fs1t, where  and F ? Rp×t contains orthonormal rows . Then,
 	,	(42)
 where {t¯} , min{m,t}. The upper bound in (42) is achieved when [Qn]1mCnF([Qn]m1CnF)T is a non-singular matrix, with Cn defined by y¯n1 = Cnx0 (as in Theorem 4).
 Proof: The effect of the random initial state to the output sequence y81 can be written as yn1 = Cnx0,
 where C. Therefore, if Q is an SVD for F n, it holds that
 	h([Dn]1nRnu1n + [Qn]1mCnFs1t)	(43)
 remains bounded, for n ? 8, if and only if limn?8 det([Qn]1mCnF([Qn]1mCnF)T ) > 0.
 Define the rank of [Qn]1mCnF as tn ? {1,... ,t¯}. If det([Qn]m1CnF([Qn]m1 CnF)T ) = 0, then the lower bound is reached by inserting (43) in (12). Otherwise, there exists L large enough such that tn = 1,
 ?n = L.
 We then proceed as the proof of Theorem 2, by considering a unitary (m × m)-matrix Hn, and a (tn × m)-matrix An such that
 	H	(44)
 This procedure allows us to conclude that , and
 that the lower limit in the latter sum equals t¯+1 when [Qn]1mCnFs1t is a full row-rank matrix. Replacing the latter into (12) finishes the proof.	
 Remark 2. If the random initial state x0 = Fs1t is generated with t = p - m, then the entropy gain introduced by an FIR minimum phase filter F is at least log?1. Otherwise, the entropy gain could be identically zero, as long as the columns of EnF(EnF)T fill only the orthogonal space to the span of the row vectors in [Qn(p)]1m, where En, F and [Qn(p)]1m are defined as in the proof of Theorem 4.
 Both results, Theorem 4 and Corollary 2, reveal that the entropy gain arises as long as the effect of the random initial state aligns with the first rows of Qn, just as in the results of the previous section.
 VI. EFFECTIVE ENTROPY GAIN DUE TO THE INTRINSIC PROPERTIES OF THE FILTER
 If there are no disturbances and the initial state is zero, then the first n output samples to an input un1 is given by (4). Therefore, the entropy gain in this case, as defined in (10), is zero, regardless of whether
 or not G is NMP.
 Despite the above, there is an interesting question which, to the best of the authors’ knowledge, has not been addressed before: Since in any LTI filter the entire output is longer than the input, what would happen if one compared the differential entropies of the complete output sequence to that of the (shorter) input sequence? As we show next, a proper definition of this question requires recasting the problem in terms of a new definition of differential entropy. After providing a geometrical interpretation of this problem, we prove that the (new) entropy gain in this case is exactly (3).
 A. Geometrical Interpretation
 Consider the random vectors u, [u1 u2]T and y, [y1 y2 y3]T related via
 	 .	(45)
 ,G? 2
 Suppose u is uniformly distributed over [0,1]×[0,1]. Applying the conventional definition of differential entropy of a random sequence, we would have that
 	h(y1,y2,y3) = h(y1,y2) + h(y3 |y1,y2) = -8,	(46)
 because y3 is a deterministic function of y1 and y2:
 y .
 In other words, the problem lies in that although the output is a three dimensional vector, it only has two degrees of freedom, i.e., it is restricted to a 2-dimensional subspace of R3. This is illustrated in Fig. 4, where the set [0,1]×[0,1] is shown (coinciding with the u-v plane), together with its image through G? 2 (as defined in (45)).
 As can be seen in this figure, the image of the square [0,1]2 through G? 2 is a 2-dimensional rhombus over which {y1,y2,y3} distributes uniformly. Since the intuitive notion of differential entropy of an ensemble of random variables (such as how difficult it is to compress it in a lossy fashion) relates to the size of the region spanned by the associated random vector, one could argue that the differential entropy of {y1,y2,y3}, far from being -8, should be somewhat larger than that of {u1,u2} (since the rhombus G? 2[0,1]2 has a larger area than [0,1]2). So, what does it mean that (and why should) h(y1,y2,y3) = -8? Simply put, the differential entropy relates to the volume spanned by the support of the probability density function. For y in our example, the latter (three-dimensional) volume is clearly zero.
 
 Figure 4.	Support of u (laying in the u-v plane) compared to that of y = G?u (the rhombus in R3).
 From the above discussion, the comparison between the differential entropies of y ? R3 and u ? R2 of our previous example should take into account that y actually lives in a two-dimensional subspace of R3. Indeed, since the multiplication by a unitary matrix does not alter differential entropies, we could consider the differential entropy of
 ?
 y˜? ?Q? ?
 , y,	(47) ?0? ?q¯T ?
 where Q? T is the 3 × 2 matrix with orthonormal rows in the singular-value decomposition of G? 2
 	G? 2 = Q? T D? R?.	(48)
 and q¯ is a unit-norm vector orthogonal to the rows of Q? (and thus orthogonal to y as well). We are now able to compute the differential entropy in R2 for y˜, corresponding to the rotated version of y such that its support is now aligned with R2.
 The preceding discussion motivates the use of a modified version of the notion of differential entropy for a random vector y ? Rn which considers the number of dimensions actually spanned by y instead
 of its length.
 Definition 2 (The Effective Differential Entropy). Let y ? Rl be a random vector. If y can be written as a linear transformation y = Su, for some u ? Rn (n = l), S ? Rl×n, then the effective differential entropy of y is defined as
 h?(y) , h(Ay),	(49)
 where S = AT TC is an SVD for S, with T Rn×n.	N
 ?
 It is worth mentioning that Shannon’s differential entropy of a vector y ? Rl, whose support’s lvolume is greater than zero, arises from considering it as the difference between its (absolute) entropy and that of a random variable uniformly distributed over an l-dimensional, unit-volume region of Rl. More precisely, if in this case the probability density function (PDF) of y = [y1 y2 ··· yl]T is Riemann integrable, then [17, Thm. 9.3.1],
 	 ,	(50)
 where y? is the discrete-valued random vector resulting when y is quantized using an l-dimensional uniform quantizer with l-cubic quantization cells with volume ?l. However, if we consider a variable y whose support belongs to an n-dimensional subspace of Rl, n < l (i.e., y = Su = AT TCu, as in Definition 2), then the entropy of its quantized version in Rl, say Hl(y?), is distinct from Hn((Ay)?), the entropy of Ay in Rn. Moreover, it turns out that, in general,
 	,	(51)
 despite the fact that A has orthonormal rows. Thus, the definition given by (50) does not yield consistent results for the case wherein a random vector has a support’s dimension (i.e., its number of degrees of freedom) smaller that its length  (If this were not the case, then we could redefine (50) replacing l by n, in a spirit similar to the one behind Renyi’s d-dimensional entropy [18].) To see this, consider the case in which u ? R distributes uniformly over [0,1] and y. Clearly, y distributes uniformly over the unit-length segment connecting the origin with the point . Then
 
 On the other hand, since in this case Ay = u, we have that
 	.	(53)
 Thus
 
 The latter example further illustrates why the notion of effective entropy is appropriate in the setup considered in this section, where the effective dimension of the random sequences does not coincide with their length (it is easy to verify that the effective entropy of y does not change if one rotates y in Rl). Indeed, we will need to consider only sequences which can be constructed by multiplying some random vector u ? Rn, with bounded differential entropy, by a tall matrix G? n ? Rn×(n+?), with ? > 0 (as in (45)), which are precisely the conditions required by Definition 2.
 B. Effective Entropy Gain
 We can now state the main result of this section:
 Theorem 5. Let the entropy-balanced random sequence u81 be the input of an LTI filter G, and let y81 be its output. Assume that G(z) is the z-transform of the (? + 1)-length sequence {gk}?k=0. Then
 (55)
 N
 Theorem 5 states that, when considering the full-length output of a filter, the effective entropy gain is introduced by the filter itself, without requiring the presence of external random disturbances or initial states. This may seem a surprising result, in view of the findings made in the previous sections, where the entropy gain appeared only when such random exogenous signals were present. In other words, when observing the full-length output and the input, the (maximum) entropy gain of a filter can be recasted in terms of the “volume” expansion yielded by the filter as a linear operator, provided we measure effective differential entropies instead of Shannon’s differential entropy.
 Proof of Theorem 5: The total length of the output l, will grow with the length n of the input, if G is FIR, and will be infinite, if G is IIR. Thus, we define the output-length function
 is FIR with i.r. length ? + 1,
 l(n) , length of y when input is u1	(56) ??8 , if G is IIR. ?
 It is also convenient to define the sequence of matrices, where G? n Rl(n)×n is Toeplitz with
 h
 G? ni = 0,?i < j, hG? ni = gi-j,?i = j. This allows one to write the entire output yl1 of a causal
 i,j	i,j
 LTI filter G with impulse response {gk}?k=0 to an input u81	as
 	y.	(57)
 Let the SVD of G? n be G?, where Q? n ? Rn×l(n) has orthonormal rows, D? n ? Rn×n is
 diagonal with positive elements, and R? n ? Rn×n is unitary.
 The effective differential entropy of y exceeds the one of un1 by
 	(58)
 = h(D? nR? nu1n) - h(u1n)	(59)
 = logdetD? n,	(60)
 where the first equality follows from the fact that u1n can be written as Inu1n, which means that h?(u1n) = h(u1n). But
 	G? TnG? n = (Q? TnD? nR? n)T (Q? TnD? nR? n) = R? TnD? nQ? nQ? TnD? nR? n = R? nTD? n2R? n.	(61)
 Since R? n is unitary, it follows that	2	, which means that .
 The product Hn ,G? TnG? n is a symmetric Toeplitz matrix, with its first column, [h0 h1 ··· hn-1]T , given by
 	.	(62)
 Thus, the sequence corresponds to the samples 0 to n- 1 of those resulting from the complete convolution g*g-, even when the filter G is IIR, where g- denotes the time-reversed (perhaps infinitely large) response g. Consequently, using the Grenander and Szego¨’s theorem [19], it holds that
 		(63)
 where G(ej?) is the discrete-time Fourier transform of {gk}lk=0.
 In order to finish the proof, we divide (58) by n, take the limit as n ? 8, and replace (63) in the
 latter.	
 VII. SOME IMPLICATIONS
 A. Rate Distortion Function for Non-Stationary Processes
 In this section we obtain a simpler proof of a result by Gray, Hashimoto and Arimoto [14]–[16], which compares the rate distortion function (RDF) of a non-stationary auto-regressive Gaussian process x81 (of
 u
 w y
 Figure 5.	Block diagram representation of how the non-stationary source x81 is built and then reconstructed as y = x+u.
 a certain class) to that of a corresponding stationary version, under MSE distortion. Our proof is based upon the ideas developed in the previous sections, and extends the class of non-stationary sources for which the results in [14]–[16] are valid.
 To be more precise, let  and be the impulse responses of two linear time-invariant filters A and A˜ with rational transfer functions
 		(64)
 	,	(65)
 where |pi| > 1, ?i = 1,...,M. From these definitions it is clear that A(z) is unstable, A˜(z) is stable, and
 |A(ej?)| = |A˜(ej?)|, ?? ? [-p,p]. Notice also that lim|z|?8 A(z) = 1 and lim z|?8 A˜(z) = 1/QMi=1 |pi|, and thus
 |
 M	(66)
 	a0 = 1,	.	(67)
 Consider the non-stationary random sequences (source) x81 and the asymptotically stationary source x˜81 generated by passing a stationary Gaussian process w81 through A(z) and A˜(z), respectively, which can be written as
 x	(68) x˜	(69)
 (A block-diagram associated with the construction of x is presented in Fig. 5.) Define the rate-distortion functions for these two sources as
 Rx(D) , lim Rx,n(D),
 n?8	,	(70)
 Rx˜(D) , lim Rx˜,n(D),
 n?8	,	(71)
 where, for each n, the minimums are taken over all the conditional probability density functions
 and  yielding E and E, respectively.
 The above rate-distortion functions have been characterized in [14]–[16] for the case in which w81	is an i.i.d. Gaussian process. In particular, it is explicitly stated in [15], [16] that, for that case,
 	.	(72)
 We will next provide an alternative and simpler proof of this result, and extend its validity for general (not-necessarily stationary) Gaussian w81 , using the entropy gain properties of non-minimum phase filters established in Section IV. Indeed, the approach in [14]–[16] is based upon asymptotically-equivalent Toeplitz matrices in terms of the signals’ covariance matrices. This restricts w81 to be Gaussian and i.i.d. and A(z) to be an all-pole unstable transfer function, and then, the only non-stationary allowed is that arising from unstable poles. For instance, a cyclo-stationarity innovation followed by an unstable filter A(z) would yield a source which cannot be treated using Gray and Hashimoto’s approach. By contrast, the reasoning behind our proof lets w81 be any Gaussian process, and then let the source be Aw, with A(z) having unstable poles (and possibly zeros and stable poles as well).
 The statement is as follows:
 Theorem 6. Let w81	be any Gaussian stationary process with bounded differential entropy rate, and let
 x81	and x˜81	be as defined in (68) and (69), respectively. Then (72) holds.	N
 Thanks to the ideas developed in the previous sections, it is possible to give an intuitive outline of the proof of this theorem (given in the appendix, page 40) by using a sequence of block diagrams. More precisely, consider the diagrams shown in Fig. 6. In the top diagram in this figure, suppose that y = C x+u realizes the RDF for the non-stationary source x. The sequence u is independent of x, and the linear filter C(z) is such that the error (y-x) ? y (a necessary condition for minimum MSE optimality). The filter B(z) is the Blaschke product of A(z) (see (168) in the appendix) (a stable, NMP filter with unit frequency response magnitude such that x˜ = B x).
 If one now moves the filter B(z) towards the source, then the middle diagram in Fig. 6 is obtained. By doing this, the stationary source x˜ appears with an additive error signal u˜ that has the same asymptotic variance as u, reconstructed as y˜ = Cx˜+u˜. From the invertibility of B(z), it also follows that the mutual information rate between x˜ and y˜ equals that between x and y. Thus, the channel y˜ = Cx˜ + u˜ has the same rate and distortion as the channel y = C x+u.
 However, if one now adds a short disturbance d to the error signal u˜ (as depicted in the bottom diagram
 
 Figure 6.	Block-diagram representation of the changes of variables in the proof of Theorem 6.
 of Fig. 6), then the resulting additive error term u¯ = u˜ + d will be independent of x˜ and will have the same asymptotic variance as u˜. However, the differential entropy rate of u¯ will exceed that of u˜ by the RHS of (72). This will make the mutual information rate between x˜ and y¯ to be less than that between x˜ and y˜ by the same amount. Hence, Rx˜(D) be at most . A similar reasoning can
 be followed to prove that .
 B. Networked Control
 Here we revisit the setup shown in Fig. 1 and discussed in Section I. Recall from (7b) that, for this general class of networked control systems, it was shown in [12, Lemma 3.2] that
 	,	(73)
 where are the poles of P(z) (the plant in Fig. 1).
 By using the results obtained in Section V we show next that equality holds in (7b) provided the feedback channel satisfies the following assumption:
 Assumption 5. The feedback channel in Fig. 1 can be written as
 	w = AB v+BF(c),	(74)
 where
 
 
 Figure 7.	Top: The class of feedback channels described by Assumption 5. Bottom: an equivalent form.
 A and B are stable rational transfer functions such that AB is biproper, ABP has the same unstable poles as P, and the feedback AB stabilizes the plant P.
 F is any (possibly non-linear) operator such that ˜c , F(c) satisfies , for all n ? N, and
 .	N
 An illustration of the class of feedback channels satisfying this assumption is depicted on top of Fig. 7. Trivial examples of channels satisfying Assumption 5 are a Gaussian additive channel preceded and followed by linear operators [20]. Indeed, when F is an LTI system with a strictly causal transfer function, the feedback channel that satisfies Assumption 5 is widely known as a noise shaper with input pre and post filter, used in, e.g. [21]–[24].
 Theorem 7. In the networked control system of Fig. 1, suppose that the feedback channel satisfies Assumption 5 and that the input u81 is entropy balanced. If the random initial state of the plant P(z), with poles , satisfies |h(x0)| < 8, then
 	.	(75)
 N
 Proof: Let P(z) = N(z)/?(z) and T(z) , A(z)B(z) = G(z)/T(z). Then, from Lemma 9 (in the appendix), the output yn1 can be written as
 	y =	?	u˜,	(76)
 ,G˜, init. state {x0,s0}
 where s0 is the initial state of T(z) and
 u˜ , u + B˜c.
 (see Fig. 7 Bottom). Then	(77)
 I(x0;yn1) = h(yn1) - h(yn1|x0)	(78)
 = h(yn1) - h(?n[G˜ nu˜1n + C˜ ns0])	(79)
 = h(?n[G˜ nu˜1n + C˜ ns0 + C¯ nx0] + Cnx0) - h(?n[G˜ nu˜1n + C˜ ns0])	(80)
 = h(?n[G˜ nu˜1n + C˜ ns0 + C¯ nx0] + Cnx0) - h(G˜ nu˜n1 + C˜ ns0),	(81)
 where C˜ 0 maps the initial state s0 to yn1, C¯ n maps the initial state x0 to the output of G˜(z), and Cn maps the initial state x0 (of ?(z)) to yn1. Since u81 is entropy balanced and ˜c81 has finite entropy rate, it follows from Lemma 2 that u˜81 is entropy balanced as well. Thus, we can proceed as in the proof of Theorem 4 to conclude that
 This completes the proof.
 C. The Feedback Channel Capacity of (non-white) Gaussian Channels
 Consider a non-white additive Gaussian channel of the form	
 yk = xk +zk,
 where the input x is subject to the power constraint	(83)
 	.	(82)
 		(84)
 and z81	is a stationary Gaussian process.
 The feedback information capacity of this channel is realized by a Gaussian input x, and is given by
 CFB = lim, (85) n?8 Kx
 
 Figure 8.	Block diagram representation a non-white Gaussian channel y = x+z and the coding scheme considered in [13].
 where Kx1n is the covariance matrix of x1n and, for every k ? N, the input xk is allowed to depend upon the channel outputs  (since there exists a causal, noise-less feedback channel with one-step delay).
 In [13], it was shown that if z is an auto-regressive moving-average process of M-th order, then CFB can be achieved by the scheme shown in Fig. 8. In this system, B is a strictly causal and stable finite-order filter and v81 is Gaussian with vk = 0 for all k > M and such that vn1 is Gaussian with a positive-definite covariance matrix KvM1 .
 Here we use the ideas developed in Section IV to show that the information rate achieved by the capacity-achieving scheme proposed in [13] drops to zero if there exists any additive disturbance of length at least M and finite differential entropy affecting the output, no matter how small. To see this, notice that, in this case, and for all n > M,
 	(86)
 = h(yn1) - h((In + Bn)zn1 + vn1|vM1 )	(87)
 = h(yn1) - h((In + Bn)z1n|vM1 )	(88)
 = h(yn1) - h((In + Bn)z1n) = h(yn1) - h(zn1 )	(89)
 	= h((I + B )z1 + v1)	h(z1),	(90)
 	n	n	n	n -	n
 since det(In+Bn) = 1. From Theorem 3, this gap between differential entropies is precisely the entropy gain introduced by In+Bn to an input z1n when the output is affected by the disturbance vM1 . Thus, from
 Theorem 3, the capacity of this scheme will correspond to , where are the zeros of 1 + B(z), which is precisely the result stated in [13, Theorem 4.1].
 However, if the output is now affected by an additive disturbance d81	not passing through B(z) such that dk = 0, ?k > M and |h(d1M)| < 8, with d, then we will have
 	yn1 = vn1 + (In + Bn)z1n + d1n.	(91)
 In this case,
 	(92)
 = h(yn1) - h((In + Bn)z1n + vn1 + d1n|vM1 )	(93)
 = h(yn1) - h((In + Bn)zn1 + d1n|vM1 )	(94)
 = h(yn1) - h((In + Bn)z1n + d1n).	(95)
 But limn?8 n1 (h((In + Bn)z1n + vn1 + dn1) - h((In + Bn)z1n + d1n)) = 0, which follows directly from applying Theorem 3 to each of the differential entropies. Notice that this result holds irrespective of how small the power of the disturbance may be.
 Thus, the capacity-achieving scheme proposed in [13] (and further studied in [25]), although of groundbreaking theoretical importance, would yield zero rate in any practical situation, since every real signal is unavoidably affected by some amount of noise.
 VIII. CONCLUSIONS
 This paper has provided a geometrical insight and rigorous results for characterizing the increase in differential entropy rate (referred to as entropy gain) introduced by passing an input random sequence through a discrete-time linear time-invariant (LTI) filter G(z) such that the first sample of its impulse response has unit magnitude. Our time-domain analysis allowed us to explain and establish under what conditions the entropy gain coincides with what was predicted by Shannon, who followed a frequencydomain approach to a related problem in his seminal 1948 paper. In particular, we demonstrated that the entropy gain arises only if G(z) has zeros outside the unit circle (i.e., it is non-minimum phase, (NMP)).
 This is not sufficient, nonetheless, since letting the input and output be u and y = Gu, the difference
  is zero for all n, yielding no entropy gain. However, if the distribution of the input process
 u satisfies a certain regularity condition (defined as being “entropy balanced”) and the output has the form y = Gu+z, with z being an output disturbance with bounded differential entropy, we have shown that the entropy gain can range from zero to the sum of the logarithm of the magnitudes of the NMP zeros of G(z), depending on how z is distributed. A similar result is obtained if, instead of an output disturbance, we let G(z) have a random initial state. We also considered the difference between the differential entropy rate of the entire (and longer) output of G(z) and that of its input, i.e.,, where ? + 1 is the length of the impulse response of G(z). For this purpose, we introduced the notion of “effective differential entropy”, which can be applied to a random sequence whose support has dimensionality smaller than its dimension. Interestingly, the effective differential entropy gain in this case, which is intrinsic to G(z), is also the sum of the logarithm of the magnitudes of the NMP zeros of G(z), without the need to add disturbances or a random initial state. We have illustrated some of the implications of these ideas in three problems. Specifically, we used the fundamental results here obtained to provide a simpler and more general proof to characterize the rate-distortion function for Gaussian non-stationary sources and MSE distortion. Then, we applied our results to provide sufficient conditions for equality in an information inequality of significant importance in networked control problems. Finally, we showed that the information rate of the capacity-achieving scheme proposed in [13] for the autoregressive Gaussian channel with feedback drops to zero in the presence of any additive disturbance in the channel input or output of sufficient (finite) length, no matter how small it may be.
 APPENDIX
 A. Proofs of Results Stated in the Previous Sections
 Proof of Proposition 1: Let su2 be the per-sample variance of u81 , thus . Let
 yn?+1 ,Fnu1n. Then Kyn?+1 = su2FnFTn = su2In-?, where In-? is the (n-?)×(n-?) identity matrix.
 As a consequence, , and thus .	
 Proof of Lemma 1: Let be the intervals (bins) in R where the sample PDF is constant. Let  be the probabilities of these bins. Define the discrete random process c81 , where c(i) = l if and
 only if ui ? bl. Let y where Fn ? R(n-?)×n has orthonormal rows. Then
 		(96)
 	,	(97)
 where the inequality is due to the fact that u1n and yn?+1 are deterministic functions of u1n, and hence c1n ?? u1n ?? yn?+1. Subtracting h(u1n) from (96) we obtain
 h(yn?+1) - h(u1n) = h(yn?+1|c1n) + I(c1n;u1n) - h(u1n)
 	= h(y?+1 c1)	h(u1 c1).	(98)
 	n	| n -	n| n	(99)
 Hence,
 		(100)
 where the last equality follows from Lemma 7 (see Appendix B) whose conditions are met because, given c1n, the sequence u1n has independent entries each of them distributed uniformly over a possibly different interval with bounded and positive measure. The opposite inequality is obtained by following the same steps as in the proof of Lemma 7, from (199) onwards, which completes the proof. 
 Proof of Lemma 2: Let yn1 , [?Tn|FTn]T wn1, where [?Tn|FTn]T ? Rn×n is a unitary matrix and where ?n ? R?×n and Fn ? R(n-?)×n have orthonormal rows. Then
 	h(yn?+1) = h(yn1) - h(y?1|yn?+1)	(101)
 	= h(wn1) - h(y?1|yn?+1)	(102)
 We can lower bound  as follows:
 h(y?1|yn?+1) = h(?nu1n + ?nvn1 |Fnu1n + Fnvn1)	(103)
 = h(?nu1n + ?nvn1 |Fnu1n + Fnvn1 , vn1)	(104)
 = h(?nu1n |Fnu1n + Fnvn1 , vn1)	(105)
 = h(?nu1n |Fnu1n, vn1)	(106)
 = h(?nu1n |Fnu1n).	(107)
 Substituting this result into (102), dividing by n and taking the limit as n ? 8, and recalling that, since
 u81 is entropy balanced, then , lead us to 
 0.
 The opposite bound over  can be obtained from
 	,	(108)
 where (wG)1n is a jointly Gaussian sequence with the same second-order moment as wn1. Therefore,
 , with sw2 (i) being the variance of the sample w(i). The fact
 that wn1 has a bounded second moment at each entry w(i), and replacing the latter inequality in (102),
 satisfy , which finishes the proof.
 
 Proof of Lemma 3: Let yn1 , [?Tn|FTn]T wn1 where [?nT|FTn]T ? Rn×n is a unitary matrix and where ?n ? R?×n and Fn ? R(n-?)×n have orthonormal rows. Since wn1 = Gnu1n, we have that
 	?nwn1 = ?nGnu1n.	(109)
 Let ?nGn = AnSnBn be the SVD of ?nGn, where An ? R?×? is an orthogonal matrix, Bn ? R?×n has orthonormal rows and Sn ? R?×? is a diagonal matrix with the singular values of ?nGn. Hence
 	h(?nwn1) = h(?nGnu1n) = h(AnSnBnun1) = logdet(Sn) + h(Bnu1n).	(110)
 It is straightforward to show that the diagonal entries in Sn are lower and upper bounded by the smallest and largest singular values of Gn, say smin(n) and smax(n), respectively, which yields
 ? logsmin(n) + h(Bnu1n) = h(?nwn1) = ? logsmax(n) + h(Bnu1n).
 But from Lemma 4, limn?8(1/n)smin(n) = limn?8(1/n)smax(n) = 0, and thus	(111)
 	,	(112)
 where the last equality is due to the fact that u81	is entropy balanced. This completes the proof.	
 	Proof of Lemma 4:	The fact that limn?8 ?n(AnATn) is upper bounded follows directly from
 the fact that A(z) is a stable transfer function. On the other hand, An is positive definite (with all its eigenvalues equal to 1), and so AnATn is positive definite as well, with limn?8 ?1(AnATn) = 0. Suppose that limn?8 ?1(AnATn) = 0. If this were true, then it would hold that limn?8 ?n(A-n1A-nT ) = 8. But A-n1 is the lower triangular Toeplitz matrix associated with A-1(z), which is stable (since A(z) is minimum phase), implying that limn?8 ?n(A-n1A-1 T ) < 8, thus leading to a contradiction. This completes the proof.	
 Proof of Lemma 5: Since Qn is unitary, we have that
 1 wn
 	,	(113)
 	vn1	z¯1n
 where
 wn1 ,vn1 + z¯1n,	(114) vn1 ,DnRnu1n,	(115)
 	¯z1n ,Qnz1n.	(116)
 Applying the chain rule of differential entropy, we get
 	.	(117)
 Notice that wm1 = [Dn]1mRnu1n+[Qn]1mz1n. Thus, it only remains to determine the limit of as n ? 8. We will do this by deriving a lower and an upper bound for this differential entropy and show that these bounds converge to the same expression as n ? 8.
 To lower bound we proceed as follows
 	(118)
 	(119)
 	(120)
 	(121)
 	(122)
 	(123)
 	(124)
 	(125)
 ,	(126)
 where (a) follows from including ¯z as well) to the conditioning set, while (b) and (d) stem from the independence between u81 and ¯z81 . Inequality (c) is a consequence of h(X +Y ) = h(X), and (e) follows from including ¯z to the conditioning set in the second term, and noting that  is not reduced upon the knowledge of zn1.
 On the other hand,
 	,	(127)
 then, by inserting (127) and (126) in (118), dividing by n, and taking the limit n ? 8, we obtain
 m
 	nlim?8 n1h(wnm+1 |wm1 ) = nlim?8 n1 h(un1) - Xi=1 logdn,i - h([Rn]m1 un1)!	(128)
 m
 ,
 where the last equality is a consequence of the fact that u81	is entropy balanced.	(129)
 1
 We now derive an upper bound for . Defining the random vector
 x,
 we can write
 		(130)
 where
 m+1[Dn]n , diag{dn,m+1,dn,m+2,...,dn,n}.
 Therefore,	(131)
 		(132)
 	.	(133)
 Notice that by Assumption 3, , and thus is restricted to the span of  of dimension ?n = ?, for all n = m + ?. Then, for n > m + ?n, one can construct a unitary matrix H, such that the rows of An ? R?×(n-m) span the space spanned by the columns of  and such that B
 0. Therefore, from (133), h(wnm+1 |wm1 ) = logdet(m+1[Dn]n) + h(Hnxmn +1 + Hn(m+1[Dn]n)-1z¯mn +1)
 = logdet(m+1[Dn]n) + h(Bnxmn +1) + h(Anxmn +1 + An(m+1[Dn]n)-1z¯mn +1|Bnxnm+1)
 
 KA
 
 where KAnxmn +1 and KAn(m+1[Dn]n)-1z¯mn +1 are the covariance matrices of A and A,
 respectively, and where the last inequality follows from [26]. The fact that ?max(Kxmn +1) and ?max(K¯zmn +1) are bounded and remain bounded away from zero for all n, and the fact that ?min(m+1[Dn]n) either grows with n or decreases sub-exponentially (since the m first singular values decay exponentially to zero, with |detDn| = 1), imply in (134) that
 	.	(134)
 But the fact that detDn = 1 implies that . This, together with the
 assumption that u81	is entropy balanced yields
 	,	(135)
 which coincides with the lower bound found in (129), completing the proof.	
 Proof of Lemma 6: The transfer function G(z) can be factored as G(z) = G˜(z)F(z), where G˜(z) is stable and minimum phase and F(z) is stable with all the non-minimum phase zeros of G(z), both being biproper rational functions. From Lemma 4, in the limit as n ? 8, the eigenvalues of G˜ TnG˜ n are lower and upper bounded by ?min(G˜ T G˜) and ?max(G˜ T G˜), respectively, where 0 < ?min(G˜ T G˜) =
 ?max(G˜ T G˜) < 8. Let G˜ and Fbe the SVDs of G˜ n and F n, respectively, with d˜n,1 = d˜n,2 = ··· = d˜n,n and dn,1 = dn,2 = ··· = dn,n being the diagonal entries of the diagonal matrices D˜ n, Dn, respectively. Then
 	G	(136)
 Denoting the i-th row of Rn by rTn,i be, we have that, from the Courant-Fischer theorem [27] that
 	vkGvk2	(137)
 	=	max		(138)
 v?span{rn,k}ik=1 :kvk=1
 	= d2n,id˜2n,n	(139)
 Likewise,
 	?i(GTnGn) = vkGvk	(140)
 	=	min		(141)
 v?span{rn,k}nk=i :kvk=1
 	= d2n,id˜2n,1	(142)
 Thus
 	.	(143)
 The result now follows directly from Lemma 8 (in the appendix).	
 Proof of Theorem 2 : In this case
 	[Dn]1mRnu1n + [Qn]1mz1n = [Dn]1mRnu1n + [Qn]1m[F]1ns1?.	(144)
 Notice that the columns of the matrix [Qn]1m[F]1n ? Rm×? span a space of dimension ?n ? {0,1,... ,?¯}, which means that one can have [Qn]1m[F]1n = 0 (if ?n = 0). In this case (i.e., if limn?8[Qn]1m[F]1n = 0) then the lower bound is reached by inserting the latter expression into (12) and invoking Lemma 6.
 We now consider the case in which limn?8[Qn]1m[F]1n =6 0. This condition implies that there exists an N sufficiently large such that ?n = 1 for all n = N. Then, for all n = N there exist unitary matrices
 	Hn ,	(145)
 
 where An ? R?n×m and An ? R(m-?n)×m have orthonormal rows, such that
 	1	1
 H(146)
 0 Thus
 	(147)
 		(148)
 
 The first differential entropy on the RHS of the latter expression is uniformly upper-bounded because u81 is entropy balanced, [Dn]1m has decaying entries, and. For the last differential entropy,
 
 notice that [Dn]1mRn = 1[Dn]m[Rn]1m. Consider the SVD An1[Dn]m[Rn]1m = V TnSnW n, with V n ? R(m-?n)×(m-?n) being unitary, Sn ? R(m-?n)×(m-?n) being diagonal, and W n ? R(m-?n)×n having orthonormal rows. We can then conclude that
 	.	(150)
 Now, the fact that
 A	WW T ST V = V T SST V
 allows one to conclude that
 	.	(151)
 Recalling that A	and that Hn ? Rm×m is unitary, it is easy to show (by using the Cauchy
 interlacing theorem [27]) that
 	,	(152)
 with equality achieved if and only if A . Substituting this into (151) and then the latter into (150) we arrive to
 	.	(153)
 Substituting this into (12), exploiting the fact that u81 is entropy balanced and invoking Lemma 6 yields the upper bound in (25). Clearly, this upper bound is achieved if, for example, [Qn]1?¯[F]1n([Qn]1?¯[F]1n)T is non-singular for all n sufficiently large, since, in that case, ?n = ¯? and we can choose An = [I?¯ 0]
 
 and An = [0Im-?¯]. This completes the proof.	
 Proof of Theorem 3 : As in (28), the transfer function G(z) can be factored as G(z) = G˜(z)F(z), where G˜(z) is stable and minimum phase and F(z) is a stable FIR transfer function with all the nonminimum-phase zeros of G(z) (m in total). Letting u˜1n ,G˜ nu1n, we have that h(yn1) = h(F nu˜n1 + z1n), h(u˜1n) = h(u1n), and that  is entropy balanced (from Lemma 3). Thus,
 	h(yn1) - h(u1n) = h(Gnu1n + z1n) - h(u1n) = h(F nu˜n1 + z1n) - h(u˜1n).	(154)
 This means that the entropy gain of Gn due to the output disturbance z81 corresponds to the entropy gain of F n due to the same output disturbance. One can then evaluate the entropy gain of Gn by applying Theorem 2 to the filter F(z) instead of G(z), which we do next.
 Since only the first m values of z81 are non zero, it follows that in this case F = [Im |0]T (see Assumption 3). Therefore, det([Qn]1m[F]1n([Qn]1m[F]1n)T ) = det(1[Qn]m(1[Qn]m)T ) and the sufficient condition given in Theorem 2 will be satisfied for ? = m if limn?8 |det(1[Qn]m)| > 0, where now QnT is the left unitary matrix in the SVD F. We will prove that this is the case by using a
 contradiction argument. Thus, suppose the contrary, i.e., that
 lim det 1[Q
 	n?8	n]m = 0.	(155)
 Then, there exists a sequence of unit-norm vectors, with vn ? Rm for all n, such that
 		(156)
 For each n ? N, define the n-length image vectors t, and decompose them as
 	t	(157)
 such that an ? Rm and ßn ? Rn-m. Then, from this definition and from (156), we have that
 	kank2 + kßnk2 = 1,	?n ? N,	(158a)
 	lim kank = 0	(158b)
 n?8
 	nlim?8 kßnk = 1	(158c)
 As a consequence,
 	,	(159)
 where the last equality follows from the fact that, by construction,  is in the span of the first m rows of Qn, together with the fact that Qn is unitary (which implies that ). Since the top m
 entries in Dn decay exponentially as n increases, we have that
 	,	(160)
 where ?n is a finite-order polynomial of n (from Lemma 8, in the Appendix). But
 		(161)
 		(162)
 		(163)
 Taking the limit as n ? 8,
 (165)
 where we have applied (158) and the fact that smax(([F n]1m)T ) is bounded and does not depend on n. Now, notice that [F n]mn +1([F n]mn +1)T is a Toeplitz matrix with the convolution of f and f- (the impulse response of F and its time-reversed version, respectively) on its first row and column. It then follows from [28, Lemma 4.1] that
 		(166)
 (the inequality is strict because all the zeros of F(z) are strictly outside the unit disk). Substituting this into (165) we conclude that
 	,	(167)
 which contradicts (160). Therefore, (155) leads to a contradiction, completing the proof.	
 Proof of Theorem 6: Denote the Blaschke product [29] of A(z) as
 	,	(168)
 which clearly satisfies		
 |B(ej?)| = 1,	?? ? [-p,p]	(169)
 	,	(170)
 where b0 is the first sample in the impulse response of B(z). Notice that (169) implies that limn?8 n1 kBnu1nk2 = limn?8 n1 ku1nk2 for every sequence of random variables u81 with uniformly bounded variance. Since B(z) has only stable poles and its zeros coincide exactly with the poles of A(z), it follows that B(z)A(z) is a stable transfer function. Thus, the asymptotically stationary process x˜81 defined in (69) can be constructed as
 	x˜1n ,Bnx1n,	(171)
 where Bn is a Toeplitz lower triangular matrix with its main diagonal entries equal to b0.
 The fact that B(z) is biproper with b0 as in (170) implies that for any u1n with finite differential entropy
 ,	(172) ,
 G
 which will be utilized next.
 For any given n = M, suppose that C(z) is chosen and x1n and u1n are distributed so as to minimize I(x1n;Cnx1n + u1n) subject to the constraint E[kyn1 - x1nk2] = E[k(Cn - I)x1nk2] + E[ku1nk]2 = D (i.e., x1n,u1n is a realization of Rx,n(D)), yielding the reconstruction
 	y.	(173)
 Since we are considering mean-squared error distortion, it follows that, for rate-distortion optimality, u1n must be jointly Gaussian with x1n. From these vectors, define
 	u˜1n ,Bnu1n,	(174)
 y˜n1 ,Bnyn1 = B1nCnBn-1x˜1n + u˜1n, (175) y¯n1 ,y˜n1 + d1n = B1nCnB-n1x˜1n + u˜1n + d1n. (176)
 where d1n is a zero-mean Gaussian vector independent of (u˜1n,x˜1n) with finite differential entropy such
 that dk = 0, ?k > M. Then, we have that2 nRx,n(D) = I(x1n;yn1) (=a) I(Bnxn1;Bnyn1) = I(x˜1n;y˜n1) (177)
 (178)
 (179)
 (180)
 (181)
 (182)
 (183)
 (184)
 (185)
 where (a) follows from Bn being invertible, (b) is due to the fact that y˜n1 = Cnx˜1n+u˜1n, (c) holds because u1n ? x1n. The equality (d) stems from h(u˜1n) = h(u1n) - nG (see (172)). Equality holds in (e) because x˜1n ? (u˜1n,d1n) and in (f) because of (176). The last inequality holds because y¯n1 = y˜n1+d1n and dn1 ? y˜n1. But from Theorem 3, limn?8 n1 (h(u˜1n +d1m)-h(u1n)) = 0, and thus Rx,n(D) = limn?8 n1 (x˜1n;y¯n1)+G.
 At the same time, the distortion for the source x˜1n when reconstructed as y¯n1 is
 
 where (a) holds because kd1nk = kd1Mk is bounded, and (b) is due to the fact that, in the limit, B(z) is a unitary operator. Recalling the definitions of Rx˜(D) and Rx˜(D), we conclude that limn?8 n1 (x˜1n;y¯n1) = Rx˜,n(D), and therefore
 	.	(188)
 In order to complete the proof, it suffices to show that . For this
 purpose, consider now the (asymptotically) stationary source x˜1n, and suppose that yˆn1 = x˜1n +u1n realizes Rx˜,n(D). Again, x˜1n and u1n will be jointly Gaussian, satisfying yˆn1 ? un1 (the latter condition is required for minimum MSE optimality). From this, one can propose an alternative realization in which the error 2The change of variables and the steps in this chain of equations is represented by the block diagrams shown in Fig. 6. sequence is u˜ ,Bnu1n, yielding an output y˜n1 = x˜1n + u˜1n with y˜n1 ? u˜1n. Then
 nRx˜,n(D) = I(x˜1n;yˆn1) = h(x˜1n) - h(x˜1n|yˆn1)	(189)
 (a) 1) - h(u1n) = h(x˜n	(190)
 (b) 1) - h(u˜1n) - nG = h(x˜n	(191)
 (c) 1) - h(u˜1n|y˜n1) - nG = h(x˜n	(192)
 (d) 1) - h(x˜1n|y˜n1) - nG = h(x˜n	(193)
 = I(x˜1n;y˜n1) - nG	(194)
 = I(Bnx1n;Bnyn1) - nG	(195)
 (e) 1;yn1) - nG, = I(xn	(196)
 where (a) follows by recalling that yˆn1 = x˜1n + u1n and because yˆn1 ? un1, (b) stems from (172), (c) is a consequence of y˜n1 ? u˜1n, (d) follows from the fact that y˜n1 = x˜n1 +u˜n1. Finally, (e) holds because Bn is invertible for all n. Since, asymptotically as n ? 8, the distortion yielded by yn1 for the non-stationary source x1n is the same which is obtained when x˜1n is reconstructed as yˆn1 (recall (169)), we conclude that
 , completing the proof.	
 B. Technical Lemmas
 Lemma 7. Let u81 be a random process with independent elements, and where each element ui is uniformly distributed over possible different intervals , such that amax > |ai| > amin > 0,?i ? N,
 for some positive and bounded amin < amax. Then u81	is entropy balanced.	N
 Proof: Without loss of generality, we can assume that ai > 1, for all i (otherwise, we could scale the input by 1/amin, which would scale the output by the same proportion, increasing the input entropy by nlog(1/amin) and the output entropy by (n - ?)log(1/amin), without changing the result). The input
 vector u1n is confined to an n-box Un (the support of un1) of volume and has entropy
 . This support is an n-box which contains -boxes of different k-volume. Each of
 these k-boxes is determined by fixing n - k entries in u1n to ±ai/2, and letting the remaining k entries sweep freely over . Thus, the k-volume of each k-box is the product of the k support sizes ai of the associated selected free-sweeping entries. But recalling that ai > 1 for all i, the volume of each k-box can be upper bounded by. With this, the added volume of all the k-boxes contained in the original n-box can be upper bounded as
 	.	(197)
 We now use this result to upper bound the entropy rate of y.
 Let y where  is a unitary matrix and where ?n ? R?×n and
 Fn ? R(n-?)×n have orthonormal rows. From this definition, y will distribute over a finite region
 , corresponding to the projection onto the k-dimensional span of the rows of Fn. Hence,  is upper bounded by the entropy of a uniformly distributed vector over the same support, i.e., by , where  is the (n - ?)-dimensional volume of this support. In turn,
  is upper bounded by the sum of the volume of all (? - k)-dimensional boxes contained in
 the n-box in which u1n is confined, which we already denoted by, and which is upper bounded
 as in (197). Therefore, !
 Dividing by n and taking the limit as n ? 8 yields
 		(198)
 On the other hand,
 	,	(199)
 where (a) follows because  is an orthogonal matrix. Letting (yG)1? correspond to the jointly Gaussian sequence with the same second-order moments as y?1, and recalling that the Gaussian distribution maximizes differential entropy for a given covariance, we obtain the upper bound
 
 where (a) follows since the  are independent, and (b) stems from the fact that ?n ? R?×n has orthonormal rows and from the Courant-Fischer theorem [27]. Since  is bounded for all n,
 we obtain by substituting (200) into (199) that . The combination of this with (198) yields , completing the proof.
 
 We re-state here (for completeness and convenience) the unnumbered lemma in the proof of [15, Theorem 1] as follows:
 Lemma 8. Let the function ? be as defined in (23) but for a transfer function G(z) with no poles and having only a finite number of zeros, m of which lie outside the unit circle. Then,
 		(201)
 	??a2n,l	,otherwise ,
 where the elements in the sequence {an,l} are positive and increase or decrease at most polynomially
 with n.	N
 Lemma 9. Let  be rational transfer function of order p with relative degree 1, with initial state x be a biproper rational transfer function of order t with initial state s0 ? Rt. Let
 y , u - P(z)T(z)y,
 where u is an exogenous signal. Then	(202)
 		(203)
 where the initial state of D(z) is x0 and the initial state of T/(TD +NG) can be taken to be [x0 s0].
 Proof: Let. Define the following variables:
 	(204) Then the recursion corresponding to P(z) is
 	,	(205)
 		(206)
 This reveals that the initial state of P(z) corresponds to			
 x0 = [x1-p x2-p	···	x0].	(207)
 Let  and . Then v = T(z)w can be written as
 X
 t
 	sk =i=1 ?isk-i + wk,	(208)
 X
 t
 vk ==0 ?isk-i,	k = 1,	(209) i
 which reveals that the initial state of T(z) can be taken to be
 	s0 , [s1-t s2-t ··· s0].	(210)
 Since yk = uk - vk, it follows that
 	.	(211)
 Combining the above recursions, it is found that y is related to the input u by the following recursion:
 	,	(212)
 	t	p
 sk =?isk-i + Xi=1 nixk-i, i=1
 p
 	yk = xk - Xi=1 dixk-i,	k = 1,
 which corresponds to	k = 1,	(213)
 (214)
 		(215)
 init. state x
 init. state [x0,s0] ﻿We characterize the rate-distortion function for zero-mean stationary Gaussian sources under the MSE fidelity criterion and subject to the additional constraint that the distortion is uncorrelated to the input. The solution is given by two equations coupled through a single scalar parameter. This has a structure similar to the well known water-filling solution obtained without the uncorrelated distortion restriction. Our results fully characterize the unique statistics of the optimal distortion. We also show that, for all positive distortions, the minimum achievable rate subject to the uncorrelation constraint is strictly larger than that given by the un-constrained rate-distortion function. This gap increases with the distortion and tends to infinity and zero, respectively, as the distortion tends to zero and infinity. 
 Many lossy source coding schemes have the property that the end-to-end reconstruction error is uncorrelated with the source. We refer to such schemes as uncorrelated distortion (UD) coders. As an example, consider a typical transform coder, as depicted in Fig. 1. Here, a random vector X ? RN is first transformed by an analysis transform T ? RN×N to yield U = TX. Then U is quantized, yielding the vector, Uˆ = Q(U). The input signal is finally approximated by Y = T˜Uˆ, where T˜ ? RN×N is the synthesis transform, cf. [1,2]. If the quantization error  is 
 uncorrelated to U, and if TT˜ = I, then it is easy to show that Y - X is uncorrelated to X, thus yielding a UD coder. 
 More generally, any quantization scheme satisfying the following two properties constitutes a UD coder: a) The error introduced by the quantizer is uncorrelated to its input; b) The linear processing (if any) before and after the quantizer yields perfect reconstruction (PR) in the absence of quantization errors. Property a) is satisfied in many cases, e.g. in high-resolution coding [3] or when a quantizer with dither (either subtractive [4] or non-subtractive [5]) is employed. On the other hand, the PR condition (Property b)) is often imposed (sometimes implicitly) in the design of filter banks [6], transform coders [1,2], and feedback quantizers [7,8]. Thus, any PR source coder using, for example, subtractively dithered quantization, is a UD coder. The rate-distortion performance of any UD coder can be compared to the underlying Shannon’s rate-distortion function R(D) of the source, for a given distortion metric. One may question whether such a comparison is, in fact, fair. After all, the additional constraint that the end-to-end distortion is uncorrelated with the source is not imposed upon R(D). With this in mind, let R?(D) denote the rate distortion function with the additional constraint that the end-to-end distortion is uncorrelated to the source. (A formal definition of R?(D) is given in Section 2). Clearly, R?(D)=R(D).  However, to the best of the authors’ knowledge, the problem of characterizing R?(D) has not been formally addressed before. Therefore, questions such as in which cases (if any) R?(D) equals R(D), and how R?(D) can be achieved, appear to be unanswered. 
 In this paper, we not only give conclusive answers to the above questions, but more importantly, we completely characterize R?(D) for the quadratic Gaussian case2 as a lower bound for the rate achievable under the uncorrelated distortion constraint . We show, in Section 2, that R?(D) can be parameterized through a single scalar variable a > 0. This is a result which parallels the conventional water-filling equations that describe R(D). We characterize the unique optimal statistics that the reconstruction error Y-X needs to have in order to achieve R?(D), for a given Gaussian source X. In particular, we show that Y-X must be Gaussian. In addition, we recast the results in a transform coding sense. More precisely, we show that if the quantization errors are Gaussian, independent both mutually and from the source, then the Karhunen-Loève Transform (KLT) is optimal among all perfect reconstruction transforms, at all rates.  A comparative analysis between R?(D) and R(D) is then presented in Section 3. There we show that R?(D) is convex and monotonically decreasing in D, and that R?(D) >R(D),?D >0, converging in the limit as R ? 8. 
 Furthermore, we show that R?(D)?0?D ?8, which is different from the well known result R(D)= 0?D=sX2 . 
 It is worth emphasizing that our results are not tied to any particular source coding architecture, but are general in the sense that any coding scheme in which the end-to-end distortion is uncorrelated with the source can do no better than R?(D). 
 Notation We use uppercase letters to represent random vectors, adding a subscript when referring to one of its elements, i.e., Xi is the i-th element of the random vector X. The expectation operator is denoted by E[·]. Uppercase bold letters are used for matrices. The positive-definite square root of a positive-definite matrix M is denoted by  . We write |M| and tr(M) for the determinant and the trace of a matrix M, respectively. The probability density function 
 (PDF) and covariance matrix of a random (column) vector X are denoted respectively by fX and K , where XT is the transpose of X. We write KX,Y for the cross-covariance matrix between two random vectors X and Y . The spectrum of a w.s.s. random process Z with autocorrelation function  is denoted by , 
 ?? ? [-p,p]. The differential entropy and the differential entropy per dimension of an N-length random vector X are denoted, respectively, by  . When X is a random process,   denotes the differential entropy rate of X. 
 We use  to refer, respectively, to the mutual information and the mutual information per dimension between two random vectors X and Y . When X and Y are random processes ,  denotes the mutual information rate between X and Y . We write a.e. for “almost everywhere”. 
 We begin by formalizing the definition of the quadratic rate-distortion function under the constraint that the end-to-end distortion be uncorrelated with the source. Then, in Section 2.1, we characterize this function for Gaussian random vectors, deferring the case of Gaussian stationary processes to Section 2.2. 
 Definition 1. The uncorrelated quadratic rate-distortion function R?(D) for a random vector (source) X ? RN is defined as 
 We now present one of the main results of this paper, namely that, for Gaussian vector sources, R?(D) is given by two equations linked through a single scalar parameter. This resembles the “water-filling” equations that describe R(D). The proof of this result, which is presented in Theorem 1, makes use of the following lemma. 
 Lemma 1. Let X ? RN ~ N(0,KX). Let Z ? RN and ZG ? RN be two random vectors with zero mean and the same covariance matrix, i.e., KZ = KZG, and having the same crosscovariance matrix with respect to X, that is, KX,Z = KX,ZG. If ZG and X are jointly Gaussian, and if Z has any distribution, then 
 If furthermore |KX+Z| = |KX+ZG| > 0, then equality is achieved in (2) iff Z ~ N(0,KZ) with Z and X being jointly Gaussian. 
 where  is the relative entropy (or Kullback-Leibler distance) between the two probability density functions f and g. The equality (a) follows from the fact that log(fZG|YG(z|y)) is a quadratic form of z and y, and from the fact that KZ,Y = KZG,YG. The inequality follows from the 
 fact thatfor all y  such that fY , with equality iff f = g. Thus, equality is achieved iff|	fZ|G|YG| =y = fGZ||Y =y0 
 Remark 1. We note that the above Lemma generalizes Lemma II.2 in [12], by relaxing the requirement that fZ|X = fZ and fZG|X = fZG, to the requirement KX,Z = KX,ZG. 
 Theorem 1. Let the source X ? RN be a zero mean Gaussian random vector with positive-definite covariance matrix KX, having eigenvalues . Then 
 Proof. Let U denote the set of all N-length random vectors uncorrelated to X, and define the sets 
 With the above definitions, (1) can be written as where (a) follows directly from Lemma 1 and where	 holds since the definition of BD (see (7)), guarantees that both h¯(X + Z) and h¯(Z) exist. 
 We now prove, by contradiction, that the minimizer of log|K-Z1KX +I| in GD, namely Z, is such that tr . For this purpose, suppose that b  , and let  be the eigenvalues of  be a Gaussian random vector with covariance matrix 
 since b >   is a strictly increasing function. Thus 	cannot be a minimizer of   unless tr . 
 The minimizer of log(|KX + KX|/|KZ|) subject to tr(KZ)/N = D can be found using a variational approach. More precisely, the covariance matrix of the minimizer, KZ, must necessarily be such that the derivative of the Lagrangian 
 with respect to KZ is zero at K , for some ß ? R, which is equivalent to the condition that the matrix differential ?L(KZ) = 0, ??KZ. Using the fact that ? log|M| = tr(M-1?M), for any positive definite matrix M, the necessary condition for Z to be a minimizer takes the form 
 The fact that KZ needs to be positive definite implies that ß > 0 and that it is infeasible to have a negative sign before the square root in (11). This, together with the change of variable , leads directly to (6), with a > 0. On the other hand, the value of a must be such that the equality constraint tr  is satisfied. From (11), and applying Lemma 2 (see appendix), this requirement is equivalent to  , which proves (5). 
 Similarly, (4) is obtained by substituting (11) into (8) and then applying Lemma 2, which yields: 
 The uniqueness of a is easily verified by noting that the right hand side of (5) is monotonically increasing with a. Since a = 1/ß is unique, it follows from (11) that the covariance matrix of  = argminZ?BD I¯(X;X + Z) is unique , completing the proof. 
 Transform Coding Realization of R?(D): Closer examination of Lemma 2, when used in (6), suggests that, for a Gaussian source X, R?(D) can be achieved by the transform coding architecture shown in Fig. 1. More precisely, an end-to-end distortion having the optimal covariance matrix KZ given by (6) is obtained by choosing the transform T such that T?T T = KX, where ? diag(?1,...,?N) (i.e., the KLT transform for X), and by having a Gaussian random vector of quantization errors E with  Interestingly, here 
 E[EET ] is not a scaled identity matrix, as is usually the case in KLT transform coding, cf. [13]. This discrepancy arises from the approximation E[Ek2] = cE[Uk2]2-2bk, commonly used to link the variance of Ek to the bit-rate bk at which each k-th transform coefficient is quantized. In this expression, c>0 is a constant that depends on the PDF of the source and on the type of quantizer. The well known optimal bit allocation analyzed, e.g., in [13], is based upon this formula, and thus minimizes the total bit-rate . On the other hand, the optimal quantization errors Ek implied by Theorem 1 need to be Gaussian, their variances being such that the end-to-end mutual information  is minimized.  Thus, the difference in the optimal values for  obtained in each case is due to the fact that . 
 The R?(D) function defined in (1) can be extended to random processes as follows: 
 Definition 2. The uncorrelated quadratic rate-distortion function R?(D) for a random process X is defined as 
 The R?(D) function for stationary Gaussian random processes can be derived from the results obtained in Section 2.1, by restricting to random vectors X ? RN having a Toeplitz covariance matrix, and then letting N ? 8. More precisely, we have the following result: 
 Theorem 2. Let the source X be a Gaussian stationary random process with spectrum SX(?) such that SX(?) > 0, a.e. on [-p,p]. Then 
 [Y1 ··· YN]T , N ? N. It is known that ??(	) = ??(	+1),?N ? N, where ??(	) and ??(	+1) are 
 the smallest eigenvalues of KX(N) and KX(N+1), respectively (see e.g. [14, Theorem 4.3.8]). This result, together with Lemma 3, in the Appendix, and the fact that SX(?) > 0, a.e. on [-p,p], implies that |KX(N)| > 0, for all N ? N. We can then apply Theorem 1 to each X(N), ?N ? N, obtaining 
 and where  denotes the set of eigenvalues of KX(N). From (14), the optimal distortion covariance matrix for X(N) is 
 Remark 2. It is interesting to note that the equations characterizing the optimal UD feedback converters derived in [8] achieve an end-to-end distortion whose spectrum is given precisely by (15). Furthermore, it is easy to show that such converters would achieve the R?(D) function if the noise due to the scalar quantization within the feedback loop were white Gaussian noise uncorrelated with the input. 
 The next theorem shows that R?(D) shares strict monotonicity and convexity with R(D), but deviates from R(D) in the asymptotic limit of large distortions. 
 Theorem 3. For any Gaussian random vector (stationary random process) X with positive definite covariance matrix Kis monotonically decreasing and convex. In addition,. 
 Proof. We present here only the proof for the case of Gaussian random vectors. The proof for 
 Gaussian stationary processes proceeds in an analogous fashion.	Monotonicity: We have that 
 , provided that   and   exist and that the latter derivative is non-zero. From (4), we obtain 
 proving that R?(·) is a strictly decreasing function (since a > 0). Convexity: The fact that a grows monotonically with increasing D, together with (22), imply that  ? ? D1 > D2, and thus R?(·) is convex. Limits: It is clear from (4) that lima?8 R? = 0 and lima?0 R? = 8. Since, as can be seen from (21), R? decreases monotonically with increasing a, ?a ? (0,8), it follows that R? ? 0 ?? a ? 8, and that R? ? 8 ?? a ? 0. 
 Similarly, it follows from (5) and the monotonicity of D with respect to a that, for fixed , D ? 8 ?? a ? 8 and D ? 0 ?? a ? 0. We then have that D? 8 ?? R? ? 0 and 
 Theorem 4. For any Gaussian random vector (stationary random process) X with positive definite covariance matrix KX (with SX(?) > 0,a.e. on [-p,p]), the following holds 
 Proof. We present here only the proof for the case of Gaussian random vectors. The proof for Gaussian stationary processes proceeds in an analogous fashion. Recall that for a Gaussian random vector X having positive covariance matrix with eigenvalues  one has that R(D) =  , with equality iff . As a consequence, 
 Equality (a) is obtained by substituting (4) and (5) into the right hand side of (24). The validity of (23)-(ii) then follows directly by taking the limit of the right hand side of (25) as a ? 0. In order to prove (23)-(i), we will show that D?(R) > D(R), where the function D?(·) is the inverse of R?(·) and D(R) is Shannon’s distortion-rate function. For this purpose, consider the random vector Y  W , where W ? RN×N is a symmetric, positive-definite matrix, and Z is as in Theorem 1-(iii). Notice that are not uncorrelated unless W = I. The mutual information per dimension between and 
 positive definite . We next show that for any D (and corresponding KZ), choosing an optimal matrix W yields a Y  whose distortion   is strictly smaller than 
 (KW , describing the distortion associated with , with the covariance matrix of Z as in (6) when R? = R. Since , we obtain, from applying Lemma 2, and after some algebraic manipulation, that 
 where the last inequality follows from the fact that . Finally, the fact that both R(D) and R?(D) are monotonically decreasing functions, together with (26), implies (23)-(i). This completes the proof. 
 that ?RL(D)/?a > 0, which implies that RL(D) increases monotonically with increasing D. On the other hand, for all D > 0, the ratio D?(R)/D(R) can be lower bounded by (26). It can be shown that this bound increases with a (and thus with D as well), tending to 8 as a ? 8, which is in agreement with Theorem 3. 
 In this work we have completely characterized R?(D), the quadratic Gaussian rate-distortion function subject to the constraint that the end-to-end distortion be uncorrelated with the source. We have further proved that this function shares convexity and monotonicity with Shannon’s rate-distortion function R(D), but R?(D) is positively bounded away from the latter, converging to R(D) only in the limit as the distortion tends to zero. We showed that the uncorrelation constraint causes the distortion to unboundedly grow as the rate tends to zero. We also discussed the achievability of R?(D) for random vectors and stationary random processes through transform coding and feedback quantization architectures. 
 with Q ? CN×N, and where Q:,k and Qk,  denote the k-th column and the k-th row of Q and 
 Q-1, respectively. If f(·) is analytic in a neighbourhood around each ?k, for k = 1,...,N, then 
 Lemma 3 ( Theorem 4.5.2 in [16] ). Let A8 be an infinite Toeplitz matrix with entry ak ? R on the k-th diagonal. Then the eigenvalues of A8 are contained in the interval m = ? = M, where m and M denote the essential infimum and supremum, respectively, of the function f(?)   . Moreover, if both m and M are finite and G(?) is any continuous function of 
 where the ?(N) are the eigenvalues of the sub-matrix A(N) ? RN×N of A8 centered about the main diagonal of A8. 
 ﻿In order that signals can be stored, transmitted or processed it is necessary that they first be converted into digital form. This, in turn, raises the problem of how to digitize data so as to achieve the best trade-off between data load and performance, i.e., “how to make the most out of a little”. Two issues are involved in this problem, namely temporal quantization (i.e., sampling) and spatial quantization. These two problems have traditionally been addressed separately. Indeed, there exists substantial literature dealing with the temporal quantization problem, covering both band-limited and non-band-limited signals. The usual underlying paradigm is that of an analysis filter, followed by a sampler, followed by a reconstruction filter. Various parts of this architecture can be optimized once other parts have been specified. On the other hand, spatial quantization has been studied extensively for a given sampling strategy, particularly in the framework of sigma delta conversion. Finally, it is also possible to formulate the joint design problem for sampling and spatial quantization. This typically leads to enhanced performance compared to that achievable by considering the two aspects separately.
 This paper will survey the general area of sampling and quantization and analyze methods for achieving efficient data representations for signal processing and control applications. We will show how, on the one hand, contemporary control theory can contribute to the design of sampling and quantization systems and, on the other hand, how these systems impact on the performance of modern feedback control systems.
 Key Words: Sampling, quantization, frames, model predictive control, constrained control, networked control systems
  
 1	INTRODUCTION
 We live in a data rich world. Most technological systems operate by first convertingcontinuoustime, continuousamplitude signals from the analog world into digital representations. This is a necessary precursor to allow signals to be stored, transmitted and processed without degradation other than that introduced by the analog-to-digital conversion itself.
 The above was indeed the motivation that led Alec Reeves to invent pulse-code modulation (PCM) seven decades ago [1]. In his 1938 patent [2], Reeves highlighted the main benefits of PCM, namely:
 
 These are remarkable statements for the time they were formulated. Indeed, most of these benefits have only become reality in recent times. Furthermore, the validity of the first two claims began to be formally determined years after they were formulated, and is still subject of ongoing research. In the pursuit of better quality at lower bit-rates (and lower costs), increasingly parsimonious methods are continually developed so as to acquire, process and represent signals digitally.
 This topic has also motivated important theoretical results, from areas such as information theory, functional analysis, optimization, communication theory, frames, wavelet theory, etc.. As we will discuss in this paper, also control theory has much to contribute to this circle of ideas. Conversely, much of the theory and techniques from digital signal processing are highly relevant to several aspects of control, e.g., networked control, where parsimonious signal representation is a key element, see, e.g., [3][4][5].
 In the present work we present some of the main strategies of sampling, quantization and reconstruction of analog, continuous-time signals. We will describe reconstruction quality and relate it to design constraints such as filter complexity, data-rate and sampling frequency. We also present some ideas concerning the joint problem of sampling-quantization, on one side, and reconstruction on the other. We limit our analysis to uniform sampling of scalar signals, sampling and reconstruction by single filters (as opposed to filter-banks), quantizers with scalar output and we will not discuss any issues related to further symbol encoding.
 The layout of the remainder of the paper is as follows: Section 2 presents the basics of PCM quantization and discusses some of the shortcomings that justify the introduction of a more general model for a samplingquantization-reconstruction system. Section 3 poses the sampling and reconstruction processes in a frame theoretic perspective. Section 4 is a review of some recent generalized results on the sampling and reconstruction problem. In Section 5 we present some basic aspects of scalar memory-less quantization and oversampling. Section 6 describes feedback quantizers. In particular, some of the basic principles of predictive and noise shaping (S?) analog-to-digital converters are presented. In Section 7 we present noise shaping quantizers that generalize S? converters based on model predictive control. Section 8 gives elements to analyze the joint problem of the quantization and sampling-reconstruction design, including some recent results and insights. In Section 9 we show how concepts related to sampling and quantization can be utilized in control problems. Section 10 draws conclusions. Finally, an Appendix is included with some of the basic concepts of frame theory necessary to understand several of the results presented in the main body of the paper.
 2	AD – CONVERSION FUNDAMENTALS
 In this section we will first describe PCM as a basic architecture used in AD–conversion applications. Various shortcomings of PCM will then motivate us to introduce later a more general framework.
 2.1	Basic PCM Scheme
 We consider the (simple) and idealized PCM system represented by the block diagram in Fig. 1.
 a(t)   a˜(t)
 	Nearest	Ideal Low-Pass
 Neighbour
 	Quantizer	Filter
 Figure 1: PCM system with ideal low-pass reconstruction filter.
 The usual paradigm associated with this setup is that the input signal a(t), t ? R, is taken to be band-limited to some frequency, say, fmax [Hz]. Then, in accordance with the Shannon-Whittaker sampling theorem [6], the sampling step is chosen as t = 1/(2fmax). Since the input signal is directly sampled, we have c[k] = a(tk), ?k ? Z. The nearest neighbour scalar quantizer in Fig. 1 corresponds to the non-linear transfer function Q?(·), defined by 
 	Q 	(1)
 where ?> 0 is the quantization step (see Fig. 2) and  denotes rounding to the closest integer value greater than a/?.
 c   u =Q?(c)
 Nearest
 Neighbour Quantizer
 Figure 2: Nearest Neighbour Scalar Quantizer.
 Thus, the output of Q? in Fig. 2 is the sequence of quantized values {u[k]}k?Z, where
 	u[k] = Q?(c[k]),	?k ? Z.	(2)
 The synthesis filter R in Fig. 1 is, in the simplest case, an ideal continuous time low-pass filter with cut-off frequency fmax = 1/(2t) [Hz] and impulse response sinc(2fmaxt), t ?
 R, where sinc . The output of R is the analog, continuous time signal ˜a, given by the mixed convolution 
 	a˜(t) = ? u[k]sinc(2fmaxt -k),	?t ? R	(3)
 k?Z
 If there were no quantization (i.e., if ?=0), then u[k] would equal c[k] for all k. In this situation, ˜a(t) in (3) would equal exactly the input a(t) for all t ? R, since, by virtue of the Shannon-Whittaker sampling theorem [6], if a(t) is bandlimited to fmax, it can be reconstructed from samples by the interpolation formula
 	a(t) = ? a(kt)sinc(2fmaxt -k),	?t ? R.	(4)
 k?Z
 In the presence of quantization, it is generally no longer true that ˜a = a. Nevertheless, it is reasonable to expect that, if the quantization step is small, then the quantized samples {u[k]}k?Z will be close to the analog samples {a(kt)}k?Z for all k, and the output of the simple PCM system of Fig. 1 will be close (in some sense) to the analog input a. Unfortunately, this and other assumptions in the above model are often far from realistic, as discussed next.
 2.2	Practical Aspects of PCM
 Whilst the PCM method described above is certainly attractive, it suffers from several shortcomings that hinder its usefulness in many practical situations. In what follows, we will describe some of the main deficiencies of this architecture.
 Synthesis Filter The ideal low-pass filter used in Fig. 1 for synthesis cannot be implemented in practice. Firstly, it is non-causal. A very close approximation of the ideal low-pass filter would still be non-causal, which rules it out from any delay sensitive application.
 Secondly, an ideal low-pass filter has an infinite impulse response length. For practical low pass filters, the closer they mimic the ideal filter, the longer the impulse response will be. One problem with a long, slow decaying impulse response is that it affects the stability of the reconstruction, in the sense that bounded errors in the samples are able to produce unbounded point-wise error in the reconstructed output. As an example, consider the ideal low-pass reconstruction in (4). It is easy to show that any bounded periodical error in the samples a(kt) of the form , with |?| > 0, will yield an unbounded reconstruction error in the L8 norm. The second difficulty with a synthesis filter with long (but finite) impulse response is cost and complexity: In applications where synthesis is accomplished via discrete-time FIR filters, longer impulse responses require higher computational complexity.
 Another problem with the ideal-low pass synthesis filter model is that, in many practical applications, the synthesis filter is not a design choice, but is prescribed by other considerations. In such cases, the synthesis filter can have almost any frequency response. An important example of this situation is that of sampled-datacontrolsystems, where the plant itself can be thought of as comprising part of the synthesis filter R in Fig. 1. We will return to this situation later in Section 9.
 Not Necessarily Band-Limited Input Signals The assumption of band-limitedness of the input signal a is also very restrictive. Most real applications have to deal with signals over a finite time interval (strictly speaking, any non-zero finite duration signal is not band-limited [7]). Even when processing a virtually infinite duration, perfectly band-limited signal, only a finite number of samples can be used for the reconstruction. This introduces truncation errors [8], i.e., part of the inter-sample behaviourof the input signal is not captured by the samples. On the other hand, it is often the case that the sampling rate cannot be made high enough to completely avoid aliasing. Whilst this is commonly dealt with by using a low-pass anti-aliasing filter before sampling, this paradigm may have significant shortcomings whenever the signal carries relevant information in the high frequency part of its spectrum, or when the reconstructionfilter is not perfectly band-limiting (see, e.g., [9, 10]). In this case other types of analysis filters should be considered.
 Availability of the Input Signal Before being able to sample the value of any physical variable, it is necessary to convert it to an electrical signal by means of a transducer, which in itself is a dynamical system. It is often the case that sampling is performed in the transducer itself. In this case, one does not have access to the underlying continuous time signal, but only to the samples taken. Depending on the situation, this can deprive further stages of knowledge of important inter-sample behaviour of the physical variable. It is then necessary to make a wise design of the synthesis stage, so that the input signal can be well approximated at the output (see, e.g., [11, 12, 13]).
 Quantization, Sampling Frequency and Data-Rate In the simple PCM system of Fig. 1, quantization is done element-wise by a nearest neighbour quantizer, see (2). Thus, if one wishes to obtain a small reconstruction error, one would naturally aim at reducing the quantization step. In practice, however, the reduction of ? is limited by cost and structural constraints. Alternatively, if the statistics of the input signal are known, then the mean square reconstruction error can often be reduced by using a quantizer in which the quantization step is not uniform along its dynamic range.
 Moreover, even though the Shannon-Whittaker sampling theorem shows that when the samples are un-quantized an increase of the sampling frequency cannot improve reconstruction (since it is already perfect), the situation with quantized coefficients is different. More precisely, when quantization is introduced, sampling abovethe Nyquist rate (oversampling) can be utilized to reduce quantization error (see Sec. 5.2) . Thus, one often has the chance to compensate the effects of coarse magnitude quantization by means of a finer time quantization, i.e., faster sampling rate. (The reader may be well aware of this in 1 bit DAC’s used in some CD players.)
 In practice, the productof the sampling rate and the number of quantization levels is often constrained by data-rate limitations. This is so because, although not explicitly shown in Fig. 1, the sequence of quantized values {u[k]}k?Z, in binary form, has to be stored or transmitted before reconstruction takes place at another location in time and space. This means that the total number of bits, or similarly, the data-rate, is limited. In principle, if the quantizer has nU ? N levels, then the data-rate will be approximately given by
 	Bit Rate   log2nU [bits/s].	(5)
 t
 It is possible, however, to reduce the data-rate by an efficient encoding of the sequence of quantized values (compression). When such encoding is applied, the data-rate limitation translates into an information-rate limitation, precisely given by the entropy of the sequence of symbols at the output of the quantizer [14]. Systems with entropy coding are also called variable-rate encoders. In this paper, however, we will not consider such coding methods. Thus, we will only consider fixed-rate encoding, and the data-rate will be given by (5).
 2.3	A More General Model for AD–Conversion
 In view of the limitations of PCM conversion discussed above, a more general model for the analysis of sampling, quantization and reconstruction systems is presented in Fig. 3.
  
 Weighting Filter Figure 3: A more general sampling, quantization and reconstruction system.
 For the remainder of this work, we will restrict our analysis to input signals a which are modeled as finite energy scalar functions of a single parameter t (i.e., a ? L2(R)). For example, we could think of t as denoting time, for the case of time varying scalar signals. Thus, the analysis filter S in Fig. 3 accounts for all the continuous-time linear processing of the input that occurs before the sampling takes place. The sampling process is assumed uniform (i.e., regular sampling), with fixed sampling interval t.
 The synthesis filter R in Fig. 3 represents the linear processing in continuous-time (possibly with some discrete-time pre-filtering) applied to the quantized samples {u[k]}k?Z. The output of R is denoted as ˜a. It approximates a in some well defined sense.
 The quantizer QU in Fig. 3 is labeled generalized because it is allowed to have access to previous and future input samples during operation, and scalar, because it generates a sequence of scalars, one at a time. We will only discuss quantizers of this type in the remainder of this paper, which justifies a moreprecise definition of the class of generalized scalar quantizers:
 Definition 1 (Generalized Scalar Quantizers). Any quantization strategy that can be devised within the following conditions
 •	The quantizer has no access to the continuous time signal a, but only to the samples {c[k]}.
 •	The quantizer outputs a sequence of scalars {u[k]} at a constant rate, one element every t units of time. The total elements in the output sequence equals the number of input analog samples.
 •	Each of the elements in the output sequence of the quantizer can take values only from a finite, given and fixed set of scalars U, i.e, the output of the quantizer satisfies
 	u[k] ? U,	?k ? Z.	(6)
 •	The quantizers has access to all past and future analog samples.
 is said to belong to the class of Generalized Scalar Quantizers.
 Note that this definition allows for the uniform, nearest neighbour scalar quantizer in (2) as a special case. The last condition in Definition 1 means that the generalized scalar quantizer in Fig. 3 is allowed, in principle, to determine the output u , for any , based upon knowledge of the entire sequence {c[k]}k?Z, i.e., it is a dynamic system. Therefore scalar quantizers with memory (such as the predictive and noise shaping quantizers to be discussed in Section 6) are special realizations of the generalized scalar quantizer  .
 In Fig. 3 an error frequency weighting filter H has been added. Inclusion of this filter reflects the fact that, depending on the application, the practical impact (or cost) of the reconstruction error is frequency dependent. Accordingly, H filters the instantaneous error a(t)- a˜(t) to produce a frequency weighted error signal eH(t). Based on the general setup illustrated in Fig. 3, throughout the remainder of this work the performance of the system will be assessed in terms of the squared L2 norm of the generated signal eH:
 	 .	(7)
 3	SAMPLING AND RECONSTRUCTION FROM A FRAME THEORETIC PERSPECTIVE
 As mentioned above, a paradigm which underlies many signal processing schemes consists of a pre-filtering (or analysis) stage, a sampling stage, a digital, discrete-time processing stage and a post-filtering (also referred to as synthesis or reconstruction) stage. It has been shown that these processes are equivalent to a sequence of mappings between Hilbert spaces (see, for example, [18, 19, 20] and [9]). This viewpoint allows one to use the powerful tools of Hilbert spaces, frames and algebraof operatorsto study and design sampling and reconstruction systems. It allows for elegant solutions to otherwise complex design optimization problems, by using innerproductsand projection operators.
 3.1	Historical notes
 To the best of our knowledge, the first author to apply Hilbert spaces theory to the sampling problem was F. Beutler in 1961. In [21] he derived sampling theorems for random stationary processes using complex exponential Fourier expansions. Further insight and results for bandlimited signals were provided by K. Yao in 1967 for other expansions, see [22]. Several publications with the Hilbert space approach to the sampling problem followed in subsequent years. Among others, a 1986 paper by Hidemitsu Ogawa [23] presented a unified approach to generalized sampling theorems. It introduced the idea of regarding the approximation of signals in a, more general, finite dimensional reconstruction space, instead of restricting to perfect reconstruction by Fourier expansions. Interestingly, in [23], a finite number of samples of a filtered signal was used, as opposed to an infinite number of “raw” samples. By the early nineties, the recently arrived wavelet theory [24, 25] began to stimulate a strong revival of sampling theory (see, for example, [26, 27, 28]), by using the mathematics of basis and frames in Hilbert spaces. This framework allowed for the re-formulation of the sampling and reconstruction problem in more general and practical situations, including, inter alia, sampling and reconstruction from finite samples [23, 29], study of arbitrary input and reconstruction spaces [11, 30, 31], sampling of non-band-limited signals [27, 10], oversampling [32, 33], non-uniform sampling [34, 18], filter-banks [35, 36], and splines and interpolation [37, 38].
 In the remainder of this section we will derive a represen-
 tation of the sampling and reconstruction processes in a Hilbert space frame theoretic context. For a more complete formal analysis, see, for example, [18, 9, 34, 28, 20, 39] .
 3.2	Sampling and Reconstruction as Frame Operators
 It will be shown next that the analysis and sampling stages, which map continuous time signals into discrete time sequences, can be seen as the analysis operator of the sampling frame. This frame is made of translates of the time reversed impulse response of the analysis filter S. Similarly, the reconstruction process, which maps discrete time sequences into continuous time signals, can be seen as the synthesis operator of a reconstruction frame. It is made of translates of the impulse response of the reconstruction filter R.
 Filtering and Sampling Consider the input signal a in the block diagram of Fig. 3. Assume that a is known to belong to some space of signals, say A ? L2. Let y be the output of the analysis filter S, which has impulse response ?(t) ? L2. Then, y(t) is given by the convolution:
 y .
 If one now creates a sequence c  by taking the values of y(t) at time instants t = kt, k ? Z (sampling process in fig. 3), one obtains c[k] = y(kt) = (a*?)(kt)
 	8	8
 (8)
 = Z a(z)?(kt-z)dz = Z a(z)f(z-kt)dz
 	-8	-8
 where  . One can see that the last integral in (8) corresponds to the conventional inner product in L2, defined in (54), between a(t) and f(t -kt). If we now define the shift operator Tkt by
 	Tk ,	(9)
 then it is possible to write (8) as c 
 Therefore, the sampled filtered input signal can be seen as the result of a sequence of inner products. From (10) and Definition 6 (see Appendix), this is indeed the process described by the analysis operator F* associated with the frame {Tktf}k?Z. As a consequence:
  
 Notice that, since {Tktf}k?Z is a frame for the Hilbert space
 S span{Tktf}k?Z ? L2, it follows that c  for all a ? L2, as required .
 Synthesis (or Reconstruction) Consider now the conversion from the discrete-time sequence {u[k]}k?Z to the continuous-time signal ˜a(t), see Fig. 3. If we denote the impulse response of R as ?(t), then the band-limited Shannon-Whittaker reconstruction scheme in (3) can be generalized to:
 	a˜(t) = ? u[k]?(t -kt) = ? u[k]Tkt?, ?t ? R	(11)
 	k?Z	k?Z
 It is clear from Definition 5 (Appendix) that the reconstruction process (11) can be represented by the synthesis operator ? associated with the frame {Tkt?}k?Z:
   a˜
 In this new setting, ?(t) becomes the generating function for the principal shift invariant reconstruction space W  span{Tkt?}k?Z, which is, in general, different from the space of band-limited signals .
 The sum in (11) can be seen as a mixed convolution [9],
 i.e.,
 	a˜(t) = (u*?)(t),	?t ? R
 which it takes a discrete time sequence u and a continuous time function ?, yielding a continuous time function ˜a. If the impulse response ?(t) is chosen such that {Tkt?}k?Z is a Bessel sequence (and therefore a frame for span{Tkt?}k?Z, see (56) ), then ? is a bounded operator, and the output ˜a(t) = ?u ? L2 for all sequences
  .
 The Combined Sampling and Reconstruction Process It follows from the above that the sampling (analysis) and reconstruction (synthesis) process can be stated as a sequence of operators between Hilbert spaces:
 •	Analysis:
  .
 In particular, c .
 •	Reconstruction:
  u
 Therefore, in the absence of quantization (i.e., if u[k] = c[k], ?k ? Z), the complete process can be expressed as
 	 ,	a˜ = ?F*a	(12)
 If the sequence {u[k]}k?Z is obtained by quantization of
 {c[k]}k?Z, then (12) becomes
 	 ,	a˜ = ?QU(F*a)
 It is interesting to note that the above results allow one to determine the ultimate limitations and capabilities of a sampling and reconstruction system in terms of the Hilbert spaces related to sampling rate and filters. More precisely, the analysis and synthesis filters alone determine, respectively, the largest class of signals that can be sensed (i.e., the sampling space) and the largest class of signals that can be generated (i.e., the reconstruction space). A rather remarkable implication is that in the intermediate (discretetime) stages one can only design the mapping between these spaces, but not expand the sampling and reconstruction spaces themselves.
 As a consequence, the design of an AD conversion scheme can be thought of as involving two aspects, namely:
 1.	Choice of the sampling and reconstruction filters (i.e.,choice of spaces).
 2.	Design of the mapping between signals in the sampling space and signals in the reconstruction space (i.e., design of discrete-time processing, including quantization).
 In what follows, we will describe aspects of the separate design of the sampling/reconstruction strategy and of the quantization method. Some aspects of the joint design problem will be discussed later in Section 8.
 4	SAMPLING AND RECONSTRUCTION WITHOUT QUANTIZATION
 In this section we discuss the effect that analysis and synthesis filters have on the reconstruction quality. We will assume that the input and output spaces are given and will neglect quantization effects. The implicit trade-off here is between the quality of the reconstruction and the computational complexity (and delay) incurred in the sampling and reconstruction processes.
 4.1	Types of Reconstruction
 As concluded in Section 3, the ultimate sampling and reconstruction capabilities of a system are limited by the sampling and reconstruction spaces. These, in turn, are entirely determined by the choice of analog filters S and R, as well as the sampling interval t. This suggests that, whenever possible, the design of S and R should focus mostly on the sampling and reconstruction spaces that one wishes to obtain. Further refinement can be achieved by careful design of discrete-time filters which can be located right after the analysis filter S and before the synthesis filter R, see Fig. 3. Interestingly enough, it has been shown that, in general, the optimal mapping is obtained by making the sampling and reconstruction frames duals of one another [9, 40]. To achieve this for a given analysis frame , one can insert a discrete-time correction filter before the synthesis filter to make the synthesis frame the dual of the analysis frame. Although, in general, the dual frame of some given frame is not unique, there exists only one shift-invariant dual frame (i.e., a unique correction filter) for each given shift-invariant frame [40]. In what follows, we will consider the following situation:
 •	H is a non-separable Hilbert space (e.g., L2(R)).
 •	A ? H is the space that contains all possible input signals.
 •	S = span{Tktf}k?Z ? H is the sampling space .
 •	W =span{Tkt?}k?Z ?H is the reconstruction space.
 Depending on the relation between the input space A, sampling space S and reconstruction space W , we will consider three types of reconstruction notions, namely: consistent, orthogonal and perfect reconstruction.
 Consistent Reconstruction The first and most generally attainable reconstruction goal is that of consistent reconstruction, first introduced in 1994 by Unser and Aldroubi, see [11] . A signal approximation is said to be consistent if it yields the same samples (observations) as the original signal when re-injected into the system, i.e. a˜ ? W is a consistent approximation of a ? A if and only if
 F*a˜ = F*a.
 The idea of consistent reconstruction is depicted in Fig. 4.a); in this figure, ˜a is projected onto W along S?, the null space of S, see (49).
  
 	(a)	(b)
 Figure 4: a) Consistent reconstruction (oblique projection); b) MSE reconstruction (orthogonal projection).
 The notion of consistent reconstruction was first introduced for Riesz bases in [11], and then extended for frames in [31, 19, 13, 41, 40] and [42].
 Orthogonal Reconstruction The second type of reconstruction is orthogonal reconstruction, also called MSE reconstruction. It requires additional conditions (see next section). In this type of reconstruction, the system generates, for any input a ? A, the output ˜a ? W that minimizes
  , i.e.:
 a˜ = arg min  . w?W
 It is well known that this notion is equivalent to an orthogonal projection of the signals of A onto the output space W (see Appendix A.1.1). The intuitive notion of orthogonal projection is illustrated in Fig. 4.b). Note that the ˜a shown in this figure is, indeed, the closest point to a in the output space W .
 Perfect Reconstruction The third, and most demanding notion is that of perfect reconstruction, i.e.,
 a˜ = a, ?a ? A.
 As will be shown below, depending on the spaces A, S and W , perfect reconstruction can still be possible, even, for example, for non band-limited signals [43, 10, 27]. In the remainder of this section we will describe conditions on the sampling and reconstruction method which ensure that each of these notions can be achieved.
 4.2	Conditions for Consistent, Optimal and Perfect Reconstruction
 Under the assumption that the sampling and reconstruction spaces satisfy the direct sum condition 
 	H = W ?S?,	(13)
 necessary and sufficient conditions have been found in order to make the sampling and reconstruction system achieve consistent reconstruction and, as particular cases, optimal and perfect reconstruction as well [19, 40, 42]. For shift invariant frames and spaces, the direct sum condition can be conveniently expressed in the frequency (Fourier) domain  based on the functions
 	A ,	A 	(14)
 	A ,	A 	(15)
 and the null sets of A? and Af, denoted, respectively, as N (A?) and N (Af), where
 	N ,	(16)
 by means of the following proposition:
 Proposition 1 ([40, Proposition 4.8]). Let ?,f ? L2(R), and assume that {Tkt?}k?Z and {Tktf}k?Z are frame sequences. Then the following are equivalent:
 (i)	L2(R) = W ?S?,
 (ii)	N (A?) = N (Af) and there exists a constant A > 0 such that
 A 
 It is shown in [42] that, if the direct sum condition is satisfied, then ˜a ? W is a consistent reconstruction of an input a ?H if and only if ˜a is the oblique projection of a onto W along S?, the null space of S (see Appendix A.1.1). Such a projector, denoted by EWS?, is defined as
 	EWS ,	EWS?h = w,
 where h = w+v, with w ? W,v ? S?
 The following defines the concept of oblique dual frame and establishes its relation with the oblique projector:
 Lemma 1 (from [40, Lemma 3.1]). Assume that {fk}k?Z and {gk}k?Z are Bessel sequences in H and let S = span{gk}k?Z, W = span{fk}k?Z. Assume that H = W ? S?. Then the following are equivalent:
 a)	w   .
 b)	EWS  .
 c)	ESW  .
 Furthermore, if the above three equivalence conditions are satisfied, then {gk}k?Z is an oblique dual frame of {fk}k?Z on S and {fk}k?Z is an oblique dual frame of {gk}k?Z on
 W .
 From Lemma 1 one can see that ?F* becomes an oblique projector if and only if {fk}k?Z is an oblique dual frame of {?k}k?Z in S.
 Although, as with the conventional case considered in Definition 7 (see Appendix A.4), the oblique dual frame within a given space is not unique, the shift-invariant oblique dual frame of a shift invariant frame is unique [40]. This means that, once reconstruction and sampling spaces are defined, there exists a unique analysis filter that makes the analysis frame the oblique dual of the reconstruction frame. Conversely, there exists a unique reconstruction filter that turns the reconstruction frame into the dual of the analysis frame. An expression in the Fourier domain for the oblique dual frame condition in terms of the frequency responses of the analysis and reconstruction filters is given in [40], which, by virtue of Proposition 1, can be rewritten as follows:
 Theorem 1 (from [40, Theorem 4.3]). Let ?,f ? L2(R) and assume that {Tkt?}k?Z and {Tktf}k?Z are frame sequences, spanning the closed spaces W and S, respectively. If L2(R) = W ?S?, then the following holds:
 (i)	There exists a unique function ?˜ ? S such that
 w  W ;
 k?Z
 (ii)	This unique function ?˜ ? S is given in the Fourier domain by 
 	?	fˆ(?)
   , if ? ?/ N (A?) ?ˆ˜ (?) = ? ? ?ˆ(?+k)fˆ*(?+k) k?Z
 	???0	, if ? ? N (A?)
 (18)
 Remark 1. In relation to (18), we note that:
 •	The function ?ˆ˜(?) in (18) is 1-periodic.
 •	The result in (18) allows one to obtain a shift invariant oblique dual frame for {Tkt?}k?Z on S for a given f by inserting a continuous or discrete-time correction filter Qf? with transfer function
 	Q 	(19)
 just before or just after the analysis filter. With such an arrangement, and provided Conditions (i) and (ii) in Theorem 1 are satisfied, the system will yield perfect reconstructionfor all inputs a?W and consistent reconstruction for all inputs a ? L2, as required.
 •	Conversely, from the reciprocity of oblique dual frames, (18) also allows one to obtain the obliquedual frame for {Tktf}k?Z on W for a given ?. This can be achieved by inserting a continuous (or discrete) time correction filter with transfer function Qf?(?) defined in (19). Notice that this correction filter does not alter the space associated to the stage in which it is inserted, i.e., if the impulse response of Qf? is qf?(t), then  .
 From the previous results it follows that, if the direct sum (17) and duality (18) conditions are met, then necessary and sufficient condition for each type of reconstruction can be stated as follows:
 Conditions for Perfect Reconstruction Perfect reconstruction only for all inputs a ? W is possible, without any further requirement
 Conditions for (MSE) Reconstruction (Orthogonal Projection) If, additionally, S = W , then EWS? becomes an orthogonal projector onto W , i.e., EWS? = PW, see Appendix A.1.1. This guarantees that the output signal ˜a will be the best approximation in W for the input signal a ? H , i.e., it will minimize .
 Conditions for Consistent Reconstruction (Oblique Projection) Consistent reconstruction will be achieved for all a ? L2 without further requirements.
 5	QUANTIZATION
 Quantization is the process of translating analog values into values which belong to a finite set. The representation of analog samples with infinite accuracy would require an infinite number of bits. Quantization allows one to achieve a controlled approximate representation of infinite analog values, which in turn can be represented with a finite number of bits. Hence, the main purpose of analog to digital conversion is to compress data, whilst aiming to obtain the best possible approximation of the analog signal. This is to be achieved within data-rate constraints and according to some fidelity criterion, i.e., “making most out of a little”. As already mentioned in Section 2.3, the quantizers to be discussed in this paper belong to the family of generalized scalar quantizers, see Definition 1. As such, quantizers generate an output sequence {u[k]}k?Z whose values are constrained to belong to a set of nU elements (see (6)) , the quantization alphabet U, now formally defined as:
 	 	(20)
 Traditionally, quantization has been analyzed only in terms of discrete-time performance, usually looking at the MSE between input samples and quantized samples. Denoting the input and outputsequencesof the quantizeras {c[k]}k?Z and {u[k]}k?Z, the MSE is given by :
 	 ?	(21)
 k?Z
 We will next briefly discuss the simplest realization of the generalized scalar quantizer in Fig. 3: the zero-memory scalar quantizer. Its performance will be analyzed in terms of the MSE as defined in (21). Other realizations of the generalized scalar quantizer, such as quantization with memory (by means of feedback) and quantization with memory and “preview”, will be analyzed in Sections 6 and 7, respectively. For a more comprehensive analysis of quantization see, e.g., [45, 16, 17].
 5.1	Scalar Quantization
 Scalar quantization is also referred to as zero-memory quantization, since each analog sample is quantized ignoring previous or future samples. Scalar quantizers partition the real line into a set of nU disjoint and consecutive intervals I = {I1,...,InU}, Ii ? R. A unique scalar from U is associated to each interval in I, usually satisfying µi ? Ii,i = 1,...,nU. Depending on the choice of the partition intervals, either a uniform or a non-uniformscalar quantizer is obtained.
 Uniform Quantizer The simplest scalar quantizer is the nearest neighbour uniform quantizer introduced in Section 2.1, where the partition of the input space (the real line) is given by (1) and the elements of U satisfy µi+1-µi = ?, i = 1,...,nU-1
 Defining the positive constants extreme output value M and extreme input value C as
 M		-µ1 = µnU	(22)
 C		M +?/2,	(23)
 the quantizer is said to be overloaded if the input |x| >C. If the probability density function of the analog samples is smooth and the quantization step is small enough, then the quantization error can be approximately modeled as a random variable with uniform distribution over [-?/2,?/2] (see [46] for precise conditions), and the mean squared error between the input x and the output u = QU(x) of the quantizer is given by the distortion measure:
 D  E ,
 where E[X] denotes the expected value of the random variable X.
 In terms of the number of bits utilized to represent each sample, we first note that
 ? = 2C2-B
 Thus, the distortion depends on the number of bits per sample B as
 	D	 C2	M2	B, for large B .	(24)
 Non-Uniform Quantizer For a given number of bits per sample, the distortion D can be further reduced if the probability density function (PDF) of the analog samples is known. This can be achieved by utilizing a non uniform quantization step. Any form of non-uniform quantization can be accomplished by placing complementary non linear elements before and after a nearest neighbour quantizer. The first block is a compressor, and its transfer function C(x) is a monotonically increasing function satisfying
 C(-C) = -C, C(C) =C, C(0) = 0
 The complementary block placed after the quantizer is called expander, and has a transfer function C -1. Adapting an expression first derived in [47], one has that, for a non uniform quantizer with a large number of quantization levels, compressor characteristic C(x) and without overload, the MSE due to quantization is given by
 Xmax
 	DC M2	B Z	 fx(x) dx	(25)
 C (x)]2
 [
 Xmin
 where fx(x) is the PDF of the analog samples and C  dC/dx. The no overload assumption implies -C = Xmin and Xmax =C, and that fx(x) = 0, ?x ?/ [Xmin,Xmax]. Notice that for C  1 (i.e., with a uniform quantization step), (25) becomes (24).
 Clearly, minimization of DC in (25) requires a compressor curve C matched to the PDF of the input signal. The optimal compressor characteristic C * is given by the solution to
 	dC *(x)	1/3
   = a[fx(x)]	(26) dx
 where a is a constant such that C(C) =C. When the solution of (26) is inserted into (25), the MSE without overload and for large B is found to be
 3
 C	Xmax/s	??? s2 ?? Z	1/3	-2B
 D	* =[fxN(x)]	dx	·2	(27)
 12
 	??Xmin/s	??
 where s2 and fxN(x) are the variance and the normalized PDF of an individual input analog sample, respectively. In relation to (27), it must also be pointed out that C (see (23) ) must be made several times larger than s for the nooverload assumption and (27) to be valid. For more details about the derivation and applications of this and other results related to scalar quantization, see, e.g., [48] and the references therein.
 5.2	Oversampling
 It is possible to further reduce the reconstruction MSE, while keeping the quantization step constant, by increasing the sampling frequency above the Nyquist frequency fN. This technique is called oversampling. For oversampling ratio r  1/(tfN) not too large, the mean square error is reduced as r-1, i.e.,
 	Dr = D1r-1	(28)
 where D1 is the MSE when r = 1 [47]. Notice that this can also be seen as a particular case of the resilience properties of redundant frame expansions discussed in Appendix A.5 (see also, e.g., [32, 49]). However, as the sampling frequency is increased, quantization noise becomes more and more correlated and the decrease rate of Dr diminishes. Furthermore, Dr asymptotically approaches a lower, strictly positive limit. The bigger ? is, the higher this limit becomes. A larger quantization step also causes the decrease rate of Dr to depart from (28) “sooner” as r is increased [47].
 The reconstruction error can be further reduced, for a given oversampling ratio, by the use of feedback . Furthermore, feedback A/D converters yield a MSE that decreases steadily as r is increased. Thus, one can obtain an arbitrarily low MSE, for a given ?, by sampling fast enough. These converters are briefly described in the next section.
 6	AD CONVERTERS WITH FEEDBACK
 Quantization schemes that use feedback can be grouped into two main families: predictive quantizers and noise shaping quantizers. Examples of the first type are the delta modulator and differential pulse code modulator (DPCM) (see, e.g., [50]). The popular S? (sigma-delta) converter, see, e.g., [51], belongs to the latter type.
 The following is a basic description of the main characteristics of both converter families, based mainly on the approach proposed in [52]. In the sequel, the quantization process is modeled as additive noise, corresponding to the quantization error of a scalar quantizer.
 6.1	Predictive Quantizers
 The general form of a predictive quantizer is shown in Fig. 5.
  
 In this diagram, U(Z) and C(z) correspond, respectively, to the Z-transforms of the analog samples sequence {c[k]} and the quantized output sequence {u[k]} depicted in Fig. 3. Thus, the quantizer contained in the dashed line rectangle in Fig. 5 is a particular realization of the generalized scalar quantizer in Fig. 3. The filter Hp-1(z) included at the end of the chain in Fig. 5 can be considered as part of the reconstruction stage in Fig. 3. The terms E(Z) and D(Z) in Fig. 5 correspond to the Z-transforms of the discrete-time signals in each of the respective nodes. N(z) is the Z-transform of the error introduced by the scalar quantizer, i.e., N(z) = U(z)-E(z). From Fig. 5, the expression for the output U(z) is found to be
 U(z) = Hp(z)[C(z)+N(z)].
 Thus, the filtered output D(z) satisfies	(29)
 D(z) =C(z)+N(z)	(30)
 The key to the noise reducing capabilities of the predictive quantizer rests on the prediction filter Hp(z). This filter is designed to minimize the variance of the prediction error
 	E(z) = Hp(z)C(z)+[1-Hp(z)]N(z),	(31)
 see Fig. 5. It is common to assume that the quantization noise is uncorrelatedto any of the signals in the loop [51] . Thus, Hp(z) is chosen so as to reduce the contribution of
 C(z) to E(z) in (31). By doing so, the variance (energy per sample) of the analog sequence that enters the quantizer is reduced. This in turn allows one to reduce the quantization step ? in the embedded scalar quantizer, without increasing the number of quantization levels needed to avoid overload. Thus, by reducing a measure of the term Hp(z)C(z) in (31), one is also reducing the quantization noise contribution, and the MSE is reduced accordingly. Of course, how much distortion reduction is achieved will ultimately depend on how predictable the sequence {c[k]} is, i.e., on the autocorrelation of {c[k]}. It will also depend on how well the prediction filter Hp(z) is able to capture this predictability
 It has been shown [52] that the MSE of the scheme in Fig. 5 decreases with the oversampling ratio not “faster” than r-(2np), where np is the order of the filter Hp(z). If an additional ideal low pass filter with cut-off frequency fN/2 is placed after Hp-1(z) (see Fig. 5), then the MSE is reduced at most as r-(2np+1) A common choice of Hp(z) is of the form (1-z-1)np.
 Note that the predictive quantizer in Fig. 5 can reduce distortion even if signals are sampled at Nyquist frequency, as long as the input analog samples are correlated. If the input samples are uncorrelated (white noise), then the predictive quantizer is unable to yield any MSE reduction at all. It is the increase in the autocorrelation of the input samples produced by oversampling which allows for the r-2np behaviour in the MSE reduction rate.
 6.2	Noise- Shaping (S? Quantizers)
 The second main category of feedback quantizers corresponds to the noise-shaping quantizers such as S? A/D converters, first proposed by Inose and Yasuda in [59]. One possible form to represent a noise shaping quantizer is depicted in Fig. 6. Again, C(z) and U(z) correspond, respectively, to the Z-transforms of {c[k]} and {u[k]} in Fig. 3. The noise shaping quantizer within the dashed line rectangle in Fig. 6 is a particular realization of the generalized scalar quantizer in Fig. 3.
  
 Figure 6: A noise-shaping quantizer.
 From this figure, it is easy to see that the output U(z) is given by
 	U(z) =C(z)+Hn(z)N(z)	(32)
 where the noise shaping filter Hn(z) constitutes a degree of freedom in the design process. Since C(z) is band-limited, and because of oversampling, it is generally convenient to choose Hn(z) to be a high-pass filter, see, e.g., [51]. With this choice, the quantization noise is attenuated within the signal band whilst increased outside of it (see Fig. 7). This compensatory increase in the off-band quantization noise is unavoidable, as determined by the Bode integral theorem [60]   Because of the frequency shaping of the quantization noise, most of its energy can be suppressed by low pass filtering U(z), leaving only the in-band portion of the quantization noise. By doing so, it is verified in [52] that the MSE decays by increasing oversampling ratio at most as r-(2nn+1), where nn is the order of the noise shaping filter Hn(z). Most common choices for Hn(z) have the form (1-z-1)nn/P(z), where P(z) is an FIR filter.
  
 Figure 7: Quantization Noise Shaping.
 As in control systems, one of the beneficial aspects of using feedback in analog-to-digitalconvertersis the increased robustness of the resultant system. Indeed, if properly designed, feedback converters allow one to achieve high accuracy quantization despite the use of inaccurate building blocks (such as the scalar quantizer itself, which can be allowed to have a very coarse and uncertain quantization step). This makes feedback quantizers the preferred choice for many practical applications.
 It should also be noted that the above mentioned decay rate of the MSE with increasing oversampling ratio is not fast enough to be rate-distortion efficient. Indeed, oversampling AD converters require, in general, a higher data-rate than a system with finer quantization and no oversampling to achieve the same distortion. This can be seen by noting that, for feedback converters, the MSE decays only polynomially with increasing the oversampling ratio, as  O(r-(2n+1)), while the MSE decreases with increasing the bits per sample (i.e., reducing ?) as O(2-2B), i.e., exponentially. Nevertheless, recent results show that the L8 norm of the reconstruction error in S? converters can be reduced as O(?-r), ? > 0, by selecting for each oversampling ratio an appropriate noise shaping filter from an infinite set of filters [64]. Following a different approach, quantization schemes based on threshold crossings exhibit a reconstruction MSE that decays exponentially with increasing oversampling ratio [65, 66], and are thus rate-distortion efficient.
 7	MOVING HORIZON QUANTIZATION
 Interestingly, control theory can be used to design the generalized scalar quantizer in Fig. 3. More precisely, since the output of the quantizer is constrained to belong to a finite alphabet of values, the situation can be regarded as a control problem with input constraints. This point of view motivated has motivated us to apply Moving Horizon Optimization (MHO) tools to achieve a more effective noise shaping quantizer. This paradigm uses Model Predictive Control, and has proved to be a powerful tool for dealing with constrained systems [67, 61, 68, 69, 70, 63]. The quantization scheme so obtained, named Multi Step Optimal Converter (MSOC) [71], typically outperforms S? quantizers, while embedding the latter as a particular case. We will present next some of the fundamental principles behind the MSOC. The remainder of this section has been basically adapted from [71],
 7.1	Noise Shaping Quantization as an Optimization Problem
 A more general formulation to analyze the discrete-time performance of noise shaping quantization can be derived from the block diagram depicted in Fig. 8.
 In Fig. 8, {c[k]} and {u[k]} represent, respectively, the input analog samples and the quantized output sequence. The motivation for quantization noise-shaping has been incorc[k]	Hd(?)  eHd [k]
 Discrete-Time Error Frequency Weighting Filter
 Figure 8: Scheme to generate the frequency weighted quantization error sequence eHd [·].
 porated by introducing a frequency weighted reconstruction error sequence, denoted by
 	eHd [k]  Hd(z)(c[k]-u[k]),	k ? Z,	(33)
 compare to (7).
 In (33), Hd is a stable, causal, linear, time-invariant filter, which can be characterized via :
 	Hd ,	(34)
 where A ? Rn×n, B ?Rn×1, C ? R1×n and n ?N is the state dimension, i.e. the order of the filter Hd. This filter can, e.g., represent the typical low-pass filter utilized in oversampled conversion, see e.g. [72], in order to decimate the converter output. In audio applications it makes sense to choose Hd as a psycho-acoustic model of the human hearing, compare also with work in [73, 74]. The performance of the quantization process in Fig. 8 will be evaluated by the measure
 	V ?[eHd [k]]2.	(35)
 k?Z
 The cost V penalizes the distortion introduced in the conversion process in a frequency-selectivemanner. If the generalized scalar quantizer in Fig. 8 is designed to minimize the performancemeasureV, then its quantizedoutput u will approximate the input c, while the un-filtered quantization error, a-u, will tend to have a spectrum similar to that of the inverse of the filter Hd. Thus, the method will shape the quantization noise spectrum, just as the S? converter discussed in Section 6 does.
 Unfortunately, minimization of V by using expression (35) is not possible in practical applications, due to the complexity of solving the associated combinatorial optimization problem. Furthermore, in the general case, an optimal quantizer would need to pre-view the entire signal c. This is clearly unsuitable for on-line applications.
 7.2	Multi Step Optimal Converter
 In order to obtain a more practical method to minimize the cost in (35), it is convenient develop a recursive conversion method, which can be implemented on-line. For that purpose, we will first introduce a cost measure over a finite horizon, to deploy later the concept of moving horizon approximation, see [63].
 Finite Horizon Formulation A practical conversion scheme, suitable for online applications, must operate sequentially, evaluating a restricted number of decision variables and considering a moderate number of future values of c. For this purpose, it is convenient to characterize eHd as the output in a state space representation of Hd
 x[k+1] = Ax[k]+B(c[k]-u[k])
 (36)
 	eHd [k] =Cx[k]+	c[k]-u[k].
 This relation follows directly from (34). In (36), x ? Rn is the state vector. Note that, due to the Markovian structure of (36), at time k =  the impact of the past trajectories of c and u on future values of eHd is exactly summarized by means of the present state, x[k].
 Given the above, we next replace the infinite horizon cost function (35) by the finite horizon cost:
 	VN  xT Px .	(37)
 k=
 In (37), N ? N determines the prediction horizon and P is a given positive semidefinite matrix.
 With a given and known current state value x  (see (36)),
 VN is a measure of the filtered distortion eHd over the prediction horizon plus a measure of the final state, x  N]. These predicted quantities are formed based upon the model (36).
 The finite horizon cost VN  proposed in (37) takes into account only a finite number N of constrained values. The value of N determines the computational complexity required for the minimization of VN . This should be compared with the infinite number of decision variables in the original cost V. Using a finite horizon N also reduces the required pre-viewing of c to N -1 samples. Since N is a design parameter, it can be chosen so that the minimization can be carried out on-line.
 Moving Horizon Approach As noted above, the optimizer to VN , contains a feasible output sequence for time instants  1. Thus, in principle, one could think of an implementation in blocks, where the minimization is carried out every N sampling instants. Unfortunately, the last few elements of  depend only on a small window of the filtered distortion, eHd . To improve performance, the multi-step optimal converter utilizes only the first element of , say u . It becomes the -th element of the converter output sequence, by setting:
 	u 	(38)
 It is also utilized to update the state according to (36), i.e.:
 	x .	(39)
 At the next sampling instant, this new state value is used to minimize the cost VN , yielding u . This procedure is repeated ad-infinitum. As illustrated in Fig. 9 for the case N = 3, the prediction horizon of the criterion VN(k) moves (slides) forward as k increases. The past is propagated forward in time via the state sequence x, thus, yielding a recursive scheme.
 The resultant architecture defines the MSOC. It constitutes an analog-to-digital converter architecture which optimizes the frequency weighted conversion distortion, based upon Model Predictive Control principles.
 Horizon at 
 Figure 9: Moving horizon principle, N = 3.
 Interestingly, it has been shown that the MSOC with N = 1 and P = 0 reduces to the S? converter, see [71]. However, it is easy to see that, in general, larger values for N provide better performance, since more data is taken into account in the decision process of allocating scalars from U to the elements in the sequence u. In fact, one can expect that, if N is chosen large enough relative to the time scale of
 Hd, then the effect of u  for j N will be negligible and the performanceof the MSOC will approach that obtained, if the infinite horizon measure of (35) were to be minimized directly (which, for the reasons explained above, is impractical). This asymptotic behaviour has been experimentally confirmed, see [71].
 In summary, the prediction horizon N allows the designer to trade-off performance versus on-line computational effort. Interestingly enough, excellent performance can often be achieved with relatively small horizons (see, e.g., [71]), thus rendering the scheme quite easy to implement in practical cases.
 Another advantage of the MSOC when compared to the S? converter resides in that the matrix P in (37) can be designed to ensure stability like properties of the MSOC, see [71].
 8	SAMPLED-DATA QUANTIZATION
 Given that digital signal processing systems have to interact with the real, physical world, the design of a quantization scheme should take into account the sampling (continuous to discrete-time) and reconstruction (discrete to continuous-time) stages between which it is to be inserted. Unfortunaly, there exists only partial understanding of how sampling-reconstruction strategies interact with a given quantization method in terms of the resulting, overall reconstruction error. Furthermore, most literature analyzes the performance of quantizers in terms of how close the input analog samples are approximated by the output, quantized samples, and not by comparing the analog, continuous-time underlying signal entering the system against the analog, continuous-time reconstructed signal that comes out of the reconstruction stage of the system. Accordingly, performance is most often measured by the 2 norm of the sample approximation error, see 21. Similarly, traditional works on sampling and reconstruction theory build their analysis based first upon ideal, nonquantized samples, incorporating later the effect of quantization viewed as the corruption of ideal samples by white additive noise. Although it has been shown that this whitenoise model of quantization is indeed accurate for small quantization steps and input samples whose PDF satisfies certain rather weak requirements, it is certainly not accurate, for example, when quantization steps are large, or when feedback structures are deployed. As presented in Sections 6 and 7, it is often the case that quantization noise is deliberately made non-white by the quantizer so as to minimize a frequency weighted measure of the reconstruction error.
 Within the setup depicted in Fig. 3, we aim to present in this section some results and additional insight related to the joint problem of designing systems that make use of the pre-filtering, sampling, quantization and reconstruction paradigm .
 8.1	Decomposition of the Reconstruction Error
 The Hilbert spaces model of the sampling and reconstruction process described in Section 3 leads to a somewhat trivial but nevertheless important result: it allows for a decomposition of the final reconstruction MSE between the analog input a and the analog output ˜a (see Fig. 3) of a sampling-quantization-reconstruction system into two terms. The first term corresponds to the error due to the “spaces mismatch”, i.e., the non coincidence of input and outputsignal spaces. The second errorterm comes fromthe deviation of the discrete-time processing (both linear and non-linear) from the optimal mapping between input and output vectors in the sampling and reconstruction spaces, respectively. The following proposition formalizes this idea:
 Proposition 2. Let ?(·) ? L2 be the impulse response of the reconstruction filter R, such that {Tkt?}k?Z is frame for W  span{Tkt?}k?Z, and let t be the sampling interval. Then, the mean square reconstruction error between any input signal a? L2 and an approximationa˜ ?W generated by the reconstruction stage can always be decomposed as follows
  PW a PW a  ,	(40) where PW a is the orthogonal projection of a onto W .
 Proof. Define w  a˜-PW a. Then we can write
  PW a 
  PW a PW a, 
 Since (a-PW a)? W ?, and because w ? W , we have that  PW a,  0 (see (50)), and (40) follows.
  
 Corollary 1. From Proposition 2, it follows that for any a ? L2, choice of quantization scheme and/or discrete time processing, the continuous time reconstruction error is lower bounded by
 	 PW a 	(41)
 We emphasize that the lower bound in (41) corresponds to the minimum continuous-time error attainable by any discrete-time scheme, once the output space is given, even if and no quantization is applied to the samples. From Proposition 2, it is clear that the performance of discrete-time processing (e.g., discrete-time filtering and quantization) should be evaluated in terms of the second term of the right hand side of (40), that is, the L2 norm of PW a -a˜. In relation to the design of quantizers, this gives rise to the question of what information is needed by a generalized scalar quantizer to minimize PW a . We have addressed this question in [79]. A summary of the analysis and results therein is presented below.
 8.2	Optimality
 As noted above, the reduction of the continuous time MSE by discrete-time processing takes place by minimizing the second term on the right hand side of (40). For the general system under study (see Fig. 3), the signal to be approximated is actually a convolved with h ? L2, the impulse response of H:
 	 ,	(42)
 as shown in Fig. 10.
 Defining ? as the impulse response of R, the approximation of a generated by the system becomes
  
 where ? is now redefined as the impulse response of the filter W  HR, i.e.:
  ,
 see, Fig. 10. The impulse response of W determines the reconstruction frame {?k}k?Z, which spans the reconstruction Hilbert space
 W  span{?k}k?Z
 As described in Section 4.2, the generation of the optimal output PW a can be accomplished by applying the pre-frame operator ? associated with {Tkt?}k?Z to the sequence of scalars , i.e.
 	PW a = ??° *a,	?a ? L2,
 where ?° * is the analysis operator associated to {Tkt?°}k?Z, the canonical dual frame of {Tkt?}k?Z (see Definition 7 in the Appendix). We will denote this optimal, un-quantized sequence of samples by
 	u?  .	(43)
 It is clear from the above that any quantization algorithm that attempts to minimize the continuous time error   needs to be able, in the first place, to obtain the target sequence u? in (43). From the results presented in Section 4.2, this implies that the first necessary condition for the feasibility of optimal quantization is that sampling and reconstruction stages be matched for orthogonal (MSE) reconstruction.
 If we now suppose that the quantizer has access to u?, then the problem of optimal quantization is that of choosing the optimal quantized sequence u, defined as
 	u = arg	min	(44)
 u[k
 The solution to (44) requires one to solve a continuous-time optimization problem with discrete-time, quantized decision variables. It is shown in [79] that this can be converted into an equivalent discrete time optimization problem. More precisely,
 	 	(45)
 The operator   is characterized by the Gram matrix (see [80, sec. 3.5]) of the reconstruction frame, which is defined element-wise as
 	G	 ,
 This matrix allows one to re write (45) in matrix notation as
 	 	(46)
 where  u and u are the vector representations of the sequences u and u?, respectively.
  
 Figure 10: The sampling, quantization and reconstruction system from Fig. 3 revisited. Impulse responses and frame
 The direct consequenceof (46) is that a quantizer can determine the optimal output sequence without full knowledge of the inter-sample behaviour of the impulse responses of the reconstruction filter. Indeed, quantization performance can be measured by the weighted 2 norm implicitly defined in (46). Note that the design of an optimal quantizer is not possible without knowledge of the matrix G?. operators are shown for each filter.
 8.3	Moving Horizon Conversion
 In general, minimization of (46) would require one to evaluate it for every sequence {u[k]}k?Z, u[k] ? U?k ? Z, that can be generated by the quantizer. This optimization programme, however,becomes intractable for sufficiently long sequences. Given the similarity of (46) and (35), one can use the ideas introduced in Section 7 and optimize over a short moving horizon of samples. Details can be found in [81, 79], where a sampled-datamulti step optimal converter is proposed. Preliminary results show that, interestingly, significant distortion reduction is obtained even when converting non band-limited signals. Indeed, since the focus is on the reduction of the total continuous-time reconstruction error, if the sampling rate is lower than the Nyquist rate, the resultant converter will attempt to reduce not only quantization noise, but also aliasing noise. Furthermore, as the horizon is made larger, the output of the converter approaches the optimal feasible output sequence, defined in (44).
 9	APPLICATIONSTO CONTROL
 In previous sections of this work we have illustrated that the power of feedback can be used in the design of ADconversion schemes. In particular, we have shown in Sections 7 and 8 that careful deployment of elements of Model Predictive Control may lead to high-performance conversion techniques. The purpose of the present section is to highlight the role played by sampling and especially quantization in feedback control applications. Efficiency in data representations plays a central role in any control system where parsimony aspects need to be taken into account. Thus, quantization and sampling are worth investigating, for example, in the following situations:
 •	when signals need to be transmitted over a digital network, i.e., in Networked Control Systems (NCS’s) [3, 82, 4];
 •	when plant inputs need to be quantized (e.g., relay feedback, on-off control, digital control, or also due to the presence of a human operator)[70];
 •	in large scale systems, such as those related to mining operations and supply chain management.
 In the following, we will briefly describe how concepts surrounding sampling and quantization translate into the design of these types of control systems.
 Sampling and Reconstruction In the design of a sampling/reconstruction scheme for a control system, traditional reconstruction criteria should be complemented with more appropriate performancenotions. Indeed, reconstruction quality is only of secondary importance. The main objective is measured at the plant output. In particular, as shown in [83, 5] for NCS’s, open loop performance measures should be replaced by closed loop ones. This can be achieved through consideration of frequency weighted measures such as (7).
 Quantization Interestingly, the noise shaping ideas described in Sections 6-8 can also be applied to control systems where signals are quantized; see, e.g., [70]. For example, when focusing on the design of controllers for plants with quantized inputs, a key point resides in realizing that the AD-conversion scheme of Fig. 3 is related to a quantized control system with plant H: The plant input u[k] is to be chosen such that the plant output Ha˜(t) tracks the reference signal Ha(t). Thus, performance can be measured via the frequency weighted error signal eH(t), see (7) and also (35).
 Details on how to apply principles of Moving Horizon to NCS’s can be found, for example, in [84]. It is interesting to note that the framework can also be enriched to incorporate dynamic scheduling into NCS’s. The resultant methodology can be regarded as incorporating sampling and quantization on demand and is, thus, highly efficient from a data representation perspective, see [84].
 10	CONCLUSIONS
 This paper has reviewed basic results and methods related to the process of sampling, quantization and reconstruction of scalar signals. With the introduction of a frame theoretic viewpoint, three notions of sampling and reconstruction have been discussed. We have described several generalized scalar quantization schemes, and have showed how control theory has contributed to signal processing theory. Furthermore, we have given insights into the joint problem of sampling, quantization and reconstruction, and have outlined how these stages interact. Finally, we have examined the role played by sampling and quantization in control systems.
 A APPENDIX
 A.1	Background on Hilbert Spaces, Riesz Bases and Isomorphisms
 Definition 2 (Hilbert Space). Let W be a vector space with an inner product  W and the induced norm ·W  . If such a space is complete under its norm then it is a Hilbert Space.
 Definition 3 (Riesz Basis). A sequence of vectors (functions) {?k}k?K, K ? Z, in a Hilbert space W is a a Riesz basis for W if and only if W = span{?k}k?K   and there exist two constants 0 < m = M < 8 such that 
  W
 This and other equivalent definitions can be found in [80, Theorem 3.6.6] .
 Remark 2. From Definition 3 one can observe that:
 •	The elements ?k in (47) are orthogonal if and only if m = M and orthonormal if and only if m = M = 1.
 •	The lower bound in (47) is equivalent to saying that {?k}k?Z is a set of linearly independent vectors.
 •	The higher bound in (47) guarantees that ?k?Kc[k]?k will be bounded for any choice of c .
 A.1.1	Orthogonal Projection
 If W ? H is a Hilbert space, then the best approximation in W (in the sense of the norm ) of any h ? H is given by the orthogonal projection of h onto W , denoted by PW h, and defined as the operator
 PW :  W ;	PW h  arg min H	(48) w?W
 The orthogonal projection from a Hilbert space H onto W ? H implicitly defines the null space of W :
 	W ?   : PW h  .	(49)
 It is easy to verify that
 	 .	(50)
 If {?k}k?K, is an orthonormal basis of W , then the orthogonal projection operator can be explicitly written as
 	PW h  H	(51)
 Orthogonal projection permits elegant solutions to some otherwise complex optimization problems in functional analysis. This makes Hilbert spaces and operators a natural framework for studying the problem of efficient sampling and quantization.
 A.1.2	Isomorphism
 A fundamental property of Hilbert spaces and operators is that they are able to define a precise form of equivalence between two different Hilbert spaces. It is called isomorphism: two different Hilbert spaces are isomorphic if they have the same dimension . An isomorphism is indeed any linear invertible  operator from one space onto the other. Of particular interest for our analysis are the isomorphisms between any separable Hilbert space W ? L2 (function space) of dimension |K|, where K ? Z, and R|K| (Euclidian space). Such an isomorphism can be stated by considering any orthonormal basis of W , namely {?k}k?K, and constructing the associated analysis operator
 	 	(52)
 The analysis operator ?* defined in (52) is an unitary isomorphism. This means that the respective images in R|K| through ?* of any group of vectors in W preserve their respective norms and relative orientations, i.e.
 	 W	(53)
 This remarkable property of isomorphic spaces allows one to study the relation between elements of a Hilbert space by looking at their images through ?* in another, more convenient Hilbert space. Actually, one can argue that all digital signal processing (including digital control) is made possible becauseof the existence of isomorphismbetween signal spaces and subspaces of 2.
 A.2	Illustrative Example
 Some of the basic concepts of Hilbert spaces of signals and bases presented so far will be illustrated by the following simple example.
 Let W be the space of all real valued functions w(t) satisfying the following conditions:
 •	w(t) is continuous.
  .
   (i.e., w(·) is square integrable
 over I).
 •	The derivatives of w(t) are constant over any of the open intervals ik = (kt,kt+t),k = 0,1,2.
 Fig. 11 a) shows three functions, s1(t),s2(t),s3(t) that belong to this space.
  
 Figure 11: Example of a functional space, an orthonormal base and a unitary analysis operator. a) Functions w1, w2 and w3 belong to the Hilbert space W ; b) The functions ?1,?2 ? S constitute an orthonormalbasis for W ; c) Image of the functions w1, w2 and w3 in R2 through the analysis operator ?*.
 With the addition of the standard L2 inner product, defined as 
 	 W	(54)
 W becomes a Hilbert space. The inner product (54) also defines a norm in W , given by
  
 It is easy to show that W is a two-dimensional space. This can be intuitively verified by noting that any function w ? W is completely determined by exactly two parameters, such as, for example, the values of the functions evaluated at t and 2t. A basis for a Hilbert space of dimension two contains two elements. Figure 11.b) shows a pair of orthonormal functions ?1,?2 in W which form an orthonormal basis for W .
 Figure 11.c) shows the images of w1, w2, w3, ?1 and ?2 through the analysis operator ?* (see (52) ) in R2. As expected, the images of the orthonormal functions ?1 and ?2 are orthonormal vectors in R2. How “close” is w1 to w2 in their spaces norm?. Since the analysis operator ?* is a unitary isomorphism between W and R2, we have, from
 (53)
  
 i.e.,   is given by the Euclidian distance between ?*w1 and ?*w2.
 Consider now the case of a function h(t), t ? R, that belongs to a space H ? L2, such that W H and h ?/ W .
 An example of such a function is shown in Fig. 12.a).
  
 Figure 12: a) Orthogonalprojection onto W ?H . a) Function h(t) belongs to H . b) Relative positions between h, ?1, ?2 and PW h represented in an isomorphic Euclidian space.
 c) Orthogonal projection of h(t) onto W in function representation.
 The magnitudes and relative directions of h with respect to an orthogonal basis for W such as  are shown in the 3 dimensional representation of Fig. 12.b). Here it can be seen that h is outside W but has a non zero orthogonal projection onto W . This orthogonal projection is the closest vector to h in W , in accordance with (48), and is given by (51). Consequently, the best approximation (in an L2 sense) of h in W is, expressed as a function of time
 (PW h)( 
 Figure 12.c) shows a plot of (PW h)(t).
 A.3	Frames
 Despite the computational convenience of bases, one often needs to study spaces generated by a set of linearly dependent vectors (over-complete basis). The concept of frames, introduced by Duffin and Schaffer [85], allows one to analyze such cases. Situations with over-complete bases arise in practice not only by chance. It has been shown that the redundancy of frames is beneficial, for it can reduce the effect of errors in the expansion coefficients, see [39]. The formal definition of a frame is given next.
 Definition 4 (Frame). A sequence {?k}k?K of elements in a Hilbert space W is a frame for W if there exist constants A,B > 0 such that
 	A W	(55)
 The largest number A and smallest number B that satisfy (55) are called frame bounds. Some important remarks about frames are as follows: is a frame for a Hilbert space W , then ?k k?K = W .
 •	A frame is said to be tight if one can choose A = B as frame bounds. If A=B=1, it is called a Parseval frame.
 •	If a frame ceases to be a frame when an arbitrary element is removed, it is called an exact frame. An exact frame is equivalent to a Riesz basis.
 •	A frame {?k}k?K in which  1 for all k ? K is called a normalized frame.
 •	If the elements of a normalized frame are linearly independent then A = 1 = B (see [39]).
 •	A frame with linearly dependent elements is said to be redundant.
 •	If A > 1 for a normalized frame then the frame is redundant.
 •	The upper frame bound Bmin of a frame {?k}k?K is greater than min .
 The redundancy of a frame with |K| vectors for a space W is defined as the ratio
  dimW r	 
 |K|
 It is easy to show that, for a normalized tight frame, r = A, where A is the lower frame bound in (55). Another important property of the elements of a frame {?k}k?K is that they are also a Bessel sequence, i.e., they
 satisfy
 	c 	(56)
 W
 where B is the upper frame bound in (55).
 From remark 2 and the above properties, orthogonal bases are a special type of Riesz basis, whilst Riesz bases are exact frames. Thus, by basing our analysis on frames, one is also including orthogonal and Riesz bases as special cases.
 A.4	Frames and their Operators
 Let H be a Hilbert space, and W = span{?k}k?K ? H .
 Definition 5 (Synthesis Operator). The synthesis (or preframe) operator for a frame {?k}k?K is defined as
  .
 Since every frame sequence is a Bessel sequence (see
 (56)), the synthesis operator for a frame with frame bounds A,B is bounded, with operator norm  B, i.e., B is the minimum constant such that  .
 Definition 6 (Analysis Operator). The analysis operator for a frame {?k}k?K is defined as
  
 Remark 3. The analysis operator ?* is the adjoint of ?, i.e., it satisfies  .
 Definition 7 (Dual Frame). Let {?k}k?K be a frame for a Hilbert space W . Another frame for W , namely, {gk}k?K that satisfies
 	w  W	(57)
 k?K
 is said to be a dual frame of {?k}k?K in W .
 As can be seen in (57), a dual frame provides an explicit method for representing any signal w ? W in terms of coefficients (samples), from which w can be exactly recovered through the synthesis frame {?k}k?K.
 Definition 8 (Frame Operator). The frame operator of a frame {?k}k?K is defined as
 	S :   ,	Sh  k	(58)
 k?K
 Lemma 2 (from [80, Lemma 5.1.5]). Let {?k}k?K be a frame with frame operator S and frame bounds A,B. Then the following holds:
 (i) S is bounded, invertible, self-adjoint, and positive.
   is a frame with bounds B-1, A-1. The
 frame operator for  is S-1
 Since  , one can derive from Lemma 2 ,
 (55) and (56) that:
 A	 	(59)
 B	 	(60)
 The frame operator defined in (58) is of particular importance for the problem of sampling and reconstruction,since it provides an explicit way to obtain a dual frame (see (57)). More precisely, with S as defined in (58), if {?k}k?K is a frame for W , then the frame   is a dual frame for {?k}k?K in W , i.e.
 	w  W	(61)
 k?K
 and
 	w  W	(62)
 k?K
 The frame  is called the canonical dual frame of {?k}k?K in W . This is a reciprocal relation, i.e.,
 {?k}k?K is the canonical dual of   in W as well.
 A.5	Noise Reduction by Redundancy of the Frame
 If the frame coefficients in (62) were contaminated by additive noise e[k], k ? K, then the reconstruction formula (62) would yield a reconstruction error
 we  
 	k?K	k?K
 Early referencesto the fact that the redundancyof the frame reduces the reconstruction error were provided in [24], whilst proofs can be found in [32] and [39]. Due to the importance of this property of redundant frames, we present next an adaptation of the result in [32], which is also illustrative of the importance of the frame bounds.
 Proposition 3. Let {?k}k?K be a frame of unit-norm vectors with frame bounds B = A > 0 and let e[k], k ? K be a sequence of independent random variables with mean zero and variance s2. Then the mean square value of we in (63)
 satisfies
 B 
 Proof. If e[k], k ? K is a sequence of independent random variables with mean zero and variance, we have
 E 
 (64)
 From (60) one can derive that
 B 
 which simplifies to
 	B A-2	(65)
 because {?k}k?K is a normalized frame. Combining (64) with (65) gives the result.	 
 Corollary 2. If the frame in Proposition 3 is also tight, then
 (dimW)s2
 	E	 
 r
 This paper studies the performance of a feedback control loop closed via an error-free digital communication channel with transmission delay. The system comprises a discrete-time noisy linear timeinvariant (LTI) plant whose single measurement output is mapped into its single control input by a causal, but otherwise arbitrary, coding and control scheme. We consider a single-input multiple-output (SIMO) channel between the encoder-controller and the decoder-controller which is lossless and imposes random time delay. We derive a lower bound on the minimum average feedback data rate that guarantees achieving a certain level of average quadratic performance over all possible realizations of the random delay. For the special case of a constant channel delay, we obtain an upper bound by proposing linear source-coding schemes that attain desired performance levels with rates that are at most 1.254 bits per sample greater than the lower bound. We give a numerical example demonstrating that bounds and operational rates are increasing functions of the constant delay. In other words, to achieve a specific performance level, greater channel delay necessitates spending higher data rate. 
 Taking communication imperfections into account for analysis and design has proved to be an interesting topic within the area of control theory during recent years. This interest is motivated by advantages of communication networks over point-to-point wiring and, on the other side, by the complexity that communication constraints impose on classical control problems [3]. Time delay, packet dropout and data rate constraints (quantization) are among prominent challenges [4]–[7]. 
 Using an information-theoretic approach, [8], [9] report primary derivations related to system performance. In these works, it is shown that the presence of a finite-capacity communication channel in a strictly causal feedback loop introduces a new performance limitation which differs from conventional Bode’s formula by a constant quantifying channel information rate. Moreover, the authors derive inequalities among entropy rate of internal signals (inside the loop) and external signals (outside the loop), resulting in a general performance bound which is affected by finite feedback capacity. Inspired by [8], [9], lower and upper bounds are derived on the minimum data rate that guarantees achieving a prescribed level of quadratic performance in [10]–[12]. These works consider noisy linear time-invariant (LTI) plants with Gaussian disturbances, controlled over an error-free digital channel without delay. In particular, [12] shows that over all causal mappings which represent coding and control, the average data rate is bounded from below by the directed information rate generated by the mappings that render the sensor input and control output jointly Gaussian. Moreover, it is proved in [12] that in an auxiliary LTI structure, the minimum signal-to-noise ratio (SNR) which guarantees stability and meeting a quadratic performance requirement gives the lower bound on the desired minimal data rate. For the upper bound analysis, [12] suggests employing entropy-coded dithered quantizers (ECDQs). Such a simple coding scheme is designed based on the aforementioned SNR-constrained optimization giving the lower bound. Inspired by [10] and [12], the authors of [13] present a method based upon semidefinite programming (SDP) to characterize the trade-off between directed information rate and linear quadratic Gaussian (LQG) performance in rate-constrained networked control systems (NCSs) with fully-observable multiple-input multiple-output (MIMO) plants. In [14], the authors derive a lower bound on the zero-delay rate distortion function associated with vector-valued Gauss-Markov processes and mean-square error distortion constraint. Based on the separation principle, this bound is in fact the lower bound on the minimum data rate required for attaining LQG performance in control of fully observable plants. Then [14] utilizes the optimal realization that corresponds to the foreshadowed class of vector-valued Gaussian sources to derive an upper bound on zero-delay rate distortion function using variable-length entropy coding with lattice quantization. Similar ideas are employed in [15] for establishing bounds on minimum mutual informations, across a delay-free channel, that guarantee achieving specific linear quadratic regulator (LQR) performance levels. Specifically, [15] derives the lower bound based on Shannon’s lower bound and power entropy inequalities whereas the upper bound is 
 NCSs subject to network-induced delays are generally analyzed according to two methodologies: robustness and adaptation [5]. The aim in the robustness framework is deriving conditions for certain stability or performance requirements by constructing Lyapunov-Krosovskii functionals that do not incorporate time-stamp information as a variable. For instance, in [16], stabilization 
 and H8 performance conditions for a singular cascade NCS are obtained. Fuzzy-model-based control is another approach in the robustness framework, where the rules are based on the size of delays, and the controller is required to be robust over the delay range [17], [18]. In the adaptation framework, one method is modelling NCSs as stochastic switched systems. The recent results on stability and H2/H8 performance of Markov jump linear systems (MJLSs) are reported in [19] and [20], respectively. The second approach in this framework is predictive control; a method which is currently quite popular in NCSs. According to this technique, the actuator selects among a sequence of control commands based on the transmission delays experienced by them [21]–[23]. 
 In all the aforementioned results on system performance, either the effect of channel delay is neglected, or the rate limitation is not taken into account. However, looking into the literature, one can find works investigating performance issues in NCSs with both rate constraints and network-induced delays (see, e.g., [24]–[27]). Even so, a few has utilized the informationtheoretic approach to treat systems with such limitations. For example, [28] derives bounds on the minimum individual (non-asymptotic) rate needed to guarantee meeting an individual performance requirement (boundedness of the maximum `2-norm of states). 
 In this paper, we study the performance of a discrete-time LTI plant with Gaussian initial state in a loop with Gaussian exogenous inputs and random or constant channel delay on the feedback path. For the setup with random delay in the channel, we seek the infimum average data rate required to achieve a prescribed qudratic performance level. We show that the average data rate over all possible realizations of the delay is lower bounded by the average directed information rate. We prove for the random channel delay case that under certain stationarity assumptions, the average directed information rate can be stated in terms of average power spectral densities of the involved signals. We obtain a lower bound on the desired minimal average data rate which is stated as the average of a function of the power spectral densities of feedback path signals over all possile realizations of the delay. To establish all these results, we utilize the tools adopted in [10] and [12]. However, compared to [12] and [10], the channel is not delay-free in our setup. In other words, we extend the information inequalities in [12] to the case where there exists a random time delay between the sensor output and the control input. 
 For the setup with known constant delay in the channel, we show that the above lower bound on the infimum average data rate required for attaining quadratic perfromance is equal to a function of infimum SNR of the channel over schemes comprised of LTI filters and AWGN channels with feedback and delay that meet the quadratic performance constraint. Our contribution in this case is showing how the presence of the channel delay affects the scheme yielding the lower bound. This gives an insight to the interplay between time delay, average data rate and performance in the considered NCS. We also prove that even over a channel with a constant delay, any admissible performance level can be achieved by an EDCQ-based linear coding scheme which generates an average data rate at most (approximately) 1.254 bits per sample away from the corresponding lower bound. We illustrate via a numerical example that lower and upper bounds as well as empirical rates and entropies are all increasing functions of channel delay. This in turn implies that channels with larger delays demand higher average data rates to allow for attaining a certain system performance. 
 Compared to our previous works in [1] and [2], first, we here study the case of random channel delay and second, we employ a simpler proof than information inequalities and identities in [1] and [2]. In this work, we also show the effect of having a delay at different places in the loop on system signals. The last departure from our previous results is that we incorporate some eliminated proofs of [1] into this paper. 
 The remainder of the paper is organized as follows. Section II introduces the notation. Section III formulates the main problem. Section IV analyzes the lower bound problem for the setup with random channel delay. Section V derives a lower bound on the desired minimal data rate in the case of constant channel delay. The analysis of upper bound problem in the constant delay case is presented in Section VI where the equivalence between systems with different delay locations is investigated. A numerical example is given in Section VII. Finally, Section IX concludes the 
 By R, we denote the set of real numbers whose subset R+ represents the set of strictly positive real numbers. The set N0 is defined as N0 ,N ? {0} where N symbolizes the set of natural numbers. The time index of every considered signal, denoted by k in most cases, belongs to N0. Symbols E, log, |.|, and k.k2 represent operators for expectation, natural logarithm, magnitude and H2-norm, respectively. Moreover, ?min(S) and ?max(S) are respectively the largest and smallest eigenvalues of the square matrix S for which the element on the i-th row and j-th column is denoted by [S]i,j. In addition, ßk is shorthand for ß(0),...,ß(k) where ß(k) denotes the k-th sample of a discrete-time signal. Furthermore, for the time-dependent set a(i),i ? N0, ak is defined as ak , a(0) × ··· × a(k). However, if a is a fixed set, then ak , a × ··· × a 
 Random variables and processes are vector valued, unless otherwise stated. Take v and q into account as two random variables with known marginal and joint probability distribution functions (PDFs). Their joint PDF is represented by f(v,q) while the marginal PDFs of v and q are symbolized by f(v) and f(q), respectively. The conditional PDf of v given q is denoted by f(v|q) and Ev(.) is the operator for the expectation with respect to the distribution of v. We define the differential entropy of v and the conditional differential entropy of v given q as h(v) , -Ev(logf(v)) and h(v|q) , -Ev,q(logf(v|q)), respectively. The mutual information between v and q is symbolized by by I(v;q) and 
 defined as I(v;q) , -Ev,q(log(f(v)f(q)/f(v,q))). Moreover, the definition of the conditional 
 mutual information between random variables v and q given the random variable r is given by I(v;q|z) , I(v,r;q) - I(r;q). All the information-theoretic definitions presented in this 
 We call the random process ? asymptotically wide-sense stationary (AWSS) if limk?8 E[?(k)] = 
 ?? and C? , R?(0) and limk?8 E[(?(k + t) - E[?(k + t)])(?(k) - E[?(k)])T ] = R?(t) hold, 
 where ?? is a finite constant. Accordingly, the steady-state covariance matrix and the steady- 
 state variance of ? are defined as s?2 , trace(C?), respectively. For the scalar random sequence 
 called asymptotically equivalent if and only if they satisfy the following expression for finite %: 
 We consider the feedback loop of Fig. 1 where the plant is LTI with one control input and one sensor output denoted by u ? R and y ? R, respectively. The plant G is disturbed by a vectorvalued zero-mean white noise which is represented by w ? Rnw and has identity covariance matrix, i.e. Cw = I. Moreover, as depicted in Fig. 1, the plant outputs the vector-valued signal z ? Rnz upon which the performance measure is characterized. The relationship between the mentioned set of inputs and outputs is described by a transfer-function matrix as follows: 
 where the dimensionality of each Gij is determined by the dimensions of corresponding pair of inputs and outputs. So nz ×nw, nz ×1, 1×nw and 1×1 are the dimensions for G11, G12, G21 
 Assumption III.1. Every entry of the transfer-function matrix in (1) is proper with no unstable hidden modes. Moreover, G22, which describes the single-input single-output (SISO) open-loop system from u to y, is strictly proper. The initial states of the plant denoted by x0 = {x(-hmax),...,x(0)} are jointly Gaussian with and independent of the disturbance signal w and has a finite differential entropy. 
 As depicted in Fig. 1, the output of the plant, y, is processed into a binary word by the encoder E and transmitted over the error-free channel. Such transmission is accompanied with a random delay. Let h(k) denote the delay experienced by the binary word yq(k) constructed at time k at the encoder. We assume that h(k) is an independent and identically distributed (i.i.d.) 
 process which has a bounded support at each time step, i.e., h(k) ? {h1,...,hm}, ?k ? N0 where hi < hi+1 (i = 1,2,...,m-1). In order to avoid unnecessary notational complexity and without loss of generality, we set h1 as h1 = 0 and hm as hm = hmax. The marginal distribution of the delay is assumed to be known and described by Pr{h(k) = hj} = aj where . 
 Such characteristics introduce a channel with the following input-output relationship: 
 for every k,i ? N0. Denoting the cardinality of S(k) by s(k), we can imply form (2) that uq(k) is a vector comprised of s(k) = hmax +1 binary words which specifies the output of the channel at time k. Note that s(k) is a random variable depending on the channel delay. We assume 
 that yq(i) is discarded at the decoder-controller side if i < 0. Moreover, under aforementioned circumstances, binary words transmitted over the considered channel are not necessarily received in the same order they were emitted. It should be also emphasized that the channel does not 
 A more detailed presentation of the feedback path in the NCS of Fig. 1 is provided by Fig. 2. As depicted, the encoder-controller is comprised of a lossy and a lossless component. The lossy 
 where ?e(k) ? ?e(k) symbolizes the side information at time k at the lossy encoder. Ek : 
 instant, the encoder is assumed to know the time delays experienced by previous binary words and the time delay of the current binary word to be sent over the channel. Therefore, hk is known at the encoder ?k ? N0. This implies that yE(k) can be reconstructed perfectly h(k) steps later at the decoder if yq(k) is constructed by using yE(k) and only those samples of  which will be already available at the decoder at time k + h(k). Note that having access to  at the decoder at the time k +h(k) is not assured. So the lossless encoder O outputs the binary 
 in which yEf(k) is a sequence comprising the elements of yEk-1 for which the associated binary words will have reached the decoder by the time k + h(k), i.e., {yE(i) : i ? N0,i = k - 1,i + 
 deterministic mapping where k - hmax + h(k) + 1 = f(k) = k + 1. So f(k) - 1 specifies the 
 cardinality of the sequence yEf(k). Note that since no dropout occurs during data transmission, 
 will certainly have been received at the decoder by the time k + h(k). In addition, A(k) is a countable set of prefix-free binary code words, which specifies the input alphabet of the channel at each time instant. 
 On the receiver side, uq(k) is available as the input to the lossless decoder. This decoder, 
 where uqf(k) is a sequence comprised of elements of that have time indices less than or equal to the largest time index of yq in uq(k), i.e.,  where m(k) = maxS(k), ?k ? N0. Such selection of data for lossless decoding is in accordance 
 , where k - hmax + 1 = g(k) = k + 1, represents an arbitrary deterministic mapping. It should be noted that according to the channel model, g(k) is a random variable denoting the cardinality of uqf(k) and   holds for all k ? N0. Moreover, based on the definition of uqf and (6), the information provided by   is enough for the lossless decoder 
 where S(k) is defined as in (3). Indeed for such reconstruction, the knowledge of the delay is required at the decoder. Hence, we further assume that the decoder is provided by S(k) through 
 for example timestamping. Finally, the decoder-controller gives the control input via 
 where ?d(k) signifies the side information available at the decoder at time k and is contained in the well-defined set ?d(k). So ?d(k) satisfies ?d(k) ? ?d(k). Moreover, 
 R, k - hmax + 1 = tu(k) = k + 1, is an arbitrary deterministic mapping where tu(k) is the cardinality of Sk. It should be noted that ?o(k) in ?o(k) ? ?o(k) is defined as ?o(k) , ?e(k)n ?d(k). We state some additional properties of the setting described above in the following 
 Remark 1. It can be implied from (5)-(9) that u(k) and uk are functions of   where lk = maxSk for every k ? N0. It thus follows from the definition of S(k) and assuming no dropout in the considered channel that k - hmax = lk = k. This implies that the controller has access to the largest and smallest amount of sensor information when the channel delay is zero and hmax, respectively. 
 Remark 2. It can be implied from the definition of S(k) in (3) that uq(k) can have at most hmax+1 entries at each time step. So the number of the words that can be received at the decoder at each time instant belongs to the set {0,...,hmax +1}. Therefore, since the channel input yq is a scalar process, (2) describes a single-input multiple-output (SIMO) channel. Moreover, S(k), as a stochastic process, cannot be i.i.d because in the considered channel, no transmitted binary word is received at the decoder more than once. 
 Assumption III.2. At each time instant k ? N0, the side information pair (?e(k),?d(k)) together with h(k), and consequently S(k), are statistically independent of (x0,w(k)). Therefore, it can 
 be implied from the dynamics of the system that I(u(k);y(k - hi) | uk-1) = 0 for any hi ? {1,...,hmax} with k - hi < 0. Moreover, upon knowledge of ui, ?di and Si, the decoder is invertible. It means that for each i ? N0, there exists a deterministic mapping Qi such that 
 Remark 3. In Appendix A, we will prove that for the architecture of Fig. 2, any encoder and non-invertible decoder with mappings E, O, O-1 and D, can be replaced by another set of mappings with the same input-output relationship and lower average data rate where the decoder is invertible . 
 For the purpose of expressing the information rate in terms of spectral densities of the signals of the system, we use the following notion of stability: 
 Definition 1. A scalar AWSS process x is called strongly asymptotically wide-sense stationary (SAWSS) if its covariance matrix is asymptotically equivalent to the covariance matrix of the wide 
 asymptotically equivalent. Furthermore, in an SAWSS NCS, all internal signals are SAWSS and their cross-covariance matrices are asymptotically equivalent to the cross-covariance matrices of corresponding WSS processes to be converged to. 
 Clearly, SAWSS-ness implies AWSS-ness but not vice versa; for both signals and systems. For each coding scheme satisfying (5)-(9) and rendering the NCS of Fig. 1 SAWSS, the steady-state variance of the output z is a random variable which depends on the realization of h(k). The same goes for the average data rate. We make explicit such dependence by writing   and R(hk), and consider the means of these variables (over all realizations of hk) as our performance measure and data rate of interest, respectively. Such notions of performance and rate, represented 
 where H denotes the support set for possible realizations of the delay h(k). Moreover, R(hk) and   indicate that the average data rate and steady-state variance are functions of delay. 
 Generally speaking, we are interested in finding the minimal Ra for which having a bounded 
 is feasible. Let Dinf(hr) denote the smallest average steady-state variance of z that can be achieved, when the random delay h(k), with the aforementioned properties, is present in the channel. Hence, Dinf(hr) is obtained by minimizing the average steady-state variance of z over all (possibly nonlinear and time-varying) settings u(k) = Kk(ylk) that render the NCS of Fig. 1 SAWSS. Note that lk is defined as in Remark 1. Under the condition that Assumption III.1 holds, 
 where D ? (Dinf(hr),8), and   represents the average staedy-state variance of the output z over all realizations of the delay. The feasible set of the optimization problem in (11) is 
 comprised of all encoder-controller and decoder-controller pairs described by (5)-(9), satisfying 
 Remark 4. It is straightforward to see from (5)-(9) that the concatenation of the decodercontroller pair and the channel in the NCS of Fig. 1 is equivalent to a decoder-controller pair with the same mapping and side information that applies a time delay with same properties as characterized in (2), on its received data, and that is followed by a delay-free channel. So the system depicted in Fig. 1 is equivalent to the feedback loop of Fig. 3 in which the encoder and the plant are the same and the inputs have the same properties as in Fig. 1. 
 The equivalence pointed out in Remark 4 between systems of Fig. 1 and the NCS of Fig. 3 
 will assist us deriving a lower bound on the average data rate Ra in the next Section. 
 In this section, we establish a lower bound on Ra(D). To do so, we derive inequalities and identities that describe the relationship between the flow of information and system performance in the NCS of Fig. 1. Therefore, we will update fundamental derivations in [2], [12] for the case where the channel delay is randomly distributed. As the first result, we show how the average data rate Ra is bounded from below in the following theorem. 
 Theorem 1. Consider the feedback loop depicted in Fig. 1 for which Assumptions III.1 and III.2 hold. Then 
 where I(.;. | .) represents conditional mutual information. According to [30],  specifies the average directed information rate across the forward channel from y to u in the NCS of Fig. 1 over all possible realizations of the channel delay. 
 Proof. It can be implied from [12, Theorem 3.1] that, for each realization h8 of the delays, the 
 Based upon the chain rule of mutual information, the bound in (13) can be restated as 
 where the definition of li is given in Remark 1. From the dynamics of the plant, we can easily conclude that the sequence  is only a function of x(0) and w, once ui-1 is given. Furthermore, it stems from (2)-(9) that upon the knowledge of yli, side informations   and   will be 
 the only variables describing u(i), ?i ? N0. Latter observations together with the fact that Assumption III.2 holds for the system of Fig. 1 yield the conclusion that the rightmost term of (14) amounts to zero, ?i ? N0. So we have 
 for the NCS of Fig. 3. Now by averaging both sides of (15) with respect to the delay realizations, as in (10), and noting that the feedback loop of Fig. 3 is equivalent to the system of Fig. 1, based on Remark 4, our claim follows immediately. 
 The next lemma shows that joint Gaussianity of two signals lowers the directed information rate between them when these are connected through a channel with random delay. 
 Lemma 1. Suppose that the NCS of Fig. 1 satisfies Assumption III.1 and Assumption III.2. For this system, if (x(0),w,u,y) represents a jointly second-order set of processes, then the following holds: 
 where yG and uG symbolize the Gaussian counterparts of y and u, respectively, in a way that (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y). 
 Proof. According to [12, Lemma 3.1], the directed information rate from sensor output to the 
 where I8(y ? u) and I8(yG ? uG) are defined as in (13). We conclude based on (2)-(9), the dynamics of the plant and the system of Fig. 3 satisfying Assumption III.2 that 
 ui-1,yli) = 0,?i ? N0. This together with the chain rule of mutual information lead to 
 Now the proof is complete by taking average over all possible realizations of the delay from both sides of (18) and considering that based on Remark 4, the system of Fig. 1 is equivalent to the feedback loop in Fig. 3. 
 If the above Gaussian signals are stationary as well, then the average directed information rate can be stated in terms of the average power spectral density of the involved signals. The next lemma will state such result formally. 
 Lemma 2. Suppose that the control input u in the NCS of Fig. 1 is SAWSS for every realization 
 of the channel delay. For each realization, assume that there exists a µ > 0 in such a way that 
 where ? is a Gaussian AWSS process that has independent samples. Such a random process is described as 
 for each realization of the random delay. Moreover, Su? represents the steady-state power spectral density of u. 
 Proof. It can be deduced from [12, Lemma 3.2] that in the NCS of Fig. 3, the following holds 
 in which n is a Gaussian AWSS process with independent samples and I8(y ? u) is defined 
 As already mentioned before, based on the plant dynamics, the knowledge of ui-1 will render 
 dependent only on x(0) and wi for any i ? N0. Moreover, according to (2)-(9), knowing 
 From (23), it can be concluded that n(k) is actually equal to ?(k) as in (20) for the NCS of Fig. 3. Now, our claim is given by taking average from both sides of upper (23) and noting that based on Remark 4, the systems in Fig. 3 and Fig. 1 are equivalent. 
 Corollary 1. Suppose that the NCS of Fig. 1 satisfies Assumption III.1. Then Ra(D) is lower bounded as follows: 
 where ? and u? are defined as in (20) and the infimum is restricted to all mappings staisfying (5)(9) and Assumption III.2, and producing signals y and u with propoerties as stated in Lemma 2. 
 In this section, we consider the same NCS as described in Section III but with a channel that imposes a known constant delay, say h steps, on the transmitted data. The corresponding feedback loop is depicted by Fig. 4. The problem we investigate here is a special case of the problem formalized in (11) where the channel delay is constant and therefore, there is only one realization for the channel delay. In this case, we consider the notation Ra(D) = R(D) and Dinf(hr) = Dinf(h). In Appendix B-A, we prove that finding R(D) is feasible if D ? 
 (Dinf(h),8). We show that in order to obtain a lower bound on R(D), one can minimize the directed information rate over an auxiliary coding scheme formed of LTI filters and an AWGN channel with feedback and delay. Inequalities and identities related to the delay-free version of this optimization derived in [12] will be extended to the case with a constant channel delay. We start by deriving a lower bound on the average data rate R in the following theorem. 
 Theorem 2. Suppose that the feedback system of Fig. 4 satisfies Assumptions III.1 and III.2. Then the average data rate R is lower bounded as follows: 
 where  is the directed information rate across the forward channel from y to u with 
 Proof. Considering that lk = h holds at any k ? N0 for the NCS of Fig. 4, we can conclude the claim immediately from Theorem 1. 
 The directed information rate in (25) will be reduced if the involved signals are jointly Gaussian. This result is formalized by the following lemma. 
 Lemma 3. Suppose that Assumptions III.1 and III.2 hold for the NCS of Fig. 4. Furthermore, consider (x(0),w,u,y) as a jointly second-order set of random processes. Denote the Gaussian counterparts of y and u by yG and uG, respectively, where (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y). Then 
 Proof. Recall that lk = h, ?k ? N0, for the considered case with constant channel delay. The claim follows immediately from Lemma 1. 
 It can be implied from Lemma 3 that by minimizing directed information rate over a scheme that renders y and u jointly Gaussian, one can obtain a lower bound on R(D). Now, we will 
 show that the directed information rate can be stated in terms of power spectral densities of the involved processes if such signals meet certain stationarity conditions. 
 Moreover, assume that u is jointly Gaussian and AWSS with the sensor output y. Then the directed information rate between u and y is expressed as 
 Proof. Immediate from Lemma 2 by noting that lk = h holds for the NCS of Fig. 4 at every time instant k ? N0. 
 It can be implied from Theorem 2 and Lemma 4 that the rate-performance pair yielded by any coding and control scheme satisfying Assumption III.2 which renders the NCS of Fig. 4 SAWSS is attainable with a lower or equal rate if there exists a scheme that generates (y,u) jointly Gaussian with (x0,w) while rendering the system SAWSS. Due to the Gaussianity of (x0,w) and the fact that the plant is LTI, a jointly Gaussian pair (y,u) can be produced by a coding-control scheme comprised of LTI filters and an AWGN noise source. Such a scheme is depicted in Fig. 5. The NCS of Fig. 5 satisfies all of the assumptions and conditions that hold for the system of Fig. 4. However, the arbitrary mappings are replaced by proper LTI filters B and J in the auxiliary feedback loop of Fig. 5. 
 In addition, for such an NCS, a delayed AWGN channel with noiseless one-sample-delayed feedback serves as communication channel. The coding-control scheme in the NCS of Fig. 5 is 
 where ? is a zero-mean white Gaussian noise with variance s?2 and independent of (x0,w), and B = [Br By]. It should be emphasized that Assumption III.1 holds for the initial states x0, the plant G and the disturbance input w in the NCS of Fig. 5. Furthermore, the initial states of the filters B and J, and the delay blocks are deterministic. As the system depicted in Fig. 5 is a special case of the structure of Fig. 4, we use apostrophes for presenting signals in Fig. 5 that have counterparts in the NCS of Fig. 4. 
 Theorem 3. If the NCS of Fig. 4 satisfies Assumption III.1 and Assumption III.2 and D ? 
 where sz20 and Su0 represent the steady-state variance of z0 and the steady-state power spectral density of u0 in Fig. 5, respectively. Moreover, the feasible set for the optimization in (29) is the set comprised of all LTI filters B and the noise ? with  that render the system of Fig. 5 
 Theorem 3 implies that doing the optimization in (29) over the auxiliary LTI system of Fig. 5, with the AWGN channel and delay, will give a lower bound on the minimal data rate required to achieve a certain performance level in the arbitrary (possibly nonlinear and time-varying) structure of Fig. 4. The following results show how the lower bound derived in (29) can be simplified to a bound which is easier to compute. 
 where  is fixed and Sr denotes the steady-state power spectral density of r. Moreover, suppose that the pair (B,J) = (B1,J1) renders the feedback loop of Fig. 5 internally stable and well-posed. Then for any ? > 0, there exists another pair with a biproper filter, say J2, and a proper one, say B2, that renders the system of Fig. 5 internally stable and well-posed, and preserves the steady-state power spectral density of z0 in a way that the following holds: 
 Intuitively speaking, the results of Theorem 3 and Lemma 5 imply that R(D) can be bounded from below by a logarithmic term as in (31) which is a function of channel SNR in the NCS 
 of Fig. 5. Such an intuition will assist us with deriving a lower bound which is computationally appealing in the following corollary. 
 Corollary 2. Take the feedback loop of Fig. 4 into account as an NCS that satisfies Assump- 
 in which st2 and sz20 symbolize the steady-state variances of t and z0 in the auxiliary system of Fig. 5, respectively. For the optimization problem in (32), a candidate solution is an LTI filter 
 pair (B,J) together with noise variance   that cause the system in Fig. 5 to become 
 In this section, we show that for any D ? (Dinf(h),8), one can always find a scheme that guarantees attaining   with an average data rate which has a distance of about 1.254 bits per sample from the theoretical lower bound. For such a scheme, we propose a design approach which utilizes the filters that together with an AWGN with feedback and delay, render the directed information rate over the channel equal to the lower bound on R(D). 
 Definition 2. We call a coding scheme with input-output relationship as in (5)-(9) in the constant channel delay case linear if and only if its dynamics can be restated as follows: 
 where B = [Br By] and J are proper LTI filters with deterministic initial condition. Moreover, ? represents a zero-mean i.i.d random sequence independent of (x0,w). The initial state of the one-step-delay feedback channel is assumed to be deterministic. 
 The realization of linear source coding schemes can be carried out by using entropy-coded dithered quantizers (ECDQs) together with LTI filters. First, implementing an ECDQ causes the 
 in which by Fq, we denote a uniform quantizer with resolution ? ? R+, Fq : R ? {i?;i ? Z}. Additionally, d(k) represents a dither signal whose access are provided to both encoder and decoder. The mapping Ok and its complementary Ok-1 formalize entropy coding for the lossless parts at the encoder and decoder, respectively. The following lemma presents an interesting property of ECDQs when being set up in an LTI feedback loop. 
 Lemma 6. Consider the feedback loop depicted in Fig. 6 and suppose that the plant G˜ is described by a proper real rational transfer-function matrix in which the transfer function from rh to t is scalar and strictly proper. For such a system, assume that the input-output relationship 
 of the ECDQ in the feedback path is given by (34) with finite and positive quantization step size ?. Moreover, take the disturbance w˜ into account as a white noise process jointly second-order with x˜0, the initial state of G˜. Then if the dither d is an i.i.d process with a uniform distribution over (-?/2,?/2) and independent of (w,˜ x˜0), the error r-t is i.i.d, uniformly distributed over (-?/2,?/2) and independent of (w,˜ x˜0). 
 It can be implied from above that combining the LTI filters in (33) with the ECDQ in (34) in a setting as depicted in Fig. 7 will lead to a linear coding scheme for the NCS of Fig. 4 as 
 long as d(k) meets the same criteria as for the dither in Lemma 6. If so, the obtained coding 
 scheme is called a linear ECDQ-based coding scheme. If such a scheme is implemented on the feedback path of the main NCS of Fig. 4, the average data rate is bounded from above by a certain value which is shown in the following lemma. 
 Lemma 7. Suppose that Assumption III.1 holds for the NCS of Fig. 4. Then the existence of an ECDQ-based linear source-coding scheme rendering the NCS of Fig. 4 SAWSS is certified in such a way that the average data rate satisfies 
 In (35), the variance of the quantization error (noise) of the ECDQ-based linear source-coding scheme is set as  . Moreover, st2 represents the steady-state variance of the signal t 
 Now, through the following theorem, we use the result of Lemma 7 to show that utilizing ECDQ-based linear coding schemes can lead to an upper bound on the desired minimal average data rate R(D). 
 Theorem 4. Let Assumption III.1 hold for the closed-loop system of Fig. 4. Then for each D ? (Dinf(h),8), one can always find an ECDQ-based linear source-coding scheme satisfying Assumption III.2 and rendering the feedback loop of Fig. 4 SAWSS in such a way that sz2 = D is resulted and the average data rate is bounded as 
 In the following remark, we state how the upper bound derived in Theorem 4 can be considered as an upper bound on Ra(D) in the case of random channel delay. 
 Remark 5. The upper bound in (36) will be an upper bound on Ra(D) in the random channel delay case if coding and control schemes are linear ECDQ-based schemes designed as in the proof of Theorem 4 for the delay hmax where the decoder-controllers have buffers installed at 
 Fig. 8: Three possible locations for the delay component in the case with constant channel delay 
 their inputs sending only yq(k - hmax) for prcessing at each time instant k ? N0. Clearly, this is due to the fact that at every time step k ? N0, yq(k - hmax) is available at the decoder. Such an upper bound does not seem to be tight since imposing a delay of hmax steps on transmitted data is actually a worst-case scenario. 
 The bounds derived in this section and the previous section limit the desired average data rate R(D) in the NCS of Fig. 4. In this system, the constant delay is induced by the digital 
 communication channel between the encoder-controller and the decoder-controller. One concern is the effect of delay location on the derived bounds. The following lemma takes a step in addressing this issue by showing how the system signals change when the time delay block is moved to a different location in the feedback loop of Fig. 4. 
 Lemma 8. Consider the NCS of Fig. 4 and two other systems each of which yielded by moving the delay component in the NCS of Fig. 4 to either the measurement path (between the sensor and the encoder-controller) or the actuation path (between the decoder-controller and the plant). Fig. 8 depicts the locations where the time delay occurs in these cases. Then systems are not necessarily equivalent across the cases if the only difference between them is the delay location. However, the equivalence can be assured by allowing the side information to change across the 
 Take the following transfer function into account as the model describing the generalized plant G in the NCS of Fig. 4: 
 Let us set the disturbance signal w and initial states x0 in such a way that Assumption III.1 is satisfied. We calculated lower and upper bounds on R(D) as derived in (32) and (36). For computing these bounds, we made use of the equivalence between the NCSs of Fig. 5 and Fig. 12, shown in the proof of Lemma 5, in that we adopted the method in [12] which solves SNR-performance optimization problems similar to the one defining ?0(D) for such systems as the NCS of Fig. 12. The bounds are computed for three different values of channel delay, h = {0,1,2}, with respect to D varying over a range from Dinf(h) to 50 for each h. Moreover, we designed actual linear ECDQ-based coding schemes, and for each selected D in the latter interval, we simulated the NCS of Fig. 4. To do so, we utilized the filters giving the lower bound on R(D) according to the procedure suggested in [12, Theorem 5.1]. The results are 
 demonstrated in Fig. 9. In this figure, the curves referred to as LB and UB present the lower 
 Fig. 9: Bounds on R(D) in (32) and actual data rates and entropies for different values of time delay h 
 and upper bounds on R(D), respectively. We can compare Dinf(h) among cases with different values of channel delay as well. As shown, greater Dinf(h) is associated with larger channel delay, as expected according to [31]. Evaluating how the bounds change in response to changes in the delay is one of the main purposes of this simulation study. We can observe from the bounds plotted in Fig. 9 that when D is fixed, increasing the delay will enlarge the bounds on R(D). In other words, the greater the delay is, the higher average data rate is to be used in order to achieve a fixed quadratic performance level. Moreover, Fig. 9 shows that the lower (upper) bound curves converge to the minimum data rate required for mean square stability as D grows larger. From [32], we know that the minimal data rate guaranteeing stabilizability of the NCS of Fig. 5 is only a function of unstable poles of the plant G. On the other hand, we use the equivalent system of Fig. 12 for the purpose of calculating bounds. So the observation with convergence of bounds to the minimal data rate needed for stability comes from the fact that incorporating time delay into the model of the plant Ga will not affect its unstable poles. 
 Simulation results are illustrated in Fig. 9 as well. The curves referred to as OR and OE present the average data rates and entropies achieved by using actual linear coding schemes. Furthermore, 106-sample-long realizations have been considered for the dither. The coding task 
 in all utilized schemes is done by memory-less Huffman coders which do not take the past information of the dither into account as prior knowledge for coding. In addition to the average data rate, the entropy of the output of the quantizer has been estimated for the aforementioned setup. The gap of around 0.4 bits per sample between the measured entropy and the lower bound indicates that for each h ? {0,1,2}, 0.4 bits per sample of the gap between the actual rates and lower bound is caused by replacing the AWGN with uniform dither and the remainder 0.25 bits per sample corresponds to sample-by-sample coding. It can be observed that the actual rates and entropies have the same properties as the properties of bounds mentioned in the previous paragraph. The most prominent property is related to the behaviour of the achieved rates and entropies as a function of channel delay, i.e., for a system with greater time delay in the channel, 
 In this paper, the trade-off between average data rate and performance in networked control systems has been studied. Two setups have been investigated, each of which incorporates an LTI plant with Gaussian disturbance and initial states, and scalar control input and sensor output. Moreover, both of them have causal, but otherwise arbitrary, mappings on their feedback paths which are responsible for coding and control. The only difference between the two considered systems is the model of the channel that carries out data transmission between the encodercontroller and the decoder-controller. In one case, the digital communication channel is SIMO and information to be exchanged are exposed to random delay. In the other system, the channel is error-free as well but it is SISO and imposes constant delay on transmitted data. For the case with random channel delay, we considered notions for rate and performance which show the average behaviour of the system over all realizations of the delay. We have shown that for such a setup, data rate is lower bounded by average directed information rate from the sensor output to control input, and if y and u are jointly Gaussian, the average directed information rate would be lowest. Moreover, we have shown that when y and u satisfy certain stationarity assumptions, the average directed information rate between them is a function of the average power spectral densities of these signals over all realizations of the channel delay. We have shown that infimum value of this function over all arbitrary coders and controllers that cause system signals have those Gaussianity and staionarity properties lower bounds the infimum average data rate required to attain a prescribed quadratic performance. 
 For the constant delay case, which is a special case of the system with random channel delay, we approximated (by deriving bounds) the minimal average data rate that certifies attaining a certain performance level. Employing the fundamental information inequalities and identities derived for the random delay case, we showed that this desired minimal average data rate is bounded from below when coder-controllers and the channel behave as a concatenation of proper LTI filters and an AWGN channel with feedback and delay. Then we showed that by approximating such schemes with simply implementable linear ECDQ-based coding schemes, one can achieve any (legitimate) performance level by actual rates which are at most 1.254 bits per sample higher than the lower bound. The results illustrated through the simulation show that bounds and empirical rates are increasing functions of channel delay for a fixed performance level. It means larger delay in the channel necessitates higher minimal average data rate that is needed for achieving a certain level of quadratic performance. 
 Future research will concern with finding closed-form solution for the lower and upper bound problems in the case of random channel delay, finding analytic expression for the desired infimum data rate, deriving lower and upper bounds with shorter gap between them, plants with model 
 Lemma 9. Consider a coding scheme described through (5)-(9) that has a non-invertible decoder, 
 and let R?(k) be defined as . For such scheme, assume that u(k) = u0(k) and R?f(k) = R?f0(k), ?k ? N0, where  . Then there exists another 
 coding scheme constructing the control input u(k) = u0(k) with an invertible decoder in such a way that R?f(k) = R?f0(k), ?k ? N0. 
 Proof. Suppose that mappings in (7)-(9) represent a non-invertible decoder at time k in a way that upon knowledge of and Si, perfect reconstruction of   from ui has been possible for all i = 
 Let S1 and S2 be associated with uD1 and uD2 respectively. Two possible cases can occur. In the first case, S1 and S2 are unequal, i.e., S1 6= S2. Since S is known at the decoder at each time step, this case does not contradict the invertibility. That is due to the fact that the knowledge of S would determine whether u(k) is caused by uD1 or uD2. However, the situation is not the same in the case where S1 = S2. Since both uD1 and uD2 are vector-valued variables, uD1 =6 uD2 means that at least one entry of uD1 is not equal to the entry with the same dimension in uD2. The corresponding elements of uD1 and uD2 that are not equal to each other are denoted by pairs (uD1j,uD2j), 1 = j = m, 1 = m = hmax + 1. Since S1 = S2, for each j, both uD1j and uD2j have been exposed to the same delay, say hj, 0 = hj = hmax. So if we denote the output of the lossy encoder that corresponds to uDnj by yEnj, then uDnj(k) = yEnj(k - hj). It should be noted that n is a positive integer which is at most equal to the size of the set As. Let pnj 
 time k -hj. The encoder-decoder set (E¯,D¯) can be defined with exactly the same properties as (E,D) but different from it in the sense that E¯ outputs only yE1j at time k -hj with probability p1j + p2j. This means having only uD1j at time k as decoder input instead of receiving either uD1j or uD2j. Let us define tj , k - hj;k = hj. Then 
 in which (aa) results from the definition of entropy and R?(k), (ab) can be concluded based on the fact that the function -ln(pnj) is monotonically decreasing, and (ac) follows from the definition of R?(k) for the scheme (E¯,D¯). So R?(tj) |(E¯,D¯) = R?(tj) |(E,D) for tj = 0, and consequently R?f(k) = R?f0(k). 
 The above procedure can be iterated for evey pair with the same characteristics as (uD1,uD2) to make sure that there are no two inputs of the reproduction decoder mapped into one identical u(k) at time instant k. Such iteration will then yield an invertible decoder. In other words, 
 u(i) = u0(i) and R?f(i) = R?f0(i), ?i = k. Our main claim now follows by repeating the above 
 Suppose that in the standard architecture depicted in Fig. 10, G, x0 and w satisfy Assumption III.1 and K follows u(k) = Kk(yk-h). Considering the Gaussianity of x0 and w and the 
 in which   denotes the steady-state variance of output z and ? is the set of all proper LTI filters which render the system of Fig. 10 internally stable and well-posed. The assumptions considered for G guarantee that finding Dinf(h) is feasible. Since Dinf(h) can be obtained, for every ? ? (0,D - Dinf(h)), there exists K1 ? ? which gives  for the system of Fig. 10. Applying K1 to this system results in a stable setting which is a special 
 case of the NCS depicted in Fig. 5 with J = 1 and r = t = K1y0 where the steady-state variance of , is finite. Therefore, since K1 ? ?, it can bring internal stability and well-posedness to the feedback loop of Fig. 5 in the presence of any additive noise ? with steady-sate 
 into account as an AWGN with finite variance s?2 for the system of Fig. 5. It should be noted that 
 ?t,?z = 0 depend only on K1. Now by choosing ? = (D - Dinf(h))/3 and the variance 
 (D - Dinf(h))/(3?z) for the AWGN, there exists K1 ? ? rendering the NCS of Fig. 5 internally 
 So considering Jensen’s inequality and concavity of logarithm, we can deduce that the problem of finding  in (29) is feasible for every D > Dinf(h). The feasibility of the problem of finding ?0(D) in (32) is inferred immediately from (40) for any D > Dinf(h). 
 Due to the validity of D > Dinf(h), one can always find at least one coding-control pair, say Eˆ and Dˆ, that while satisfying Assumption III.2, renders the NCS of Fig. 4 SAWSS in such a 
 In (41), processes zˆ, yˆ and uˆ are the counterparts of z, y and u in Fig. 4, respectively. Moreover, 
 the inequalities and identities in (41) stem from Theorem 2 if conditions in Lemma 3 and Lemma 4 are satisfied. Therefore, (yˆG,uˆG) are jointly Gaussian counterparts of (y,ˆ uˆ) as in Lemma 3 and Su? represents the steady-state power spectral density of uˆG as in Lemma 4. The 
 pair (yˆG,uˆG) with conditions stated in Lemma 3 can be generated by a scheme which certifies 
 in which ?ˆG(k) denotes a Gaussian noise with zero mean and independent of  . Since Lk is a linear and causal mapping, we can redescribe  as 
 It follows from causality in (43) that ?k ? N, Bk and Gk are lower triangular matrices with Bk-1 and Gk-1 on the top left corners. This together with the fact that (yˆG,uˆG) are jointly SAWSS allow us to conclude that based on transitivity of asymptotic equivalence for products and sum of the matrices in [34], the sequences {Qk} and {Pk} are asymptotically equivalent to sequences of lower triangular Toeplitz matrices. Furthermore, using Lk as in (42) will bring internal stability and well-posed-ness to the corresponding NCS. Now let us set J = 1 and B as a concatenation of linear filters with the same behaviour as steady-state behaviour of Lk in (42) for the auxiliary system of Fig. 5. Moreover, suppose that ? has a variance equal to s?2ˆG. So based on the asymptotic equivalence between the matrix representations of L and {Lk}, choosing J, B and ? as above will render the system of Fig. 5 well-posed and internally stable. More specifically, the latter set of filters and the noise will give WSS processes to which uˆG and zˆG converge. Therefore, for the control input u0 and error signal z0 in the feedback loop of Fig. 5, Su0 = Su? and   hold. Then based on Lemma 4, the directed information rate in the NCS of Fig. 5 can be expressed as 
 First, we can deduce that any pair (E,ˆ Dˆ) with properties stated above has a counterpart comprised of LTI filter B, J = 1 and the white Gaussian noise ? in architecture of Fig. 5 in such a way that I8(y0 ? u0) = I8(yˆ ? uˆ) and . Secondly, the main problem is finding the 
 Fig. 11: The LTI system whose internal stability guarantees the internal stability of the auxiliary system in Fig. 5 
 infimum of R over all mappings (E,ˆ Dˆ). With all of this in mind, it can be implied from (44) and (41) that the lower bound for R(D) would be equal to the rightmost term of (29) which 
 The necessary and sufficient condition for the feedback loop of Fig. 5 to be internally stable and well-posed is that every entry of the transfer function matrix from input [?,w,?1,?2]T to outputs [z0,y0,r,u0]T in the system of Fig. 11 belongs to RH8 [35]. Such a transfer function matrix, which we denote by T, is described as follows: 
 Now, let us shift the delay block in the system of Fig. 5 to the plant model in a way that for 
 Such an auxiliary NCS is depicted by Fig. 12. Except for the plant model (47), everything 
 Fig. 13: The auxiliary feedback loop characterizing the internal stability of the NCS of Fig. 12 
 in the feedback loop of Fig. 12 is assumed to be the same as in the system of Fig. 5. The 
 internal stability and well-posed-ness of the feedback loop of Fig. 12 is guaranteed if and only 
 Fig. 13 belongs to RH8. It is straightforward to see that Ta = T. So an equivalence holds between internal stability and well-posed-ness of the system of Fig. 12 and the NCS of Fig. 5. In other words, every triplet   rendering the feedback loop of Fig. 12 internally stable and well-posed, will bring internal stability and well-posed-ness to the NCS of Fig. 5 as well. One other implication of Ta = T is that using an stabilizing   commonly for NCSs of Fig. 5 and Fig. 12 will lead to an identical  . This is due to the properties of 
 LTI systems exposed to Gaussian and stationary inputs. Furthermore, those properties lead to deriving the following H2-norm expressions for SNR and variance of the output z0 in the NCS of Fig. 5: 
 in which N , JByz-h(1 - Brz-1)-1. Likewise, the SNR and variance of the output z in the NCS of Fig. 12 is formalized in terms of H2-norms as follows: 
 and . Therefore, upon using the same stabilizing triplet (B,J,s?2), the channel SNR and the variance of the output characterizing performance will be the same for the NCSs of Fig. 5 and Fig. 12. 
 According to [12, Lemma 4.1], for any pair (B,J) = (B1,J1) that renders the feedback loop of Fig. 12 internally stable and well-posed, there exists another pair with the same properties as for (B2,J2) in this lemma. Then our claims follow immediately from the above equivalences between the NCS of Fig. 12 and the NCS of Fig. 5. 
 The feasibility of obtaining  , caused by D belonging to (Dinf(h),8), certifies the existence of a triplet, say  , that leads to for the system of Fig. 5. In the latter triplet, B? is assumed to be a proper LTI filter and  . This together with the definition 
 Moreover, the triplet   with aforementioned properties meets the conditions in Lemma 5. Therefore, another triplet, say  , exists in such a way that implementing it brings 
 for the LTI feedback loop of Fig. 5. Note that J˜? is a biproper filter while B˜? only needs to be proper. Now the fact that (51) holds for any ?,? > 0, the definition of ?0(D) in (32), and the claim of Theorem 3 complete the proof. 
 Let G˜h denote the transfer-function matrix from [w˜ r]T to [z˜ t]T in Fig. 6. Since rh is related to r by rh = rz-h, we can conclude that G˜h meets the conditions of being proper and real rational, and containing a strictly proper SISO open-loop transfer function from r to t. Now having the schemes described via (33) and (34) in mind, we can deduce our claim immediately from [12, Lemma 5.1]. 
 Let us assume that a linear source coding scheme is implemented in the feedback path of the main system in Fig. 4. Due to the feasibility of finding ?0(D), which necessitates satisfaction of Assumption III.1, we can conclude the existence of proper LTI filters B and J that together with an AWGN, say ?, render the NCS of Fig. 4 SAWSS. It stems from some properties of internal 
 stability that the system will still be stable if one keeps the latter filters B and J and only sets ? as ? = 0. This signifies that in the case of unity feedback (t = r), internal stability and wellposed-ness are guaranteed for the open-loop system between r and t. We come immediately to the conclusion that (35) holds based on [10, Corollary 5.3] and statistical characteristics of the dither mentioned in Lemma 6. 
 Considering the feasibility of finding ?0(D), results of Lemma 5, lemma 6, and Lemma 7, and invertibility of the decoder, we conclude the claim by following the same steps as in [12, 
 One of the common feedback loop components across the considered cases in Fig. 8 is the 
 where x ? Rnx represents plant states and u, w, y and z are inputs and outputs defined as in 
 (1). Moreover, A, B1, B2, C1, C2, D11, D12, and D21 are time-invariant matrices of appropriate dimensions. According to the recursion in (52), the states and outputs of the plant at each time instant i ? N0 can be expressed in terms of initial conditions, disturbance and control inputs as follows: 
 For the case where the time delay is imposed by the error-free digital channel between the encoder-controller and the decoder-controller, the relationship between the control input and the sensor output is characterized based on (5)-(9). The dynamics described by (5)-(9) can be 
 where Ek and Dk represent causal, but otherwise arbitrary, mappings at each k ? N0. It follows 
 from (55) that uk can be stated as an arbitrary function, say Nk, of  , i.e., uk = 
 k ? N0, x(k) is a function of   is a function of  , and y(k) is a function of  . 
 In the second case, it is the link between the decoder-controller and the plant that induces the 
 time delay. For such a setting, Ek, Dk, ?e(k) and ?d(k) yield a scheme with following dynamics: 
 It follows from (56) that in this case, uk can be expressed as  , ?k ? N0, where Mk is an arbitrary mapping which is specified by   and  . 
 Substituting such an expression into (53) an by induction, we can rederive x(k), z(k), and y(k) as 
 As the third case, we focus on a structure in which the delay is introduced by the path between the sensor and the encoder-controller. In this situation, the coding scheme is described by causal 
 and based on induction, we come to the conclusion that for the closed-loop system considered in 
 According to the above observations, comparing system states x, sensor output y, and the output z at each time instant indicates that such signals are not necessarily equal across the three cases studied above if the systems share the design (mappings for coding and control and side information) and have the same initial conditions and exogenous inputs. So values of each signal change by relocating the delay component in the NCS of Fig. 4. However, it is straightforward to see from the structure of the variables describing processes x, z, and y that the equivalence over cases can be obtained under the condition that everything is the same across 
 the cases except for side information which can be considered as decision variable. 
 [1]	M. Barforooshan, J. Østergaard, and M. S. Derpich, “Interplay between transmission delay, average data rate, and performance in output feedback control over digital communication channels,” in American Control Conference (ACC), May 2017, pp. 1691–1696. 
 [2]	M. Barforooshan, J. Østergaard, and P. A. Stavrou, “Achievable performance of zero-delay variable-rate coding in rateconstrained networked control systems with channel delay,” in IEEE 56th Annual Conference on Decision and Control (CDC), Dec. 2017, pp. 5991–5996. 
 [3]	X.-M. Zhang, Q.-L. Han, and X. Yu, “Survey on recent advances in networked control systems,” IEEE Transactions on 
 [4]	G. N. Nair, F. Fagnani, S. Zampieri, and R. J. Evans, “Feedback control under data rate constraints: An overview,” 
 [5]	L. Zhang, H. Gao, and O. Kaynak, “Network-induced constraints in networked control systems—a survey,” IEEE 
 [6]	J. Baillieul and P. J. Antsaklis, “Control and communication challenges in networked real-time systems,” Proceedings of 
 [7]	A. S. Matveev and A. V. Savkin, Estimation and control over communication networks.	Springer Science & Business 
 [8]	N. C. Martins and M. A. Dahleh, “Feedback control in the presence of noisy channels: Bode-like fundamental limitations 
 of performance,” IEEE Transactions on Automatic Control, vol. 53, no. 7, pp. 1604–1615, 2008. 
 [9]	N. C. Martins, M. A. Dahleh, and J. C. Doyle, “Fundamental limitations of disturbance attenuation in the presence of side 
 information,” IEEE Transactions on automatic control, vol. 52, no. 1, pp. 56–66, 2007. 
 [10]	E. I. Silva, M. S. Derpich, and J. Østergaard, “A framework for control system design subject to average data-rate 
 constraints,” IEEE Transactions on Automatic Control, vol. 56, no. 8, pp. 1886–1899, 2011. 
 [11]	——, “An achievable data-rate region subject to a stationary performance constraint for LTI plants,” IEEE Transactions 
 [12]	E. I. Silva, M. S. Derpich, J. Østergaard, and M. A. Encina, “A characterization of the minimal average data rate that guarantees a given closed-loop performance level,” IEEE Transactions on Automatic Control, vol. 61, no. 8, pp. 2171–2186, 2016. 
 [13]	T. Tanaka, P. M. Esfahani, and S. K. Mitter, “LQG control with minimum directed information: Semidefinite programming approach,” IEEE Transactions on Automatic Control, vol. 63, no. 1, pp. 37–52, 2018. 
 [14]	P. A. Stavrou, J. Østergaard, and C. D. Charalambous, “Zero-delay rate distortion via filtering for vector-valued Gaussian 
 sources,” IEEE Journal of Selected Topics in Signal Processing (to appear), 2018. 
 [15]	V. Kostina and B. Hassibi, “Rate-cost tradeoffs in control,” 2016. [Online]. Available: http://arxiv.org/abs/1612.02126v2 
 [16]	Z. Du, D. Yue, and S. Hu, “H-infinity stabilization for singular networked cascade control systems with state delay and 
 disturbance,” IEEE Transactions on Industrial Informatics, vol. 10, no. 2, pp. 882–894, 2014. 
 [17]	M. A. Khanesar, O. Kaynak, S. Yin, and H. Gao, “Adaptive indirect fuzzy sliding mode controller for networked control systems subject to time-varying network-induced time delay,” IEEE Transactions on Fuzzy Systems, vol. 23, no. 1, pp. 205–214, 2015. 
 [18]	R. Lu, H. Cheng, and J. Bai, “Fuzzy-model-based quantized guaranteed cost control of nonlinear networked systems,” 
 [19]	J. Xiong, J. Lam, Z. Shu, and X. Mao, “Stability analysis of continuous-time switched systems with a random switching 
 signal,” IEEE Transactions on Automatic Control, vol. 59, no. 1, pp. 180–186, 2014. 
 [20]	L. Qiu, Y. Shi, F. Yao, G. Xu, and B. Xu, “Network-based robust H2/H8 control for linear systems with two-channel 
 random packet dropouts and time delays,” IEEE transactions on cybernetics, vol. 45, no. 8, pp. 1450–1462, 2015. 
 [21]	Z.-H. Pang, G.-P. Liu, D. Zhou, and M. Chen, “Output tracking control for networked systems: A model-based prediction 
 approach,” IEEE Transactions on Industrial Electronics, vol. 61, no. 9, pp. 4867–4877, 2014. 
 [22]	H. Li and Y. Shi, “Network-based predictive control for constrained nonlinear systems with two-channel packet dropouts,” 
 IEEE Transactions on Industrial Electronics, vol. 61, no. 3, pp. 1574–1582, 2014. 
 [23]	W. Yao, L. Jiang, J. Wen, Q. Wu, and S. Cheng, “Wide-area damping controller for power system interarea oscillations: A networked predictive control approach,” IEEE Transactions on Control Systems Technology, vol. 23, no. 1, pp. 27–36, 2015. 
 [24]	Y. Nakahira, “LQ vs. `8 in controller design for systems with delay and quantization,” in IEEE 55th Conference on 
 [25]	Q.-L. Han, Y. Liu, and F. Yang, “Optimal communication network-based H8 quantized control with packet dropouts for a class of discrete-time neural networks with distributed time delay,” IEEE transactions on neural networks and learning 
 [26]	K. Liu, E. Fridman, K. H. Johansson, and Y. Xia, “Quantized control under round-robin communication protocol,” IEEE 
 [27]	W. M. H. Heemels, A. R. Teel, N. Van de Wouw, and D. Nesic, “Networked control systems with communication constraints: Tradeoffs between transmission intervals, delays and performance,” IEEE Transactions on Automatic control, vol. 55, no. 8, 
 [28]	J. Zhang and C.-C. Wang, “On the rate-cost of Gaussian linear control systems with random communication delays,” in 
 2018 IEEE International Symposium on Information Theory, June 2018, pp. 2441–2445. 
 [29]	T. M. Cover and J. A. Thomas, Elements of information theory.	John Wiley & Sons, 2012. 
 [30]	M. S. Derpich, E. I. Silva, and J. Østergaard, “Fundamental inequalities and identities involving mutual and directed 
 informations in closed-loop systems,” 2013. [Online]. Available: http://arxiv.org/abs/1301.6427 
 [31]	K. Hashikura, “H2/H8 controller design for input-delay and preview systems based on state decomposition approach,” 
 Ph.D. dissertation, Department of Human Mechatronics Systems, Tokyo Metropolitan University, Japan, 2014. 
 [32]	E. Johannesson, “Control and communication with signal-to-noise ratio constraints,” Ph.D. dissertation, Department of 
 [33]	K. J. Astr°	om,¨	Introduction to stochastic control theory.	Courier Corporation, 2012. 
 [34]	R. M. Gray, “Toeplitz and circulant matrices: A review,” Foundations and Trends in Communications and Information 
 [35]	B. A. Francis, A course in H8 control theory.	Berlin; New York: Springer-Verlag, 1987. 
 ﻿ In order that can be , or it is necessary that they first be converted into digital form . This , in turn , the problem of how to digitize data so as to achieve the best trade off between data load and performance , i . e ., how to make the most out of a little . Two are involved in this problem , namely temporal quantization i . e ., sampling and spatial quantization . These two have traditionally been separately . Indeed , there substantial literature dealing with the temporal quantization problem , covering both band limited and non band limited . The usual underlying paradigm is that of an analysis filter , by a sampler , by a reconstruction filter . Various of this architecture can be once other have been . On the other hand , spatial quantization been studied extensively for a given sampling strategy , particularly in the framework of sigma delta conversion . Finally , it is also possible to formulate the joint design problem for sampling and spatial quantization . This typically to enhanced performance to that achievable by considering the two separately . 
 This paper will survey the general area of sampling and quantization and analyze for efficient data for signal and control . We will show how , on the one hand , contemporary control theory can contribute to the design of sampling and quantization and , on the other hand , how these impact on the performance of modern feedback control . 
 Key : Sampling , quantization model predictive control , constrained control , control 
  
 INTRODUCTION 
 We live in a data rich world . Most technological operate by first , from the world into digital . This is a necessary precursor to allow to be , and without degradation other than that by the to digital conversion itself . 
 The above was indeed the motivation that led Alec to invent pulse code modulation seven ago . In his patent , the main of , namely : 
  
 These are remarkable for the time they were . Indeed , most of these have only become reality in recent times . Furthermore , the validity of the first two to be formally determined after they were , and is still subject of ongoing research . In the pursuit of better quality at lower bit and lower , increasingly parsimonious are continually so as to acquire , process and represent digitally . 
 This topic also important theoretical , from such as information theory , functional analysis , optimization , communication theory wavelet theory . As we will discuss in this paper , also control theory much to contribute to this circle of . Conversely , much of the theory and from digital signal are highly relevant to several of control , e .., control , where parsimonious signal representation is a key element , see , e .., . 
 In the present work we present some of the main of sampling , quantization and reconstruction of , continuous time . We will describe reconstruction quality and relate it to design such as filter complexity , data rate and sampling frequency . We also present some concerning the joint problem of sampling quantization , on one side , and reconstruction on the other . We limit our analysis to uniform sampling of scalar , sampling and reconstruction by single as opposed to filter , with scalar output and we will not discuss any related to further symbol . 
 The layout of the remainder of the paper is as : Section the of quantization and some of the that justify the introduction of a more general model for a reconstruction system . Section the sampling and reconstruction in a frame theoretic perspective . Section is a review of some recent generalized on the sampling and reconstruction problem . In Section we present some basic of scalar memory less quantization and . Section feedback . In particular , some of the basic of predictive and noise shaping to digital are . In Section we present noise shaping that generalize based on model predictive control . Section to analyze the joint problem of the quantization and sampling reconstruction design , some recent and . In Section we show how related to sampling and quantization can be in control . Section . Finally , an Appendix is included with some of the basic of frame theory necessary to understand several of the in the main body of the paper . 
 AD CONVERSION 
 In this section we will first describe as a basic architecture used in AD conversion . Various of will then motivate us to introduce later a more general framework . 
 . Basic Scheme 
 We consider the simple and system by the block diagram in Fig . . 
 a a 
 Nearest Ideal Low Pass 
  
 Filter 
 Figure : system with ideal low pass reconstruction filter . 
 The usual paradigm associated with this setup is that the input signal a ,, is taken to be band limited to some frequency , say , . Then , in accordance with the sampling theorem , the sampling step is chosen as . Since the input signal is directly , we have a , . The nearest scalar in Fig . to the non linear transfer function , defined by 
 Q 
 where is the quantization step see Fig . and rounding to the integer value greater than a . 
 c u 
 Nearest 
  
 Figure : Nearest Scalar . 
 Thus , the output of in Fig . is the sequence of u , where 
 u , . 
 The synthesis Fig . is , in the case , an ideal continuous time low pass filter with cut off frequency t and impulse response , 
 R , where . The output the , continuous time signal a , given by the mixed convolution 
 a u , 
 k Z 
 If there were no quantization i . e ., if , then u would equal for all . In this situation , a in would equal exactly the input a for all , since , by virtue of the sampling theorem , if a is to , it can be reconstructed from by the interpolation formula 
 a a , . 
 k Z 
 In the presence of quantization , it is generally no longer true that a a . Nevertheless , it is reasonable to expect that , if the quantization step is small , then the u will be close to the a for all , and the output of the simple system of Fig . will be close in some sense to the input a . Unfortunately , this and other in the above model are often far from realistic , as next . 
 . Practical of 
 Whilst the method above is certainly attractive , it from several that hinder its usefulness in many practical . In what , we will describe some of the main of this architecture . 
 Synthesis Filter The ideal low pass filter used in Fig . for synthesis cannot be in practice . Firstly , it is non causal . A very close approximation of the ideal low pass filter would still be non causal , which it out from any delay sensitive application . 
 Secondly , an ideal low pass filter an infinite impulse response length . For practical low pass , the closer they mimic the ideal filter , the longer the impulse response will be . One problem with a long , slow impulse response is that it the stability of the reconstruction , in the sense that bounded in the are able to produce unbounded point wise error in the reconstructed output . As an example , consider the ideal low pass reconstruction in . It is easy to show that any bounded periodical error in the a of the form , with , will yield an unbounded reconstruction error in the L norm . The second difficulty with a synthesis filter with long but finite impulse response is cost and complexity : In where synthesis is accomplished via discrete time FIR , longer impulse require higher computational complexity . 
 Another problem with the ideal low pass synthesis filter model is that , in many practical , the synthesis filter is not a design choice , but is by other . In such , the synthesis filter can have almost any frequency response . An important example of this situation is that of , where the plant itself can be thought of as part of the synthesis Fig . . We will return to this situation later in Section . 
 Not Necessarily Band Limited Input The assumption of band limitedness of the input signal a is also very restrictive . Most real have to deal with over a finite time interval strictly speaking , any non zero finite duration signal is not band limited . Even when a virtually infinite duration , perfectly band limited signal , only a finite number of can be used for the reconstruction . This truncation , i . e ., part of the inter sample the input signal is not by the . On the other hand , it is often the case that the sampling rate cannot be made high enough to completely avoid . Whilst this is commonly dealt with by a low pass anti filter before sampling , this paradigm may have significant whenever the signal relevant information in the high frequency part of its spectrum , or when the is not perfectly band limiting see , e .., , . In this case other of analysis should be considered . 
 Availability of the Input Signal Before being able to sample the value of any physical variable , it is necessary to convert it to an electrical signal by of a transducer , which in itself is a dynamical system . It is often the case that sampling is in the transducer itself . In this case , one does not have access to the underlying continuous time signal , but only to the taken . Depending on the situation , this can deprive further of knowledge of important inter sample behaviour of the physical variable . It is then necessary to make a wise design of the synthesis stage , so that the input signal can be well at the output see , e .., , , . 
 Quantization , Sampling Frequency and Data Rate In the simple system of Fig . , quantization is done element wise by a nearest , see . Thus , if one to obtain a small reconstruction error , one would naturally aim at reducing the quantization step . In practice , however , the reduction of is limited by cost and structural . Alternatively , if the statistics of the input signal are known , then the mean square reconstruction error can often be reduced by a in which the quantization step is not uniform along its dynamic range . 
 Moreover , even though the sampling theorem that when the are un an increase of the sampling frequency cannot improve reconstruction since it is already perfect , the situation with is different . More precisely , when quantization is , sampling rate can be to reduce quantization error see Sec . . . Thus , one often the chance to compensate the effects of coarse magnitude quantization by of a finer time quantization , i . e ., faster sampling rate . The reader may be well aware of this in bit used in some . 
 In practice , the the sampling rate and the number of quantization is often constrained by data rate . This is so because , although not explicitly shown in Fig . , the sequence of u , in binary form , to be or before reconstruction place at another location in time and space . This that the total number of , or similarly , the data rate , is limited . In principle , if the nU , then the data rate will be approximately given by 
 Bit Rate . 
 t 
 It is possible , however , to reduce the data rate by an efficient of the sequence of compression . When such is applied , the data rate limitation into an information rate limitation , precisely given by the entropy of the sequence of at the output of the . with entropy are also variable rate . In this paper , however , we will not consider such . Thus , we will only consider fixed rate , and the data rate will be given by . 
 . A More General Model for AD Conversion 
 In view of the of conversion above , a more general model for the analysis of sampling , quantization and reconstruction is in Fig . . 
  
 Weighting Filter Figure : A more general sampling , quantization and reconstruction system . 
 For the remainder of this work , we will restrict our analysis to input a which are as finite energy scalar of a single parameter i . e ., a L . For example , we could think time , for the case of time scalar . Thus , the analysis Fig . for all the continuous time linear of the input that before the sampling place . The sampling process is assumed uniform i . e ., regular sampling , with fixed sampling interval . 
 The synthesis Fig . the linear in continuous time possibly with some discrete time filtering applied to the u . The output as a . It a in some well defined sense . 
 The in Fig . is generalized because it is to have access to previous and future input during operation , and scalar , because it a sequence of , one at a time . We will only discuss of this type in the remainder of this paper , which a definition of the class of generalized scalar : 
 Definition Generalized Scalar . Any quantization strategy that can be within the following 
 • The no access to the continuous time signal a , but only to the . 
 • The a sequence of u at a constant rate , one element time . The total in the output sequence the number of input . 
 • Each of the in the output sequence of the can take only from a finite , given and fixed set of U , i . e , the output of the 
 u U , . 
 • The access to all past and future . 
 is said to belong to the class of Generalized Scalar . 
 Note that this definition for the uniform , nearest scalar in as a special case . The last condition in Definition that the generalized scalar in Fig . is , in principle , to determine the output u , for any , based upon knowledge of the entire sequence , i . e ., it is a dynamic system . Therefore scalar with memory such as the predictive and noise shaping to be in Section are special of the generalized scalar . 
 In Fig . an error frequency weighting added . Inclusion of this filter the fact that , depending on the application , the practical impact or cost of the reconstruction error is frequency dependent . Accordingly , the instantaneous error a a to produce a frequency weighted error signal eH . Based on the general setup in Fig . , throughout the remainder of this work the performance of the system will be assessed in of the squared L norm of the signal eH : 
 . 
 SAMPLING AND RECONSTRUCTION FROM A FRAME THEORETIC PERSPECTIVE 
 As above , a paradigm which many signal of a filtering or analysis stage , a sampling stage , a digital , discrete time stage and a post filtering also to as synthesis or reconstruction stage . It been shown that these are equivalent to a sequence of between see , for example , , , and . This viewpoint one to use the powerful of , and study and design sampling and reconstruction . It for elegant to otherwise complex design optimization , by projection . 
 . Historical 
 To the best of our knowledge , the first author to apply theory to the sampling problem was . in . In he derived sampling for random stationary complex exponential . Further insight and for were provided by . in for other , see . Several with the space approach to the sampling problem in subsequent . Among , a paper by a unified approach to generalized sampling . It the idea of regarding the approximation of in a , more general , finite dimensional reconstruction space , instead of to perfect reconstruction by . Interestingly , in , a finite number of of a signal was used , as opposed to an infinite number of raw . By the early , the recently wavelet theory , to stimulate a strong revival of sampling theory see , for example , , , , by the mathematics of basis and in . This framework for the re formulation of the sampling and reconstruction problem in more general and practical inter , sampling and reconstruction from finite , , study of arbitrary input and reconstruction , , , sampling of non band limited , , , , non uniform sampling , , filter , , and and interpolation , . 
 In the remainder of this section we will derive a 
 of the sampling and reconstruction in a space frame theoretic context . For a more complete formal analysis , see , for example , , , , , , . 
 . Sampling and Reconstruction as Frame 
 It will be shown next that the analysis and sampling , which map continuous time into discrete time , can be seen as the analysis operator of the sampling frame . This frame is made of of the time reversed impulse response of the analysis filter . Similarly , the reconstruction process , which discrete time into continuous time , can be seen as the synthesis operator of a reconstruction frame . It is made of of the impulse response of the reconstruction filter . 
 Filtering and Sampling Consider the input signal a in the block diagram of Fig . . Assume that a is known to belong to some space of , say A L . the output of the analysis filter , which impulse response L . Then , is given by the convolution : 
 y . 
 If one now a taking the of at time , sampling process in fig . , one a 
  
  
 a a 
  
 where . One can see that the last integral in to the conventional inner product in L , defined in , between a and . If we now define the shift operator by 
 , 
 then it is possible to write as c 
 Therefore , the input signal can be seen as the result of a sequence of inner . From and Definition see Appendix , this is indeed the process by the analysis operator associated with the frame . As a consequence : 
  
 Notice that , since is a frame for the space 
 S span L , it all a L , as . 
 Synthesis or Reconstruction Consider now the conversion from the discrete time sequence u to the continuous time signal a , see Fig . . If we denote the impulse response , then the band limited reconstruction scheme in can be generalized to : 
 a u u , 
 k Z 
 It is clear from Definition Appendix that the reconstruction process can be by the synthesis operator associated with the frame : 
 a 
 In this new setting , becomes the generating function for the principal shift invariant reconstruction , which is , in general , different from the space of band limited . 
 The sum in can be seen as a mixed convolution , 
 i . e ., 
 a u , R 
 which it a discrete time sequence u and a continuous time function , yielding a continuous time function a . If the impulse response is chosen such that is a sequence and therefore a frame for span , see , then is a bounded operator , and the output a u L for all 
 . 
 The Combined Sampling and Reconstruction Process It from the above that the sampling analysis and reconstruction synthesis process can be stated as a sequence of between : 
 • Analysis : 
 . 
 In particular ,. 
 • Reconstruction : 
 u 
 Therefore , in the absence of quantization i . e ., if u , , the complete process can be expressed as 
 , a a 
 If the sequence u is by quantization of 
 , then becomes 
 , a a 
 It is interesting to note that the above allow one to determine the ultimate and of a sampling and reconstruction system in of the related to sampling rate and . More precisely , the analysis and synthesis alone determine , respectively , the class of that can be sensed i . e ., the sampling space and the class of that can be i . e ., the reconstruction space . A rather remarkable implication is that in the intermediate one can only design the between these , but not expand the sampling and reconstruction themselves . 
 As a consequence , the design of an AD conversion scheme can be thought of as two , namely : 
 . Choice of the sampling and reconstruction i . e ., choice of . 
 . Design of the between in the sampling space and in the reconstruction space i . e ., design of discrete time , quantization . 
 In what , we will describe of the separate design of the sampling reconstruction strategy and of the quantization method . Some of the joint design problem will be later in Section . 
 SAMPLING AND RECONSTRUCTION WITHOUT QUANTIZATION 
 In this section we discuss the effect that analysis and synthesis have on the reconstruction quality . We will assume that the input and output are given and will neglect quantization effects . The implicit trade off here is between the quality of the reconstruction and the computational complexity and delay in the sampling and reconstruction . 
 . of Reconstruction 
 As in Section , the ultimate sampling and reconstruction of a system are limited by the sampling and reconstruction . These , in turn , are entirely determined by the choice , as well as the sampling interval . This that , whenever possible , the design focus mostly on the sampling and reconstruction that one to obtain . Further refinement can be by careful design of discrete time which can be right after the analysis before the synthesis filter , see Fig . . Interestingly enough , it been shown that , in general , the optimal is by making the sampling and reconstruction of one another , . To achieve this for a given analysis frame , one can insert a discrete time correction filter before the synthesis filter to make the synthesis frame the dual of the analysis frame . Although , in general , the dual frame of some given frame is not unique , there only one shift invariant dual frame i . e ., a unique correction filter for each given shift invariant frame . In what , we will consider the following situation : 
 • is a non separable space e .., L . 
 • A is the space that all possible input . 
 • span is the sampling space . 
 • span is the reconstruction space . 
 Depending on the relation between the input space A , sampling reconstruction space , we will consider three of reconstruction , namely : consistent , orthogonal and perfect reconstruction . 
 Consistent Reconstruction The first and most generally attainable reconstruction goal is that of consistent reconstruction , first in by and , see . A signal approximation is said to be consistent if it the same as the original signal when re into the system , i . e . a is a consistent approximation of a A if and only if 
 F a a . 
 The idea of consistent reconstruction is in Fig . . a ; in this figure , a is , the null space of , see . 
  
 a 
 Figure : a Consistent reconstruction oblique projection ; reconstruction orthogonal projection . 
 The notion of consistent reconstruction was first for bases in , and then extended for in , , , , and . 
 Orthogonal Reconstruction The second type of reconstruction is orthogonal reconstruction , also reconstruction . It additional see next section . In this type of reconstruction , the system , for any input a A , the output a that 
 , i . e .: 
 a min . W 
 It is well known that this notion is equivalent to an orthogonal projection of the of A onto the output space see Appendix A . . . The intuitive notion of orthogonal projection is in Fig . .. Note that the a shown in this figure is , indeed , the point to a in the output space . 
 Perfect Reconstruction The third , and most demanding notion is that of perfect reconstruction , i . e ., 
 a a , a A . 
 As will be shown below , depending on the A , and , perfect reconstruction can still be possible , even , for example , for non band limited , , . In the remainder of this section we will describe on the sampling and reconstruction method which ensure that each of these can be . 
 . for Consistent , Optimal and Perfect Reconstruction 
 Under the assumption that the sampling and reconstruction satisfy the direct sum condition 
 H , 
 necessary and sufficient have been found in order to make the sampling and reconstruction system achieve consistent reconstruction and , as particular , optimal and perfect reconstruction as well , , . For shift invariant and , the direct sum condition can be conveniently expressed in the frequency domain based on the 
 A , A 
 A , A 
 and the null of A and respectively , as A and , where 
 N , 
 by of the following proposition : 
 Proposition , Proposition . . Let , L , and assume that and are frame . Then the following are equivalent : 
 i L , 
 A and there a constant A such that 
 A 
 It is shown in that , if the direct sum condition is satisfied , then a is a consistent reconstruction of an input a if and only if a is the oblique projection of a , the null space of see Appendix A . . . Such a projector , by , is defined as 
 , , 
 where , with , 
 The following the concept of oblique dual frame and its relation with the oblique projector : 
 Lemma from , Lemma . . Assume that and are let span , span . Assume that . Then the following are equivalent : 
 a . 
 b . 
 c . 
 Furthermore , if the above three equivalence are satisfied , then is an oblique dual frame of is an oblique dual frame of on 
 W . 
 From Lemma one can see that becomes an oblique projector if and only if is an oblique dual frame of in . 
 Although , as with the conventional case considered in Definition see Appendix A . , the oblique dual frame within a given space is not unique , the shift invariant oblique dual frame of a shift invariant frame is unique . This that , once reconstruction and sampling are defined , there a unique analysis filter that the analysis frame the oblique dual of the reconstruction frame . Conversely , there a unique reconstruction filter that turns the reconstruction frame into the dual of the analysis frame . An expression in the domain for the oblique dual frame condition in of the frequency of the analysis and reconstruction is given in , which , by virtue of Proposition , can be as : 
 Theorem from , Theorem . . Let , L and assume that and are frame , the , respectively . If L , then the following : 
 i There a unique function such that 
 w ; 
 k Z 
 This unique function is given in the domain by 
  
 , if A Z 
 , if A 
  
 Remark . In relation to , we note that : 
 • The function in is periodic . 
 • The result in one to obtain a shift invariant oblique dual frame for a a continuous or discrete time correction filter with transfer function 
 Q 
 just before or just after the analysis filter . With such an arrangement , and provided i and in Theorem are satisfied , the system will yield perfect all a and consistent reconstruction for all a L , as . 
 • Conversely , from the reciprocity of oblique dual , also one to obtain the frame for a given . This can be by a continuous or discrete time correction filter with transfer function defined in . Notice that this correction filter does not alter the space associated to the stage in which it is inserted , i . e ., if the impulse response of is , then . 
 From the previous it that , if the direct sum and duality are met , then necessary and sufficient condition for each type of reconstruction can be stated as : 
 for Perfect Reconstruction Perfect reconstruction only for all a is possible , without any further requirement 
 for Reconstruction Orthogonal Projection If , additionally ,, then becomes an orthogonal projector onto , i . e ., , see Appendix A . . . This that the output signal a will be the best approximation the input signal a , i . e ., it will minimize . 
 for Consistent Reconstruction Oblique Projection Consistent reconstruction will be for all a L without further . 
 QUANTIZATION 
 Quantization is the process of into which belong to a finite set . The representation of with infinite accuracy would require an infinite number of . Quantization one to achieve a approximate representation of infinite , which in turn can be with a finite number of . Hence , the main purpose of to digital conversion is to compress data , whilst aiming to obtain the best possible approximation of the signal . This is to be within data rate and according to some fidelity criterion , i . e ., making most out of a little . As already in Section . , the to be in this paper belong to the family of generalized scalar , see Definition . As such , generate an output sequence u whose are constrained to belong to a set of nU see , the quantization alphabet U , now formally defined as : 
  
 Traditionally , quantization been only in of discrete time performance , usually looking at the between input and . the input and the and u , the is given by : 
  
 k Z 
 We will next briefly discuss the realization of the generalized scalar in Fig . : the zero memory scalar . Its performance will be in of the as defined in . Other of the generalized scalar , such as quantization with memory by of feedback and quantization with memory and preview , will be in and , respectively . For a more comprehensive analysis of quantization see , e .., , , . 
 . Scalar Quantization 
 Scalar quantization is also to as zero memory quantization , since each sample is previous or future . Scalar partition the real line into a set of nU disjoint and consecutive I I ,..., , . A unique scalar from U is associated to each interval in I , usually satisfying , i ,..., nU . Depending on the choice of the partition , either a uniform or a non is . 
 Uniform The scalar is the nearest uniform in Section . , where the partition of the input space the real line is given by and the of U satisfy , i ,..., nU 
 the positive extreme output extreme input 
 M 
 C , 
 the is said to be if the input . If the probability density function of the is smooth and the quantization step is small enough , then the quantization error can be approximately as a random variable with uniform distribution over , see for precise , and the mean squared error between the the output u of the is given by the distortion measure : 
 D E , 
 where E the value of the random variable . 
 In of the number of to represent each sample , we first note that 
 C B 
 Thus , the distortion on the number of per 
 D C M , for large . 
 Non Uniform For a given number of per sample , the be further reduced if the probability density function of the is known . This can be by a non uniform quantization step . Any form of non uniform quantization can be accomplished by complementary non linear before and after a nearest . The first block is a compressor , and its transfer function is a monotonically increasing function satisfying 
 C , , 
 The complementary block after the is expander , and a transfer function . an expression first derived in , one that , for a non uniform with a large number of quantization , compressor characteristic and without overload , the due to quantization is given by 
  
  
 C 
  
  
 where is the of the and . The no overload assumption and , and that , Notice that for i . e ., with a uniform quantization step , becomes . 
 Clearly , minimization of in a compressor the of the input signal . The optimal compressor characteristic is given by the solution to 
  
 a 
 where a is a constant such that . When the solution of is inserted into , the without overload and for found to be 
  
 C s B 
 D 
  
  
 where s and are the variance and the of an individual input sample , respectively . In relation to , it must also be pointed out that see must be made several times the assumption and to be valid . For more about the derivation and of this and other related to scalar quantization , see , e .., and the therein . 
 . 
 It is possible to further reduce the reconstruction , while keeping the quantization step constant , by increasing the sampling frequency above the frequency . This technique is . For ratio not too large , the mean square error is reduced as , i . e ., 
  
 where D is the when . Notice that this can also be seen as a particular case of the resilience of redundant frame in Appendix A . see also , e .., , . However , as the sampling frequency is , quantization noise becomes more and more correlated and the decrease rate of . Furthermore , asymptotically a lower , strictly positive limit . The bigger is , the higher this limit becomes . A quantization step also the decrease rate of to depart from sooner . 
 The reconstruction error can be further reduced , for a given ratio , by the use of feedback . Furthermore , feedback A yield a that steadily . Thus , one can obtain an arbitrarily low , for a given , by sampling fast enough . These are briefly in the next section . 
 AD WITH FEEDBACK 
 Quantization that use feedback can be grouped into two main : predictive and noise shaping . of the first type are the delta modulator and differential pulse code modulator see , e .., . The popular sigma delta converter , see , e .., , to the latter type . 
 The following is a basic description of the main of both converter , based mainly on the approach in . In the sequel , the quantization process is as additive noise , corresponding to the quantization error of a scalar . 
 . Predictive 
 The general form of a predictive is shown in Fig . . 
  
 In this diagram , U and correspond , respectively , to the of the sequence and the output sequence u in Fig . . Thus , the in the dashed line rectangle in Fig . is a particular realization of the generalized scalar in Fig . . The filter included at the end of the chain in Fig . can be considered as part of the reconstruction stage in Fig . . The E and in Fig . correspond to the of the discrete time in each of the respective . is the transform of the error by the scalar , i . e ., U E . From Fig . , the expression for the output U is found to be 
 U . 
 Thus , the output 
 D 
 The key to the noise reducing of the predictive on the prediction filter . This filter is designed to minimize the variance of the prediction error 
 E , 
 see Fig . . It is common to assume that the quantization noise is any of the in the loop . Thus , is chosen so as to reduce the contribution of 
 C to E in . By doing so , the variance energy per sample of the sequence that the is reduced . This in turn one to reduce the quantization step in the scalar , without increasing the number of quantization to avoid overload . Thus , by reducing a measure of the term in , one is also reducing the quantization noise contribution , and the is reduced accordingly . Of course , how much distortion reduction is will ultimately depend on how predictable the sequence is , i . e ., on the of . It will also depend on how well the prediction filter is able to capture this predictability 
 It been shown that the of the scheme in Fig . with the ratio not faster than , where is the order of the filter . If an additional ideal low pass filter with cut off frequency is after see Fig . , then the is reduced at most as A common choice of is of the form . 
 Note that the predictive in Fig . can reduce distortion even if are at frequency , as long as the input are correlated . If the input are uncorrelated white noise , then the predictive is unable to yield any reduction at all . It is the increase in the of the input produced by which for the behaviour in the reduction rate . 
 . Noise Shaping 
 The second main category of feedback to the noise shaping such as A , first by and in . One possible form to represent a noise shaping is in Fig . . Again , and U correspond , respectively , to the of and u in Fig . . The noise shaping within the dashed line rectangle in Fig . is a particular realization of the generalized scalar in Fig . . 
  
 Figure : A noise shaping . 
 From this figure , it is easy to see that the output U is given by 
 U 
 where the noise shaping filter a degree of freedom in the design process . Since is band limited , and because of , it is generally convenient to choose to be a high pass filter , see , e .., . With this choice , the quantization noise is within the signal band whilst outside of it see Fig . . This compensatory increase in the off band quantization noise is unavoidable , as determined by the Bode integral theorem Because of the frequency shaping of the quantization noise , most of its energy can be suppressed by low pass filtering U , leaving only the in band portion of the quantization noise . By doing so , it is in that the by increasing ratio at most as , where is the order of the noise shaping filter . Most common for have the form , where is an FIR filter . 
  
 Figure : Quantization Noise Shaping . 
 As in control , one of the beneficial of feedback in to the robustness of the resultant system . Indeed , if properly designed , feedback allow one to achieve high accuracy quantization despite the use of inaccurate building such as the scalar itself , which can be to have a very coarse and uncertain quantization step . This feedback the preferred choice for many practical . 
 It should also be noted that the above decay rate of the with increasing ratio is not fast enough to be rate distortion efficient . Indeed , AD require , in general , a higher data rate than a system with finer quantization and no to achieve the same distortion . This can be seen by that , for feedback , the only with increasing the ratio , as O n , while the with increasing the per sample i . e ., reducing as O B , i . e ., exponentially . Nevertheless , recent show that the L norm of the reconstruction error in can be reduced as O , , by for each ratio an appropriate noise shaping filter from an infinite set of . Following a different approach , quantization based on threshold exhibit a reconstruction that exponentially with increasing ratio , , and are thus rate distortion efficient . 
 MOVING HORIZON QUANTIZATION 
 Interestingly , control theory can be used to design the generalized scalar in Fig . . More precisely , since the output of the is constrained to belong to a finite alphabet of , the situation can be as a control problem with input . This point of view us to apply Moving Horizon Optimization MHO to achieve a more effective noise shaping . This paradigm Model Predictive Control , and proved to be a powerful tool for dealing with constrained , , , , , . The quantization scheme so , Step Optimal Converter , typically , while the latter as a particular case . We will present next some of the fundamental behind the . The remainder of this section been basically from , 
 . Noise Shaping Quantization as an Optimization Problem 
 A more general formulation to analyze the discrete time performance of noise shaping quantization can be derived from the block diagram in Fig . . 
 In Fig . , and u represent , respectively , the input and the output sequence . The motivation for quantization noise shaping been 
 Discrete Time Error Frequency Weighting Filter 
 Figure : Scheme to generate the frequency weighted quantization error sequence . 
 by a frequency weighted reconstruction error sequence , by 
 u ,, 
 compare to . 
 In , is a stable , causal , linear , time invariant filter , which can be via : 
 , 
 where A , , R the state dimension , i . e . the order of the filter . This filter can , e .., represent the typical low pass filter in conversion , see e .. , in order to decimate the converter output . In audio it sense to choose as a acoustic model of the human hearing , compare also with work in , . The performance of the quantization process in Fig . will be by the measure 
 V . 
 k Z 
 The distortion in the conversion process in a frequency . If the generalized scalar in Fig . is designed to minimize the , then its u will approximate the input , while the un quantization error , a u , will tend to have a spectrum similar to that of the inverse of the filter . Thus , the method will shape the quantization noise spectrum , just as the converter in Section does . 
 Unfortunately , minimization expression is not possible in practical , due to the complexity of the associated combinatorial optimization problem . Furthermore , in the general case , an optimal would need to view the entire signal . This is clearly unsuitable for on line . 
 . Step Optimal Converter 
 In order to obtain a more practical method to minimize the cost in , it is convenient develop a recursive conversion method , which can be on line . For that purpose , we will first introduce a cost measure over a finite horizon , to deploy later the concept of moving horizon approximation , see . 
 Finite Horizon Formulation A practical conversion scheme , suitable for , must operate sequentially , a restricted number of decision and considering a moderate number of future of . For this purpose , it is convenient to characterize as the output in a state space representation of 
 x Ax u 
  
 u . 
 This relation directly from . In , is the state vector . Note that , due to the structure of , at time the impact of the past u on future of is exactly by of the present state ,. 
 Given the above , we next replace the infinite horizon cost function by the finite horizon cost : 
 . 
 k 
 In , the prediction horizon a given positive semidefinite matrix . 
 With a given and known current state value see , 
 is a measure of the distortion over the prediction horizon plus a measure of the final state , x . These are formed based upon the model . 
 The finite horizon cost in into account only a finite constrained . The value computational complexity for the minimization of . This should be with the infinite number of decision in the original cost . a finite the . a design parameter , it can be chosen so that the minimization can be carried out on line . 
 Moving Horizon Approach As noted above , the to , a feasible output sequence for time . Thus , in principle , one could think of an implementation in , where the minimization is carried out . Unfortunately , the last few of depend only on a small window of the distortion To improve performance , the step optimal converter only the first element of , say u . It becomes the th element of the converter output sequence , by setting : 
 u 
 It is also to update the state according to , i . e .: 
 x . 
 At the next sampling instant , this new state value is used to minimize the cost , yielding u . This procedure is repeated ad . As in Fig . for the case , the prediction horizon of the criterion forward as . The past is forward in time via the state sequence , thus , yielding a recursive scheme . 
 The resultant architecture the . It an to digital converter architecture which the frequency weighted conversion distortion , based upon Model Predictive Control . 
 Horizon at 
 Figure : Moving horizon principle , . 
 Interestingly , it been shown that the with and to the converter , see . However , it is easy to see that , in general , better performance , since more data is taken into account in the decision process of from U to the in the sequence u . In fact , one can expect that , chosen large enough relative to the time scale of 
 , then the effect of u for be negligible and the the will approach that , if the infinite horizon measure of were to be directly which , for the above , is impractical . This asymptotic behaviour been experimentally confirmed , see . 
 In summary , the prediction designer to trade off performance versus on line computational effort . Interestingly enough , excellent performance can often be with relatively small see , e .., , thus rendering the scheme quite easy to implement in practical . 
 Another advantage of the when to the converter in that the can be designed to ensure stability like of the , see . 
 DATA QUANTIZATION 
 Given that digital signal have to interact with the real , physical world , the design of a quantization scheme should take into account the sampling continuous to discrete time and reconstruction discrete to continuous time between which it is to be inserted there only partial understanding of how sampling reconstruction interact with a given quantization method in of the resulting , overall reconstruction error . Furthermore , most literature the performance of in of how close the input are by the output and not by the , continuous time underlying signal entering the system against the , continuous time reconstructed signal that comes out of the reconstruction stage of the system . Accordingly , performance is most often measured by the norm of the sample approximation error , see . Similarly , traditional works on sampling and reconstruction theory build their analysis based first upon ideal later the effect of quantization as the corruption of ideal by white additive noise . Although it been shown that this model of quantization is indeed accurate for small quantization and input whose certain rather weak , it is certainly not accurate , for example , when quantization are large , or when feedback are . As in and , it is often the case that quantization noise is deliberately made non white by the so as to minimize a frequency weighted measure of the reconstruction error . 
 Within the setup in Fig . , we aim to present in this section some and additional insight related to the joint problem of designing that make use of the filtering , sampling , quantization and reconstruction paradigm . 
 . Decomposition of the Reconstruction Error 
 The model of the sampling and reconstruction process in Section to a somewhat trivial but nevertheless important result : it for a decomposition of the final reconstruction between the input a and the output a see Fig . of a sampling quantization reconstruction system into two . The first term to the error due to the mismatch , i . e ., the non coincidence of input and . The second comes deviation of the discrete time both linear and non linear from the optimal between input and output in the sampling and reconstruction , respectively . The following proposition this idea : 
 Proposition . Let L be the impulse response of the reconstruction filter , such that is frame , and the sampling interval . Then , the mean square reconstruction error between any input signal a L and an by the reconstruction stage can always be decomposed as 
 a a , where a is the orthogonal projection of a onto . 
 Proof . a . Then we can write 
 a 
 a a , 
 Since a a , and because , we have that a , see , and . 
  
 Corollary . From Proposition , it that for any a L , choice of quantization scheme and or discrete time , the continuous time reconstruction error is lower bounded by 
 a 
 We emphasize that the lower bound in to the minimum continuous time error attainable by any discrete time scheme , once the output space is given , even if and no quantization is applied to the . From Proposition , it is clear that the performance of discrete time e .., discrete time filtering and quantization should be in of the second term of the right hand side of , that is , the L norm of a a . In relation to the design of , this rise to the question of what information is by a generalized scalar to minimize a . We have this question in . A summary of the analysis and therein is below . 
 . 
 As noted above , the reduction of the continuous time by discrete time place by the second term on the right hand side of . For the general system under study see Fig . , the signal to be is actually a with L , the impulse response of : 
 , 
 as shown in Fig . . 
 as the impulse response of , the approximation of a by the system becomes 
  
 where is now as the impulse response of the filter , i . e .: 
 , 
 see , Fig . . The impulse response reconstruction frame , which the reconstruction space 
 W span Z 
 As in Section . , the generation of the optimal output a can be accomplished by the frame operator associated with to the sequence of , i . e . 
 a a , a L , 
 where is the analysis operator associated to , the canonical dual frame of see Definition in the Appendix . We will denote this optimal , un sequence of by 
 u . 
 It is clear from the above that any quantization algorithm that to minimize the continuous time error needs to be able , in the first place , to obtain the target sequence u in . From the in Section . , this that the first necessary condition for the feasibility of optimal quantization is that sampling and reconstruction be for orthogonal reconstruction . 
 If we now suppose that the access to u , then the problem of optimal quantization is that of choosing the optimal sequence u , defined as 
 u min 
 u k 
 The solution to one to solve a continuous time optimization problem with discrete time , decision . It is shown in that this can be converted into an equivalent discrete time optimization problem . More precisely , 
  
 The operator is by the Gram matrix see , sec . . of the reconstruction frame , which is defined element wise as 
 G , 
 This matrix one to re write in matrix notation as 
  
 where u and u are the vector of the u and u , respectively . 
  
 Figure : The sampling , quantization and reconstruction system from Fig . . Impulse and frame 
 The direct is that a can determine the optimal output sequence without full knowledge of the inter sample behaviour of the impulse of the reconstruction filter . Indeed , quantization performance can be measured by the weighted norm implicitly defined in . Note that the design of an optimal is not possible without knowledge of the matrix . are shown for each filter . 
 . Moving Horizon Conversion 
 In general , minimization of would require one to evaluate it for every sequence u , u U , that can be by the . This optimization , however , becomes intractable for sufficiently long . Given the similarity of and , one can use the in Section and optimize over a short moving horizon of . can be found in , , where a step optimal converter is . Preliminary show that , interestingly , significant distortion reduction is even when converting non band limited . Indeed , since the focus is on the reduction of the total continuous time reconstruction error , if the sampling rate is lower than the rate , the resultant converter will attempt to reduce not only quantization noise , but also noise . Furthermore , as the horizon is made , the output of the converter the optimal feasible output sequence , defined in . 
 CONTROL 
 In previous of this work we have that the power of feedback can be used in the design of . In particular , we have shown in and that careful deployment of of Model Predictive Control may lead to high performance conversion . The purpose of the present section is to highlight the role by sampling and especially quantization in feedback control . Efficiency in data a central role in any control system where parsimony need to be taken into account . Thus , quantization and sampling are worth investigating , for example , in the following : 
 • when need to be over a digital network , i . e ., in Control , , ; 
 • when plant need to be e .., relay feedback , on off control , digital control , or also due to the presence of a human operator ; 
 • in large scale , such as those related to mining and supply chain management . 
 In the following , we will briefly describe how surrounding sampling and quantization translate into the design of these of control . 
 Sampling and Reconstruction In the design of a sampling reconstruction scheme for a control system , traditional reconstruction criteria should be with more appropriate . Indeed , reconstruction quality is only of secondary importance . The main objective is measured at the plant output . In particular , as shown in , for , open loop performance should be by closed loop . This can be through consideration of frequency weighted such as . 
 Quantization Interestingly , the noise shaping in can also be applied to control where are ; see , e .., . For example , when on the design of for with , a key point in realizing that the AD conversion scheme of Fig . is related to a control system with plant : The plant input u is to be chosen such that the plant output Ha the reference signal Ha . Thus , performance can be measured via the frequency weighted error signal eH , see and also . 
 on how to apply of Moving Horizon to can be found , for example , in . It is interesting to note that the framework can also be to incorporate dynamic into . The resultant methodology can be as sampling and quantization on demand and is , thus , highly efficient from a data representation perspective , see . 
  
 This paper basic and related to the process of sampling , quantization and reconstruction of scalar . With the introduction of a frame theoretic viewpoint , three of sampling and reconstruction have been . We have several generalized scalar quantization , and have how control theory to signal theory . Furthermore , we have given into the joint problem of sampling , quantization and reconstruction , and have outlined how these interact . Finally , we have the role by sampling and quantization in control . 
 A APPENDIX 
 A . Background on , Bases and 
 Definition Space . a vector space with an inner the induced norm . If such a space is complete under its norm then it is a Space . 
 Definition Basis . A sequence of ,, in a a a basis and only if span and there exist two such that 
 W 
 This and other equivalent can be found in , Theorem . . . 
 Remark . From Definition one can observe that : 
 • The in are orthogonal if and only if and only if . 
 • The lower bound in is equivalent to saying that is a set of linearly independent . 
 • The higher bound in that will be bounded for any choice of . 
 A . . Orthogonal Projection 
 a space , then the best approximation in in the sense of the norm of given by the orthogonal projection , by , and defined as the operator 
 :; min W 
 The orthogonal projection from a the null space of : 
 W :. 
 It is easy to verify that 
 . 
 If , is an basis of , then the orthogonal projection operator can be explicitly written as 
 h 
 Orthogonal projection elegant to some otherwise complex optimization in functional analysis . This and a natural framework for the problem of efficient sampling and quantization . 
 A . . Isomorphism 
 A fundamental property of and is that they are able to define a precise form of equivalence between two different . It is isomorphism : two different are isomorphic if they have the same dimension . An isomorphism is indeed any linear invertible operator from one space onto the other . Of particular interest for our analysis are the between any separable space L function space of dimension , where , and space . Such an isomorphism can be stated by considering any basis of , namely , and the associated analysis operator 
  
 The analysis operator defined in is an unitary isomorphism . This that the respective in through of any group of their respective and relative , i . e . 
 W 
 This remarkable property of isomorphic one to study the relation between of a space by looking at their through in another , more convenient space . Actually , one can argue that all digital signal digital control is made possible the existence of signal and of . 
 A . Illustrative Example 
 Some of the basic of of and bases so far will be by the following simple example . 
 the space of all real valued satisfying the following : 
 • is continuous . 
 . 
 i . e ., is square integrable 
 over I . 
 • The of are constant over any of the open , , , , . 
 Fig . a three , s , s , s that belong to this space . 
  
 Figure : Example of a functional space , an base and a unitary analysis operator . a w , w and w belong to the space ; The , constitute an for ; Image of the w , w and w in R through the analysis operator . 
 With the addition of the standard L inner product , defined as 
 W 
 W becomes a space . The inner product also a norm in , given by 
  
 It is easy to show a two dimensional space . This can be intuitively by that any completely determined by exactly two , such as , for example , the of the t . A basis for a space of dimension two two . Figure . a pair of , form an basis for . 
 Figure . the of w , w , w , and through the analysis operator see in R . As , the of the and are in R . How close is w to w in their norm . Since the analysis operator is a unitary isomorphism R , we have , from 
  
  
 i . e ., is given by the distance between w and w . 
 Consider now the case of a function ,, that to a space L , such that Wand . 
 An example of such a function is shown in Fig . . a . 
  
 Figure : a onto . a Function to . Relative between , , an isomorphic space . 
 c Orthogonal projection of function representation . 
 The and relative respect to an orthogonal basis as are shown in the dimensional representation of Fig . .. Here it can be seen a non zero orthogonal projection onto . This orthogonal projection is the vector , in accordance with , and is given by . Consequently , the best approximation in an L sense , expressed as a function of time 
  
 Figure . a plot of . 
 A . 
 Despite the computational convenience of bases , one often needs to study by a set of linearly dependent over complete basis . The concept of , by and , one to analyze such . with over complete bases arise in practice not only by chance . It been shown that the redundancy of is beneficial , for it can reduce the effect of in the expansion , see . The formal definition of a frame is given next . 
 Definition Frame . A sequence of in a a frame there exist A , such that 
 A 
 The number A and satisfy are frame . Some important about are as : is a frame for a space , then k . 
 • A frame is said to be tight if one can choose A as frame . If A , it is a frame . 
 • If a frame to be a frame when an arbitrary element is removed , it is an exact frame . An exact frame is equivalent to a basis . 
 • A frame in which for a frame . 
 • If the of a frame are linearly independent then A see . 
 • A frame with linearly dependent is said to be redundant . 
 • If A for a frame then the frame is redundant . 
 • The upper frame bound of a frame is greater than min . 
 The redundancy of a frame with for a defined as the ratio 
 r 
  
 It is easy to show that , for a tight frame , A , where A is the lower frame bound in . Another important property of the of a frame is that they are also a sequence , i . e ., they 
 satisfy 
 c 
 W 
 the upper frame bound in . 
 From remark and the above , orthogonal bases are a special type of basis , whilst bases are exact . Thus , by our analysis on , one is also orthogonal and bases as special . 
 A . and their 
 a space , and span . 
 Definition Synthesis Operator . The synthesis or operator for a frame is defined as 
 . 
 Since every frame sequence is a sequence see 
 , the synthesis operator for a frame with frame A , is bounded , with operator norm , i . e ., is the minimum constant such that . 
 Definition Analysis Operator . The analysis operator for a frame is defined as 
  
 Remark . The analysis operator is the adjoint of , i . e ., it . 
 Definition Dual Frame . Let be a frame for a space . Another frame for , namely , that 
 w 
 k K 
 is said to be a dual frame of in . 
 As can be seen in , a dual frame an explicit method for any of , from be exactly through the synthesis frame . 
 Definition Frame Operator . The frame operator of a frame is defined as 
 S : , Sh 
 k K 
 Lemma from , Lemma . . . Let be a frame with frame frame A ,. Then the following : 
 i is bounded , invertible , self adjoint , and positive . 
 is a frame with , A . The 
 frame operator for is 
 Since , one can derive from Lemma , 
 and that : 
 A 
 B 
 The frame operator defined in is of particular importance for the problem of sampling and reconstruction , since it an explicit way to obtain a dual frame see . More precisely , defined in , if is a frame for , then the frame is a dual frame for in , i . e . 
 w 
 k K 
 and 
 w 
 k K 
 The frame is the canonical dual frame of in . This is a reciprocal relation , i . e ., 
 is the canonical dual of well . 
 A . Noise Reduction by Redundancy of the Frame 
 If the frame in were by additive noise e ,, then the reconstruction formula would yield a reconstruction error 
 we 
 k K 
 Early the fact that the the frame the reconstruction error were provided in , whilst can be found in and . Due to the importance of this property of redundant , we present next an adaptation of the result in , which is also illustrative of the importance of the frame . 
 Proposition . Let be a frame of unit norm with frame A and let e , be a sequence of independent random with mean zero and variance s . Then the mean square value of we in 
  
 B 
 Proof . If e , is a sequence of independent random with mean zero and variance , we have 
 E 
  
 From one can derive that 
 B 
 which to 
 B A 
 because is a frame . Combining with the result . 
 Corollary . If the frame in Proposition is also tight , then 
 s 
 E 
 r 
  
 ﻿We present empirical results on the achievable cellular system throughput gains stemming from the use of remote radio heads (RRH) in an urban environment. Our work is based on simultaneous path-loss measurements of the base station (BS) and RRH links to outdoor street-level users. We calculated the increase in received power, when a RRH is added to improve the coverage achieved by a BS. We consider diverse settings for the RRH and diverse expected coverage areas for the mobile station (MS), evaluating the effect of RRH height and position with respect to the intended users. We also compare the power gains that would be obtained in practice from combining schemes such as selection combining (SC) and maximum ratio combining (MRC). 
 We conclude that under practical conditions, the benefits of using RRHs will depend very strongly on the existence of line-ofsight links between the RRH and the intended users. For RRHs placed at low heights, below the clutter, only users in a “streetcanyon” position with respect to the RRH will obtain a significant benefit. Our data also shows that the gains in signal-to-noise ratio achieved when using MRC are only marginally better than those of the much simpler SC. 
 Index Terms—Channel modeling, maximum ratio combining, remote radio heads, selection combining, small cells, wireless repeaters. 
 NE of the great challenges for today’s wireless communication networks is to provide adequate spatial coverage in a cost-effective way, while achieving bandwidth efficiency and interference levels adequate for high frequency re-use [1]. To meet this challenge there has been a growing interest in the study of femtocells, relay stations (RSs) and low-complexity repeaters [2], [3]. These may constitute relatively simple, lowcost and easy-to-install alternatives when compared to the deployment of an additional base station (BS) to serve mobile stations (MSs) within the cell. 
 While femtocells are conceived as a small–coverage BS with low transmit power connected to a wired network, a RS is 
 connected to a BS through a wireless link, being able to repeat or re-code the data by using amplify-and-forward or decodeand-forward algorithms, among others [4]. A repeater may be thought of as an amplify-and-forward relay with no decoding or scheduling ability [3]. In this context, there are two basic types of repeaters: wireless-repeaters (WRs), and remote radio heads (RRHs), also known as fiber repeaters, connected to a BS by an optical fiber [3]. WRs will suffer large and small scale fading at both BS-RS/WR and RS/WR-MS links. In contrast, for RRH assisted wireless networks, achievable gains will only depend on the quality of the RRH-MS connection, which makes it an attractive solution to provide connectivity to wireless users in dense urban areas, where the links to the BS may experience significant shadowing. 
 Proper placement of a RRH is obviously a fundamental factor in defining the compromise between the desired gains and the cost associated with its installation. In this regard, in an urban environment it is reasonable to expect that while coverage will improve with antenna height, this will at the same time have a negative impact on deployment costs and on co-channel interference, thus affecting frequency reuse in a large system [5], [6]. If the RRH is to cover an area of a size comparable to a small cell, then it is reasonable to assume that this is best achieved by placing its antenna in positions normally used by BSs. In such cases well-established propagation models such as those of Hata [7] and Erceg et al. [8] will be adequate to predict the coverage achieved by the RRH. Alternatively, they may be positioned at lower heights with the aim of achieving improvements in more limited regions, i.e., generating microcells within a large cell. Coverage in microcells has generated growing interest as it can provide local increases in signal-to-noise ratio (SNR) and thus high data-rates [2], [9]. This may be achieved without generating excessive interference in neighboring cells and at lower implementation costs when compared to a BS [2], [10], [11], particularly when placed below clutter height. Numerous empirical models have been proposed for the statistical characterization of path-loss in relatively low-height (3-20 m) radio links [12]–[17]. Analytical models based on optical geometry have also been proposed for microcells in urban environments [18], [19]. Goldsmith et al. [20] proposed, based on a collection of measurements, a mathematical description of the radius of coverage of a microcell for an urban environment with and without line-of-sight (LOS). Contours of local mean attenuation are obtained for two neighborhoods, and it is found that these contours have the shape of “convex diamonds” [20]. Haneda et al. [21] analyzed relays in an indoor-outdoor environment operating under Amplify-and-Forward (AF) and Decode-and-Forward (DF) schemes. In [3], the authors studied different uses of repeaters and RRHs in cellular networks, concluding that careful placement of repeaters can improve capacity substantially, transferring traffic from heavily loaded to lightly loaded sectors, thereby improving network capacity. They also conclude, based on their simulations, that it is best to place repeaters on a horseshoe layout with a sum transmission power of all repeaters comparable to the transmission power of a single macro-base transceiver station. 
 The IEEE 802.16 Relay Task Group [22] and the WINNER Consortium [23] have suggested the use of both empirical and theoretical models to predict path-losses for wireless links with transmission units below the clutter of scatterers, also applicable to RRH-MS links. While the accuracy of pathloss models for diverse environments has been the subject of extensive studies, their use in RRH-assisted wireless networks requires more than treating each link individually, since the joint statistics of BS-MS and RRH-MS links may not correspond to those of independent random variables. 
 In this work, we report an empirical study that includes the joint statistics for the path-losses of the links involved in a cellular system assisted by RRHs. Specifically, our results are based on simultaneous path-loss measurements of the BS-MS and RRH-MS links in an outdoor urban environment. Based on our measurements we evaluate diverse aspects related to the performance improvement that can be expected when adding a RRH in an area served by a BS. We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user. Our results will be useful to calculate the achievable gains in transmission rates experienced by mobile terminals. 
 Our empirical study firstly considered the influence of the RRH height. Toward that end, we started by placing the RRH in a tall-building rooftop position, typical for an urban radio cell, and then lowering it stepwise to a height of 5 m. In this study, we considered street-level user positions spread over a disk with a radius of 1000 m centered at the RRH. Path-loss data for all links involved was collected at each RRH height. The results allowed us to quantify the tradeoff between RRH height and the achievable gains in coverage and received power. For our test conditions, it was found that -on average- an increase in height of the RRH entails an increase in received power of about 1 dB for every 10 m. Overall, the obtained results suggest that the effectiveness of a RRH assisted system is quite limited if a large area is to be covered, unless the RRH is positioned at such a height and operates at such powers that in practice it becomes another urban BS. This conclusion led to the second part of our study, where we restricted measurements to a disk with a radius of only 200 m centered at the RRH. In addition, we placed the RRH at lamp-post heights, considering that this type of setting will be typical for a practical deployment of low-cost repeaters. In this part of the study we also considered two types of RRH placements. One was chosen so as to maximize the likelihood of users having LOS to the RRH and the other chosen in close proximity to the first, but obstructed from direct street view by construction. In this way we were able to evaluate the effect of elements blocking a RRH, as may occur when a surrounding construction is modified after the placement of the fiber repeater. Our results are only directly applicable to narrow-band transmission, for example a single carrier of an OFDM transmission, as our measurements were done with unmodulated carriers. We also report results that involve spatial averaging, i.e., the elimination of small-scale fades. This is equivalent to having spatial diversity, which often yields similar results to wideband frequency diversity [24]. 
 From the joint data of path-loss for the BS-MS and RRHMS links we were able to compare the effectiveness of Selection Combining (SC) and Maximal Ratio Combining (MRC) [25] at the mobile terminal. To this effect we considered typical transmit powers and antenna gains at the BS and RRH and calculated received signal power at the MS under the condition of equal noise power for both schemes. It was found that the statistical gain of MRC over SC is less than 1 dB under all conditions tested, a consequence of the random power imbalance among the links. 
 Our measurements also allowed us to calculate correlations between fades for the BS-MS and RRH-MS links for all RRH heights. The low correlations observed validate the approach based on considering them as independent random variables. 
 The rest of this paper is organized as follows: In Section II we present a detailed description of the measurement scenario and procedures. The results related to the effect of RRH height on the achievable gains are discussed in Section III. In Section IV, we present the empirical results obtained when positioning a RRH at lamp-post height. Conclusions are presented in Section V. 
 The measurement campaign extended over a period of 4 months during summertime. The urban area used as testbed in Viña del Mar, Chile, contains a mix of high-rise buildings and two-story houses with heights ranging from 12 to 62 m, built on a plane region at sea level. Measurements of received power at MS positions were carried out at streetlevel. The area is overlooked by nearby hills that allowed positioning a transmitter at a location typical for a BS covering a relatively large urban area. The BS height was 180 m above the measurement region at a distance of 2 km to the RRH position. The RRH was mounted at various heights on the external wall of a 62 m high building as will be described later. The streets in the measurement area are lined with trees with heights ranging from 6 to 8 m. All BS-MS links were nonline-of-sight (NLOS), with blockages due to the surrounding construction. A schematic description of the scenario and terrain profile is presented in Figs. 1 and 2. 
 The second scenario was based on the same urban locations for both base and RRH but with all users within a disk with a radius of 200 m centered at the position of a RRH. The RRH was this time placed at 3 m above street level. The disposition of the user locations is shown in Fig. 3. 
 Fig. 1. Placement of BS, RRH and MS in urban environment for the first part of this study. (a) Tested positions, (b) 3D-view. 
 A block diagram of the measurement system is shown in Fig. 4. At the BS position, a continuous wave (CW) source based on a synthesized oscillator transmits a 3.5 GHz unmodulated carrier with 19 dBm output power through a 17 dBi sector-type antenna, corresponding to an EIRP of 36 dBm. The antenna used had 60? azimuth and 8? elevation 
 Fig. 3. Placement of RRH and MS units in urban environment for the second part of this study. 
 beamwidths at -3 dB points. All MS locations were within this beamwidth. At the RRH, the antenna used was a vertically polarized dipole with 2 dBi gain, transmitting a CW signal at 3.57 GHz with 19 dBm output power, corresponding to an EIRP of 21 dBm. The choice of frequencies was based on the fact that they fall inside a band used for Metropolitan Area Networks such as WiMAX and that considerable previous work in this band has been reported [26], providing a reference framework on propagation in urban environments. The RRH was configured to contain a CW transmitter and a narrowband receiver tuned to the BS frequency. This allowed us to remotely detect any possible anomaly at the BS, which was located at a considerable distance from our measurement scenario. The 70 MHz separation between BS and RRH frequencies provided enough isolation to allow simultaneous transmission and reception by the RRH. 
 Power measurements at the MS positions were carried out using an Agilent model N1996A-506 spectrum analyzer that simultaneously tracked the carriers at 3.5 and 3.57 GHz. The antenna used was a 2 dBi gain vertically polarized dipole placed at a 1.8 m height. The dynamic range of the measurement setup is limited from below by the system noise-floor. Using only the spectrum analyzer as receiver, this proved to be inadequate for several of the envisioned NLOS user locations and would have biased our results if we had only selected the MS placements where measurements were feasible. We thus included a 25 dB gain pre-amplifier at the mobile-antenna terminal to achieve a system noise figure of 9 dB and thus a noise floor of -125 dBm in a 10 kHz bandwidth. The resulting link budget allowed us to measure path-losses of up to 145 dB in the RRH-MS link and 160 dB in the BS-MS link. At all selected locations the average received power exceeded the noise floor by at least 10 dB. The MS measurement system included a laptop computer that acquired the spectrum analyzer scans at a rate of 2 per second. The received power from the BS and RRH transmissions allowed us to calculate the corresponding path-losses for both links. 
 The two measurement campaigns involved moving the MS along a straight path of about 4.5 m at each of the diverse locations in the region tested. This was done using a very slow-moving vehicle on which our measurement equipment was installed. We traversed each 4.5 m sector at a constant speed in such a way that approximately 300 equally-spaced power samples were collected. At the specified frequency, this implies that more than 100 of the samples are at positions spaced at least half a wavelength apart. From these measurements we obtained the average path-loss at the location and the small-scale fade statistics with respect to this average. 
 The first measurement campaign was aimed at determining the effect of RRH height. This involved placing the RRH at 20 different heights, ranging from 5 to 62 m above street-level. 29 measurement locations for the MS were selected at various distances from the RRH: 9 within 300 m, 10 between 300 and 600 m and 10 between 600 and 1000 m. These are marked with diamonds in Fig. 1. Measurements for RRH heights in the range from 5 to 14 m did in some locations result in RRHMS path-losses exceeding the reliable measurement range for received MS power. Thus, to avoid biasing our results, the statistics presented for this part of our study exclude data obtained for RRH heights below 14 m. 
 The second set of measurements, aimed at evaluating the practical performance gains for low-RRH, was based on 30 MS locations, but all within 200 m of the RRH, as illustrated in Fig. 3. Of these, 8 were within 60 m, 8 between 60 and 100 m, and 14 between 100 and 200 m of the RRH. The RRH at a height of 3 m above street level, was placed at two different positions: one at an intersection so as to simultaneously illuminate two streets and the other one within 20 m of the first placement, but with no direct path to the adjacent streets. In the first case there are LOS and NLOS user locations, while in the second all MS positions are NLOS. 
 In this section we quantify the effect of lowering the RRH antenna from a position where in practice it can be considered another BS, to heights close to street level. To characterize our scenario, we first contrast the measured RRH-MS path-losses with the corresponding values predicted by wellestablished propagation models. In [22] various models for path-loss prediction are proposed, some of which correspond to scenarios similar to the one used here as a test-bed. These models do not explicitly consider the height variation of the BS or relay, but are based on the relative position of these with respect to the user, i.e., LOS/NLOS, above or below clutter. Among these models, we chose the one that best fits our conditions. Specifically we considered the alternative WINNER model specified for urban Type-E NLOS links, with a BS above rooftop height and mobile users below rooftops. The predictions by this model were compared to the RRH-MS path-loss link-measurements, considering the various RRH heights. The distances for these links range to 1000 m. The comparison is shown in Fig. 5 where measurements at all locations and for all RRH heights are included. For this comparison we averaged out small-scale fades, thus generating one path-loss value for each combination of RRH height and MS position, which is to be compared to the model prediction. The separation of the empirical data into LOS and NLOS was based on whether or not the line-of-sight path was blocked by a large structure such as a building. We considered that a tree in the direct path does not alter a LOS condition. To avoid cluttering the figure we combined the measurements for the various RRH heights into three groups: 14 m to 29 m, 30 m to 45 m and 46 m to 62 m. The corresponding pathloss predicted by the model is plotted considering a BS at a height in the center of the range of heights for each group. We note that the model is little affected by this height. We also show for reference the free-space path-loss. As can be seen, the NLOS path-loss measurements are within range of the model predictions, although we observed some path-loss reduction with RRH height, not accounted for by the model. The same figure also shows the results of the LOS path-loss measurements considering all heights, which as expected fall well below the NLOS model prediction. 
 We further compared the statistics of our empirical path-losses for each RRH height to those of the classical log-distance model with log-normal shadow fading and Rice/Rayleigh small-scale fades. As is common practice [25], we chose the model to adjust to the free-space path-loss at the reference distance of 1 m. We found that this model provided a very good fit, with a path-loss exponent ranging from 2.3 to 2.5 when considering LOS locations. When considering only the NLOS locations, the path-loss exponent varied between 2.6 and 3.5. The lowest values were obtained at the highest RRH position, with a clearly observable increase as the RRH was lowered towards street level. The standard deviation with respect to the regression line was in the range of 4 to 8 dB, with little difference between LOS and NLOS links. For all of the NLOS locations, the statistics of the small-scale fades fit the Rayleigh distribution, within the accuracy limits that result from a finite data set. For the LOS links the Rice distribution 
 Fig. 5. Empirical RRH-MS path loss compared to WINNER model according to IEEE 802.16j recommendation. 
 provided a similarly good fit, with K-factors that were quite small, no larger than 12 dB. 
 The above results indicate that our choice of measurement scenario reflects the propagation behavior of an urban environment, characterized as described in the literature, by shadowing and by strong multipath propagation. 
 To evaluate the achievable received power gain as a function of the RRH height, we calculated the received power at the MS locations, assuming realistic conditions for transmit power and antenna gains. We note that once the path-losses for all links have been obtained, we are free in subsequent calculations to assume any transmit power for the BS and the RRH, not necessarily those used before in the channel sounding process. In the calculation of received power at the MS, we thus used the actual measured path-losses for the links, and transmit-power levels that we deemed reasonable for an actual deployment. From the collection of calculated power values at the MS, we generated the cumulative distribution function (CDF) of the received power. For this we considered transmission by BS alone, by RRH alone, and transmission by both BS and RRH with SC or MRC at the MS. We considered a BS with 45 dBm of transmission power, typical for a macrocell [27]. The BS and MS antennas are assumed to be the same as the ones used in our measurements (17 dBi and 2 dBi, respectively). For the RRH we assumed that in most practical cases the RRH would be attached to a wall and consequently only illuminate a sector of 180? in azimuth. All our MS locations are in fact within a sector of such angular width. We thus considered that a typical 7 dBi, 180? sector antenna would be used, consistent with calculations reported in [28]. For the transmit power at the RRH we used two values that cover what could be considered the high and low extremes of a practical deployment: 40 and 20 dBm. The lower value is representative for femtocells, while the high value is in the power range mentioned for relays and RRHs [27]. 
 Figure 6 shows the CDFs of the received power assuming that the RRH transmits at 40 dBm. The CDFs consider all power measurements at all MS locations. We obtained CDFs 
 Fig. 6. CDF of received power at MS for 1,000 m coverage radius and 40 dBm RRH power. 
 for all RRH heights between 14 and 62 m but we here only show the ones corresponding to 62 m. The CDFs for 14 m essentially match the CDF of the power received from the BS alone, with a shift to the right of less than 1 dB at all probability levels. We do not include them in the figure to avoid cluttering the graph. All other CDFs fall between these two extremes and are also omitted for the sake of clarity. The improvement in coverage over that achieved by the BS alone was only found to be significant when the RRH occupies a high position. Not shown here is the case where the RRH operates at 20 dBm, which results in virtually no gain at any RRH position. 
 Fig. 6 shows that the received power gain of MRC over SC is almost insignificant. This is due to the fact that the likelihood of similar received power from BS and RRH is quite small, and as well known, even for equal power the gain is only 3 dB. We repeated these calculations for diverse ratios of BS to RRH power with the result that the improvement in received power of MRC over SC under all conditions was no greater than 1 dB. As mentioned before, this assumes equal receiver noise in both cases, thus the power gain is equivalent to SNR gain. Based on the above results, we describe below the benefits that were observed for the RRH-assisted link in this setting. All our subsequent results are based on the assumption of a SC at the user terminal, but for all practical purposes the results would be no different using a MRC. 
 From Fig. 6 it is seen that, under the stated conditions, 50% of users will receive at least -61 dBm when only the BS is operating. Adding a RRH at the maximum height, transmitting at 40 dBm improves this coverage to 70%. At the other extreme, a RRH at 14 m will provide virtually no statistically significant improvement in coverage, as the corresponding CDF of received power was found to exhibit a right-shift of less than 1 dB. Obviously lower heights will further reduce any existing RRH benefits. We note that these rather limited gains were achieved considering a relatively 
 Fig. 7. Power gain of SC over BS alone ?, for diverse availabilities as function of RRH height with RRH transmission power of 40 dBm. 
 high RRH transmit power, only 5 dB less than that of the base, which does however have a 15 dB EIRP advantage over the RRH, due to the 10 dB difference in the corresponding antenna gains. 
 To quantify the gains we define as ?(y) the dB difference in received power that can be guaranteed to a proportion of users y. We refer to this proportion as the availability level associated with that power gain. We denote the CDFs of received power P [dBm] from the BS alone and from SC of BS and RRH transmissions as FBS (P) and FSC (P) respectively. The gain in received power using SC is then: 
 The same definition of course applies to the gain obtained through MRC. We now use (1) to calculate, at various availability levels y, the power gain resulting from the use of a RRH. This is shown in Fig. 7 for the full range of RRH placements at availabilities of 10%, 50% and 90%, considering 40 dBm of transmit power at the RRH. To construct this figure we considered the empirical CDFs as illustrated in Fig. 6, but calculated for each RRH height. In Fig. 7 we also indicate as a reference the results that would have been obtained if instead of using the measured CDFs for path-losses, these had been calculated using at each location a fixed path-loss equal to the average value predicted by the models discussed before. Specifically, for the RRH-MS link we used the alternative WINNER model for type E in [22] while at the longerrange BS-MS links we used the Hata model, extrapolated to a frequency of 3.5 GHz. Since height dependence is not part of the model in [22], the resulting gain, which we found to be 2.7 dB, is only a function of the MS placements in the region. In contrast, the empirical gains show considerable height dependence. 
 From the data that Fig. 7 is based on, we calculated by linear regression the increase in received power (?) vs. height of the RRH. The result was that under the stated conditions, 
 Fig. 8. LOS and NLOS power gains of SC over BS alone ?, for diverse availabilities as function of RRH height with RRH transmission power of 40 dBm. 
 the power gain is approximately 1 dB for every 10 m in height increase, at availabilities of 90% and 50%. If the RRH power is reduced to 20 dBm this gain becomes only 0.2 dB for every 10 m. We note that these values represent the range of height-related gains for our particular choice of MS placements and thus cannot be easily generalized. From a qualitative perspective however, the results may suggest that the gain with height is mostly due to the fact that at the higher RRH positions, the number of MS locations that exhibit a LOS link to the RRH is larger. To investigate this aspect further, we segmented the results, separating the MS locations into those with, and those without LOS, for each RRH height. The resulting power gain vs. RRH height is shown in Fig. 8. In this figure we also show the percentage of LOS zones. As seen, for the NLOS zones there is little gain improvement as the RRH height increases. On the other hand, for the LOS zones, there is more evidence of gain increase with height. The main conclusion that may be derived from this figure is that the LOS locations benefit significantly more from increasing the height of the RRH than those that are NLOS. We also note some gain increases with height, even when the percentage of LOS location reaches a saturation level. Thus, the growth in the percentage of RRH-MS links with LOS does not by itself account for the increased gains with RRH height. 
 Finally, to evaluate the degree of dependence of the fades [28] in the BS-MS and RRH-MS links we calculate the crosscorrelation coefficient ? between these fades for each RRH height as 
 where PL(BS) and PL(RRH) represent respectively the path-losses, including the small-scale fades, from the BS and the RRH to a specific MS position. E[PL(BS)] and E[PL(RRH)] are the statistical averages of these path losses at the distance under consideration, obtained from a linear regression of path loss vs. distance. sBS and sRRH are the standard deviations of the fades with respect to these averages. 
 This calculation was repeated for each RRH height. The values obtained were very small. Only at the lowest height (14 m) did ? approach +0.1, while at all others it was less than 0.05 in modulus. We also calculated the correlations of the shadow fades, i.e. the fades after averaging out the small-scale fades. This would be more representative for links with space or frequency diversity, which tends to suppress small-scale fades. In this case ? was somewhat larger, but still not significant, reaching +0.5 at the lowest height and fluctuating between positive and negative values less than 0.3 in modulus at heights above 17 m. 
 The results of the previous section suggest that even with a RRH position at a height that is comparable to that of an urban cell BS, only modest gains can be achieved at a large coverage radius. This happens even at a relatively high transmit power. In this section we therefore explore more practical configurations, limited to much smaller coverage regions and a lower-height RRH (3 m above street level), as we have described in Section II. In this setup we also explored the effect of choosing a RRH position with an uncluttered path to two intersecting streets, in contrast with placing it in a nearby position obstructed by construction. The first placement is denoted here as “Non-Obstructed RRH” (N-RRH) and the second as “Obstructed RRH” (O-RRH). The N-RRH position provides LOS links to some of the chosen MS locations. For a setting where both antennas are below clutter, LOS links are described as being in a “street-canyon” [29]. Correspondingly, we refer to the NLOS links as “non-street-canyon”. In the 200 m radius, half (15) of the N-RRH links are of the streetcanyon type while all O-RRH links are non-street-canyon. Within the 100 m radius all links to the N-RRH become of street-canyon type. This classification (according to the RRHMS link characteristics) is aimed, as before, at determining which users can be expected in practice to benefit most from a given RRH placement. We again first verified that the chosen MS locations are such that the path-loss statistics for the environment correspond to well-established propagation models. 
 For the NLOS links, we found a good match to the COSTHata model [25] and to the alternative WINNER path loss model for the type F-NLOS case suggested in [22], using an operating frequency of 3.57 GHz. With respect to these models, our measured losses were slightly higher, 2 and 4 dB on average, with root mean square (r.m.s.) errors of 6 and 10 dB respectively. The measured losses for the street-canyon links were far lower than those for NLOS links at the same distance, from 4 to 34 dB, with 18 dB on average. They were however higher by about 8 dB on average than those predicted by the advanced LOS model for Type-F LOS scenarios (with both RS and MS below rooftops) [22]. The path-loss graphs illustrating these comparisons are shown in Fig. 9. As before, we also verified that our data fit the statistics of the singleslope log-distance model, with log-normal shadow fading and Rice/Rayleigh small-scale fades. The resulting slopes were 2.6 and 3.8 for LOS and NLOS links respectively. We note that, as mentioned before, most of our LOS links were at least partially obstructed by relatively large trees. 
 Fig. 9. Empirical RRH-MS path-loss measurements compared to reference models for low height RRH positions. 
 Fig. 10. CDF of received power at MS for 200 m coverage radius and 20 dBm RRH power. RRH height was 3 m. 
 Fig. 10 presents the CDFs of the received power at the MS as previously discussed for Fig. 6 but within the 200 m radius. We illustrate as an example, only the case where the RRH transmits at 20 dBm for both RRH placements. All other parameters are as described before. As seen, the achievable gains very much depend on the specific placement of the RRH. For example, at 50% coverage, the BS will guarantee a power of at least -63 dBm. Using an O-RRH will guarantee the same power to 56% of users, while an N-RRH will increase this coverage to 76%. When repeating this calculation under the assumption of a RRH transmit power of 40 dBm, the coverage increases to 77% and 84% respectively. We summarize the RRH gains for various conditions in what follows. 
 To evaluate the effect of coverage range we divided the locations where the power measurements were carried out into three groups with disk radii of 200 m, 100 m and 60 m, referred to the RRH. Table I presents the power gain ? (as 
 defined in (1)), obtained with a SC at the availability levels of 90%, 50% and 10%, for two RRH transmit powers and for the two placements described. The results using MRC are almost identical. We recall that for a radius of 100 m or less the N-RRH placement provides street-canyon-type links to all users, while only 50% of the users are in that condition for the 200 m radius. The tabulated results provide some interesting insight into the benefits that can be expected from the use of a RRH. We first note that for 50% and 90% availability, there are consistent increases in power gain as the coverage radius decreases, particularly when all user locations become of street-canyon type with respect to the N-RRH. This increase is especially sharp at 90% availability. The values at 10% availability, while not as relevant in practice, serve to illustrate the gains available to a minority of users that have a particularly low path-loss to the RRH, for example due to an unobstructed path at a short range. We in fact see that the gains available to 10% of users at a 200 m radius are comparable to those observed for 90% within the shorter ranges. The two smaller radii include only street-canyon type links for the N-RRH case, however the variation in distance from the various user positions to the RRH is obviously larger for the 100 m than for the 60 m case. At high availability this implies significantly increased power gains for the smaller radius, as it will exclude most user locations with higher path-losses to the RRH. Not as obvious is the fact that when considering 10% availability, the power gains can actually decrease with the radius, as seen in the table. This is due to the fact that the smaller radius may also exclude some user positions that exhibit particularly low path-losses to the RRH. It is precisely these few well-placed users (with respect to the RRH) that dominate the achievable gains for low availability. In the same context, we observe that for the N-RRH, a 20 dB increase in transmit power results in almost exactly the same increase in received power to the 10% best-placed users. This again indicates that for such users, the received power is completely dominated by that of the RRH. 
 Both Fig. 10 and Table I illustrate the importance of proper positioning of the RRH. This can also be associated with the fact that the N-RRH position provides street-canyon type links, particularly at the radius of 100 m and 60 m. We further explore this below. 
 Based on our above observations we divided the results for all links with up to 200 m length into street-canyon and non-street-canyon types, rather than by distance as before. Table II presents the results for the power gain ? under this classification, using again the parameters previously detailed in Section III. 
 The N-RRH columns illustrate that very considerable power gains are available for street-canyon links, even at low RRH powers. In contrast, the row corresponding to the non-streetcanyon links shows very modest gains regardless of the RRH position. The latter results were obtained considering only MS positions that are of non-street-canyon type under both RRH placements, so that comparisons are based on a common set of data. 
 SELECTION COMBINING GAIN WITH LOW-HEIGHT RRH, WITH ?(y) = F-SC1(1-y)- F-BS1(1-y). 
 To illustrate the gains achievable under the most favorable RRH and MS placements, we present in Fig. 11 the CDFs for received power considering all street-canyon locations within the disk of 200 m radius, at a RRH power of 20 dBm. When comparing this figure with Fig. 10, it becomes clear that the street-canyon locations within the 200 m radius are those that contribute most significantly to the observed increases in received power. For example the BS will provide at least -65 dBm to 50% of users. This same minimum power will be available to 96% of users if a RRH is included. Repeating this for an assumed 40 dBm transmit power increases coverage to virtually 100%. 
 Finally we again calculated correlation of fades for the BSMS and RRH-MS links, for O-RRH and N-RRH. In both cases these correlations were found to be below 0.01. When smallscale fades are averaged out, this increases to 0.27. 
 Our work suggests that in an urban environment a RRH cannot be expected to yield significant benefits for an ensemble of users that is distributed over a disk region with a radius 
 Fig. 11. CDF of received power at MS street-canyon positions for 200 m coverage radius and 20 dBm RRH power. 
 of 1000 m centered at the RRH. Under our assumptions, based on typical values of power and antenna gains, only limited improvements in received power or coverage are available even when the RRH is located at considerable height and operates at only 5 dB less power than the BS. Limiting the coverage radius to 200 m, significantly improves the benefits obtained from a RRH. It allows important gains even when placed at heights as low as 3 m and operating at the relatively low RRH power of 20 dBm. However, these gains are basically associated with the existence of LOS (or, equivalently, streetcanyon) links to users, and thus heavily depend on site-specific RRH and user placements. 
 In all scenarios, the gains achievable using MRC are only marginally larger than those obtained by a SC, attesting to the fact that, in practice, the likelihood of the RRH and the base providing comparable powers is very small. 
 Our results show that the fades of the RRH-user link and the base-user link are essentially independent. Moreover, measured path-losses for such links were found to be quite consistent with average values obtained from accepted models for the corresponding urban setting. Thus such models can lead to a basic prediction of the gains achievable from the use of a RRH. 
 ﻿We present empirical results on the achievable cellular system throughput gains stemming from the use of remote radio heads (RRH) in an urban environment. Our work is based on simultaneous path-loss measurements of the base station (BS) and RRH links to outdoor street-level users. We calculated the increase in received power, when a RRH is added to improve the coverage achieved by a BS. We consider diverse settings for the RRH and diverse expected coverage areas for the mobile station (MS), evaluating the effect of RRH height and position with respect to the intended users. We also compare the power gains that would be obtained in practice from combining schemes such as selection combining (SC) and maximum ratio combining (MRC).
 We conclude that under practical conditions, the benefits of using RRHs will depend very strongly on the existence of line-ofsight links between the RRH and the intended users. For RRHs placed at low heights, below the clutter, only users in a “streetcanyon” position with respect to the RRH will obtain a significant benefit. Our data also shows that the gains in signal-to-noise ratio achieved when using MRC are only marginally better than those of the much simpler SC.
 Index Terms—Channel modeling, maximum ratio combining, remote radio heads, selection combining, small cells, wireless repeaters.
 I. INTRODUCTION
 O
 NE of the great challenges for today’s wireless communication networks is to provide adequate spatial coverage in a cost-effective way, while achieving bandwidth efficiency and interference levels adequate for high frequency re-use [1]. To meet this challenge there has been a growing interest in the study of femtocells, relay stations (RSs) and low-complexity repeaters [2], [3]. These may constitute relatively simple, lowcost and easy-to-install alternatives when compared to the deployment of an additional base station (BS) to serve mobile stations (MSs) within the cell.
 While femtocells are conceived as a small–coverage BS with low transmit power connected to a wired network, a RS is
 connected to a BS through a wireless link, being able to repeat or re-code the data by using amplify-and-forward or decodeand-forward algorithms, among others [4]. A repeater may be thought of as an amplify-and-forward relay with no decoding or scheduling ability [3]. In this context, there are two basic types of repeaters: wireless-repeaters (WRs), and remote radio heads (RRHs), also known as fiber repeaters, connected to a BS by an optical fiber [3]. WRs will suffer large and small scale fading at both BS-RS/WR and RS/WR-MS links. In contrast, for RRH assisted wireless networks, achievable gains will only depend on the quality of the RRH-MS connection, which makes it an attractive solution to provide connectivity to wireless users in dense urban areas, where the links to the BS may experience significant shadowing.
  2013 IEEE
 Proper placement of a RRH is obviously a fundamental factor in defining the compromise between the desired gains and the cost associated with its installation. In this regard, in an urban environment it is reasonable to expect that while coverage will improve with antenna height, this will at the same time have a negative impact on deployment costs and on co-channel interference, thus affecting frequency reuse in a large system [5], [6]. If the RRH is to cover an area of a size comparable to a small cell, then it is reasonable to assume that this is best achieved by placing its antenna in positions normally used by BSs. In such cases well-established propagation models such as those of Hata [7] and Erceg et al. [8] will be adequate to predict the coverage achieved by the RRH. Alternatively, they may be positioned at lower heights with the aim of achieving improvements in more limited regions, i.e., generating microcells within a large cell. Coverage in microcells has generated growing interest as it can provide local increases in signal-to-noise ratio (SNR) and thus high data-rates [2], [9]. This may be achieved without generating excessive interference in neighboring cells and at lower implementation costs when compared to a BS [2], [10], [11], particularly when placed below clutter height. Numerous empirical models have been proposed for the statistical characterization of path-loss in relatively low-height (3-20 m) radio links [12]–[17]. Analytical models based on optical geometry have also been proposed for microcells in urban environments [18], [19]. Goldsmith et al. [20] proposed, based on a collection of measurements, a mathematical description of the radius of coverage of a microcell for an urban environment with and without line-of-sight (LOS). Contours of local mean attenuation are obtained for two neighborhoods, and it is found that these contours have the shape of “convex diamonds” [20]. Haneda et al. [21] analyzed relays in an indoor-outdoor environment operating under Amplify-and-Forward (AF) and Decode-and-Forward (DF) schemes. In [3], the authors studied different uses of repeaters and RRHs in cellular networks, concluding that careful placement of repeaters can improve capacity substantially, transferring traffic from heavily loaded to lightly loaded sectors, thereby improving network capacity. They also conclude, based on their simulations, that it is best to place repeaters on a horseshoe layout with a sum transmission power of all repeaters comparable to the transmission power of a single macro-base transceiver station.
 The IEEE 802.16 Relay Task Group [22] and the WINNER Consortium [23] have suggested the use of both empirical and theoretical models to predict path-losses for wireless links with transmission units below the clutter of scatterers, also applicable to RRH-MS links. While the accuracy of pathloss models for diverse environments has been the subject of extensive studies, their use in RRH-assisted wireless networks requires more than treating each link individually, since the joint statistics of BS-MS and RRH-MS links may not correspond to those of independent random variables.
 In this work, we report an empirical study that includes the joint statistics for the path-losses of the links involved in a cellular system assisted by RRHs. Specifically, our results are based on simultaneous path-loss measurements of the BS-MS and RRH-MS links in an outdoor urban environment. Based on our measurements we evaluate diverse aspects related to the performance improvement that can be expected when adding a RRH in an area served by a BS. We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user. Our results will be useful to calculate the achievable gains in transmission rates experienced by mobile terminals.
 Our empirical study firstly considered the influence of the RRH height. Toward that end, we started by placing the RRH in a tall-building rooftop position, typical for an urban radio cell, and then lowering it stepwise to a height of 5 m. In this study, we considered street-level user positions spread over a disk with a radius of 1000 m centered at the RRH. Path-loss data for all links involved was collected at each RRH height. The results allowed us to quantify the tradeoff between RRH height and the achievable gains in coverage and received power. For our test conditions, it was found that -on average- an increase in height of the RRH entails an increase in received power of about 1 dB for every 10 m. Overall, the obtained results suggest that the effectiveness of a RRH assisted system is quite limited if a large area is to be covered, unless the RRH is positioned at such a height and operates at such powers that in practice it becomes another urban BS. This conclusion led to the second part of our study, where we restricted measurements to a disk with a radius of only 200 m centered at the RRH. In addition, we placed the RRH at lamp-post heights, considering that this type of setting will be typical for a practical deployment of low-cost repeaters. In this part of the study we also considered two types of RRH placements. One was chosen so as to maximize the likelihood of users having LOS to the RRH and the other chosen in close proximity to the first, but obstructed from direct street view by construction. In this way we were able to evaluate the effect of elements blocking a RRH, as may occur when a surrounding construction is modified after the placement of the fiber repeater. Our results are only directly applicable to narrow-band transmission, for example a single carrier of an OFDM transmission, as our measurements were done with unmodulated carriers. We also report results that involve spatial averaging, i.e., the elimination of small-scale fades. This is equivalent to having spatial diversity, which often yields similar results to wideband frequency diversity [24].
 From the joint data of path-loss for the BS-MS and RRHMS links we were able to compare the effectiveness of Selection Combining (SC) and Maximal Ratio Combining (MRC) [25] at the mobile terminal. To this effect we considered typical transmit powers and antenna gains at the BS and RRH and calculated received signal power at the MS under the condition of equal noise power for both schemes. It was found that the statistical gain of MRC over SC is less than 1 dB under all conditions tested, a consequence of the random power imbalance among the links.
 Our measurements also allowed us to calculate correlations between fades for the BS-MS and RRH-MS links for all RRH heights. The low correlations observed validate the approach based on considering them as independent random variables.
 The rest of this paper is organized as follows: In Section II we present a detailed description of the measurement scenario and procedures. The results related to the effect of RRH height on the achievable gains are discussed in Section III. In Section IV, we present the empirical results obtained when positioning a RRH at lamp-post height. Conclusions are presented in Section V.
 II. MEASUREMENT SCENARIO AND PROCEDURES
 A. Description of the Urban Environment Tested
 The measurement campaign extended over a period of 4 months during summertime. The urban area used as testbed in Viña del Mar, Chile, contains a mix of high-rise buildings and two-story houses with heights ranging from 12 to 62 m, built on a plane region at sea level. Measurements of received power at MS positions were carried out at streetlevel. The area is overlooked by nearby hills that allowed positioning a transmitter at a location typical for a BS covering a relatively large urban area. The BS height was 180 m above the measurement region at a distance of 2 km to the RRH position. The RRH was mounted at various heights on the external wall of a 62 m high building as will be described later. The streets in the measurement area are lined with trees with heights ranging from 6 to 8 m. All BS-MS links were nonline-of-sight (NLOS), with blockages due to the surrounding construction. A schematic description of the scenario and terrain profile is presented in Figs. 1 and 2.
 The second scenario was based on the same urban locations for both base and RRH but with all users within a disk with a radius of 200 m centered at the position of a RRH. The RRH was this time placed at 3 m above street level. The disposition of the user locations is shown in Fig. 3.
  
 (b)
 Fig. 1. Placement of BS, RRH and MS in urban environment for the first part of this study. (a) Tested positions, (b) 3D-view.
  
 B. Measurement System
 A block diagram of the measurement system is shown in Fig. 4. At the BS position, a continuous wave (CW) source based on a synthesized oscillator transmits a 3.5 GHz unmodulated carrier with 19 dBm output power through a 17 dBi sector-type antenna, corresponding to an EIRP of 36 dBm. The antenna used had 60? azimuth and 8? elevation
  
 Fig. 3. Placement of RRH and MS units in urban environment for the second part of this study.
  
 Fig. 4.	Measurement system block diagram.
 beamwidths at -3 dB points. All MS locations were within this beamwidth. At the RRH, the antenna used was a vertically polarized dipole with 2 dBi gain, transmitting a CW signal at 3.57 GHz with 19 dBm output power, corresponding to an EIRP of 21 dBm. The choice of frequencies was based on the fact that they fall inside a band used for Metropolitan Area Networks such as WiMAX and that considerable previous work in this band has been reported [26], providing a reference framework on propagation in urban environments. The RRH was configured to contain a CW transmitter and a narrowband receiver tuned to the BS frequency. This allowed us to remotely detect any possible anomaly at the BS, which was located at a considerable distance from our measurement scenario. The 70 MHz separation between BS and RRH frequencies provided enough isolation to allow simultaneous transmission and reception by the RRH.
 Power measurements at the MS positions were carried out using an Agilent model N1996A-506 spectrum analyzer that simultaneously tracked the carriers at 3.5 and 3.57 GHz. The antenna used was a 2 dBi gain vertically polarized dipole placed at a 1.8 m height. The dynamic range of the measurement setup is limited from below by the system noise-floor. Using only the spectrum analyzer as receiver, this proved to be inadequate for several of the envisioned NLOS user locations and would have biased our results if we had only selected the MS placements where measurements were feasible. We thus included a 25 dB gain pre-amplifier at the mobile-antenna terminal to achieve a system noise figure of 9 dB and thus a noise floor of -125 dBm in a 10 kHz bandwidth. The resulting link budget allowed us to measure path-losses of up to 145 dB in the RRH-MS link and 160 dB in the BS-MS link. At all selected locations the average received power exceeded the noise floor by at least 10 dB. The MS measurement system included a laptop computer that acquired the spectrum analyzer scans at a rate of 2 per second. The received power from the BS and RRH transmissions allowed us to calculate the corresponding path-losses for both links.
 C. Measurement Procedure
 The two measurement campaigns involved moving the MS along a straight path of about 4.5 m at each of the diverse locations in the region tested. This was done using a very slow-moving vehicle on which our measurement equipment was installed. We traversed each 4.5 m sector at a constant speed in such a way that approximately 300 equally-spaced power samples were collected. At the specified frequency, this implies that more than 100 of the samples are at positions spaced at least half a wavelength apart. From these measurements we obtained the average path-loss at the location and the small-scale fade statistics with respect to this average.
 The first measurement campaign was aimed at determining the effect of RRH height. This involved placing the RRH at 20 different heights, ranging from 5 to 62 m above street-level. 29 measurement locations for the MS were selected at various distances from the RRH: 9 within 300 m, 10 between 300 and 600 m and 10 between 600 and 1000 m. These are marked with diamonds in Fig. 1. Measurements for RRH heights in the range from 5 to 14 m did in some locations result in RRHMS path-losses exceeding the reliable measurement range for received MS power. Thus, to avoid biasing our results, the statistics presented for this part of our study exclude data obtained for RRH heights below 14 m.
 The second set of measurements, aimed at evaluating the practical performance gains for low-RRH, was based on 30 MS locations, but all within 200 m of the RRH, as illustrated in Fig. 3. Of these, 8 were within 60 m, 8 between 60 and 100 m, and 14 between 100 and 200 m of the RRH. The RRH at a height of 3 m above street level, was placed at two different positions: one at an intersection so as to simultaneously illuminate two streets and the other one within 20 m of the first placement, but with no direct path to the adjacent streets. In the first case there are LOS and NLOS user locations, while in the second all MS positions are NLOS.
 III. EMPIRICAL RESULTS: EFFECT OF RRH HEIGHT
 In this section we quantify the effect of lowering the RRH antenna from a position where in practice it can be considered another BS, to heights close to street level. To characterize our scenario, we first contrast the measured RRH-MS path-losses with the corresponding values predicted by wellestablished propagation models. In [22] various models for path-loss prediction are proposed, some of which correspond to scenarios similar to the one used here as a test-bed. These models do not explicitly consider the height variation of the BS or relay, but are based on the relative position of these with respect to the user, i.e., LOS/NLOS, above or below clutter. Among these models, we chose the one that best fits our conditions. Specifically we considered the alternative WINNER model specified for urban Type-E NLOS links, with a BS above rooftop height and mobile users below rooftops. The predictions by this model were compared to the RRH-MS path-loss link-measurements, considering the various RRH heights. The distances for these links range to 1000 m. The comparison is shown in Fig. 5 where measurements at all locations and for all RRH heights are included. For this comparison we averaged out small-scale fades, thus generating one path-loss value for each combination of RRH height and MS position, which is to be compared to the model prediction. The separation of the empirical data into LOS and NLOS was based on whether or not the line-of-sight path was blocked by a large structure such as a building. We considered that a tree in the direct path does not alter a LOS condition. To avoid cluttering the figure we combined the measurements for the various RRH heights into three groups: 14 m to 29 m, 30 m to 45 m and 46 m to 62 m. The corresponding pathloss predicted by the model is plotted considering a BS at a height in the center of the range of heights for each group. We note that the model is little affected by this height. We also show for reference the free-space path-loss. As can be seen, the NLOS path-loss measurements are within range of the model predictions, although we observed some path-loss reduction with RRH height, not accounted for by the model. The same figure also shows the results of the LOS path-loss measurements considering all heights, which as expected fall well below the NLOS model prediction.
 We further compared the statistics of our empirical path-losses for each RRH height to those of the classical log-distance model with log-normal shadow fading and Rice/Rayleigh small-scale fades. As is common practice [25], we chose the model to adjust to the free-space path-loss at the reference distance of 1 m. We found that this model provided a very good fit, with a path-loss exponent ranging from 2.3 to 2.5 when considering LOS locations. When considering only the NLOS locations, the path-loss exponent varied between 2.6 and 3.5. The lowest values were obtained at the highest RRH position, with a clearly observable increase as the RRH was lowered towards street level. The standard deviation with respect to the regression line was in the range of 4 to 8 dB, with little difference between LOS and NLOS links. For all of the NLOS locations, the statistics of the small-scale fades fit the Rayleigh distribution, within the accuracy limits that result from a finite data set. For the LOS links the Rice distribution
  
  
 Fig. 5. Empirical RRH-MS path loss compared to WINNER model according to IEEE 802.16j recommendation.
 provided a similarly good fit, with K-factors that were quite small, no larger than 12 dB.
 The above results indicate that our choice of measurement scenario reflects the propagation behavior of an urban environment, characterized as described in the literature, by shadowing and by strong multipath propagation.
 To evaluate the achievable received power gain as a function of the RRH height, we calculated the received power at the MS locations, assuming realistic conditions for transmit power and antenna gains. We note that once the path-losses for all links have been obtained, we are free in subsequent calculations to assume any transmit power for the BS and the RRH, not necessarily those used before in the channel sounding process. In the calculation of received power at the MS, we thus used the actual measured path-losses for the links, and transmit-power levels that we deemed reasonable for an actual deployment. From the collection of calculated power values at the MS, we generated the cumulative distribution function (CDF) of the received power. For this we considered transmission by BS alone, by RRH alone, and transmission by both BS and RRH with SC or MRC at the MS. We considered a BS with 45 dBm of transmission power, typical for a macrocell [27]. The BS and MS antennas are assumed to be the same as the ones used in our measurements (17 dBi and 2 dBi, respectively). For the RRH we assumed that in most practical cases the RRH would be attached to a wall and consequently only illuminate a sector of 180? in azimuth. All our MS locations are in fact within a sector of such angular width. We thus considered that a typical 7 dBi, 180? sector antenna would be used, consistent with calculations reported in [28]. For the transmit power at the RRH we used two values that cover what could be considered the high and low extremes of a practical deployment: 40 and 20 dBm. The lower value is representative for femtocells, while the high value is in the power range mentioned for relays and RRHs [27].
 Figure 6 shows the CDFs of the received power assuming that the RRH transmits at 40 dBm. The CDFs consider all power measurements at all MS locations. We obtained CDFs
  
 Fig. 6. CDF of received power at MS for 1,000 m coverage radius and 40 dBm RRH power.
 for all RRH heights between 14 and 62 m but we here only show the ones corresponding to 62 m. The CDFs for 14 m essentially match the CDF of the power received from the BS alone, with a shift to the right of less than 1 dB at all probability levels. We do not include them in the figure to avoid cluttering the graph. All other CDFs fall between these two extremes and are also omitted for the sake of clarity. The improvement in coverage over that achieved by the BS alone was only found to be significant when the RRH occupies a high position. Not shown here is the case where the RRH operates at 20 dBm, which results in virtually no gain at any RRH position.
 Fig. 6 shows that the received power gain of MRC over SC is almost insignificant. This is due to the fact that the likelihood of similar received power from BS and RRH is quite small, and as well known, even for equal power the gain is only 3 dB. We repeated these calculations for diverse ratios of BS to RRH power with the result that the improvement in received power of MRC over SC under all conditions was no greater than 1 dB. As mentioned before, this assumes equal receiver noise in both cases, thus the power gain is equivalent to SNR gain. Based on the above results, we describe below the benefits that were observed for the RRH-assisted link in this setting. All our subsequent results are based on the assumption of a SC at the user terminal, but for all practical purposes the results would be no different using a MRC.
 A. Coverage Extension with Respect to that Achievable by BS
 From Fig. 6 it is seen that, under the stated conditions, 50% of users will receive at least -61 dBm when only the BS is operating. Adding a RRH at the maximum height, transmitting at 40 dBm improves this coverage to 70%. At the other extreme, a RRH at 14 m will provide virtually no statistically significant improvement in coverage, as the corresponding CDF of received power was found to exhibit a right-shift of less than 1 dB. Obviously lower heights will further reduce any existing RRH benefits. We note that these rather limited gains were achieved considering a relatively
  
 RRH height [m]
 Fig. 7. Power gain of SC over BS alone ?, for diverse availabilities as function of RRH height with RRH transmission power of 40 dBm.
 high RRH transmit power, only 5 dB less than that of the base, which does however have a 15 dB EIRP advantage over the RRH, due to the 10 dB difference in the corresponding antenna gains.
 B. Received-Power Gain
 To quantify the gains we define as ?(y) the dB difference in received power that can be guaranteed to a proportion of users y. We refer to this proportion as the availability level associated with that power gain. We denote the CDFs of received power P [dBm] from the BS alone and from SC of BS and RRH transmissions as FBS (P) and FSC (P) respectively. The gain in received power using SC is then:
 	?(y) = F-SC1(1 - y) - F-BS1(1 - y)	(1)
 The same definition of course applies to the gain obtained through MRC. We now use (1) to calculate, at various availability levels y, the power gain resulting from the use of a RRH. This is shown in Fig. 7 for the full range of RRH placements at availabilities of 10%, 50% and 90%, considering 40 dBm of transmit power at the RRH. To construct this figure we considered the empirical CDFs as illustrated in Fig. 6, but calculated for each RRH height. In Fig. 7 we also indicate as a reference the results that would have been obtained if instead of using the measured CDFs for path-losses, these had been calculated using at each location a fixed path-loss equal to the average value predicted by the models discussed before. Specifically, for the RRH-MS link we used the alternative WINNER model for type E in [22] while at the longerrange BS-MS links we used the Hata model, extrapolated to a frequency of 3.5 GHz. Since height dependence is not part of the model in [22], the resulting gain, which we found to be 2.7 dB, is only a function of the MS placements in the region. In contrast, the empirical gains show considerable height dependence.
 From the data that Fig. 7 is based on, we calculated by linear regression the increase in received power (?) vs. height of the RRH. The result was that under the stated conditions,
  
 Fig. 8. LOS and NLOS power gains of SC over BS alone ?, for diverse availabilities as function of RRH height with RRH transmission power of 40 dBm.
 the power gain is approximately 1 dB for every 10 m in height increase, at availabilities of 90% and 50%. If the RRH power is reduced to 20 dBm this gain becomes only 0.2 dB for every 10 m. We note that these values represent the range of height-related gains for our particular choice of MS placements and thus cannot be easily generalized. From a qualitative perspective however, the results may suggest that the gain with height is mostly due to the fact that at the higher RRH positions, the number of MS locations that exhibit a LOS link to the RRH is larger. To investigate this aspect further, we segmented the results, separating the MS locations into those with, and those without LOS, for each RRH height. The resulting power gain vs. RRH height is shown in Fig. 8. In this figure we also show the percentage of LOS zones. As seen, for the NLOS zones there is little gain improvement as the RRH height increases. On the other hand, for the LOS zones, there is more evidence of gain increase with height. The main conclusion that may be derived from this figure is that the LOS locations benefit significantly more from increasing the height of the RRH than those that are NLOS. We also note some gain increases with height, even when the percentage of LOS location reaches a saturation level. Thus, the growth in the percentage of RRH-MS links with LOS does not by itself account for the increased gains with RRH height.
 Finally, to evaluate the degree of dependence of the fades [28] in the BS-MS and RRH-MS links we calculate the crosscorrelation coefficient ? between these fades for each RRH height as
  	(2)
 E[{PL(BS) - E[PL(BS)]}{PL(RRH) - E[PL(RRH)]}]
 where PL(BS) and PL(RRH) represent respectively the path-losses, including the small-scale fades, from the BS and the RRH to a specific MS position. E[PL(BS)] and E[PL(RRH)] are the statistical averages of these path losses at the distance under consideration, obtained from a linear regression of path loss vs. distance. sBS and sRRH are the standard deviations of the fades with respect to these averages.
 This calculation was repeated for each RRH height. The values obtained were very small. Only at the lowest height (14 m) did ? approach +0.1, while at all others it was less than 0.05 in modulus. We also calculated the correlations of the shadow fades, i.e. the fades after averaging out the small-scale fades. This would be more representative for links with space or frequency diversity, which tends to suppress small-scale fades. In this case ? was somewhat larger, but still not significant, reaching +0.5 at the lowest height and fluctuating between positive and negative values less than 0.3 in modulus at heights above 17 m.
 IV. LOW-HEIGHT RRH
 The results of the previous section suggest that even with a RRH position at a height that is comparable to that of an urban cell BS, only modest gains can be achieved at a large coverage radius. This happens even at a relatively high transmit power. In this section we therefore explore more practical configurations, limited to much smaller coverage regions and a lower-height RRH (3 m above street level), as we have described in Section II. In this setup we also explored the effect of choosing a RRH position with an uncluttered path to two intersecting streets, in contrast with placing it in a nearby position obstructed by construction. The first placement is denoted here as “Non-Obstructed RRH” (N-RRH) and the second as “Obstructed RRH” (O-RRH). The N-RRH position provides LOS links to some of the chosen MS locations. For a setting where both antennas are below clutter, LOS links are described as being in a “street-canyon” [29]. Correspondingly, we refer to the NLOS links as “non-street-canyon”. In the 200 m radius, half (15) of the N-RRH links are of the streetcanyon type while all O-RRH links are non-street-canyon. Within the 100 m radius all links to the N-RRH become of street-canyon type. This classification (according to the RRHMS link characteristics) is aimed, as before, at determining which users can be expected in practice to benefit most from a given RRH placement. We again first verified that the chosen MS locations are such that the path-loss statistics for the environment correspond to well-established propagation models.
 For the NLOS links, we found a good match to the COSTHata model [25] and to the alternative WINNER path loss model for the type F-NLOS case suggested in [22], using an operating frequency of 3.57 GHz. With respect to these models, our measured losses were slightly higher, 2 and 4 dB on average, with root mean square (r.m.s.) errors of 6 and 10 dB respectively. The measured losses for the street-canyon links were far lower than those for NLOS links at the same distance, from 4 to 34 dB, with 18 dB on average. They were however higher by about 8 dB on average than those predicted by the advanced LOS model for Type-F LOS scenarios (with both RS and MS below rooftops) [22]. The path-loss graphs illustrating these comparisons are shown in Fig. 9. As before, we also verified that our data fit the statistics of the singleslope log-distance model, with log-normal shadow fading and Rice/Rayleigh small-scale fades. The resulting slopes were 2.6 and 3.8 for LOS and NLOS links respectively. We note that, as mentioned before, most of our LOS links were at least partially obstructed by relatively large trees.
  
 Fig. 9. Empirical RRH-MS path-loss measurements compared to reference models for low height RRH positions.
  
 Fig. 10. CDF of received power at MS for 200 m coverage radius and 20 dBm RRH power. RRH height was 3 m.
 Fig. 10 presents the CDFs of the received power at the MS as previously discussed for Fig. 6 but within the 200 m radius. We illustrate as an example, only the case where the RRH transmits at 20 dBm for both RRH placements. All other parameters are as described before. As seen, the achievable gains very much depend on the specific placement of the RRH. For example, at 50% coverage, the BS will guarantee a power of at least -63 dBm. Using an O-RRH will guarantee the same power to 56% of users, while an N-RRH will increase this coverage to 76%. When repeating this calculation under the assumption of a RRH transmit power of 40 dBm, the coverage increases to 77% and 84% respectively. We summarize the RRH gains for various conditions in what follows.
 A. RRH Gains as Function of Coverage Radius
 To evaluate the effect of coverage range we divided the locations where the power measurements were carried out into three groups with disk radii of 200 m, 100 m and 60 m, referred to the RRH. Table I presents the power gain ? (as
  
 defined in (1)), obtained with a SC at the availability levels of 90%, 50% and 10%, for two RRH transmit powers and for the two placements described. The results using MRC are almost identical. We recall that for a radius of 100 m or less the N-RRH placement provides street-canyon-type links to all users, while only 50% of the users are in that condition for the 200 m radius. The tabulated results provide some interesting insight into the benefits that can be expected from the use of a RRH. We first note that for 50% and 90% availability, there are consistent increases in power gain as the coverage radius decreases, particularly when all user locations become of street-canyon type with respect to the N-RRH. This increase is especially sharp at 90% availability. The values at 10% availability, while not as relevant in practice, serve to illustrate the gains available to a minority of users that have a particularly low path-loss to the RRH, for example due to an unobstructed path at a short range. We in fact see that the gains available to 10% of users at a 200 m radius are comparable to those observed for 90% within the shorter ranges. The two smaller radii include only street-canyon type links for the N-RRH case, however the variation in distance from the various user positions to the RRH is obviously larger for the 100 m than for the 60 m case. At high availability this implies significantly increased power gains for the smaller radius, as it will exclude most user locations with higher path-losses to the RRH. Not as obvious is the fact that when considering 10% availability, the power gains can actually decrease with the radius, as seen in the table. This is due to the fact that the smaller radius may also exclude some user positions that exhibit particularly low path-losses to the RRH. It is precisely these few well-placed users (with respect to the RRH) that dominate the achievable gains for low availability. In the same context, we observe that for the N-RRH, a 20 dB increase in transmit power results in almost exactly the same increase in received power to the 10% best-placed users. This again indicates that for such users, the received power is completely dominated by that of the RRH.
 Both Fig. 10 and Table I illustrate the importance of proper positioning of the RRH. This can also be associated with the fact that the N-RRH position provides street-canyon type links, particularly at the radius of 100 m and 60 m. We further explore this below.
 B. RRH Gains for Street-Canyon and Non-Street-Canyon Links
 Based on our above observations we divided the results for all links with up to 200 m length into street-canyon and non-street-canyon types, rather than by distance as before. Table II presents the results for the power gain ? under this classification, using again the parameters previously detailed in Section III.
 The N-RRH columns illustrate that very considerable power gains are available for street-canyon links, even at low RRH powers. In contrast, the row corresponding to the non-streetcanyon links shows very modest gains regardless of the RRH position. The latter results were obtained considering only MS positions that are of non-street-canyon type under both RRH placements, so that comparisons are based on a common set of data.
 TABLE I
 SELECTION COMBINING GAIN WITH LOW-HEIGHT RRH, WITH ?(y) = F-SC1(1-y)- F-BS1(1-y).
           RRH 
 Transmitted  
                power 
  
 Coverage  
 radius              	PtxRRH= 40 dBm 	PtxRRH = 20 dBm 
 	O-RRH 	N-RRH 	O-RRH 	N-RRH 
 200 m 	y = 90% 	5.5 	7.5 	1.6 	4.8 
 	y = 50% 	6.2 	16.3 	1.2 	6.1 
 	y = 10% 	13.5 	29.4 	1.0 	9.4 
 100 m 	y = 90% 	11.8 	24.7 	2.4 	11.0 
 	y = 50% 	11.8 	31.9 	2.1 	13.6 
 	y = 10% 	21.2 	33.0 	2.5 	12.9 
 60 m 	y = 90% 	18.2 	29.3 	6.0 	14.0 
 	y = 50% 	19.5 	36.4 	3.8 	17.0 
 	y = 10% 	20.4 	30.1 	1.9 	10.1 
 TABLE II
 SELECTION COMBINING GAIN FOR STREET-CANYON AND
 NON-STREET-CANYON, WITH ?(y) = F-SC1(1-y)- F-BS1(1-y)
  
 To illustrate the gains achievable under the most favorable RRH and MS placements, we present in Fig. 11 the CDFs for received power considering all street-canyon locations within the disk of 200 m radius, at a RRH power of 20 dBm. When comparing this figure with Fig. 10, it becomes clear that the street-canyon locations within the 200 m radius are those that contribute most significantly to the observed increases in received power. For example the BS will provide at least -65 dBm to 50% of users. This same minimum power will be available to 96% of users if a RRH is included. Repeating this for an assumed 40 dBm transmit power increases coverage to virtually 100%.
 Finally we again calculated correlation of fades for the BSMS and RRH-MS links, for O-RRH and N-RRH. In both cases these correlations were found to be below 0.01. When smallscale fades are averaged out, this increases to 0.27.
 V. CONCLUSIONS
 Our work suggests that in an urban environment a RRH cannot be expected to yield significant benefits for an ensemble of users that is distributed over a disk region with a radius
  
 Fig. 11. CDF of received power at MS street-canyon positions for 200 m coverage radius and 20 dBm RRH power.
 of 1000 m centered at the RRH. Under our assumptions, based on typical values of power and antenna gains, only limited improvements in received power or coverage are available even when the RRH is located at considerable height and operates at only 5 dB less power than the BS. Limiting the coverage radius to 200 m, significantly improves the benefits obtained from a RRH. It allows important gains even when placed at heights as low as 3 m and operating at the relatively low RRH power of 20 dBm. However, these gains are basically associated with the existence of LOS (or, equivalently, streetcanyon) links to users, and thus heavily depend on site-specific RRH and user placements.
 In all scenarios, the gains achievable using MRC are only marginally larger than those obtained by a SC, attesting to the fact that, in practice, the likelihood of the RRH and the base providing comparable powers is very small.
 Our results show that the fades of the RRH-user link and the base-user link are essentially independent. Moreover, measured path-losses for such links were found to be quite consistent with average values obtained from accepted models for the corresponding urban setting. Thus such models can lead to a basic prediction of the gains achievable from the use of a RRH.
 ﻿ The objective of this note is to report some potentially useful mutual information . 
 Throughout this section , and unless otherwise stated ,, xi , i N ,, continuous random taking in appropriate of . We assume that they all have well defined probability density , which we denote by , and , respectively , and well defined joint by . We also use the notation to refer to the conditional of , given . All and in this section are standard and can be found in . 
 Definition Differential entropy The differential entropy defined via 
 independent , then eh eh eh eh is entropy power of . This property is entropy power inequality . 
 Definition Mutual information The mutual information defined via 
 Definition chain The random , said to form a chain in that order if and only if , , i . e ., if and only conditionally independent given . If that is the case , we write 
 Theorem Data inequality If , then I ; I ;. Equality if and only if , in addition ,. ¤¤¤ Definition Divergence between The divergence of the distribution respect to the distribution of in short , the divergence is defined by 
 If is a second order random variable any other random variable with the same mean and covariance matrix , then 
 Remark Conditional divergence It will prove useful to consider an extension of the definition of divergence . Given two joint and , we define the conditional divergence between them via 
 If and are jointly random joint , arbitrary random a joint with the same first and second order as , then 
 We end this section with an extension of the notion of differential entropy to random . 
 Definition Differential entropy rate Consider an asymptotically stationary process . The differential entropy rate defined by 
 stationary , then it is clear that , with equality if and only a sequence of independent random recall Fact . 
 Theorem Differential entropy rate see , e .., , If a stationary process is by a stable filter frequency response , then the filter entropy rate given by 
 Lemma Consider the situation in Figure , dimensional random that have arbitrary . independent , and and denote independent dimensional random the same mean and covariance matrix , respectively , then 
 Proof : and , the independence of , and and the definition of , it is easy to see that 
 Lemma Consider the situation in Figure , dimensional random , is arbitrary distribution . If an dimensional random variable , jointly with , the same mean and covariance matrix as , and such that the cross covariance cross covariance between and , then 
 with equality if the covariance matrix non singular , and jointly with . 
 Use of the in Remark the first part of the result . Clearly , , then equality in . The proof of the converse can be found in . ¤¤¤ 
 Lemma Consider the situation in Figure , independent scalar random with arbitrary . If and denote independent scalar random the same mean and covariance matrix , and , then 
 Proof : We will use the proof of Lemma . If the right hand side in equality a in were positive , then the result would be true . Thus , we will start examining the difference 
 where we have used Fact , the independence of , and . On the other hand , the entropy power inequality one to conclude that , since , are independent , 
 and , since the variance of the and non random is the same , we have from 
 where a from Fact and from and , and the fact that the variance of the and non random is the same . The result and in equality a in . ¤¤¤ 
 Definition Consider two . We define if the exist the mutual information rate 
 Lemma Consider the feedback system in Figure , where is stable and strictly proper i . e ., , is a random process , an i . i .. sequence that is independent of the initial state of . Then , 
 Proof : By definition of mutual information rate and the chain rule of mutual information we have that 
 on , it that I i ; wi I i ; di wi . Thus , 
 where a from Fact , from the definition Fact , from the fact that , by definition of , wi , di ni , di for every random variable , and from the fact that , independent of the initial state of , is independent , thus , i ni di . We also have that 
 where a from the definition of in Figure , from Fact and the fact that , by definition , wi , , di for every random variable , and from the fact that both the initial state of independent of , being i . i .., and being strictly proper that i . 
 Use of Theorem , and the Bode integral theorem see , e .., the result . ¤¤¤ 
  
 ﻿ The objective of this note is to report some potentially useful mutual information . 
  
 Throughout this section , and unless otherwise stated ,, xi , i N ,, continuous random taking in appropriate of . We assume that they all have well defined probability density , which we denote by , and , respectively , and well defined joint by . We also use the notation to refer to the conditional of , given . All and in this section are standard and can be found in . 
 Definition Differential entropy The differential entropy defined via 
 Z 
 h , u u . 
 The conditional differential entropy of , given , is defined via 
 Z , u , u , . 
 ¤¤ 
 The differential entropy the following : 
 Fact of 
 h with equality if and only independent . 
 h . 
 If a , then ax a . 
 this property is chain rule for differential 
 entropy . 
 independent , then eh eh eh eh is entropy power of . This property is entropy power inequality . 
 ¤¤¤ 
 Definition Mutual information The mutual information defined via 
  
 The conditional mutual information , given , is defined via 
  
 Mutual information the following : 
 Fact of I 
 I ; I ;. 
 I ; , , I ;. 
 I ; with equality if and only independent . 
 I ,; I ; I ; chain rule of mutual information . ¤¤ 
 ¤¤¤ 
 Definition chain The random , said to form a chain in that order if and only if , , i . e ., if and only conditionally independent given . If that is the case , we write 
 x . 
 ¤¤ 
 Theorem Data inequality If , then I ; I ;. Equality if and only if , in addition ,. ¤¤¤ Definition Divergence between The divergence of the distribution respect to the distribution of in short , the divergence is defined by 
  
 ¤¤ 
 Relevant of are below : 
 Fact of 
 D with equality if and only if almost everywhere a . e .. 
 If is a second order random variable any other random variable with the same mean and covariance matrix , then 
 D ax , 
 where a is any real number . 
 ¤¤¤ 
 Remark Conditional divergence It will prove useful to consider an extension of the definition of divergence . Given two joint and , we define the conditional divergence between them via 
  
 It is possible to show that the following : 
 D . 
 If and are jointly random joint , arbitrary random a joint with the same first and second order as , then 
 D . 
 ¤¤ 
 We end this section with an extension of the notion of differential entropy to random . 
 Definition Differential entropy rate Consider an asymptotically stationary process . The differential entropy rate defined by 
 . 
 ¤¤ 
 stationary , then it is clear that , with equality if and only a sequence of independent random recall Fact . 
 Theorem Differential entropy rate see , e .., , If a stationary process is by a stable filter frequency response , then the filter entropy rate given by 
  
 ¤¤¤ 
  
 This section the main of this note . 
 Lemma Consider the situation in Figure , dimensional random that have arbitrary . independent , and and denote independent dimensional random the same mean and covariance matrix , respectively , then 
 I ; I ; , 
 with equality if and only jointly . 
 Proof : and , the independence of , and and the definition of , it is easy to see that 
 I ; I ; 
  
 a 
 , 
 where the last inequality from Fact . The result is now immediate . ¤¤¤ 
  
 Figure : Additive channel . 
 Lemma Consider the situation in Figure , dimensional random , is arbitrary distribution . If an dimensional random variable , jointly with , the same mean and covariance matrix as , and such that the cross covariance cross covariance between and , then 
 I ; I ;, 
 with equality if the covariance matrix non singular , and jointly with . 
 Proof : Fact it is possible to write 
 I ; I ; . 
 Use of the in Remark the first part of the result . Clearly , , then equality in . The proof of the converse can be found in . ¤¤¤ 
 Lemma Consider the situation in Figure , independent scalar random with arbitrary . If and denote independent scalar random the same mean and covariance matrix , and , then 
 D and I ; I ;, 
 with equality if and only jointly . 
 Proof : We will use the proof of Lemma . If the right hand side in equality a in were positive , then the result would be true . Thus , we will start examining the difference 
  
 where we have used Fact , the independence of , and . On the other hand , the entropy power inequality one to conclude that , since , are independent , 
  
  
 Figure : Feedback system considered in Lemma . 
 Use of in 
  
 and , since the variance of the and non random is the same , we have from 
 that 
  
 where a from Fact and from and , and the fact that the variance of the and non random is the same . The result and in equality a in . ¤¤¤ 
 Definition Consider two . We define if the exist the mutual information rate 
 , 
 and the average mutual information 
 . 
 ¤¤ 
 Lemma Consider the feedback system in Figure , where is stable and strictly proper i . e ., , is a random process , an i . i .. sequence that is independent of the initial state of . Then , 
 , 
 where the set of non minimum phase of . ¤¤¤ 
 Proof : By definition of mutual information rate and the chain rule of mutual information we have that 
 . 
 on , it that I i ; wi I i ; di wi . Thus , 
 I ; I . 
 Define , and note that 
 n . 
 We first note that 
 I i ; di wi I i ; wi a i wi ; i wi , di 
 i wi ; i wi , di 
 i wi ; i ni , di 
 i wi ; i ni , 
 where a from Fact , from the definition Fact , from the fact that , by definition of , wi , di ni , di for every random variable , and from the fact that , independent of the initial state of , is independent , thus , i ni di . We also have that 
 h i wi , a i i wi , 
 i , 
 i , 
 where a from the definition of in Figure , from Fact and the fact that , by definition , wi , , di for every random variable , and from the fact that both the initial state of independent of , being i . i .., and being strictly proper that i . 
 From , , and Fact it that 
 I ; I . 
 Use of Theorem , and the Bode integral theorem see , e .., the result . ¤¤¤ 
  
 We study the increase in per-sample differential entropy rate of random sequences and processes after being passed through a non minimum-phase (NMP) discrete-time, linear time-invariant (LTI) filter G. For LTI discrete-time filters and random processes, it has long been established that this entropy gain, G(G), equals the integral of  . It is also known that, if the first sample of the impulse response of G has unit-magnitude, then the latter integral equals the sum of the logarithm of the magnitudes of the non-minimum phase zeros of G (i.e., its zeros outside the unit circle), say B(G). These existing results have been derived in the frequency domain as well as in the time domain. In this note, we begin by showing that existing time-domain proofs, which consider finite length-n sequences and then let n tend to infinity, have neglected significant mathematical terms and, therefore, are inaccurate. We discuss some of the implications of this oversight when considering random processes. We then present a rigorous time-domain analysis of the entropy gain of LTI filters for random processes. In particular, we show that the entropy gain between equal-length input and output sequences is upper bounded by B(G) and arises if and only if there exists an output additive disturbance with finite differential entropy (no matter how small) or a random initial state. Unlike what happens with linear maps, the entropy gain in this case depends on the distribution of all the signals involved. Instead, when comparing the input differential entropy to that of the entire (longer) output of G, the entropy gain equals B(G) irrespective of the distributions and without the need for additional exogenous random signals. We illustrate some of the consequences of these results by presenting their implications in three different problems. Specifically: a simple derivation of the rate-distortion function for Gaussian non-stationary sources, conditions for 
 In his seminal 1948 paper [1], Claude Shannon gave a formula for the increase in differential entropy per degree of freedom that a continuous-time, band-limited random process u(t) experiences after passing through a linear time-invariant (LTI) continuous-time filter. In this formula, if the input process is bandlimited to a frequency range [0,B], has differential entropy rate (per degree of freedom) h¯(u), and the LTI filter has frequency response G(j?), then the resulting differential entropy rate of the output process y(t) is given by [1, Theorem 14] 
 The last term on the right-hand side (RHS) of (1) can be understood as the entropy gain (entropy amplification or entropy boost) introduced by the filter G(j?). Shannon proved this result by arguing that an LTI filter can be seen as a linear operator that selectively scales its input signal along infinitely many frequencies, each of them representing an orthogonal component of the source. The result is then obtained by writing down the determinant of the Jacobian of this operator as the product of the frequency response of the filter over n frequency bands, applying logarithm and then taking the limit as the number of frequency components tends to infinity. 
 An analogous result can be obtained for discrete-time input {u(k)} and output {y(k)} processes, and an LTI discrete-time filter G(z) by relating them to their continuous-time counterparts, which yields 
 is the differential entropy rate of the process {u(k)}. Of course the same formula can also be obtained by applying the frequency-domain proof technique that Shannon followed in his derivation of (1). 
 The rightmost term in (2), which corresponds to the entropy gain of G(z), can be related to the structure of this filter. It is well known that if G is causal with a rational transfer function G(z) such that limz?8 |G(z)| = 1 (i.e., such that the first sample of its impulse response has unit magnitude), then 
 where {?i} are the zeros of G(z) and D, {z ? C : |z| < 1} is the open unit disk on the complex 
 plane. This provides a straightforward way to evaluate the entropy gain of a given LTI filter with rational transfer function G(z). In addition, (3) shows that, if limz?8 |G(z)| = 1, then such gain is greater than one if and only if G(z) has zeros outside D. A filter with the latter property is said to be non-minimum phase (NMP); conversely, a filter with all its zeros inside D is said to be minimum phase (MP) [2]. 
 NMP filters appear naturally in various applications. For instance, any unstable LTI system stabilized via linear feedback control will yield transfer functions which are NMP [2], [3]. Additionally, NMP-zeros also appear when a discrete-time with ZOH (zero order hold) equivalent system is obtained from a plant whose number of poles exceeds its number of zeros by at least 2, as the sampling rate increases [4, Lemma 5.2]. On the other hand, all linear-phase filters, which are specially suited for audio and imageprocessing applications, are NMP [5], [6]. The same is true for any all-pass filter, which is an important building block in signal processing applications [5], [7]. 
 An alternative approach for obtaining the entropy gain of LTI filters is to work in the time do- 
 where yn1 , [y1 y1 ··· yn]T and the random vector u1n is defined likewise. From this, it is clear that 
 regardless of whether G(z) (i.e., the polynomial g0 +g1z-1 +···) has zeros with magnitude greater than one, which clearly contradicts (2) and (3). Perhaps surprisingly, the above contradiction not only has been overlooked in previous works (such as [8], [9]), but the time-domain formulation in the form of (4) has been utilized as a means to prove or disprove (2) (see, for example, the reasoning in [10, p. 568]). 
 A reason for why the contradiction between (2), (3) and (6) arises can be obtained from the analysis developed in [11] for an LTI system P within a noisy feedback loop, as the one depicted in Fig. 1. In 
 Figure 1.	Left: LTI system P within a noisy feedback loop. Right: equivalent system when the feedback channel is noiseless and has unit gain. 
 this scheme, C represents a causal feedback channel which combines the output of P with an exogenous (noise) random process c81 to generate its output. The process c81 is assumed independent of the initial state of P, represented by the random vector x0, which has finite differential entropy. For this system, it is shown in [11, Theorem 4.2] that 
 with equality if w is a deterministic function of v. Furthermore, it is shown in [12, Lemma 3.2] that if 
 |h(x0)| < 8 and the steady state variance of system P remains asymptotically bounded as k ? 8, then 
 where {pi} are the poles of P. Thus, for the (simplest) case in which w = v, the output y81 is the result of filtering u81 by a filter (as shown in Fig. 1-right), and the resulting entropy rate of {y(k)} will exceed that of {u(k)} only if there is a random initial state with bounded differential entropy (see (7a)). Moreover, under the latter conditions, [11, Lemma 4.3] implies that if G(z) is stable and |h(x0)| < 8, then this entropy gain will be lower bounded by the right-hand side (RHS) of (3), which is greater than zero if and only if G is NMP. However, the result obtained in (7b) does not provide conditions under which the equality in the latter equation holds. 
 Additional results and intuition related to this problem can be obtained from in [13]. There it is shown that if {y(k)} is a two-sided Gaussian stationary random process generated by a state-space recursion of the form 
 for some A ? CM×M, g ? CM×1, h ? CM×1, with unit-variance Gaussian i.i.d. innovations u, then its entropy rate will be exactly  (i.e., the differential entropy rate of u) plus the RHS of (3) (with {?i} now being the eigenvalues of A outside the unit circle). However, as noted in [13], if the same system with zero (or deterministic) initial state is excited by a one-sided infinite Gaussian 
 i.i.d. process u81 with unit sample variance, then the (asymptotic) entropy rate of the output process y81 is just  (i.e., there is no entropy gain). Moreover, it is also shown that if vl1 is a Gaussian random sequence with positive-definite covariance matrix and l = M, then the entropy rate of y also exceeds that of u81 by the RHS of (3). This suggests that for an LTI system which admits a statespace representation of the form (8), the entropy gain for a single-sided Gaussian i.i.d. input is zero, and that the entropy gain from the input to the output-plus-disturbance is (3), for any Gaussian disturbance of length M with positive definite covariance matrix (no matter how small this covariance matrix may 
 The previous analysis suggests that it is the absence of a random initial state or a random additive output disturbance that makes the time-domain formulation (4) yield a zero entropy gain. But, how would the addition of such finite-energy exogenous random variables to (4) actually produce an increase in the differential entropy rate which asymptotically equals the RHS of (3)? In a broader sense, it is not clear from the results mentioned above what the necessary and sufficient conditions are under which an entropy gain equal to the RHS of (3) arises (the analysis in [13] provides only a set of sufficient conditions and relies on second-order statistics and Gaussian innovations to derive the results previously described). Another important observation to be made is the following: it is well known that the entropy gain introduced by a linear mapping is independent of the input statistics [1]. However, there is no reason to assume such independence when this entropy gain arises as the result of adding a random signal to the input of the mapping, i.e., when the mapping by itself does not produce the entropy gain. Hence, it remains to characterize the largest set of input statistics which yield an entropy gain, and the magnitude of this gain. 
 The first part of this paper provides answers to these questions. In particular, in Section III explain how and when the entropy gain arises (in the situations described above), starting with input and output sequences of finite length, in a time-domain analysis similar to (4), and then taking the limit as the length tends to infinity. In Section IV it is shown that, in the output-plus-disturbance scenario, the entropy gain is at most the RHS of (3). We show that, for a broad class of input processes (not necessarily Gaussian or stationary), this maximum entropy gain is reached only when the disturbance has bounded differential entropy and its length is at least equal to the number of non-minimum phase zeros of the filter. We provide upper and lower bounds on the entropy gain if the latter condition is not met. A similar result is shown to hold when there is a random initial state in the system (with finite differential entropy). In addition, in Section IV we study the entropy gain between the entire output sequence that a filter yields as response to a shorter input sequence (in Section VI). In this case, however, it is necessary to consider a new definition for differential entropy, named effective differential entropy. Here we show that an effective entropy gain equal to the RHS of (3) is obtained provided the input has finite differential entropy rate, even when there is no random initial state or output disturbance. 
 In the second part of this paper (SectionVII) we apply the conclusions obtained in the first part to three problems, namely, networked control, the rate-distortion function for non-stationary Gaussian sources, and the Gaussian channel capacity with feedback. In particular, we show that equality holds in (7b) for the feedback system in Fig. 1-left under very general conditions (even when the channel C is noisy). For the problem of finding the quadratic rate-distortion function for non-stationary auto-regressive Gaussian sources, previously solved in [14]–[16], we provide a simpler proof based upon the results we derive in the first part. This proof extends the result stated in [15], [16] to a broader class of non-stationary sources. For the feedback Gaussian capacity problem, we show that capacity results based on using a short random sequence as channel input and relying on a feedback filter which boosts the entropy rate of the end-to-end channel noise (such as the one proposed in [13]), crucially depend upon the complete absence of any additional disturbance anywhere in the system. Specifically, we show that the information rate of such capacity-achieving schemes drops to zero in the presence of any such additional disturbance. As a consequence, the relevance of characterizing the robust (i.e., in the presence of disturbances) feedback capacity of Gaussian channels, which appears to be a fairly unexplored problem, becomes evident. 
 For any LTI system G, the transfer function G(z) corresponds to the z-transform of the impulse response g0, g1,..., i.e., . For a transfer function G(z), we denote by Gn ? Rn×n 
 the lower triangular Toeplitz matrix having [g0 ··· gn-1]T as its first column. We write as a shorthand for the sequence {x1,...,xn} and, when convenient, we write in vector form as x1n , [x1 x2 ··· xn]T , where ()T denotes transposition. Random scalars (vectors) are denoted using non-italic characters, such as x (non-italic and boldface characters, such as x). For matrices we use upper-case boldface symbols, such as A. We write ?i(A) to the note the i-th smallest-magnitude eigenvalue of A. If An ? Cn×n, then 
 Figure 2.	Linear, causal, stable and time-invariant system G with input and output processes, initial state and output disturbance. 
 Ai,j denotes the entry in the intersection between the i-th row and the j-th column. We write , with i1 = i2 = n, to refer to the matrix formed by selecting the rows i1 to i2 of A. The expression m1[A]m2 corresponds to the square sub-matrix along the main diagonal of A, with its top-left and bottom-right corners on Am1,m1 and Am2,m2, respectively. A diagonal matrix whose entries are the elements in D is denoted as diagD 
 Consider the discrete-time system depicted in Fig. 2. In this setup, the input u81 is a random process and the block G is a causal, linear and time-invariant system with random initial state vector x0 and random output disturbance z81 . In vector notation, 
 where y¯n1 is the natural response of G to the initial state x0. We make the following further assumptions about G and the signals around it: 
 Assumption 1. G(z) is a causal, stable and rational transfer function of finite order, whose impulse 
 It is worth noting that there is no loss of generality in considering g0 = 1, since otherwise one can write G(z) as G'(z) = g0 ·G(z)/g0, and thus the entropy gain introduced by G'(z) would be logg0 plus the entropy gain due to G(z)/g0, which has an impulse response where the first sample equals 1. 
 Assumption 3. The disturbance z81 is independent of u81 and belongs to a ?-dimensional linear subspace, for some finite ? ? N. This subspace is spanned by the ? orthonormal columns of a matrix F ? R|N|×? 
 (where |N| stands for the countably infinite size of N), such that |h(FT z1 )| < 8. Equivalently, z1 = 
 Fs1?, where the random vector s1? ,FT z18 has finite differential entropy and is independent of u18. 
 As anticipated in the Introduction, we are interested in characterizing the entropy gain G of G in the presence (or absence) of the random inputs u, denoted by 
 In the next section we provide geometrical insight into the behaviour of for the situation where there is a random output disturbance and no random initial state. A formal and precise treatment of this scenario is then presented in Section IV. The other scenarios are considered in the subsequent 
 In this section we provide an intuitive geometric interpretation of how and when the entropy gain defined in (10) arises. This understanding will justify the introduction of the notion of an entropybalanced random process (in Definition 1 below), which will be shown to play a key role in this and in related problems. 
 Suppose for the moment that G in Fig. 2 is an FIR filter with impulse response g0 = 1, g1 = 2, gi = 0, ?i = 2. Notice that this choice yields G(z) = (z - 2)/z, and thus G(z) has one non-minimum phase zero, at z = 2. The associated matrix Gn for n = 3 is 
 whose determinant is clearly one (indeed, all its eigenvalues are 1). Hence, as discussed in the introduction, h(G3u13) = h(u13), and thus G3 (and Gn in general) does not introduce an entropy gain by itself. However, an interesting phenomenon becomes evident by looking at the singular-value decomposition (SVD) 
 of G3, given by G, where Q3 and R3 are unitary matrices and D3 , diag{d1,d2,d3}. In this case, D3 = diag{0.19394, 1.90321, 2.70928}, and thus one of the singular values of G3 is much smaller than the others (although the product of all singular values yields 1, as expected). As will be shown in Section IV, for a stable G(z) such uneven distribution of singular values arises only when G(z) has non-minimum phase zeros. The effect of this can be visualized by looking at the image of the cube 
 [0,1]3 through G3 shown in Fig. 3. If the input u13 were uniformly distributed over this cube (of unit 
 Figure 3.	Image of the cube [0,1]3 through the square matrix with columns [1 2 0]T, [0 1 2]T and [0 0 1]T. 
 volume), then G3u13 would distribute uniformly over the unit-volume parallelepiped depicted in Fig. 3, and hence h(G3u13) = h(u13). 
 Now, if we add to G3u13 a disturbance z13 = Fs, with scalar s uniformly distributed over [-0.5, 0.5] independent of u13, and with F ? R3×1, the effect would be to “thicken” the support over which the resulting random vector y31 = G3u13 +z13 is distributed, along the direction pointed by F. If F is aligned with the direction along which the support of G3u13 is thinnest (given by q3,1, the first row of Q3), then the resulting support would have its volume significantly increased, which can be associated with a large increase in the differential entropy of y31 with respect to u13. Indeed, a relatively small variance of s and an approximately aligned F would still produce a significant entropy gain. 
 The above example suggests that the entropy gain from u1n to yn1 appears as a combination of two factors. The first of these is the uneven way in which the random vector Gnu1n is distributed over Rn. The second factor is the alignment of the disturbance vector z1n with respect to the span of the subset {qn,i}i??n of columns of Qn, associated with smallest singular values of Gn, indexed by the elements in the set ?n. As we shall discuss in the next section, if G has m non-minimum phase zeros, then, as n increases, there will be m singular values of Gn going to zero exponentially. Since the product of the singular values of Gn equals 1 for all n, it follows that Qi/??n dn,i must grow exponentially with n, where dn,i is the i-th diagonal entry of Dn. This implies that Gnu1n expands with n along the span of {qn,i}i/??n, compensating its shrinkage along the span of {qn,i}i??n, thus keeping h(Gnun1) = h(u1n) for all n. Thus, as n grows, any small disturbance distributed over the span of {qn,i}i??n, added to Gnu1n, will keep the support of the resulting distribution from shrinking along this subspace. Consequently, the expansion of Gnu1n with n along the span of {qn,i}i/??n is no longer compensated, yielding an entropy increase proportional to log(Qi/??n dn,i). 
 The above analysis allows one to anticipate a situation in which no entropy gain would take place even when some singular values of Gn tend to zero as n ? 8. Since the increase in entropy is made possible by the fact that, as n grows, the support of the distribution of Gnu1n shrinks along the span of {qn,i}i??n, no such entropy gain should arise if the support of the distribution of the input u1n expands accordingly along the directions pointed by the rows {rn,i}i??n of Rn. 
 An example of such situation can be easily constructed as follows: Let G(z) in Fig. 2 have nonminimum phase zeros and suppose that u81 is generated as, where u˜81 is an i.i.d. random process with bounded entropy rate. Since the determinant of G-n1 equals 1 for all n, we have that h(u1n) = h(u˜1n), for all n. On the other hand, yn1 = GnG-n1u˜1n + z1n = u˜1n + z1n. Since z1n = [F]1ns1? for some finite ? 
 The preceding discussion reveals that the entropy gain produced by G in the situation shown in Fig. 2 depends on the distribution of the input and on the support and distribution of the disturbance. This stands in stark contrast with the well known fact that the increase in differential entropy produced by an invertible linear operator depends only on its Jacobian, and not on the statistics of the input [1]. We have also seen that the distribution of a random process along the different directions within the Euclidean space which contains it plays a key role as well. This motivates the need to specify a class of random processes which distribute more or less evenly over all directions. The following section introduces a rigorous definition of this class and characterizes a large family of processes belonging to it. 
 We begin by formally introducing the notion of an “entropy-balanced” process u81 , being one in which, for every finite ? ? N, the differential entropy rate of the orthogonal projection of un1 into any subspace of dimension n - ? equals the entropy rate of u. This idea is precisely in the following 
 Definition 1. A random process is said to be entropy balanced if, for every ? ? N, 
 for every sequence of matrices, with orthonormal rows.	N Equivalently, a random process {v(k)} is entropy balanced if every unitary transformation on vn1 yields 
 sequence yn1 means that one cannot predict its last ? samples with arbitrary accuracy by using its previous n - ? samples, even if n goes to infinity. 
 We now characterize a large family of entropy-balanced random processes and establish some of their properties. Although intuition may suggest that most random processes (such as i.i.d. or stationary processes) should be entropy balanced, that statement seems rather difficult to prove. In the following, we show that the entropy-balanced condition is met by i.i.d. processes with per-sample probability density function (PDF) being uniform, piece-wise constant or Gaussian. It is also shown that adding to an entropy-balanced process an independent random processes independent of the former yields another entropy-balanced process, and that filtering an entropy-balanced process by a stable and minimum phase filter yields an entropy-balanced process as well. 
 Proposition 1. Let u81	be a Gaussian i.i.d. random process with positive and bounded per-sample 
 Lemma 1. Let u81 be an i.i.d. process with finite differential entropy rate, in which each ui is distributed according to a piece-wise constant PDF in which each interval where this PDF is constant has measure greater than o, for some bounded-away-from-zero constant o. Then u81 is entropy balanced. N 
 Lemma 2. Let u81	and v81	be mutually independent random processes. If u81	is entropy balanced, then 
 The working behind this lemma can be interpreted intuitively by noting that adding to a random process another independent random process can only increase the “spread” of the distribution of the former, which tends to balance the entropy of the resulting process along all dimensions in Euclidean space. In addition, it follows from Lemma 2 that all i.i.d. processes having a per-sample PDF which can be constructed by convolving uniform, piece-wise constant or Gaussian PDFs as many times as required are entropy balanced. It also implies that one can have non-stationary processes which are entropy balanced, since Lemma 2 imposes no requirements for the process v81 . 
 Our last lemma related to the properties of entropy-balanced processes shows that filtering by a stable and minimum phase LTI filter preserves the entropy balanced condition of its input. 
 Lemma 3. Let u81 be an entropy-balanced process and G an LTI stable and minimum-phase filter. Then the output w is also an entropy-balanced process.	N 
 This result implies that any stable moving-average auto-regressive process constructed from entropybalanced innovations is also entropy balanced, provided the coefficients of the averaging and regression correspond to a stable MP filter. 
 We finish this section by pointing out two examples of processes which are non-entropy-balanced, namely, the output of a NMP-filter to an entropy-balanced input and the output of an unstable filter to an entropy-balanced input. The first of these cases plays a central role in the next section. 
 In this section we formalize the ideas which were qualitatively outlined in the previous section. Specifically, for the system shown in Fig. 2 we will characterize the entropy gain defined in (10) for the case in which the initial state x0 is zero (or deterministic) and there exists an output random disturbance of (possibly infinite length) z81 which satisfies Assumption 3. The following lemmas will be instrumental for that purpose. 
 Lemma 4. Let A(z) be a causal, finite-order, stable and minimum-phase rational transfer function with 
 Lemma 5. Consider the system in Fig. 2, and suppose z81 satisfies Assumption 3, and that the input process u81 is entropy balanced. Let Gbe the SVD of Gn, where Dn = diag{dn,1,...,dn,n} are the singular values of Gn, with dn,1 = dn,2 = ··· = dn,n, such that |detGn| = 1 ?n. Let m be the number of these singular values which tend to zero exponentially as n ? 8. Then 
 The previous lemma precisely formulates the geometric idea outlined in Section III. To see this, notice that no entropy gain is obtained if the output disturbance vector z1n is orthogonal to the space spanned by the first m columns of Qn. If this were the case, then the disturbance would not be able fill the subspace along which Gnu1n is shrinking exponentially. Indeed, if [Qn]1nz1n = 0 for all n, then 
 , and the latter sum cancels out the one on the RHS of (12), while limn?8 n1 h([Rn]1nu1n) = 0 since u81 is entropy balanced. On the contrary (and loosely speaking), if the projection of the support of z1n onto the subspace spanned by the first m rows of Qn is of dimension m, then h([Dn]1mRnu1n +[Qn]1mz1n) remains bounded for all 
 n, and the entropy limit of the sum  on the RHS of (12) yields the largest possible entropy gain. Notice that  (because det(Gn) = 1), and thus 
 this entropy gain stems from the uncompensated expansion of Gnu1n along the space spanned by the 
 Lemma 5 also yields the following corollary, which states that only a filter G(z) with zeros outside the unit circle (i.e., an NMP transfer function) can introduce entropy gain. 
 Corollary 1 (Minimum Phase Filters do not Introduce Entropy Gain). Consider the system shown in Fig. 2 and let u81 be an entropy-balanced random process with bounded entropy rate. Besides Assumption 1, suppose that G(z) is minimum phase. Then 
 Proof: Since G(z) is minimum phase and stable, it follows from Lemma 4 that the number of singular values of Gn which go to zero exponentially, as n ? 8, is zero. Indeed, all the singular values vary polynomially with n. Thus m = 0 and Lemma 5 yields directly that the entropy gain is zero (since the RHS of (12) is zero). 
 In this section we show that random disturbances satisfying Assumption 3, when added to the input u81 (i.e., before G), do not introduce entropy gain. This result can be obtained from Lemma 5, as stated in the following theorem: 
 Theorem 1 (Input Disturbances do not Introduce Entropy Gain). Let G satisfy Assumption 1. Suppose that u81 is entropy balanced and consider the output 
 where b18 = ?a1?, with a1? being a random vector satisfying h(a1?) < 8, and where ? ? R|N|×? has orthonormal columns. Then, 
 Proof: In this case, the effect of the input disturbance in the output is the forced response of G to it. This response can be regarded as an output disturbance z. Thus, the argument of the differential entropy on the RHS of (12) is 
 The proof is completed by substituting this result into the RHS of (12) and noticing that 
 Remark 1. An alternative proof for this result can be given based upon the properties of an entropybalanced sequence, as follows. Since det(Gn) = 1, ?n, we have that h(Gn(u1n + bn1)) = h(un1 + b1n). 
 Let Tn ? R?×n and  be a matrices with orthonormal rows which satisfy and such that  is a unitary matrix. Then 
 which is upper bounded for all n because h(a1n) < 8 and h(Tnu1n) < 8, the latter due to u81 being entropy balanced. On the other hand, since b1n is independent of u1n, it follows that h(u1n +bn1) = h(u1n), 
 We show here that the entropy gain of a transfer function with zeros outside the unit circle is at most the sum of the logarithm of the magnitude of these zeros. To be more precise, the following assumption is required. 
 Assumption 4. The filter G satisfies Assumption 1 and its transfer function G(z) has p poles and p zeros, m of which are NMP-zeros. Let M be the number of distinct NMP zeros, given by, i.e., such that |?1| > |?2| > ··· > |?M| > 1, with li being the multiplicity of the i-th distinct zero. We denote by ?(i), where ? : {1,... ,m} ? {1,...,M}, the distinct zero of G(z) associated with the i-th non-distinct zero of G(z), i.e., 
 As can be anticipated from the previous results in this section, we will need to characterize the asymptotic behaviour of the singular values of Gn. This is accomplished in the following lemma, which relates these singular values to the zeros of G(z). This result is a generalization of the unnumbered lemma in the proof of [15, Theorem 1] (restated in the appendix as Lemma 8), which holds for FIR 
 transfer functions, to the case of infinite-impulse response (IIR) transfer functions (i.e., transfer functions having poles). 
 where the elements in the sequence	an,l	are positive and increase or decrease at most polynomially 
 Theorem 2. In the system of Fig. 2, suppose that u81 is entropy balanced and that G(z) and z81 satisfy assumptions 4 and 3, respectively. Then 
 where ?¯ , min{?,m} and ? is as defined in Assumption 3. Both bounds are tight. The upper bound is achieved if limn?8 det([Qn]1?¯[F]1n([Qn]1?¯[F]1n)T ) > 0, where the unitary matrices Qhold the 
 Theorem 3. In the system of Fig. 2, suppose that u81	is entropy balanced and that G(z) satisfies 
 Assumption 4. Let z81 be a random output disturbance, such that z(i) = 0, ?i > m, and that. Then 
 Here we analyze the case in which there exists a random initial state x0 independent of the input u81 , and zero (or deterministic) output disturbance. 
 The effect of a random initial state appears in the output as the natural response of G to it, namely the sequence y¯n1. Thus, yn1 can be written in vector form as 
 This reveals that the effect of a random initial state can be treated as a random output disturbance, which allows us to apply the results from the previous sections. 
 Recall from Assumption 4 that G(z) is a stable and biproper rational transfer function with m NMP zeros. As such, it can be factored as 
 where P(z) is a biproper filter containing only all the poles of G(z), and N(z) is a FIR biproper filter, containing all the zeros of G(z). 
 We have already established (recall Theorem 1) that the entropy gain introduced by the minimum phase system P(z) is zero. It then follows that the entropy gain can be introduced only by the NMP-zeros of N(z) and an appropriate output disturbance y¯81 . Notice that, in this case, the input process w81 to N (i.e., the output sequence of P due to a random input u81 ) is independent of y¯81 (since we have placed the natural response y¯81 after the filters P and N, hose initial state is now zero). This condition allows us to directly use Lemma 5 in order to analyze the entropy gain that u81 experiences after being filtered 
 Theorem 4. Consider a stable p-th order biproper filter G(z) having m NMP-zeros, and with a random initial state x0, such that |h(x0)| < 8. Then, the entropy gain due to the existence of a random initial 
 Proof: Being a biproper and stable rational transfer function, G(z) can be factorized as 
 where P(z) is a stable biproper transfer function containing only all the poles of G(z) and with all its zeros at the origin, while N(z) is stable and biproper FIR filter, having all the zeros of G(z). Let C˜ nx0 and Cnx0 be the natural responses of the systems P and N to their common random initial state x0, respectively, where C˜ n,Cn ? Rn×p. Then we can write 
 Since P(z) is stable and MP, it follows from Corollary 1 that h(wn1) = h(u1n) for all n, and therefore 
 Therefore, we only need to consider the entropy gain introduced by the (possibly) non-minimum filter N due to a random output disturbance z1n = y¯n1 = NnC˜ nx0 +Cnx0, which is independent of the input wn1. Thus, the conditions of Lemma 5 are met considering Gn = Nn, where now N is 
 the SVD for Nn, and dn,1 = dn,2 = ··· = dn,n. Consequently, it suffices to consider the differential entropy on the RHS of (12), whose argument is 
 where vn1 , u1n + C˜ nx0 has bounded entropy rate and is entropy balanced (since C˜ nx0 is the natural response of a stable LTI system and because of Lemma 2). We remark that, in (35), vn1 is not independent of x0, which precludes one from using the proof of Theorem 2 directly. 
 On the other hand, since N(z) is FIR of order (at most) p, we have that C, where 
 Ep ? Rp×p is a non-singular upper-triangular matrix independent of n. Hence, Cnx0 can be written as 
 [F]1ns1p, where  and s1p , Epx0. According to (35), the entropy gain in (25) arises as long as h([Qn]1mCnx0) is lower bounded by a finite constant (or if it decreases sub-linearly as n grows). Then, we need [Qn]1m[F]1n to be a full row-ranked matrix in the limit as n ? 8. However, 
 where [Qn(p)]1m denotes the first p columns of the first m rows in Qn. We will now show that these 
 determinants do not go to zero as n ? 8. Define the matrix Qn ? Rm×(p-m) such that [Qn(p)]1m = . Then, it holds that ?x ? Rn, 
 Hence, the minimum singular value of [Q(np)]1m is lower bounded by the smallest singular value of 1[Qn]m, for all n = m. But it was shown in the proof of Theorem 3 (see page 39) that limn?8 ?min(1[Qn]m(1[Qn]m)T ) > 0. Using this result in (37) and taking the limit, we arrive to 
 is upper and lower bounded by a constant independent of n because v81 is entropy balanced, [Dn]1m has decaying entries, and h(sp1) < 8, which means that the entropy rate in the RHS of (12) decays to zero. The proof is finished by invoking Lemma 6. 
 Theorem 4 allows us to formalize the effect that the presence or absence of a random initial state has on the entropy gain using arguments similar to those utilized in Section IV. Indeed, if the random initial state x0 ? Rp has finite differential entropy, then the entropy gain achieves (3), since the alignment between x0 and the first m rows of Qn is guaranteed. This motivates us to characterize the behavior of the entropy gain (due only to a random initial state), when the initial state x0 can be written as [F]1ps1t, with t = p, which means that x0 has an undefined (or -8) differential entropy. 
 Corollary 2. Consider an FIR, p-order filter F(z) having m NMP-zeros, such that its random initial state can be written as x0 = Fs1t, where  and F ? Rp×t contains orthonormal rows . Then, 
 where {t¯} , min{m,t}. The upper bound in (42) is achieved when [Qn]1mCnF([Qn]m1CnF)T is a non-singular matrix, with Cn defined by y¯n1 = Cnx0 (as in Theorem 4). 
 Proof: The effect of the random initial state to the output sequence y81 can be written as yn1 = Cnx0, 
 remains bounded, for n ? 8, if and only if limn?8 det([Qn]1mCnF([Qn]1mCnF)T ) > 0. 
 Define the rank of [Qn]1mCnF as tn ? {1,... ,t¯}. If det([Qn]m1CnF([Qn]m1 CnF)T ) = 0, then the lower bound is reached by inserting (43) in (12). Otherwise, there exists L large enough such that tn = 1, 
 We then proceed as the proof of Theorem 2, by considering a unitary (m × m)-matrix Hn, and a (tn × m)-matrix An such that 
 that the lower limit in the latter sum equals t¯+1 when [Qn]1mCnFs1t is a full row-rank matrix. Replacing the latter into (12) finishes the proof. 
 Remark 2. If the random initial state x0 = Fs1t is generated with t = p - m, then the entropy gain introduced by an FIR minimum phase filter F is at least log?1. Otherwise, the entropy gain could be identically zero, as long as the columns of EnF(EnF)T fill only the orthogonal space to the span of the row vectors in [Qn(p)]1m, where En, F and [Qn(p)]1m are defined as in the proof of Theorem 4. 
 Both results, Theorem 4 and Corollary 2, reveal that the entropy gain arises as long as the effect of the random initial state aligns with the first rows of Qn, just as in the results of the previous section. 
 If there are no disturbances and the initial state is zero, then the first n output samples to an input un1 is given by (4). Therefore, the entropy gain in this case, as defined in (10), is zero, regardless of whether 
 Despite the above, there is an interesting question which, to the best of the authors’ knowledge, has not been addressed before: Since in any LTI filter the entire output is longer than the input, what would happen if one compared the differential entropies of the complete output sequence to that of the (shorter) input sequence? As we show next, a proper definition of this question requires recasting the problem in terms of a new definition of differential entropy. After providing a geometrical interpretation of this problem, we prove that the (new) entropy gain in this case is exactly (3). 
 Suppose u is uniformly distributed over [0,1]×[0,1]. Applying the conventional definition of differential entropy of a random sequence, we would have that 
 In other words, the problem lies in that although the output is a three dimensional vector, it only has two degrees of freedom, i.e., it is restricted to a 2-dimensional subspace of R3. This is illustrated in Fig. 4, where the set [0,1]×[0,1] is shown (coinciding with the u-v plane), together with its image through G? 2 (as defined in (45)). 
 As can be seen in this figure, the image of the square [0,1]2 through G? 2 is a 2-dimensional rhombus over which {y1,y2,y3} distributes uniformly. Since the intuitive notion of differential entropy of an ensemble of random variables (such as how difficult it is to compress it in a lossy fashion) relates to the size of the region spanned by the associated random vector, one could argue that the differential entropy of {y1,y2,y3}, far from being -8, should be somewhat larger than that of {u1,u2} (since the rhombus G? 2[0,1]2 has a larger area than [0,1]2). So, what does it mean that (and why should) h(y1,y2,y3) = -8? Simply put, the differential entropy relates to the volume spanned by the support of the probability density function. For y in our example, the latter (three-dimensional) volume is clearly zero. 
 Figure 4.	Support of u (laying in the u-v plane) compared to that of y = G?u (the rhombus in R3). 
 From the above discussion, the comparison between the differential entropies of y ? R3 and u ? R2 of our previous example should take into account that y actually lives in a two-dimensional subspace of R3. Indeed, since the multiplication by a unitary matrix does not alter differential entropies, we could consider the differential entropy of 
 where Q? T is the 3 × 2 matrix with orthonormal rows in the singular-value decomposition of G? 2 
 and q¯ is a unit-norm vector orthogonal to the rows of Q? (and thus orthogonal to y as well). We are now able to compute the differential entropy in R2 for y˜, corresponding to the rotated version of y such that its support is now aligned with R2. 
 The preceding discussion motivates the use of a modified version of the notion of differential entropy for a random vector y ? Rn which considers the number of dimensions actually spanned by y instead 
 Definition 2 (The Effective Differential Entropy). Let y ? Rl be a random vector. If y can be written as a linear transformation y = Su, for some u ? Rn (n = l), S ? Rl×n, then the effective differential entropy of y is defined as 
 It is worth mentioning that Shannon’s differential entropy of a vector y ? Rl, whose support’s lvolume is greater than zero, arises from considering it as the difference between its (absolute) entropy and that of a random variable uniformly distributed over an l-dimensional, unit-volume region of Rl. More precisely, if in this case the probability density function (PDF) of y = [y1 y2 ··· yl]T is Riemann integrable, then [17, Thm. 9.3.1], 
 where y? is the discrete-valued random vector resulting when y is quantized using an l-dimensional uniform quantizer with l-cubic quantization cells with volume ?l. However, if we consider a variable y whose support belongs to an n-dimensional subspace of Rl, n < l (i.e., y = Su = AT TCu, as in Definition 2), then the entropy of its quantized version in Rl, say Hl(y?), is distinct from Hn((Ay)?), the entropy of Ay in Rn. Moreover, it turns out that, in general, 
 despite the fact that A has orthonormal rows. Thus, the definition given by (50) does not yield consistent results for the case wherein a random vector has a support’s dimension (i.e., its number of degrees of freedom) smaller that its length  (If this were not the case, then we could redefine (50) replacing l by n, in a spirit similar to the one behind Renyi’s d-dimensional entropy [18].) To see this, consider the case in which u ? R distributes uniformly over [0,1] and y. Clearly, y distributes uniformly over the unit-length segment connecting the origin with the point . Then 
 The latter example further illustrates why the notion of effective entropy is appropriate in the setup considered in this section, where the effective dimension of the random sequences does not coincide with their length (it is easy to verify that the effective entropy of y does not change if one rotates y in Rl). Indeed, we will need to consider only sequences which can be constructed by multiplying some random vector u ? Rn, with bounded differential entropy, by a tall matrix G? n ? Rn×(n+?), with ? > 0 (as in (45)), which are precisely the conditions required by Definition 2. 
 Theorem 5. Let the entropy-balanced random sequence u81 be the input of an LTI filter G, and let y81 be its output. Assume that G(z) is the z-transform of the (? + 1)-length sequence {gk}?k=0. Then 
 Theorem 5 states that, when considering the full-length output of a filter, the effective entropy gain is introduced by the filter itself, without requiring the presence of external random disturbances or initial states. This may seem a surprising result, in view of the findings made in the previous sections, where the entropy gain appeared only when such random exogenous signals were present. In other words, when observing the full-length output and the input, the (maximum) entropy gain of a filter can be recasted in terms of the “volume” expansion yielded by the filter as a linear operator, provided we measure effective differential entropies instead of Shannon’s differential entropy. 
 Proof of Theorem 5: The total length of the output l, will grow with the length n of the input, if G is FIR, and will be infinite, if G is IIR. Thus, we define the output-length function 
 It is also convenient to define the sequence of matrices, where G? n Rl(n)×n is Toeplitz with 
 G? ni = 0,?i < j, hG? ni = gi-j,?i = j. This allows one to write the entire output yl1 of a causal 
 Let the SVD of G? n be G?, where Q? n ? Rn×l(n) has orthonormal rows, D? n ? Rn×n is 
 where the first equality follows from the fact that u1n can be written as Inu1n, which means that h?(u1n) = h(u1n). But 
 G? TnG? n = (Q? TnD? nR? n)T (Q? TnD? nR? n) = R? TnD? nQ? nQ? TnD? nR? n = R? nTD? n2R? n.	(61) 
 The product Hn ,G? TnG? n is a symmetric Toeplitz matrix, with its first column, [h0 h1 ··· hn-1]T , given by 
 Thus, the sequence corresponds to the samples 0 to n- 1 of those resulting from the complete convolution g*g-, even when the filter G is IIR, where g- denotes the time-reversed (perhaps infinitely large) response g. Consequently, using the Grenander and Szego¨’s theorem [19], it holds that 
 In order to finish the proof, we divide (58) by n, take the limit as n ? 8, and replace (63) in the 
 In this section we obtain a simpler proof of a result by Gray, Hashimoto and Arimoto [14]–[16], which compares the rate distortion function (RDF) of a non-stationary auto-regressive Gaussian process x81 (of 
 Figure 5.	Block diagram representation of how the non-stationary source x81 is built and then reconstructed as y = x+u. 
 a certain class) to that of a corresponding stationary version, under MSE distortion. Our proof is based upon the ideas developed in the previous sections, and extends the class of non-stationary sources for which the results in [14]–[16] are valid. 
 To be more precise, let  and be the impulse responses of two linear time-invariant filters A and A˜ with rational transfer functions 
 where |pi| > 1, ?i = 1,...,M. From these definitions it is clear that A(z) is unstable, A˜(z) is stable, and 
 |A(ej?)| = |A˜(ej?)|, ?? ? [-p,p]. Notice also that lim|z|?8 A(z) = 1 and lim z|?8 A˜(z) = 1/QMi=1 |pi|, and thus 
 Consider the non-stationary random sequences (source) x81 and the asymptotically stationary source x˜81 generated by passing a stationary Gaussian process w81 through A(z) and A˜(z), respectively, which can be written as 
 (A block-diagram associated with the construction of x is presented in Fig. 5.) Define the rate-distortion functions for these two sources as 
 where, for each n, the minimums are taken over all the conditional probability density functions 
 The above rate-distortion functions have been characterized in [14]–[16] for the case in which w81	is an i.i.d. Gaussian process. In particular, it is explicitly stated in [15], [16] that, for that case, 
 We will next provide an alternative and simpler proof of this result, and extend its validity for general (not-necessarily stationary) Gaussian w81 , using the entropy gain properties of non-minimum phase filters established in Section IV. Indeed, the approach in [14]–[16] is based upon asymptotically-equivalent Toeplitz matrices in terms of the signals’ covariance matrices. This restricts w81 to be Gaussian and i.i.d. and A(z) to be an all-pole unstable transfer function, and then, the only non-stationary allowed is that arising from unstable poles. For instance, a cyclo-stationarity innovation followed by an unstable filter A(z) would yield a source which cannot be treated using Gray and Hashimoto’s approach. By contrast, the reasoning behind our proof lets w81 be any Gaussian process, and then let the source be Aw, with A(z) having unstable poles (and possibly zeros and stable poles as well). 
 Theorem 6. Let w81	be any Gaussian stationary process with bounded differential entropy rate, and let 
 Thanks to the ideas developed in the previous sections, it is possible to give an intuitive outline of the proof of this theorem (given in the appendix, page 40) by using a sequence of block diagrams. More precisely, consider the diagrams shown in Fig. 6. In the top diagram in this figure, suppose that y = C x+u realizes the RDF for the non-stationary source x. The sequence u is independent of x, and the linear filter C(z) is such that the error (y-x) ? y (a necessary condition for minimum MSE optimality). The filter B(z) is the Blaschke product of A(z) (see (168) in the appendix) (a stable, NMP filter with unit frequency response magnitude such that x˜ = B x). 
 If one now moves the filter B(z) towards the source, then the middle diagram in Fig. 6 is obtained. By doing this, the stationary source x˜ appears with an additive error signal u˜ that has the same asymptotic variance as u, reconstructed as y˜ = Cx˜+u˜. From the invertibility of B(z), it also follows that the mutual information rate between x˜ and y˜ equals that between x and y. Thus, the channel y˜ = Cx˜ + u˜ has the same rate and distortion as the channel y = C x+u. 
 However, if one now adds a short disturbance d to the error signal u˜ (as depicted in the bottom diagram 
 Figure 6.	Block-diagram representation of the changes of variables in the proof of Theorem 6. 
 of Fig. 6), then the resulting additive error term u¯ = u˜ + d will be independent of x˜ and will have the same asymptotic variance as u˜. However, the differential entropy rate of u¯ will exceed that of u˜ by the RHS of (72). This will make the mutual information rate between x˜ and y¯ to be less than that between x˜ and y˜ by the same amount. Hence, Rx˜(D) be at most . A similar reasoning can 
 Here we revisit the setup shown in Fig. 1 and discussed in Section I. Recall from (7b) that, for this general class of networked control systems, it was shown in [12, Lemma 3.2] that 
 By using the results obtained in Section V we show next that equality holds in (7b) provided the feedback channel satisfies the following assumption: 
 Figure 7.	Top: The class of feedback channels described by Assumption 5. Bottom: an equivalent form. 
 A and B are stable rational transfer functions such that AB is biproper, ABP has the same unstable poles as P, and the feedback AB stabilizes the plant P. 
 F is any (possibly non-linear) operator such that ˜c , F(c) satisfies , for all n ? N, and 
 An illustration of the class of feedback channels satisfying this assumption is depicted on top of Fig. 7. Trivial examples of channels satisfying Assumption 5 are a Gaussian additive channel preceded and followed by linear operators [20]. Indeed, when F is an LTI system with a strictly causal transfer function, the feedback channel that satisfies Assumption 5 is widely known as a noise shaper with input pre and post filter, used in, e.g. [21]–[24]. 
 Theorem 7. In the networked control system of Fig. 1, suppose that the feedback channel satisfies Assumption 5 and that the input u81 is entropy balanced. If the random initial state of the plant P(z), with poles , satisfies |h(x0)| < 8, then 
 Proof: Let P(z) = N(z)/?(z) and T(z) , A(z)B(z) = G(z)/T(z). Then, from Lemma 9 (in the appendix), the output yn1 can be written as 
 where C˜ 0 maps the initial state s0 to yn1, C¯ n maps the initial state x0 to the output of G˜(z), and Cn maps the initial state x0 (of ?(z)) to yn1. Since u81 is entropy balanced and ˜c81 has finite entropy rate, it follows from Lemma 2 that u˜81 is entropy balanced as well. Thus, we can proceed as in the proof of Theorem 4 to conclude that 
 The feedback information capacity of this channel is realized by a Gaussian input x, and is given by 
 Figure 8.	Block diagram representation a non-white Gaussian channel y = x+z and the coding scheme considered in [13]. 
 where Kx1n is the covariance matrix of x1n and, for every k ? N, the input xk is allowed to depend upon the channel outputs  (since there exists a causal, noise-less feedback channel with one-step delay). 
 In [13], it was shown that if z is an auto-regressive moving-average process of M-th order, then CFB can be achieved by the scheme shown in Fig. 8. In this system, B is a strictly causal and stable finite-order filter and v81 is Gaussian with vk = 0 for all k > M and such that vn1 is Gaussian with a positive-definite covariance matrix KvM1 . 
 Here we use the ideas developed in Section IV to show that the information rate achieved by the capacity-achieving scheme proposed in [13] drops to zero if there exists any additive disturbance of length at least M and finite differential entropy affecting the output, no matter how small. To see this, notice that, in this case, and for all n > M, 
 since det(In+Bn) = 1. From Theorem 3, this gap between differential entropies is precisely the entropy gain introduced by In+Bn to an input z1n when the output is affected by the disturbance vM1 . Thus, from 
 Theorem 3, the capacity of this scheme will correspond to , where are the zeros of 1 + B(z), which is precisely the result stated in [13, Theorem 4.1]. 
 However, if the output is now affected by an additive disturbance d81	not passing through B(z) such that dk = 0, ?k > M and |h(d1M)| < 8, with d, then we will have 
 But limn?8 n1 (h((In + Bn)z1n + vn1 + dn1) - h((In + Bn)z1n + d1n)) = 0, which follows directly from applying Theorem 3 to each of the differential entropies. Notice that this result holds irrespective of how small the power of the disturbance may be. 
 Thus, the capacity-achieving scheme proposed in [13] (and further studied in [25]), although of groundbreaking theoretical importance, would yield zero rate in any practical situation, since every real signal is unavoidably affected by some amount of noise. 
 This paper has provided a geometrical insight and rigorous results for characterizing the increase in differential entropy rate (referred to as entropy gain) introduced by passing an input random sequence through a discrete-time linear time-invariant (LTI) filter G(z) such that the first sample of its impulse response has unit magnitude. Our time-domain analysis allowed us to explain and establish under what conditions the entropy gain coincides with what was predicted by Shannon, who followed a frequencydomain approach to a related problem in his seminal 1948 paper. In particular, we demonstrated that the entropy gain arises only if G(z) has zeros outside the unit circle (i.e., it is non-minimum phase, (NMP)). 
 This is not sufficient, nonetheless, since letting the input and output be u and y = Gu, the difference 
 is zero for all n, yielding no entropy gain. However, if the distribution of the input process 
 u satisfies a certain regularity condition (defined as being “entropy balanced”) and the output has the form y = Gu+z, with z being an output disturbance with bounded differential entropy, we have shown that the entropy gain can range from zero to the sum of the logarithm of the magnitudes of the NMP zeros of G(z), depending on how z is distributed. A similar result is obtained if, instead of an output disturbance, we let G(z) have a random initial state. We also considered the difference between the differential entropy rate of the entire (and longer) output of G(z) and that of its input, i.e.,, where ? + 1 is the length of the impulse response of G(z). For this purpose, we introduced the notion of “effective differential entropy”, which can be applied to a random sequence whose support has dimensionality smaller than its dimension. Interestingly, the effective differential entropy gain in this case, which is intrinsic to G(z), is also the sum of the logarithm of the magnitudes of the NMP zeros of G(z), without the need to add disturbances or a random initial state. We have illustrated some of the implications of these ideas in three problems. Specifically, we used the fundamental results here obtained to provide a simpler and more general proof to characterize the rate-distortion function for Gaussian non-stationary sources and MSE distortion. Then, we applied our results to provide sufficient conditions for equality in an information inequality of significant importance in networked control problems. Finally, we showed that the information rate of the capacity-achieving scheme proposed in [13] for the autoregressive Gaussian channel with feedback drops to zero in the presence of any additive disturbance in the channel input or output of sufficient (finite) length, no matter how small it may be. 
 yn?+1 ,Fnu1n. Then Kyn?+1 = su2FnFTn = su2In-?, where In-? is the (n-?)×(n-?) identity matrix. 
 Proof of Lemma 1: Let be the intervals (bins) in R where the sample PDF is constant. Let  be the probabilities of these bins. Define the discrete random process c81 , where c(i) = l if and 
 where the inequality is due to the fact that u1n and yn?+1 are deterministic functions of u1n, and hence c1n ?? u1n ?? yn?+1. Subtracting h(u1n) from (96) we obtain 
 where the last equality follows from Lemma 7 (see Appendix B) whose conditions are met because, given c1n, the sequence u1n has independent entries each of them distributed uniformly over a possibly different interval with bounded and positive measure. The opposite inequality is obtained by following the same steps as in the proof of Lemma 7, from (199) onwards, which completes the proof. 
 Proof of Lemma 2: Let yn1 , [?Tn|FTn]T wn1, where [?Tn|FTn]T ? Rn×n is a unitary matrix and where ?n ? R?×n and Fn ? R(n-?)×n have orthonormal rows. Then 
 Substituting this result into (102), dividing by n and taking the limit as n ? 8, and recalling that, since 
 where (wG)1n is a jointly Gaussian sequence with the same second-order moment as wn1. Therefore, 
 that wn1 has a bounded second moment at each entry w(i), and replacing the latter inequality in (102), 
 Proof of Lemma 3: Let yn1 , [?Tn|FTn]T wn1 where [?nT|FTn]T ? Rn×n is a unitary matrix and where ?n ? R?×n and Fn ? R(n-?)×n have orthonormal rows. Since wn1 = Gnu1n, we have that 
 Let ?nGn = AnSnBn be the SVD of ?nGn, where An ? R?×? is an orthogonal matrix, Bn ? R?×n has orthonormal rows and Sn ? R?×? is a diagonal matrix with the singular values of ?nGn. Hence 
 It is straightforward to show that the diagonal entries in Sn are lower and upper bounded by the smallest and largest singular values of Gn, say smin(n) and smax(n), respectively, which yields 
 where the last equality is due to the fact that u81	is entropy balanced. This completes the proof. 
 Proof of Lemma 4:	The fact that limn?8 ?n(AnATn) is upper bounded follows directly from 
 the fact that A(z) is a stable transfer function. On the other hand, An is positive definite (with all its eigenvalues equal to 1), and so AnATn is positive definite as well, with limn?8 ?1(AnATn) = 0. Suppose that limn?8 ?1(AnATn) = 0. If this were true, then it would hold that limn?8 ?n(A-n1A-nT ) = 8. But A-n1 is the lower triangular Toeplitz matrix associated with A-1(z), which is stable (since A(z) is minimum phase), implying that limn?8 ?n(A-n1A-1 T ) < 8, thus leading to a contradiction. This completes the proof. 
 Notice that wm1 = [Dn]1mRnu1n+[Qn]1mz1n. Thus, it only remains to determine the limit of as n ? 8. We will do this by deriving a lower and an upper bound for this differential entropy and show that these bounds converge to the same expression as n ? 8. 
 where (a) follows from including ¯z as well) to the conditioning set, while (b) and (d) stem from the independence between u81 and ¯z81 . Inequality (c) is a consequence of h(X +Y ) = h(X), and (e) follows from including ¯z to the conditioning set in the second term, and noting that  is not reduced upon the knowledge of zn1. 
 then, by inserting (127) and (126) in (118), dividing by n, and taking the limit n ? 8, we obtain 
 nlim?8 n1h(wnm+1 |wm1 ) = nlim?8 n1 h(un1) - Xi=1 logdn,i - h([Rn]m1 un1)!	(128) 
 where the last equality is a consequence of the fact that u81	is entropy balanced.	(129) 
 Notice that by Assumption 3, , and thus is restricted to the span of  of dimension ?n = ?, for all n = m + ?. Then, for n > m + ?n, one can construct a unitary matrix H, such that the rows of An ? R?×(n-m) span the space spanned by the columns of  and such that B 
 0. Therefore, from (133), h(wnm+1 |wm1 ) = logdet(m+1[Dn]n) + h(Hnxmn +1 + Hn(m+1[Dn]n)-1z¯mn +1) 
 where KAnxmn +1 and KAn(m+1[Dn]n)-1z¯mn +1 are the covariance matrices of A and A, 
 respectively, and where the last inequality follows from [26]. The fact that ?max(Kxmn +1) and ?max(K¯zmn +1) are bounded and remain bounded away from zero for all n, and the fact that ?min(m+1[Dn]n) either grows with n or decreases sub-exponentially (since the m first singular values decay exponentially to zero, with |detDn| = 1), imply in (134) that 
 Proof of Lemma 6: The transfer function G(z) can be factored as G(z) = G˜(z)F(z), where G˜(z) is stable and minimum phase and F(z) is stable with all the non-minimum phase zeros of G(z), both being biproper rational functions. From Lemma 4, in the limit as n ? 8, the eigenvalues of G˜ TnG˜ n are lower and upper bounded by ?min(G˜ T G˜) and ?max(G˜ T G˜), respectively, where 0 < ?min(G˜ T G˜) = 
 ?max(G˜ T G˜) < 8. Let G˜ and Fbe the SVDs of G˜ n and F n, respectively, with d˜n,1 = d˜n,2 = ··· = d˜n,n and dn,1 = dn,2 = ··· = dn,n being the diagonal entries of the diagonal matrices D˜ n, Dn, respectively. Then 
 Denoting the i-th row of Rn by rTn,i be, we have that, from the Courant-Fischer theorem [27] that 
 Notice that the columns of the matrix [Qn]1m[F]1n ? Rm×? span a space of dimension ?n ? {0,1,... ,?¯}, which means that one can have [Qn]1m[F]1n = 0 (if ?n = 0). In this case (i.e., if limn?8[Qn]1m[F]1n = 0) then the lower bound is reached by inserting the latter expression into (12) and invoking Lemma 6. 
 We now consider the case in which limn?8[Qn]1m[F]1n =6 0. This condition implies that there exists an N sufficiently large such that ?n = 1 for all n = N. Then, for all n = N there exist unitary matrices 
 The first differential entropy on the RHS of the latter expression is uniformly upper-bounded because u81 is entropy balanced, [Dn]1m has decaying entries, and. For the last differential entropy, 
 notice that [Dn]1mRn = 1[Dn]m[Rn]1m. Consider the SVD An1[Dn]m[Rn]1m = V TnSnW n, with V n ? R(m-?n)×(m-?n) being unitary, Sn ? R(m-?n)×(m-?n) being diagonal, and W n ? R(m-?n)×n having orthonormal rows. We can then conclude that 
 Recalling that A	and that Hn ? Rm×m is unitary, it is easy to show (by using the Cauchy 
 with equality achieved if and only if A . Substituting this into (151) and then the latter into (150) we arrive to 
 Substituting this into (12), exploiting the fact that u81 is entropy balanced and invoking Lemma 6 yields the upper bound in (25). Clearly, this upper bound is achieved if, for example, [Qn]1?¯[F]1n([Qn]1?¯[F]1n)T is non-singular for all n sufficiently large, since, in that case, ?n = ¯? and we can choose An = [I?¯ 0] 
 Proof of Theorem 3 : As in (28), the transfer function G(z) can be factored as G(z) = G˜(z)F(z), where G˜(z) is stable and minimum phase and F(z) is a stable FIR transfer function with all the nonminimum-phase zeros of G(z) (m in total). Letting u˜1n ,G˜ nu1n, we have that h(yn1) = h(F nu˜n1 + z1n), h(u˜1n) = h(u1n), and that  is entropy balanced (from Lemma 3). Thus, 
 This means that the entropy gain of Gn due to the output disturbance z81 corresponds to the entropy gain of F n due to the same output disturbance. One can then evaluate the entropy gain of Gn by applying Theorem 2 to the filter F(z) instead of G(z), which we do next. 
 Since only the first m values of z81 are non zero, it follows that in this case F = [Im |0]T (see Assumption 3). Therefore, det([Qn]1m[F]1n([Qn]1m[F]1n)T ) = det(1[Qn]m(1[Qn]m)T ) and the sufficient condition given in Theorem 2 will be satisfied for ? = m if limn?8 |det(1[Qn]m)| > 0, where now QnT is the left unitary matrix in the SVD F. We will prove that this is the case by using a 
 Then, there exists a sequence of unit-norm vectors, with vn ? Rm for all n, such that 
 such that an ? Rm and ßn ? Rn-m. Then, from this definition and from (156), we have that 
 where the last equality follows from the fact that, by construction,  is in the span of the first m rows of Qn, together with the fact that Qn is unitary (which implies that ). Since the top m 
 where we have applied (158) and the fact that smax(([F n]1m)T ) is bounded and does not depend on n. Now, notice that [F n]mn +1([F n]mn +1)T is a Toeplitz matrix with the convolution of f and f- (the impulse response of F and its time-reversed version, respectively) on its first row and column. It then follows from [28, Lemma 4.1] that 
 (the inequality is strict because all the zeros of F(z) are strictly outside the unit disk). Substituting this into (165) we conclude that 
 which contradicts (160). Therefore, (155) leads to a contradiction, completing the proof. 
 where b0 is the first sample in the impulse response of B(z). Notice that (169) implies that limn?8 n1 kBnu1nk2 = limn?8 n1 ku1nk2 for every sequence of random variables u81 with uniformly bounded variance. Since B(z) has only stable poles and its zeros coincide exactly with the poles of A(z), it follows that B(z)A(z) is a stable transfer function. Thus, the asymptotically stationary process x˜81 defined in (69) can be constructed as 
 where Bn is a Toeplitz lower triangular matrix with its main diagonal entries equal to b0. 
 The fact that B(z) is biproper with b0 as in (170) implies that for any u1n with finite differential entropy 
 For any given n = M, suppose that C(z) is chosen and x1n and u1n are distributed so as to minimize I(x1n;Cnx1n + u1n) subject to the constraint E[kyn1 - x1nk2] = E[k(Cn - I)x1nk2] + E[ku1nk]2 = D (i.e., x1n,u1n is a realization of Rx,n(D)), yielding the reconstruction 
 Since we are considering mean-squared error distortion, it follows that, for rate-distortion optimality, u1n must be jointly Gaussian with x1n. From these vectors, define 
 y˜n1 ,Bnyn1 = B1nCnBn-1x˜1n + u˜1n, (175) y¯n1 ,y˜n1 + d1n = B1nCnB-n1x˜1n + u˜1n + d1n. (176) 
 where d1n is a zero-mean Gaussian vector independent of (u˜1n,x˜1n) with finite differential entropy such 
 that dk = 0, ?k > M. Then, we have that2 nRx,n(D) = I(x1n;yn1) (=a) I(Bnxn1;Bnyn1) = I(x˜1n;y˜n1) (177) 
 where (a) follows from Bn being invertible, (b) is due to the fact that y˜n1 = Cnx˜1n+u˜1n, (c) holds because u1n ? x1n. The equality (d) stems from h(u˜1n) = h(u1n) - nG (see (172)). Equality holds in (e) because x˜1n ? (u˜1n,d1n) and in (f) because of (176). The last inequality holds because y¯n1 = y˜n1+d1n and dn1 ? y˜n1. But from Theorem 3, limn?8 n1 (h(u˜1n +d1m)-h(u1n)) = 0, and thus Rx,n(D) = limn?8 n1 (x˜1n;y¯n1)+G. 
 At the same time, the distortion for the source x˜1n when reconstructed as y¯n1 is 
 where (a) holds because kd1nk = kd1Mk is bounded, and (b) is due to the fact that, in the limit, B(z) is a unitary operator. Recalling the definitions of Rx˜(D) and Rx˜(D), we conclude that limn?8 n1 (x˜1n;y¯n1) = Rx˜,n(D), and therefore 
 purpose, consider now the (asymptotically) stationary source x˜1n, and suppose that yˆn1 = x˜1n +u1n realizes Rx˜,n(D). Again, x˜1n and u1n will be jointly Gaussian, satisfying yˆn1 ? un1 (the latter condition is required for minimum MSE optimality). From this, one can propose an alternative realization in which the error 2The change of variables and the steps in this chain of equations is represented by the block diagrams shown in Fig. 6. sequence is u˜ ,Bnu1n, yielding an output y˜n1 = x˜1n + u˜1n with y˜n1 ? u˜1n. Then 
 where (a) follows by recalling that yˆn1 = x˜1n + u1n and because yˆn1 ? un1, (b) stems from (172), (c) is a consequence of y˜n1 ? u˜1n, (d) follows from the fact that y˜n1 = x˜n1 +u˜n1. Finally, (e) holds because Bn is invertible for all n. Since, asymptotically as n ? 8, the distortion yielded by yn1 for the non-stationary source x1n is the same which is obtained when x˜1n is reconstructed as yˆn1 (recall (169)), we conclude that 
 Lemma 7. Let u81 be a random process with independent elements, and where each element ui is uniformly distributed over possible different intervals , such that amax > |ai| > amin > 0,?i ? N, 
 Proof: Without loss of generality, we can assume that ai > 1, for all i (otherwise, we could scale the input by 1/amin, which would scale the output by the same proportion, increasing the input entropy by nlog(1/amin) and the output entropy by (n - ?)log(1/amin), without changing the result). The input 
 vector u1n is confined to an n-box Un (the support of un1) of volume and has entropy 
 these k-boxes is determined by fixing n - k entries in u1n to ±ai/2, and letting the remaining k entries sweep freely over . Thus, the k-volume of each k-box is the product of the k support sizes ai of the associated selected free-sweeping entries. But recalling that ai > 1 for all i, the volume of each k-box can be upper bounded by. With this, the added volume of all the k-boxes contained in the original n-box can be upper bounded as 
 Fn ? R(n-?)×n have orthonormal rows. From this definition, y will distribute over a finite region 
 , corresponding to the projection onto the k-dimensional span of the rows of Fn. Hence,  is upper bounded by the entropy of a uniformly distributed vector over the same support, i.e., by , where  is the (n - ?)-dimensional volume of this support. In turn, 
 is upper bounded by the sum of the volume of all (? - k)-dimensional boxes contained in 
 the n-box in which u1n is confined, which we already denoted by, and which is upper bounded 
 where (a) follows because  is an orthogonal matrix. Letting (yG)1? correspond to the jointly Gaussian sequence with the same second-order moments as y?1, and recalling that the Gaussian distribution maximizes differential entropy for a given covariance, we obtain the upper bound 
 where (a) follows since the  are independent, and (b) stems from the fact that ?n ? R?×n has orthonormal rows and from the Courant-Fischer theorem [27]. Since  is bounded for all n, 
 we obtain by substituting (200) into (199) that . The combination of this with (198) yields , completing the proof. 
 We re-state here (for completeness and convenience) the unnumbered lemma in the proof of [15, Theorem 1] as follows: 
 Lemma 8. Let the function ? be as defined in (23) but for a transfer function G(z) with no poles and having only a finite number of zeros, m of which lie outside the unit circle. Then, 
 where the elements in the sequence {an,l} are positive and increase or decrease at most polynomially 
 Lemma 9. Let  be rational transfer function of order p with relative degree 1, with initial state x be a biproper rational transfer function of order t with initial state s0 ? Rt. Let 
 where the initial state of D(z) is x0 and the initial state of T/(TD +NG) can be taken to be [x0 s0]. 
 Combining the above recursions, it is found that y is related to the input u by the following recursion: 
 ﻿ We improve the achievable rate for causal and for zero delay source of stationary under an average mean squared error distortion measure . To begin with , we find a closed form expression for the information theoretic causal rate distortion function under such distortion measure , by , for first order Gauss . is a lower bound to the optimal performance theoretically attainable by any causal source code , namely . We show that , for , the latter can also be upper bounded as sample . In order to 
 analyze for arbitrary zero mean stationary , we introduce , the information theoretic causal when the reconstruction error is jointly stationary with the source . Based upon , we derive three closed form upper to the additive rate loss defined as , where . Two of these are strictly smaller than . sample at all . These differ from one another in their tightness and ease of evaluation ; the the bound , the more involved its evaluation . We then show that , for any source spectral density and any positive distortion , can be by an additive white noise channel surrounded by a unique set of causal , post , and feedback . We show that finding such a convex optimization problem . In order to solve the latter , we propose an iterative optimization procedure that the optimal and is to converge to . Finally , by a connection to feedback quantization , we design a causal and a zero delay scheme which , for , an 
 operational rate lower than and 
 sample , respectively . This that the among all zero delay source , by , is upper bounded as sample . 
 Causality , convex optimization , differential modulation , entropy quantization , noise shaping , rate distortion theory , sequential . 
 I . INTRODUCTION 
 I 
 N zero delay source , the reconstruction of each input sample must take place at the same time instant the corresponding input sample been . Zero delay source is desirable in many , e .., in real time where one cannot afford to have large , or in feedback , in which the current input on the previous . A notion closely related to the principle behind zero delay is that of causal source , wherein the reproduction of the present source sample only on the present and past source but not on the future source , . This notion does not preclude the use of entropy , and thus , it does not guarantee zero delay reconstruction . Nevertheless , any zero delay source code must also be causal . 
 It is known that , in general , causal cannot achieve the rate distortion function of the source , which is the optimal performance theoretically attainable in the absence of causality . However , it is in general not known how close to one can get when attention to the class of causal or zero delay source , except , for causal , when dealing with memory less , stationary at high resolution , or first order Gauss under a per sample mean squared error distortion metric . 
 For the case of memory less , it was shown by and Gilbert that the optimum rate distortion performance of causal source , say , is by time at most two memory less scalar by entropy . In this case , the rate loss due to causality was shown to be given by the space filling loss of the , i . e ., the loss is at most sample . For the case of stationary with memory and distortion , and that the information theoretic causal , here by 
 to be defined formally in Section and which , to as the distortion goes to zero , . The possible gap between the of causal source and this information theoretic causal was not assessed . Since operational data are lower bounded by the mutual information between the source and its reconstruction , we also have that . 
 . 
 On the other hand , for arbitrary stationary with finite differential entropy and under high resolution , it was shown in that the rate loss of causal i . e ., the difference between their and is at most the space filling loss of a uniform scalar . With the exception of memory less and first order Gauss , the price of causality at general rate for other stationary remains an open problem . However , it is known that for any source , the mutual information across an additive white noise channel and across a scalar entropy quantization channel do not exceed by more than . and . sample , respectively , . This immediately the and . 
 In causal source , it is generally difficult to provide a constructive proof of since random construction , which upon jointly long of source , is not directly applicable even in the case of memory less . Thus , even if one could obtain an outer bound for the achievable region based on an information theoretic , finding the inner bound , i . e ., the , would still remain being a challenge . 
 There exist other related to the information theoretic causal , in which is not . The minimum sum rate necessary to sequentially block encode and block decode two scalar correlated random under a coupled fidelity criterion was studied in . A closed form expression for this minimum rate is given in , Th . for the special case of a squared error distortion measure and a per variable as opposed to a sum or average distortion constraint . In , the minimum rate for causally and source under either a per sample or average distortion was given the name sequential rate distortion function . Under a per sample distortion constraint , it was also shown in ,. that for a first order Gauss source , a zero mean white process with variance , the information theoretic the form 
  
 for all . No are known for for higher order Gauss . Also , with the exception of memory less with its average distortion constraint than a per sample constraint , not been . 
 In this paper , we improve the inner and outer rate distortion for causal and for zero delay source of zero mean stationary and average distortion . We start by showing that , for any zero mean source with bounded differential entropy rate , the causal by less than approximately . sample . Then , we revisit the problem for first order Gauss under a per sample distortion constraint schedule and find the explicit expression for the corresponding by of an alternative , constructive derivation . This expression , which turns out to differ from the one found in , bottom of . , us to show that for first order 
 Gauss , the information theoretic causal for an average as opposed to per sample distortion 
 measure with . In order to upper bound for general stationary , we introduce the information theoretic causal when the distortion is jointly stationary with the source and denote it by . We then derive three closed form upper bounding to the rate loss 
 , which can be applied to any stationary random process . Two of these are , at all , strictly than the best previously known general bound of . sample . Since , by definition we have that 
  
 and thus all three bounding also upper bound the gap 
 . As we shall see , equality would hold in if could be by a test channel with distortion jointly stationary with the source , which a reasonable conjecture for stationary . 
 We do not provide a closed form expression for except for first order Gauss , and thus the upper bound on the right hand side of the bound in this paper is not analytically for the general case . However , we propose an iterative procedure that can be numerically and which one to evaluate , for any source power spectral density and 
 , with any desired accuracy . This procedure is based upon the iterative optimization of causal , post , and feedback around an channel . A key result in this paper and its second main contribution is showing that such filter optimization problem is convex in the frequency of all the . This that the mutual information rate between source and reconstruction by our iterative procedure monotonically to as the number of and the order of the tend to infinity . This equivalence between the solution to a convex filter design optimization problem and the troublesome minimization over mutual , thus making it possible to actually compute in practice , for general stationary . We then make the link between and the of causal and zero delay . More precisely , when the channel is by a uniform scalar by memory less entropy , the with the iterative procedure yield a causal source system whose operational rate is below sample . If the entropy coder in this system is restricted to encode individually as opposed to long of them , then this system zero delay operation with an operational rate sample . This directly into an upper bound to the of zero delay source , namely . To illustrate our , we present an example for a zero mean AR and a zero mean AR source , for which we evaluate the closed form and obtain an approximation of numerically by the iterative procedure herein . 
 This paper is organized as . In Section , we review some preliminary . We prove in Section that the for does not exceed the information theoretic by more than approximately . sample . Section the derivation of a closed form expression for for first order Gauss . In formally introduce and derive the three closed form upper bounding for the information theoretic rate loss of causality . Section the iterative procedure to calculate , after the proof of convexity that its convergence . The two are provided in Section . Finally , Section . Most of the of our are given in . 
 A . Notation 
 denote , respectively , the set of real and the set of real . and denote , respectively , the of and positive . We use lower case , such as , to denote scalar random , and and to denote and matrices , respectively . We use and to denote the , the column span , and the null space of the matrix , respectively . The expectation operator is by . The notation to the variance of . The notation a one sided random process , which may also be written simply as . We write to refer to the sequence . The of a wide sense stationary process is by 
 . Notice that . For any two we write the standard squared norm and inner product as and , 
 respectively , where complex conjugation . For one sided random and , the term the 
 mutual information rate between and , provided the limit . Similarly , for a stationary random process 
 , the differential entropy rate of . 
 . 
 A source pair a source into binary , from which a reconstruction is . The end to end effect 
 of any pair can be by a series of reproduction , such that , for every 
  
 where we write as a short notation for . Following , we say that an pair is causal if and only if it the following definition . 
 Definition Causal Source Coder : An pair is said to be causal if and only if its reproduction are such that 
  
  
 It also from Definition that an pair is causal if and only if the following chain for every possible random input process : 
  
 It is worth that if the are random , then this equivalent causality constraint must require that is satisfied for each realization of the 
 . 
 be the total number of that the received when it the output subsequence . Define as the random binary sequence that the that the received when is . Notice that is , in general , a function of all source , since the binary may be , i . e ., may be only after the received enough to reproduce 
 , with . We highlight the fact that even though may contain which depend on with , the random may still satisfy , i . e ., the pair can still be causal . Notice also a random variable , which on , the , and on the manner in which the source is into the binary sequence sent to the . 
 For further analysis , we define the average operational rate of an pair as 
  
 In the sequel , we focus only on the as the distortion measure . Accordingly , we define the average distortion associated with an pair as 
  
 The allow us to define the operational causal as : 
 Definition : is defined as 
  
  
 We note that the operational causal defined previously to the of all causal . 
 In order to find a meaningful information theoretical counter 
  
 Also , from the data inequality , it immediately that 
  
 where the last inequality turns into equality for a causal pair , since in that case . Thus , combining , , and 
  
 This lower bound the study of an information theoretic causal , as defined in the following . 
 Definition : The information theoretic causal for a source , with respect to the average distortion measure , is defined as 
  
 where the is over all such that and such that . 
 The definition is a special case of the nonanticipative epsilon entropy by and , which was shown to converge to , for stationary and in the limit as the rate goes to infinity , 
 . 
 In the case , it is known that for any source and for any single letter distortion measure , the the information theoretic . Unfortunately , such a strong equivalence between the and the information theoretic 
 does not seem to be possible in the causal case i . e ., for 
 . One exception is if one is to jointly and causally encode an asymptotically large number of parallel , of causal . Nevertheless , as outlined in Section I , it is possible to obtain lower and upper to the of causal from . Indeed , and to begin with , since , it directly from and that 
  
 The previous inequality in is strict , in general , and becomes equality when the source is white or when the rate to infinity . Also , as it will be shown in Section , for , does not exceed by more than 
 . sample , and thus an upper bound to can be from . 
 For completeness , and for future reference , we recall that for any distortion , the for a stationary source with is equal to the associated , given by the reverse water filling a 
 b 
 Although , in general , it is not known by how much , for stationary one can readily 
  
 can be causally . 
 More generally , it is known that , for any source , the mutual information across an channel which noise with variance , 
 most . sample , see , e .., . Thus , we have 
  
  
 Until now it been an open question whether a bound than can be for with memory and at general rate . In , we show that for , this is indeed the case . But before on upper for , its operational importance will be established by showing in the following section that , for , the does not exceed by more than approximately . sample . 
 . UPPER TO 
 In this section , we show that , for any source 
 and , an upper bound to can be readily from by approximately . per sample to . This result is first formally stated and proved for finite of any source . Then , it is extended to stationary . We start with two . 
 Definition : The causal information theoretic for a zero mean random vector of length is defined as 
  
 where the is taken over all output satisfying the causality constraint 
  
  
 Definition : The operational causal for a zero mean random vector of length is defined as 
  
  
 We will also need the following result , Lemma : 
 Lemma : Let be two random with zero mean and the same covariance matrix , i . e . and the same cross covariance matrix with respect to , that is If 
 and are jointly , and if any distribution , then 
  
 If furthermore , then equality is in if and only if with and being jointly . 
  
 Notice that if one Lemma to a reconstruction error with which the output sequence the causality constraint , then the version of the same reconstruction error will also produce an output causally related with the input . To see this , let 
  
 be the factorization of the covariance matrix of the random vector where is a lower triangular matrix . This one to write as 
  
 where satisfy . Then , there a set of reproduction satisfying the of Definition which generate each partial vector . Specifically , for any given , there a function , such that . From and given that 
 is lower triangular , we have that is fully determined by 
 and thus , for some function . From this and the fact that is independent of , we have that 
  
 where probabilistic independence . On the other hand , for each let be the minimum mean square error linear estimator of 
 given . Then , the notation for the left corner of a matrix , we have that 
  
  
 where from and all the subsequent stem from and from the fact that is lower triangular . Therefore , since the residual is uncorrelated to , it that 
  
 Now , since have the same second order statistics as , it from that 
  
 which , that is jointly with , that satisfy too . 
 We are now in the position to state the first main result of this section . 
 Lemma : 
  
 The proof of Lemma is in Section . 
 The result stated in Lemma for random vector is extended to stationary in the following theorem the second main result of this section . 
 Theorem : For a zero mean stationary source , and 
  
  
 The proof of Theorem can be found in Section . 
 The fact that for one to find upper to the of causal by explicitly finding or upper bounding . This is accomplished in the following . 
 . FOR FIRST ORDER GAUSS 
 In this section , we will find when the source is a Gauss process . More precisely , we will show that the information theoretic causal , which is associated with an average distortion constraint , with the expression for the on the of in for a per sample distortion constraint . To do so , and to provide also a constructive method of realizing the as well as , we will start by an alternative derivation of the for scalar source of length . In this case , from its definition in . . ,. , the the following form : 
  
 where the is over all conditional of given satisfying the causality constraint and the distortion schedule 
  
 Before proceeding , it will be convenient to introduce some . , to denote the random column vector and adopt the shorter notation . For any two random , 
 , we define 
 It was already stated in Lemma that the reconstruction vector which mutual information between a source vector and for any given distortion constraint , must be jointly with the source . This , in particular , for a realization of the with distortion schedule . In the next theorem , we will obtain an explicit expression for this and prove that in its realization , the sample 
  
 Fig . . Recursive Procedure at its th iteration . Starting from known covariance matrices , their next partial and are found . The indicate the step in the algorithm which the corresponding part of the matrix . 
 step is responsible of revealing the partial and by number in the figure . 
 The are formally stated in the following theorem , which also an exact expression for the of first order Gauss . 
 the . Under the latter interpretation , nothing one from choosing an arbitrarily large value for , say , yielding an arbitrarily large value for the second term in the summation on the of , which is , of course , inadequate . 
 We are now in a position to find the expression for for first order Gauss . This is done in the following theorem , whose proof is in Section . 
 Theorem : For a stationary process 
 where 
 The technique applied to prove and does not seem to be extensible to Gauss of order greater than . In the sequel , we will find upper to for arbitrary any order stationary . 
 V . CLOSED FORM UPPER 
 In order to upper bound the difference between and for arbitrary stationary , we will start this section by an upper bounding function for , by . then derive three closed form upper bounding to the rate loss , applicable to any stationary process . Two of these are 
 strictly smaller than . bit sample for all 
 . 
 We begin with the following definition . 
 Definition Causal Stationary : For a stationary , the information theoretic causal 
 jointly stationary with the source ; 
 chain . 
  
 Next , we derive three closed form upper bounding to that are applicable to arbitrary zero mean stationary with finite differential entropy rate . This result is stated in the following theorem , proved in Section : 
 Theorem : Let be a zero mean stationary source with with bounded differential entropy rate and variance . Let denote for given by , and let denote the quadratic for source uncorrelated for the source defined in . Let denote the information theoretic causal see Definition . Then , for all 
  
 are given in , respectively , 
 shown at the bottom of the page , where 
  
 with being any scalar with which and such that . 
 Notice that is independent of , being therefore numerically simpler to evaluate than the other bounding in Theorem . However , as is away from and , becomes very loose . In fact , it can be seen from a that for , the gap between and is actually upper bounded by , which is of course than , but one to evaluate . 
 It is easy to see that time between two causal an output process which causality with a rate distortion pair corresponding to the linear combination of , 
 . Thus , in some , one could get a bound than by considering the boundary of the convex hull of the region above and then . However , such bound would be much more involved to compute , since it to evaluate not only , but also the already convex hull . 
 It is also worth that the first term within the on the of becomes smaller when is reduced . This difference , which from inequality is 
  
 Fig . . channel within a perfect reconstruction system by the causal filter . 
 always , could be taken as a measure of the of the of especially when . Indeed , as a white process , to zero . 
 It can be seen from that the upper bound for the information theoretic among all so far . Although it does not seem to be feasible to obtain a closed form expression for , we show in the next section how to get arbitrarily close to it . 
 . 
 In this section , we present an iterative procedure that one to calculate with arbitrary accuracy , for any . 
 In addition , we will see that this procedure a characterization of the in a feedback that which sample . 
 A . Equivalent Problem 
 To derive the previously , we will work on a scheme of an channel and a set of causal , as in Fig . . In this scheme , the source is and stationary , with , and is assumed to 
 have finite differential entropy rate . In Fig . , the noise 
 is a zero mean process with i . i . independent of . Thus , between and the channel . The filter is stable and strictly causal , i . e ., it at least a one sample delay . The are causal and stable . The idea , to be in the remainder of this section , is to first show that with the that minimize the variance of the reconstruction error for a fixed ratio , the system of Fig . a mutual information rate between source and reconstruction equal to , with a reconstruction equal to . We will then show that finding such is a convex optimization problem , which naturally an iterative procedure to solve it . 
 In order to analyze the system in Fig . , and for notational convenience , we define 
  
  
  
 Fig . . Equivalent block diagram the output as the sum of 
 and , where is an i . i .. zero mean process independent of . 
 see Fig . . Therefore , is the signal transfer function of the system . 
 The perfect reconstruction condition a division , convenient of the optimization problem associated with it . On the one hand , because of , the net effect of the channel and the and is to introduce 
 colored stationary additive noise , namely , independent of the source . The of this noise is given by 
  
 The diagram in Fig . how the signal transfer function and the noise transfer function act upon and to yield the output process . 
 On the other hand , by looking at Fig . , one can see that also the role of a filter , which can be to reduce additive noise at the expense of linear upon the stationary corrupted by additive stationary noise 
 On the of , the first term is the variance of the additive , source independent , noise . The second term to the error due to linear distortion , that is , from the deviation of from a unit gain . 
 Since we will be interested in , for any given and , the and in Fig . are chosen so as to minimize in , while still satisfying . From the viewpoint of the subsystem comprised of the and and the channel , as an error frequency weighting filter , see . Thus , for any and , the that minimize are those in , Prop . , by setting in b equal to . With the minimizer in , the variance of the source independent error term is given by 
  
 On the other hand , the filter needs to be strictly causal and stable . As a consequence , it that 
  
 which from formula see also the Bode Integral Theorem in . 
 Thus , from and , if one to minimize the reconstruction by choosing appropriate causal in the 
 , and for any given and the 
 a 
 where the space of all frequency that can be with causal . 
 Now , we can establish the equivalence between 
 , if the , 
  
 From the lemma , whose proof can be found in Section , one can find either by the minimization in Definition or by Optimization Problem . In the following , we will pursue the latter approach . As we shall see , our formulation of Optimization Problem a convenient of its decision . In fact , it defined in a with respect to the set of all causal frequency involved . That result can be directly from the following key lemma , proved in Section : 
 Lemma : Define the of 
 We can now prove the convexity of Optimization Problem . 
 Lemma : For all and for all , Optimization Problem is convex . 
 Proof : With the change of in , we obtain , see . With this , Optimization Problem to finding the and that 
 a 
 b 
 where 
  
 Clearly , the space of frequency associated with causal transfer is a convex set . This that is a convex set . In addition , is also a convex set , and from Lemma , is a convex functional . Therefore , the optimization problem stated in , and thus 
 Optimization Problem , are convex . This the proof . 
  
 B . Finding Numerically 
 Lemma and the in Optimization Problem allow one to define an iterative algorithm that , as will be shown later , the information theoretic causal . Such algorithm is in iterative Procedure . 
  
 Step : Return to step . 
 Notice that after Step in the first iteration of Procedure , the is comprised of only additive noise independent of the source . Step then the by source independent noise at the expense of linear distortion . Each step the until a local or global minimum of the is . Based upon the Problem , the following theorem , which is the main technical result in this section , convergence to the global minimum of the , say , for a given end to end mutual information . Since all the in Optimization Problem are causal , the mutual information at this global minimum is equal to . 
 Theorem Convergence of Iterative Procedure : Iterative Procedure monotonically to the unique and that realize . More precisely , denote the 
 Indeed , after Step for the first time , the resulting rate is the quadratic for source uncorrelated in see also . 
  
 Fig . . Uniform scalar and dither forming an , the channel of the system from Fig . . 
  
 Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . 
 after the th iteration of Iterative Procedure at a target rate , we have that 
  
 and 
  
  
 Proof : The result directly from the fact that Optimization Problem is strictly convex in and , which was shown in Lemma , and from Lemma . 
 The theorem that the stationary information theoretic causal can be by Iterative Procedure . In practice , this that an approximation arbitrarily for a given can be if sufficient of the procedure are carried out . 
 The feasibility of running Iterative Procedure on being able to solve each of the minimization sub involved in and . We next show how these can be . 
 C . Step 
 If is given , the minimization problem in Step of Iterative Procedure is equivalent to a feedback design problem with the constraint and with error weighting filter . Therefore , the solution to Step is given in closed form by , , and b , where in b is by . The latter 
  
 Fig . . in sample and several upper bounding for for zero mean unit variance white noise through . The resulting source variance is . . 
 in characterize the frequency response of the optimal , and given . The existence of rational transfer and arbitrarily close in an sense to such frequency response is also shown in . 
 D . Step 
 Finding the causal frequency response that for a given is equivalent to 
  
 for a given , where is as defined in . Since are convex , is a convex optimization problem . As such , its global solution can always be found iteratively . In particular , if is constrained to be an th order finite impulse response FIR filter with impulse response , such that the discrete time 
 transform , then 
  
 is a convex functional . The latter directly from the convexity of and the linearity of . As a consequence , one can solve the minimization problem in Step , to any degree of accuracy , by over the of the impulse response of , standard convex optimization see , e .., . This approach also the benefit of being amenable to numerical computation . 
 It is interesting to note that if the order of the filter were not a restricted , then , after Iterative Procedure 
 to , the is the causal 
 magnitude of back distance , see , e .., ,. . The inequality in 
 Wiener filter i . e ., the causal estimator for the noisy signal that comes out of the perfect reconstruction system that . Notice also that one can get the system in Fig . to yield a realization of Iterative Procedure by simply to be . This would yield a system equivalent to the one that was analytically in . An important observation is that one could not obtain a realization such a system in one step by simply a Wiener filter by the causal estimator that is , a causal Wiener filter . To see , and would no longer be optimal for . One would then have to change , and then again , and so on , thus to carry out infinitely many recursive optimization . However , a causally truncated version of the non causal Wiener filter that could be used as an alternative starting guess in Step of the iterative procedure . 
 E . Sample Causally 
 If the channel in the system of Fig . is by an , as shown in Fig . , then instead of the noise 
 , we will have an i . i .. process independent of , whose are uniformly distributed over the quantization interval . The dither signal , by , is an i . i .. sequence of uniformly distributed random , independent of the source . Let be the output of the . Denote the resulting input and the output to the , before and after the dither , respectively , as and , and let be the quantization noise by the . Notice that the of are independent , both mutually and from the source . However , unlike and , the and are not , since they contain of the uniformly distributed process . We then have the following . 
 Theorem : If the scheme shown in Fig . the by Iterative Procedure , and if long of the output of this system are entropy conditioned to the dither in a memoryless fashion , then an operational rate satisfying 
  
 the last line of is strict since the distribution of is not . 
 The result directly by combining with Lemma and Theorem . 
 , and linear time invariant a reconstruction error jointly stationary with the source , it that the operational rate distortion performance of the feedback thus is within 
 from the best performance achievable by any pair within this class . 
 Remark : When the rate goes to infinity , so does . In that limiting case , the transfer function to unity , and it from that the optimal asymptotically satisfy 
 . 
 Moreover , when , the system of Fig . which , in this asymptotic regime , with 
 , with tending to . 
 F . Sample With Zero Delay 
 If the requirement of zero delay , which is than that of causality , was to be satisfied , then it would not be possible to apply entropy to long of . This would entail an excess bit rate not greater than bit per sample , see , e .., , Sec . . . Consequently , we have the following result . 
 Theorem : The of zero delay , say , can be upper bounded by the operational rate of the scheme of Fig . when each output value is entropy independently , conditioned to the current dither value . Thus 
  
 The . sample in , commonly to as the space filling loss of scalar quantization , can be reduced by vector quantization , . Vector quantization could be applied while causality and without delay if the of the source were dimensional . This would also allow for the use of entropy over 
 dimensional of , which the extra bit sample at the end of to sample see , Th . . . . 
 G . Additive Rate Loss of Causality From Two 
 It is worth that Lemma and the an interesting fact : the rate loss due to causality for with memory , i . e ., the difference between the of causal and , is upper bounded by the sum of two . The first term is . sample , and from the space filling loss associated with scalar quantization , as was also pointed out in for the high resolution situation . This term is associated only with the . For a scalar stationary source , such excess rate can only be by jointly of consecutive source vector quantization , i . e ., by for or by several parallel . The second term can be to the reduced of causal , to those of or smoothing . The contribution of the causal filtering aspect to the total rate loss is indeed . This latter gap can also be associated with the performance loss of causal . 
 As a final remark , we note that the architecture of Fig . , which us to pose the search of as a convex optimization problem , is by no the only scheme capable of the upper and . For instance , it can be shown that the same performance can be removing either or in the system of Fig . , provided an entropy coder with infinite memory is used . Indeed , the theoretical among causal of the differential pulse code , with estimation at the end , been shown in a different setting . 
 . EXAMPLE 
 To illustrate the upper in the previous , we here evaluate and , and calculate an approximation of via Iterative Procedure , for two zero mean AR and AR . These were by the recursion 
  
 where the of the process are i . i .. zero mean unit variance random . 
 Iterative Procedure was carried out by to be an eight tap FIR filter . For each of the target considered , the procedure was stopped after four complete . 
 The first order source Source was chosen by setting the of the in to be . This to zero mean , unit variance white noise through the coloring transfer function . The second order source Source of zero mean , unit variance white noise through the coloring transfer function . The resulting upper for Source and Source are shown in . and , respectively . As by and , all the upper for derived in to 
 in the limit of both large and small i . e ., when and , respectively . 
 For both , the gap between and is significantly smaller than . sample , for all at which was . Indeed , this gap is smaller than . 
 bit sample for both . 
 For the first order source , the magnitude of the of the FIR filter rapidly with coefficient index . For example , when running five of Iterative Procedure , a tenth order FIR filter for , for Source at sample , the was 
 Such fast decay of the impulse response of that , at least for AR , there is little to be by be an FIR filter of order . It is worth that , in the iterative procedure , the initial guess for is a unit scalar gain . The frequency response magnitude of is plotted in Fig . , together with and the resulting frequency after for a target rate of . 
 Notice that for Source , after four of Iterative Procedure , the for are almost identical to 
 , according to . This that Iterative Procedure fast convergence . For example , when four of Iterative Procedure to Source with a target rate of . sample , the after each iteration were . , . , . , and . , respectively . For the same source with a target rate of . sample , the distortion took the . , . , . , and . as the . A similar behavior is for other target , and for other of in as well . Thus , at least for AR , one close to the global optimum after just three . 
 . CONCLUSION 
 In this paper , we have and upper to the causal and zero delay rate distortion function for stationary and as the distortion measure . We first that for with bounded differential entropy rate , the causal does not exceed the information theoretic by more than approximately . sample . After that , we derived an explicit expression for the information theoretic under per sample distortion a constructive method . This result was then for a closed form formula for the causal information theoretic of first order Gauss under an average distortion constraint . 
 We then derived three closed form upper bounding to the difference between and . Two of these bounding are than the previously best known bound of . sample , at all . We also provided a fourth upper bound to that is constructive . More precisely , we provide a practical scheme that this bound , based on a noise shaped predictive coder of an channel surrounded by , post , and feedback . For a given source spectral density and desired distortion , the design of the is convex in their frequency . We an iterative algorithm , which is to converge to the optimal set of unique . Moreover , the mutual information across the channel , monotonically to . Thus , one to solve the more complicated minimization of the mutual information over all possible conditional satisfying the distortion constraint . To achieve the upper on the operational , one may simply replace the channel by a scalar and memoryless entropy conditioned to the dither . 
 . PROOF OF LEMMA 
 We will first show that can be by a vector channel between two square matrices . It was already established in Lemma that an output to a realization of only if it is jointly with the source 
 . From this condition , the estimator of 
 from , say , is given by 
  
 where the inverse of from the fact that bounded differential entropy . It is clear from and the joint between and that the causality condition is satisfied if and only if the matrix 
  
 On the other hand , the distortion constraint can be expressed as 
  
 From the definition of , for every , there an 
 output vector jointly with such that and satisfy , and 
  
 We will now describe a simple scheme which is capable of the joint statistics between and any given jointly with satisfying . 
 Suppose is first by a matrix yielding the random vector . Then a vector with i . i .. with unit variance , independent from , say , is added to , to yield the random vector . Finally , this result is by a matrix to yield the output 
  
 On the other hand , the joint second order statistics between and are fully by the matrices 
  
  
 It can be seen from these that all that is for the system previously to reproduce any given pair of covariance matrices is that the matrices and 
 satisfy 
  
  
  
  
 Thus , can be chosen , for example , as the lower triangular matrix in a factorization of . With this , a tentative solution for could be as , which would satisfy if and only if . The latter if and only if recall that is nonsingular since bounded differential entropy . We will now show that this condition actually by a contradiction argument . Suppose 
 . Since , the former supposition is equivalent to . If this were the case , then there would exist such that and . The latter , combined with , would imply 
 . One could then construct the scalar random variable , which would have nonzero variance . The of from is given by 
  
 From this , and in view of the fact that is with nonzero variance , we conclude that would be unbounded . However , by construction , the chain , and therefore by the Data Inequality we would have that 
 , that is unbounded too . This the assumption that is a realization of , leading to the conclusion that . There 
 fore , the choice 
  
 is to satisfy , and thus for every , there exist matrices and which yield an output vector satisfying . 
 On the other hand , we have that 
  
 The first equality from the data inequality and the fact that is from . To prove that the second equality in , we will prove that 
 . We first have , from , that , which combined with the identity immediately that 
  
 Second , we note that and that can be decomposed 
 as 
  
 where the orthogonal projection operator onto a given subspace . Since and is orthogonal to the other two on the of , we have that 
  
 where the last equality from the fact that is i . i .., which that is independent of the other two in the expression . On the other hand , from 
  
 Thus , we have , shown at the bottom of the page , where comes from the data inequality , and 
 from the fact that and from . To complete the proof of the second equality in , we note that the data inequality also . 
 Therefore , if and yield an output such that 
 , then . 
 Finally , and and replace the noise by the vector of noise with unit variance by independently operating , with their being jointly conditioned to the dither , then the operational data rate would be upper bounded by 
 where is the output of the channel . Since the distortion by the is the same as that with the original channel , we conclude that 
  
 X . PROOF OF Theorem We will start by showing that 
  
 First , following exactly the same proof as in Lemma in the Appendix , it is straightforward to show that 
  
 Now , consider the following family of . For some positive integer , the entire source sequence is in of contiguous . and of each block is independent of the and of any other block . As in the scheme in the second part of the proof of Lemma , each source block is by the optimal matrix , the resulting block being and parallel and independent , with their jointly entropy conditioned to the dither . When , the result is then by the optimal post matrix in the proof of Lemma . 
 For such an pair , and from , the operational rate after have been reconstructed is 
  
 where rounding to the nearest integer since the th sample is reconstructed only after of length are . On the other hand , since the variance of each reconstruction error sample cannot be than the variance source , we with the first is upper bounded as 
  
 where rounding to the nearest smaller integer . Therefore , for any finite , the average distortion of this scheme when i . e ., when we consider the entire source process . Also , from and we conclude that 
  
 for every finite . Our aim is to use this result to show that . Since is valid only for 
 finite of , we must resort to the convergence of as . First of all , since is bounded , 
  
 such 
 that 
  
 Since 
 XI . PROOF OF THEOREM 
 From Lemma , for any given reconstruction error covariance matrix , the mutual information is if and only if the output is jointly with the source . In addition , for any information between and a jointly 
 , the variance of every reconstruction error sample is if and only if is the estimation error resulting from from , that is , if and only if 
  
 which for are 
 dent , and therefore 
  
 Thus , hereafter we restrict the analysis to output jointly with and causally related also satisfy . For any such output process , say the following : 
  
 . 
 and thus equality in if and only if the following chain is satisfied : 
  
 Finally , and follow because for all 
 . 
 Thus , the mutual information of every output that 
 is a candidate to constitute a realization of 
 is lower bounded by the of , which in turn only on the error associated with . We shall now see that this lower bound is by a unique set of error , and then show that the resulting bound is achievable while these error . , we have that 
 With this , and since the of when any error variance , the minimum value of the of subject to the 
  
 is when these satisfy , for 
 see . Therefore , for all causally related to and jointly with satisfying the distortion con 
 Now , we will show that for any distortion schedule , the output by the recursive algorithm of Procedure is such that the lower bound , thus being a realization of . 
 We will first demonstrate that the causality chain 
  
 and the , and Source Past Independence which are necessary and sufficient to attain equality in . 
 Causality Condition : Let 
 pose causality . Then , since , it from that the top left square is lower triangular , being 
 given by 
  
 Then , Step of the algorithm is equivalent to 
  
  
  
 This that the top in the th column of depend only on the of above its th row . that , we conclude that is also lower triangular , and thus also causality . Notice that for any given causality up to sample , the by Step is the only vector consistent with satisfying causality up to the th sample . 
 : for . , and mean all . Therefore , the reconstruction vector by the above algorithm for all . 
 Source Past Independence : Since all are jointly , condition is equivalent to 
 From , and it that 
 . Substitution of this into and 
 the result into directly to . Thus , is satisfied for all . 
 Since the above algorithm an output which , , and , for all , this output equality in , thus being a realization of . Notice that once the are given , each step in the recursive algorithm the only and that satisfy , and . Therefore , for any given distortion schedule , the latter algorithm the unique output that . This the proof . 
 . PROOF OF THEOREM 
 Consider the first of input and output . The average distortion constraint here the form 
  
 Then , we have , shown at the bottom of the page , where the last inequality from inequality and the fact that is a convex function of . Equality is if and 
  
 . Given that the of is when constraint is active i . e ., by making , we can attain equality in and minimize its by 
  
 For this choice to be feasible , the distortion must satisfy 
 , which into the constraint 
  
 . PROOF OF THEOREM 
 The first inequality in directly from and . For a plain channel with noise variance , the mutual information between source and reconstruction is 
  
 In both the end to end distortion can be reduced by a scalar gain after the test channel . The optimal minimum gain is . The mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it . However , now the resulting end to end distortion is . Therefore , for a given end to end distortion , the distortion between the source and the signal before the optimal scalar gain is 
  
 the distortion in when is substituted by , we find that 
  
  
  
 where from , , and and by that , from , and from inequality . Notice that the of the first term on the of . 
 The middle term on the of directly from . Finally , for close to , a bound than can be from a as : 
  
 b 
 which is precisely the third term on the of . In the above , a trivially since , and 
 b from inequality . Therefore , equality in b if and only if is white . The validity of the chain of in directly from and . This the proof . 
 . PROOF OF LEMMA 
 The idea of the proof is to first show that if the distortion 
  
  
 Immediately afterward we prove that , despite the distortion and causality , the scheme in Fig . enough of freedom to turn all the above into . That that if we are able to globally over the of the system while satisfying the distortion and causality , then that , say , must satisfy 
 . 
 We now proceed to demonstrate the validity of and to state the under which are . The first equality in from the fact that is a i . i .. process . Inequality from the following : 
  
  
  
  
  
  
  
  
  
  
  
 where is the signal at the output of , see Fig . . In the above , from the fact that and are independent and from the fact that is strictly causal . As a consequence , is independent of , for all . Inequality from the property , with equality if and only if and are independent , i . e ., if and only if is white . Similarly , since the of are independent . By that is a linear combination of and , it immediately that is independent from upon knowledge of , which to . On the other hand , from the fact that 
 . Equality in from the fact that , if is known , then can be from 
 , and vice , see Fig . . Equality from the fact that there no feedback from to , and thus the chain . On the other hand with equality if and only if is invertible for all for which . Finally , directly from the 
 Data Inequality , with equality if and only if 
 is invertible for all for which . 
 Since is by definition an , it that , for every , there an output process jointly with , satisfying the causality and distortion and such that . 
 Such output can be by its noise , say , and its signal transfer function , say , by the model in Fig . . 
 Therefore , all that is for the system in Fig . to achieve 
  
 chosen in a for simplicity and because , as we shall see next , we have enough of freedom to do so without compromising rate distortion performance . the system of formed by a , c , and b , we obtain 
  
 that their squared equal their in . To do so , we will make use of the Wiener theorem Theorem in the Appendix . 
 To begin with , we notice from Fig . , and since is 
  
 Substitution of the of the second equation of c into the above , together with the Wiener theorem , that there a causal , stable and minimum phase transfer 
 that satisfy , throughout and 
  
 where 
  
  
  
 all not satisfying . This the proof . 
 APPENDIX 
 Lemma : For any zero mean stationary source 
  
  
 Proof : Suppose does not hold , i . e ., that 
  
 for some . The definition of in that there such that 
  
 Combining this inequality with , we arrive to 
  
  
 Since can be chosen to be arbitrarily small , it can always be chosen so that , which . Therefore 
 . 
 Lemma : Let 
  
 where lated to 
  
 Then , for any first order Gauss source , the following : 
  
  
 Proof : In Lemma in the Appendix , it is shown that 
  
 so all we need to demonstrate is that 
 . To do this , we simply observe 
 from Theorem that if we construct an output process by the recursive algorithm of that theorem , with the choice , for all , then this output process is such that . 
 Therefore concluding the proof . 
 Proposition Column Correspondence : Let be a random vector source with covariance matrix . A reconstruction random vector 
  
 if and only if 
  
  
 Proof : We have that 
  
 The proof is by that if and only if . 
 Lemma Triangular Correspondence : Let 
 , with , be a random source vector with covariance matrix . A reconstruction random vector 
  
 if and only if 
  
  
 Proof : Let us first introduce the notation , the top left of any given square matrix 
 , with . From Proposition , it immediately that , for every 
  
 which is equivalent to . 
 Lemma that , if the reconstruction is the output of a causal Wiener filter applied to the noisy source for some noise vector a condition equivalent to , then and have identical on and above their main . 
 Wiener Theorem : 
 Theorem see ,. : Let be a function defined on . There a unique stable , causal and minimum phase transfer function such that if and only if 
  
  
 and is such that , then 
  
  
 Proof : Let . From inequality and the fact that , we have 
 In ,. , it is stated that is a sufficient condition for such a to exist . However , from , Note ,. and the discrete continuous equivalence in ,. , it that is also necessary . 
  
  
 ﻿The authors present an improved achievable rate region for two-pair bidirectional Gaussian relay networks based on successive compute-and-forward method. In these networks, one relay helps in the communication between two pairs of users. In their proposed scheme, the authors use nested lattice codes for encoding the messages at the users, and Gaussian random codes for the encoding at the relay. They use the successive compute-and-forward strategy to decode two integer linear combinations of the lattice codewords in the uplink, and successive interference cancellation for decoding the Gaussian codewords in the downlink. The downlink channel can be considered as a broadcast channel with two receiver groups, but within each group, a pair of users is considered as an additive white Gaussian noise channel (instead of a broadcast channel) because each node knows its own transmitted message. It is shown that for all channel gains of downlink channels and all channel gains of symmetric uplink channel pairs, the strategy achieves rates to within constant gaps of 1/2 and 3/4 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted models, respectively. These gaps are tighter than those previously obtained for this network, which have not exploited the successive compute-and-forward method.
  
  
 1	Introduction
 Nowadays, the capacity of wireless networks is one of the main research topics in network information theory. As an interesting and important application, two-way communications via relays have been introduced, where the two-way relay channel (TRC) is used as a building block. Several coding strategies were proposed for the TRC in [1–3], by extending and combining the basic methods for the relay channel [4], such as decode-and-forward (DF), compress-and-forward (CF) and amplify-and-forward (AF). When using CF or AF, the effect of the noise remains on the signal transmitted by the relay. In contrast, by applying DF at the relay, the noise is completely removed from the signal. However, DF suffers from multiplexing loss [2].
 Compute-and-forward is another strategy proposed for the TRC which uses lattice codebooks. In this method, the receiver need not decode messages directly; instead, it computes a linear equation of the messages based on the fact that integer combinations of codewords also belong to the codebook [5–12]. In [5], an achievable scheme was proposed for the TRC based on nested lattice codes, which was shown to achieve within 1/2 bit from the cut-set bound, for all channel parameters. This strategy attains noise suppression without multiplexing loss. In [6], the capacity of the TRC with relay private messages has been achieved within constant gaps of 2/3 and 5/6 bit/s/Hz per user from the single-sided genie-aided upper bound for restricted and non-restricted channel models, respectively.
 The extension of the TRC approaches to networks has been done in [8–10], where more than two users communicate via a relay in a multi-directional manner. The Gaussian multi-way relay channel (MRC) was studied in [8], where upper and lower bounds for the capacity were given. In this setting, users are divided into several clusters, where each user in a cluster has a single message intended to other users in the same cluster. A similar symmetric setup with one cluster and equal channel gains, noise variances and power constraints was considered in [9], where the sum capacity was obtained. In [10], a type of three-user Gaussian MRC was considered, where each user has two independent messages, each of them intended to one of the other users.
 In [11], a compute-and-forward approach was proposed to exploit interference between the transmitters in an additive white Gaussian noise (AWGN) network. In their approach, relays are free to select integer coefficients that match the channel coefficients as closely as possible, thus reducing the effective noise and increasing the achievable rates. In [12], the successive compute-and-forward approach was presented, where after decoding a linear combination, the relay can combine it with its channel observation to obtain a new effective channel that can be better for decoding the next targeted linear combination.
 In [13], the capacity regions of two-pair two-way relay networks (TPTRNs) for both linear shift deterministic and complex Gaussian channel models were obtained. In these networks, a relay node facilitates the communication between two pairs of users. It was shown that the cut-set upper bound is tight for the linear shift deterministic channel model. The strategy for the deterministic channel was translated to a specific superposition of lattice codes and random Gaussian codes at the source nodes and successive interference cancellation at the receiving nodes for the Gaussian network. It was shown that for all channel gains, that strategy achieves the cut-set upper bound to within 3 bit/s/Hz per user. In that approach, the lattice relations of [14] were exploited, so it was necessary for them to assume equal power constraints for lattice codebooks of one pair, and to use Gaussian codebooks for the excess part of the transmitted signal with extra power. However, the authors did not show that successive interference cancellation was actually feasible for lattice and Gaussian codewords together.
 In this paper, we study two-pair two-way (bidirectional) relay networks, using the same model considered in [13], where one relay can help the communication between two pairs of users (see Fig. 1). For brevity, here we name it TPTRN. Here, we present a larger achievable rate region for a class of full-duplex Gaussian TPTRN by exploiting successive compute-and-forward method, which has the ability to efficiently decode multiple linear combinations of the signals. In contrast to the method proposed in
  
 Fig. 1	Asymmetric TPTRN
 [13], which is based on the superposition of Gaussian random codes and lattice codes at the senders, and successive interference cancellation at the decoders, in our method, it is enough to consider only lattice codes for encoding at the users, and successive compute-and-forward for decoding at the relay in the uplink. In the downlink, Gaussian random codes are used for encoding at the relay, and successive interference cancellations are applied for decoding at the users. Moreover, in contrast to [8], we do not use timesharing and assume that all users can transmit simultaneously. It is shown that for all downlink channel gains and symmetric uplink channel pairs, the proposed method achieves within 1/2 and 3/4 bit/s/Hz per user of cut-set upper bound for restricted and non-restricted TPTRN, respectively, which are tighter than the gaps 2 and 3 bit/s/Hz per user obtained in [13].
 The remainder of this paper is organised as follows. In Section 2, the system model, the cut-set upper bound and some preliminaries about compute-and-forward strategies are reviewed. The achievable rate region for Gaussian TPTRN is characterised in Section 3. Capacity gap calculations and some simulations are presented in Sections 4 and 5, respectively. Some concluding remarks are provided in Section 6.
 2	System model and definitions
 In this section, we review the system model for the Gaussian asymmetric TPTRN and its associated cut-set upper bound. We also review some concepts about lattice codes, and outline the successive CF strategy. The material of this section, presented for completeness, is a brief summary of the notions and results from [13–15], in which the interested reader can find a detailed exposition. Consider an asymmetric TPTRN as shown in Fig. 1, where nodes 1 and 2 (a pair of users) can communicate with each other via node r (a relay). There is no direct link between users. The same situation applies for nodes 3 and 4. The relay is assumed to be able to listen and transmit at the same time. We use a real-valued AWGN channel model for all channels in this network. Let log+(·) = max (log2(·), 0), C(x) = 1/2log2(1 + x), U = 1, 2, 3, 4, S = 1, 2, 3, 4, r and for each i ? U, let Oi denote the node on the other side of the relay which communicates with i. The forward and backward channels of each pair are assumed to be different. Channel gains are written on each link in Fig. 1 as hij, where i, j ? S and i ? j.
 Node i ? U has messages miVi [ MiVi = {1, 2, ..., M} to be delivered to node Oi, where M [Z. Each user is assumed to choose its transmit message independently of the other users. The user at node i ? U encodes its message into a codeword xni = [xi1, xi2, ..., xin] [Rn using an encoding function xik = fik(miVi , yki -1), for k ? 1, …, n, where n is the codeblock length (in channel uses), and the messages of the relay node r are encoded into a codeword xnr = [xr1, xr2, ..., xrn] [Rn using an encoding function xrk = frk(ykr-1), for k ? 1, …, n, xik is a realisation of a real random variable Xik, satisfying the power constraint 1/n nk=1 E  P for i ? S, and yik -1 [yi1, yi2, ..., yi,k-1]alldenotes the previously received symbols at nodenodes transmit at the same time, =beginningi. Notethe that
 transmission of each codeword simultaneously. In this setup, called non-restricted encoding, some dependency between the signals transmitted by different users can be introduced. In contrast, in restricted encoding, users apply the encoding functions xni = fi(miVi ), thus yielding independent transmitted signals for all i.
 The rate RiVi of an (M, n) code is defined (in bit/s/Hz) as
 1
 RiVi =  log2M n
 The rate RiVi is said to be achievable if, for any ? > 0 and for all n sufficiently large, there exists an (M, n) code with M = 2nRiVi such that the probability of error is less than ?.
 The received signals at node i ? U and at the relay node r are given by the following equations, respectively
 yik = hrixrk + zik,	k = 1, 2, ..., n	(1)
 4
 yrk =	hirxik + zrk,	k = 1, 2, ..., n	(2)
 i=1
 where zik is a realisation of independent and identically distributed real-valued Gaussian noise N˜ (0, 1) (i.e. with zero mean and unit variance). For each i ? U, hri and hir denote the downlink and uplink channel gains, respectively. Each node i ? U, uses a decoding function gi to decode mVii as mˆ Vii = gi(yni , miVi ), where yni = [yi1, yi2, ..., yin].
 2.1	Cut-set upper bound
 The cut-set upper bound for the TPTRN was obtained in [13]. We rewrite a reduced and equivalent version of it for both restricted and non-restricted cases in the following theorem and corollary. For the proof and further details, the reader can refer to [13].
 Theorem 1: The capacity region of the non-restricted Gaussian TPTRN is upper bounded by
 RiVi = minC(|hir|2P), C(|hrVi |2P),	i [ U	(3)
 RiVi + RkVk = min{C((|hir| + |hkr|)2P),
 	C((|hrVi |2 + |hrVk |2)P)},	i, k [ U,	i =Vk	(4)
 Corollary 1: The capacity region of the restricted Gaussian TPTRN is upper bounded by
 	RiVi = minC(|hir|2P), C(|hrVi |2P),	i [ U	(5)
 RiVi + RkVk = min{C((|hir|2 + |hkr|2)P),	(6) C(max(|hrVi |2, |hrVk |2)P)},	i, k [ U, i =Vk
 Remark 1: In [13, lemma 1], it is proved that (4) and (6) are within 1 bit/s/Hz per user of each other. The latter was shown by the
 following inequalities
 2C{(|h1r| + |h2r|)2P} = 2C{(2|h1r|2 + 2|h2r|2)P}
 = 2C{(|h1r|2 + |h2r|2)P} + 1
 (7)
 2C{(|hr1|2 + |hr2|2)P} = 2C{max(2|hr1|2, 2|hr2|2)P}
 = 2C{max(|hr1|2, |hr2|2)P} + 1
 When the channel is complex valued, the capacity corresponds to 2C (·) so the previous inequalities yield a gap of 1 bit/s/Hz for two users or, equivalently, 1/2 bit/s/Hz per user. However, in our problem, we have assumed real-valued AWGN channels, for which the capacity is simply C(·). Thus, the gap between upper bounds for restricted and non-restricted channels would be 1/2 bit/s/Hz for two users or, equivalently, 1/4 bit/s/Hz per user.
 2.2	Compute-and-forward approach
 Next, we review some preliminaries on lattice codes and the compute-and-forward approach. For more details see [11–15] and the references therein.
 A nested lattice code is defined in terms of two n-dimensional lattices Lnf and Lnc, which form a lattice partition Lnf /Lnc, that is, Lnc #Lnf . (Lnf and Lnc are usually called fine and coarse lattices, respectively.) The nested lattice code is a lattice code which uses Lnf as codewords and the Voronoi region of Lnc as a shaping region which is denoted by Vc. For Lnf /Lnc, the set of coset leaders is defined as C = {Lnf mod Lnc} = Lfn > Vc. Lnc is chosen so that its average power per dimension is P, that is, the second moment per dimension of a random variable uniformly distributed over Vc is P. The coarse and fine lattices should be good for quantisation and channel coding, respectively (for definition of lattice goodness see [15]). The coding rate of the nested lattice code is defined as
 1
 R =  log|C|	(8) n
 Let d, called dither, be a random variable uniformly distributed over c	C
 . Given the message t [ , the dithered encoder sends
 	X = [t - d] mod Lnc	(9)
 In [12, Theorem 2], an achievable rate was proposed for real-valued AWGN networks based on the compute-and-forward strategy to decode one linear combination of codewords. In [12], the successive compute-and-forward method was proposed, where decoding more than one linear combination of codewords is desirable. We rephrase these theorems in the following theorem for the case of L transmitters and one receiver.
 Theorem 2: For real-valued AWGN networks with L transmitters, one receiver and channel coefficient vectors h = [h1, h2, ..., hL] [ RL and equation coefficient vector a = [a1, a2, ..., aL] [ZL, the following computation rate is achievable
 	Ral = 1 log+?? a2 -	 +(hTa)2hP2-1??,	al = 0	(10)
 	2	1	P
 To successfully decode two linear combinations, for example, a, b [ZL, the following computation rate is achievable (see (11))
 Finally, the rate of transmitter l is obtained as
 Rl = minl (Ral , Rbl ), al, bl = 0 (12) a ,bl
 In other words, the relay can first aim to obtain a linear combination that is easy to decode and then use it to create a better effective channel for decoding the second linear combination [12]. In Theorem 2, a and b are selected according to the design options where decoding different linear combinations of messages are desirable. In the next section, we present an application for that approach.
 3	Achievable rate for the Gaussian TPTRN
 In this section, we present an achievable rate region for the Gaussian TPTRN. In our scheme, we consider restricted encoding functions. The obtained achievable rate region is also an achievable region for the non-restricted Gaussian TPTRN.
 In the uplink, based on the fact that the TPTRN consists of two TRC pairs, we use the idea of successive interference cancellation for lattice codes introduced in [11] which was reviewed in the previous section. By considering appropriate values for a and b in (10) and (11), the linear function of the messages of each pair is decoded, respectively, at the relay.
 We would like to mention that in [13] the superposition of Gaussian and lattice codes from four transmitters is received at the relay in the TPTRN. The relay then uses the successive interference cancellation method (originally thought for Gaussian codewords) for all codewords, regardless of the fact that the codewords are of different kinds, that is, they have used the equation of [14] for decoding the lattice codes of each pair but they consider the other codewords as interferences in the Gaussian rate equation. There is no proof for this performance and it does not seem to be a correct approach.
 In contrast, we use the interference cancellation method that has been proposed for lattice codes in [11]. Moreover, in this method, nested lattice codes and dithered encoding have been used instead of lattice codes alone. The motivation for this is that the former method has been shown in [5] to yield the rate log+(1/2 + SNR) for equal power constraints which is higher than the one proposed in [14] which is log+(SNR).
 In the downlink, we extend the method proposed for the TRC in [5] to the two pairs of TRCs present in the TPTRN. In this method, each pair of TRCs can be treated as an AWGN channel instead of a broadcast channel. This is due to the fact that each user of each pair knows its own signal, thus it can subtract this part from the received signal and can extract the message of the other user of that pair. This method yields higher rates than the method used in [13] which considers TPTRN as a broadcast channel with four receivers. The full strategy is explained next.
 3.1	Encoding at the users
 According to the described strategy, the transmit signal at user i ? U is given by xniVi, which denotes a dithered lattice code ensemble of nested lattice codewords (Lnj Vn) with size 2nRiVi , where Ln ,Ln1 ,Ln2 ,Ln3 ,Ln4. ?n should be good for quantisation and its second moment per dimension is P. {Lni }4i=1 should be good for channel coding. We consider the following uplink channel vector between users and the relay
 	h =  h1r h2r h3r h4r	(13)
 3.2	Uplink: decoding at the relay
 The relay decodes two linear combinations of lattice code ensembles t = a1xn12 + a2xn21 and f = b3x34n + b4xn43, with the following parameters
 a =  a1 a2 0 0, b = 0 0 b3 b4 ,	a1, a2 [Z b3, b4 [Z	(14)
 (15)
 	 	
 Rbl =	???? b2 - (aab)2 - 1 +(PP(h- h(a2h-/(aTah)2)/a)ba)2?? ?? bl = 0
 	T	-1
 The coefficient vectors a and b are decoded by successive compute-and-forward individually with transmitter rates as shown by the following equations
 Rt		 	2	1	
 	(	a	h	h	a h	)
 (16)
 	?	2	2	a2h1r)2P?
 Rf
 	2	???	P(	b4)	???
 (17)
 These bounds can be obtained via the following computations
 	aTh	T	T
 (h -   a2 a) b = h b
  a2 = a21 + a22
  h2 = h21r + h22r + h32r + h24r hTb = b3h3r + b4h4r hTa = a1h1r + a2h2r aTh = hTa aTb = 0
 By substituting the above relations in (10) and (11), (16) and (17) are obtained, respectively. The details are shown below
 t?	2	(hTa)2P -1??	2 R? a -1+P h2
 2
 	=21 log+a12 +a22 -	 (a1h1r +a2h2r) P	P-1
  
 1
 =2 log+(a12 +a22)(11++((hh1223rr ++hh2242rr)+P)h+23r (+a1hh242rr)P-a2h1r)2P-1
 =21log+ )(11++((hh2123rr ++hh2242rr)+P)h+23r (+a1hh242rr)P-a2h1r)2P
 (18)
 	T	2	T	2	T	2 -1
 Rf ???b2 -(aab)2 -1+P((Ph-h(a2h-/(aTah)2)/a)ab) 2?? ??
 1
 =2 log+??b2 - a2(1P+Pah2(h2T)b-)2P(aTh)2-1??
 -1
 1
 =2 log+??????????b32 +b24 -  P(a( a12 +1a2a2)(122r)(b3h2(h3r31rr+2b4hr4)rP)2)????? ?????
 +(a h -a h ) P
 =12log+????(a12 +Pa(a221221)(1++a2222()(hb23r4+h13rh2-24rr)bP3)h2+4r)1(2ra+12h(2br 32-+a2bh241)r)2P?????
 	 	(a +a +(a h -a h ) P)
 (19)
 Remark 2: As stated in [11], the closer a = [a1 a2 a3 a4] and h = [h1r h2r h3r h4r] are, the higher the rate obtained by computing (10). However, in our problem, the two last elements of a are fixed to be zero, that is, (a3, a4) = (0, 0), so as shown in the simulation section, not necessarily those values of (a1, a2) which are closer to (h1r, h2r) result in higher rates.
 Remark 3: Consider the case where each pair of TRCs in the TPTRN has symmetric uplink channel gains, that is, h1r = h2r and h3r = h4r. For this case, decoding a = [1100] and b = [0011] yields higher rates as shown by the following computations
 Rt   (a21 + a22)(11 ++((hh1232rr ++hh2224rr)+P)h+23r (+a1hh422rr)P- a2h1r)2P
 	 	
 	 1		1	
 (20)
  2	b4)
 (21)
 More generally, let us consider the channel gain vector as h = [va1rh1r va2rh2r va3rh3r va4rh4r], where 0 =air = 1 for all i. It is possible to find 0 =afr, atr = 1, hfr and htr such that we have symmetric uplink channels for each pair of TRCs, that is,
 vv	= v	= v	andh4r =
 a h	h = h r, we can consider	r and
 . For this case, by assuming = [1100] and = [0011], (16) and (17) are simplified as
 	Rt	 2log+12 + 1 +a2tra|hfrtr||h2Pfr|2P	(22)
 	Rf	  afr|hfr|2P	(23)
 3.3	Encoding at the relay
 The relay maps two lattice codes t and f to Gaussian codewords {xnt }, {xnf } from codebooks of sizes {2nRrt } and {2nRrf }, respectively. The signal transmitted by the relay is given by
 	xnr = art xnrt + arfxrnf , art +arf = 1	(24)
 3.4	Downlink: decoding at the users
 The two following cases of all different orders of downlink channel gains are sufficient for the analysis to be carried out. The other cases can be obtained and will lead to similar conclusions by changing the indices.
 3.4.1 Case |hr2| = |hr1| = |hr4| = |hr3|: It is shown that, for any choice of channel gains, this can be done successfully as long as the following equations are satisfied.
 Decoding at user 2
 =+arf |htr|2|2P| 
 R12 = C ar |hr2|2P	(26)
 R34, R43	Car hr2 2P	(25) 1  t	
 Proof: User 2 first decodes xnf by treating xnt as noise. This can be done with arbitrarily small probability of error as long as (25) is satisfied. Then, the signal xnf is cancelled from the received signal and xnt is decoded. This can be done with arbitrarily small probability of error as long as (26) is satisfied, because by the same conclusion reached in [5] for TRC, in the downlink, each pair of TRCs in the TPTRN can be treated as an AWGN channel instead of a broadcast channel. As explained before, this is due to the fact that each user of each pair knows its own signal, thus it can subtract this part from the received signal and can extract the message of the other user of that pair. Hence, the TPTRN can be interpreted as a broadcast channel with two receiver groups (1, 2) and (3, 4) instead of a broadcast channel with four receivers, as was considered in [13].	?
 The proof of the other following parts can be done similarly.
 Decoding at user 1
 =+arf |htr|1|2P| 
 	R34, R43	Car hr1 2P	(27)
 1
 Decoding at user 3	R21 = Cart |hr1|2P	(28)
 	=	 +arf |htr|3|2P| 
 	R43	Car hr3 2P	(29)
 1
 Decoding at user 4
 	=	 +arf |htr|4|2P| 
 	R34	Car hr4 2P	(30)
 1
 Clearly, (25) and (27) are redundant in view of (29) and (30), respectively, because C(x/1 + x) is an increasing function of x.
 3.4.2 Case |hr2| = |hr4| = |hr1| = |hr3|: It is shown that for any choice of channel gains, this can be done successfully as long as the following equations are satisfied.
 Decoding at user 2
 =+arf |htr|2|2P| 
 	R34, R43	Car hr2 2P	(31)
 1
 Decoding at user 4	R12 = Cart |hr2|2P	(32)
 	=	 +arf |htr|4|2P| 
 	R34	Car hr4 2P	(33)
 1
 Decoding at user 1
 	=	 +art |hfr|1|2P 
 	R21	Car hr1|2P	(34)
 1
 Decoding at user 3
 =+arf |htr|3|2P| 
 	R43	Car hr3 2P .	(35)
 1
 Clearly, (31) is redundant in view of (33) and (35).
 4	Capacity gap calculations
 In this section, we compute the capacity gaps between the achievable rates and the cut-set upper bound by using the described strategy. For the uplink, we obtain the capacity gaps by considering symmetric channel gains for each pair of TRCs. For the downlink, we consider the general case.
 Lemma 1: (Uplink): For the TPTRN with symmetric uplink channel gains for each pair of TRCs, for any rate tuple r = rf, rt satisfying
 rt = C(|htr|2P) - 1/2	(36)
 rf = C(|hfr|2P) - 1/2	(37)
 rt + rf = C((|htr|2 + |hfr|2)P) - 1	(38)
 there exists a choice of power assignments such that decoding of
 codewords with rates {error probability for all channel gains.rij}i[U,j=Vi can be done with arbitrary small
 Proof: From (23) we have
 = 22|rf -| 1/2 (37)= 1 + |hfr||2P|/2 - 1/2 =
 afr	2	2	1	(39) hfr	P	hfr	P
 From (22) and (23) we have
 = 2 · 22rf|(22|rt - 1/2) = 2 · 22(|rf +r|t) - 22rf atr	2	2 htr	P	htr	P
 (40) (38)= 1 + (|hfr|2|+ ||htr|2)P/2 - 1 = |htr||2P|- 1/2 =
 2	2 1 htr P	htr P
 Table 1 Optimised values for(a1, a2),(b3, b4),Rt andRf for each channel coefficient vector h
 No.	hUL	(a1, a2)	(b3, b4)	Rt = max
 (R12, R21)	Rf = max
 (R34, R43)
 1	[8, 7, 5, 1]	(1, 1)	(2, 1)	0.67	0.37
 2	[2, 4, 1, 3]	(1, 1)	(1, 1)	0.12	0.18
 3	[4, 8, 1, 3]	(1, 2)	(1, 1)	0.36	0.44
 4	[8, 16, 1, 3]	(1, 2)	(1, 1)	1.29	0.44
 5	[20, 40, 1, 3]	(1, 2)	(1, 1)	2.60	0.44
 6	[4, 8, 1, 2]	(1, 2)	(1, 1)	0.76	0.5
 7	[5, 10, 1, 2]	(1, 2)	(1, 1)	1.06	0.5
 8	[10, 20, 1, 2]	(1, 2)	(1, 1)	2.04	0.5
 9	[12.5, 25, 2.5, 5]	(1, 2)	(1, 2)	1.17	1.35
 10	[2, 2, 1, 1]	(1, 1)	(1, 1)	0.44	0.29
 11	[4, 4, 1, 1]	(1, 1)	(1, 1)	1.27	0.29
 12	[10, 10, 1, 1]	(1, 1)	(1, 1)	2.54	0.29
 13	[12.5, 12.5, 2.5, 2.5]	(1, 1)	(1, 1)	1.80	1.38
 Lemma 2: (Downlink): For any rate tuple r = {riVi }i[U satisfying
 	riVi = C(|hrVi |2P) - 1/2,	i [ U	(41)
 riVi + rkVk = C(max(|hrVi |2, |hrVk |2)P) - 1/2,
 (42)
 	i, k [ U,	i =Vk
 there exists a choice of power assignments such that decoding of
 codewords with rates {error probability for all channel gains.rij}i[U,j=Vi can be done with arbitrary small
 Proof: Consider (26) and (28)–(30) for downlink:
 From (26), we have
 	art	2	2	1	(43)
 	|hr2| P	| r2|
 From (28), we have
 	art	2	2	1	(44)
 	|hr1| P	| r1|
 Substituting (43) in (29), we have
 	arf =	 	|hr3| P	
 	=  |hr3|-2P +	- 2 2 -	+
 	= |2hrr3|-2P + 2r +2r|hr-2|2P-	+
 	 |	|	| |
 	1	hr3 2	2	1	1	2P/2	1
 	|	||	|
 	1	 
 	= 1	 |	|	|	|
 Similarly, substituting (44) into (29) results in
 	P/	hr1
 	arf	|hr3| P	+	|hr1| P
 substituting (43) into (30) results in
 	P/	hr2
 	arf	|hr4| P	+	|hr2| P
  
  
 Fig. 2	Non-restricted and restricted cut-set upper bounds and achievable rates obtained for hUL = [8, 16, 1, 3] and different values for a and b
 Remark 4: Note that if R > 1/2I, (I: unit vector), satisfies (5) and (6), then the rate tuple r = R- 1/2I satisfies the conditions of Lemmas 1 and 2. Thus, the capacity of the restricted Gaussian TPTRN with symmetric uplink channel pairs can be obtained within 1/2 bit/s/Hz per user of the cut-set upper bound. By comparing against (7), we see that the gap is <3/4 bit/s/Hz with respect to the cut-set upper bound for non-restricted Gaussian TPTRN.
 5	Simulation
 In this section, we illustrate the performance of the proposed method for the TPTRN via simulations and discuss the results. All the rate units shown in the figures and the table are in bit/s/Hz.
 	5.1	Uplink
 Fig. 3 Cut-set upper bounds and achievable rates obtained for hUL = [12.5, Assuming P = 1, we consider several values for h as shown in 12.5, 2.5, 2.5] and (a1, a2) = (1, 1) and (b3, b4) = (1, 1) Table 1, and for each case, we compute the values of the decoding coefficients (a1, a2) and (b3, b4) which yield the highest rates Rt and Rf. As it is observed from the results, not necessarily decoding
 and substituting (44) into (30), results in those values of (a1, a2) and (b3, b4) which are closer to (h1r, h2r) and (h3r, h4r) result in higher rates. The first row of the table
 arf  | |P/ + h|r1 | shows the optimised values of (which are (1, 1) and (2, 1), respectively. Rows 2a1, a2) and (b3, b4)–5forof the tableh = [8751] hr4 P hr1 P show the results for h2r/h1r = 2 and h4r/h3r = 3. As it is seen, not necessarily decoding with (a1, a2) = (1, 2) and (b3, b4) = (1, 3) Other cases can be proved similarly. ? results in higher rates. Rows 6–9 of the table shows the results for
  
 Fig. 4	Non-restricted and restricted cut-set upper bounds and achievable rates obtained for hDL = [7, 10, 4, 6] by our method and the method of [13] for the downlink communication
  
 h2r/h1r = h4r/h3r = 2. It can be seen that, for most cases, decoding (a1, a2) = (1, 2) results in higher rates. Rows 10–13 of the table show the results for h1r = h2r and h3r = h4r, in this case, decoding with (a1, a2) = (1, 1) and (b3, b4) = (1, 1) results in higher rates.
 Fig. 2 shows the cut-set upper bound and the achievable rates obtained for an uplink channel gain vector equal to hUL = [8, 16, 1, 3] and different values for a and b. It is seen that the choice (a1, a2) = (1, 2) and (b3, b4) = (1, 1) results in higher rates. This result is also shown at row 4 of the table.
 Fig. 3 shows the cut-set upper bound and the achievable rates obtained for a symmetric uplink channel gain vector equal to hUL = [12.5, 12.5, 2.5, 2.5], and for (a1, a2) = (1, 1) and (b3, b4) = (1, 1), which yield the largest rate region. As it is seen from the figure, the distance between the achievable rate (Rt, Rf) = (1.80, 1.38) (the last row of Table 1) and the cut-set bound is <1 bit/ s/Hz, which is tighter than the gaps found in Lemma 1.
 5.2	Downlink
 Assume the downlink channel gain vector to be hDL = [7, 10, 4, 6], and Pr = 1. Fig. 4 shows the cut-set bound, the achievable rates attained by the proposed method and the results of [13] for the downlink communication. As it is seen from the figure, for most cases, the proposed method yields gaps that are within those predicted by Lemma 2.
 6	Conclusion
 In this paper, we studied the capacity region of an asymmetric full-duplex Gaussian TPTRN with separated users (no direct link between users). We established an achievable rate for this network by using nested lattice codes for encoding at the users and Gaussian random codes for encoding at the relay. For decoding, we exploited successive compute-and-forward and successive interference cancellation at the relay and at the users, respectively. It was shown that, for the TPTRN with symmetric uplink channel pairs, the achievable rate is within constant gaps of 1/2 and 3/4 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted TPTRNs, respectively. In this paper, we have used real-valued AWGN channels. For complex-valued AWGN channels, we have 2C(x) instead of C(x) in the capacity expression, so the gaps would be twice the gaps obtained for real-valued channels, that is, 1 and 1.5 bit/s/Hz per user of the cut-set upper bound for restricted and non-restricted TPTRN, respectively. These gaps are one-half of those obtained in [13], that is, 2 and 3 bit/s/Hz per user for restricted and non-restricted TPTRNs, respectively.
 ﻿ We present empirical results on the achievable gains stemming from the use of wireless remote radio heads (RRH) in a typical urban environment. Our work is based on simultaneous path-loss measurements of the base station and RRH links to outdoor street level users. We statistically characterize the increase in received power, when a RRH is added to improve the coverage achieved by a base station. We consider diverse expected coverage areas for the mobile terminal, evaluating the effect of RRH position with respect to the intended users. We also compare the power gains that would be obtained in practice from combining the signals from the base with those of the RRH, using schemes such as selection combining and maximum ratio combining. We conclude that under practical conditions, the benets of using RRHs will depend very strongly on the existence of line-of-sight links between the RRH and the intended users. For RRHs placed at low heights, below the clutter, only users in a street-canyon position with respect to the RRH will obtain a signicant benet. Our data also shows that the gains in signalto-noise ratio achieved when using maximum ratio combining are only marginally better than those of the much simpler selection combining.
 I. INTRODUCTION
 One of the great challenges for today’s wireless communication networks is to provide adequate spatial coverage in a cost-effective way, while achieving bandwidth efciency and interference levels adequate for high frequency re-use [1]. To meet this challenge, during the last few years there has been a growing interest in the study of femtocells, relay stations (RSs) and low-complexity repeaters [2], [3]. These may constitute relatively simple, low cost and easy-to-install alternatives when compared to the deployment of an additional base station to serve mobile stations (MSs) within the cell.
 While femtocells are conceived around a small-coverage base station (BS) with low transmit power connected to a wired network, a RS is connected to a BS through a wireless link, being able to repeat or re-code the data by using amplify-andforward or decode-and-forward algorithms, among others [4]. A repeater may be thought of as an amplify-and-forward relay with no decoding or scheduling ability [3]. In this context, there are two basic types of repeaters: wireless-repeaters (WRs), and remote radio heads (RRHs, also known as ber repeaters, connected to a BS by an optical ber) [3]. WRs
 will suffer large and small scale fading at both BS-RS/WR and RS/WR-MS links. In contrast, for RRH assisted wireless networks, achievable gains will only depend on the quality of the RRH-MS connection, which makes it an attractive solution to provide connectivity to wireless users in dense urban areas, where the links to the BS may experience signicant shadowing and ber connectivity for the RRH may be readily available.
 Proper placement of a RRH is obviously a fundamental factor in dening the compromise between the desired gains and the cost associated with its installation. In this regard, in an urban environment it is reasonable to expect that while coverage will improve with antenna height, this will at the same time have a negative impact on deployment costs and on co-channel interference, thus affecting frequency reuse in a large system [5], [6]. If the RRH is to cover an area of a size comparable to a small cell, then it is reasonable to assume that this is best achieved by placing its antenna in positions normally used by base stations (i.e above the clutter height). In such cases well-established propagation models such as those of Hata [7] and Erceg et al. [8] will be adequate to predict the coverage achieved by the RRH. Alternatively, they may be positioned at lower heights with the aim of achieving improvements in more limited regions, i.e. generating microcells within a large cell. Coverage in microcells has generated growing interest as it can provide local increases in signal to noise ratio (SNR) and thus higher data rates [2], [9]. This may be achieved without generating excessive interference in neighboring cells and at lower implementation costs when compared to a base station [2], [10], [11], particularly when placed below the surrounding clutter height. Numerous empirical models have been proposed for the statistical characterization of path-loss in relatively lowheight (3-20 m) radio links [12]–[17]. Analytical models based on optical geometry have also been proposed for microcells in urban environments [18], [19]. Goldsmith et al. [20] proposed, based on a collection of measurements, a mathematical description of the radius of coverage of a microcell. Such models may be used to characterize the RRH-MS link portion of a repeater. Haneda et al. [21] analyzed relays based on outage capacity for a very short-range outdoor-indoor environment. In [3], the authors simulated different uses of repeaters and RRHs in cellular networks, concluding that careful placement can improve capacity substantially by transferring trafc from heavily loaded to lightly loaded sectors.
 The IEEE 802.16 Relay Task Group [22] and the WINNER Consortium [23] have suggested the use of both empirical and theoretical models to predict path-losses for wireless links with transmission units below the clutter of scatterers, also applicable to RRH-MS links. While the accuracy of pathloss models for diverse environments has been the subject of extensive studies, their use in RRH-assisted wireless networks requires more than treating each link individually, since the joint statistics of BS-MS and RRH-MS links may not correspond to those of independent random variables. Statistics for power gains stemming from the use of relays or RRHs based on simultaneous measurements in urban environments, as presented here, have to the best of our knowledge not been published.
 In this work, we report on path-loss measurements of the BS-MS and RRH-MS links in a typical outdoor urban environment. We evaluate diverse aspects related to the performance improvement that can be expected when adding a RRH in an area served by a base station. We focus on modeling the statistics of the radio links involved and the resulting gains in received power by the mobile user. Our results will be useful to calculate the achievable benets in coverage and in transmission rates experienced by mobile terminals. Measurements were carried out in an area within a range of 200 m of the RRH, which is placed at lamp-post height (3 m), considering that this type of setting will be typical for a practical deployment of low complexity repeaters. To avoid biasing our statistics we randomly chose LOS and NLOS placements for the MS at various distances from the RRH. The maximum range was dened by the requirement that at all placements, particularly those lacking LOS, the received power would be within the limits imposed by our channel sounding system. We note that our system was capable of measuring path-losses that would considerably exceed those compatible with a typical link budget in a wireless network. We also considered two types of RRH placements. One was chosen so as to maximize the likelihood of users having LOS to the RRH and the other chosen in close proximity to the rst, but obstructed from direct street view by construction. In this way we were able to consider the effect of elements blocking a RRH, as may occur when a surrounding construction is modied after the placement of the ber repeater. Thus our statistics allow us to quantify the benets of using a RRH for randomly placed mobile users within a given range considering improvements in coverage and in transmission rates. From the joint data of path-loss for the BS-MS and RRH-MS links we were able to compare the effectiveness of Selection Combining (SC) and Maximal Ratio Combining (MRC) [24] at the mobile terminal. To this effect we considered typical transmit powers and antenna gains at the BS and RRH and calculated received signal power at the MS under the condition of equal noise power for both schemes. It was found that the statistical gain of MRC over SC is less than 1 dB under all conditions tested, due to the random power imbalance between the links.
 Our measurements also allowed us to calculate correlations between fades for the BS-MS and RRH-MS links. The low values observed validate an approach based on considering them as independent random variables.
 II. MEASUREMENT SCENARIO AND PROCEDURES
 A. Description of the Urban Environment Tested
 The measurement campaign extended over a period of 4 months during summertime. The urban area used as test-bed in Vin˜a del Mar, Chile, contains a mix of high-rise buildings and two-story houses with altitudes ranging between 8 and 60 m, built on a plane region at sea level. Measurements of received power at MS positions were carried out at street-level. The area is overlooked by nearby hills that allowed positioning a transmitter at a location typical for a base station covering a relatively large urban area. The BS transmitter height was 180 m above the measurement region at a distance of 2 km to the RRH position. The RRH was mounted in two positions on the exterior walls of a 62 m high building. The streets in the measurement area are lined with trees with heights ranging between 6 and 8 m. All BS-MS links were non-line-of-sight (NLOS), with blockages due to the surrounding constructions. All users were located within a radius of 200 m centered at the low-height RRH. Regarding the distance of these locations to the RRH, 8 were within 60 m, 8 between 60 and 100 m, and 14 between 100 and 200 m, which allowed us to also obtain range-dependent statistics. A schematic description of the terrain prole and of our measurement scenario are presented in Figs. 1 and 2.
 Base Station
 (BS)
 m
 150
 100
 50
 0.0	0.5	1.0	1.5	2.0 km
 	Figure 1.	Terrain prole
  
 	Figure 2.	Placement of RRH and MS units
 B. Measurement System
 At the BS position, a continuous wave (CW) transmitter based on a synthesized oscillator transmits at 3.5 GHz with 19 dBm output power through a sector-type antenna. The antenna used had 60? azimuth and 8? elevation beamwidths at -3 dB points. All MS locations were within this beamwidth. At the RRH, the antenna used was a vertically polarized dipole with 2 dBi gain, transmitting a CW signal at 3.57 GHz with 19 dBm output power. Power measurements at the MS positions were carried out using an Agilent model N1996A-506 spectrum analyzer that simultaneously tracked the carriers at 3.5 and 3.57 GHz. A noise oor of -125 dBm in a 10 kHz bandwidth was achieved using low-noise preampliers. The antenna used was a 2 dBi gain vertically polarized dipole placed at a 1.8 m height. The MS measurement system included a laptop computer that acquired the spectrum analyzer scans at a rate of 2 per second. Within the selected area all power measurements exceeded the noise oor by at least 10 dB.
 C. Measurement Procedure
 The RRH was placed at an altitude of 3 m at two different positions: one at a street intersection so as to simultaneously illuminate two streets and the other one within 5 m of the rst placement but with no direct path to the adjacent streets. In the rst case there are LOS and NLOS user locations, while in the second all MS positions are NLOS. The measurement campaign for both RRH placements involved moving the MS along a straight path of about 4.5 m at each of the 30 locations that were selected at random within 200 m of the RRH, as illustrated in Fig. 2. This was done using a very slow-moving vehicle in such a way that approximately 300 equally-spaced power samples were collected. At the specied frequency, this implies more than 100 received power samples at positions spaced at least half a wavelength apart. From these measurements we obtained the path-loss at each receiver position, which allowed the calculation of average path-loss and small-scale fade statistics at each of the 30 locations.
 D. Data Analysis and Received Power Gain
 To evaluate achievable power gains under realistic conditions we calculated the received power at the mobile station assuming typical values for transmit powers and antenna gains. Using the path-losses for all points measured we then generated the cumulative distribution function (CDF) of the received power at the MS, considering the BS alone, the RRH alone, the BS with the RRH under a selection combining scheme (SC) and the BS with the RRH considering a maximum-ratio combiner (MRC). We assumed the existence of a BS with 45 dBm of transmission power. The BS and MS antennas are assumed to be the same as the ones used in our measurements (with gains of 17dBi and 2dBi, respectively). For the RRH we assumed that in most practical cases the RRH would be attached to a wall and consequently only illuminate a sector of 180 in azimuth. All our MS locations are in fact within a sector of such angle. We thus considered that a typical 7 dBi, 180? sector antenna would be used, consistent with calculations reported in [25]. For the transmit powers at the RRH we considered two values that cover what could be considered the high and low extremes of a practical deployment: 20 and 40 dBm.
 To quantify the gains we dene as ?(y) the dB increase in received power that can be guaranteed to users at a given availability level y. Let P[dBm] be the received power at the MS. We denote the CDFs of power received P[dBm] from the BS alone as FBS(P) and the CDF of the power received under SC as FSC(P). The gain in received power when using SC is then:
 	?(y) = FSC-1(y) - FBS-1(y)	(1)
 The same type of denition of course applies to the gain obtained through MRC. We will use (1) to calculate, at various availability levels, the power gain resulting from the use of a RRH.
 III. EMPIRICAL RESULTS
 In this section discuss the results that can be expected when using a low altitude RRH (3 m height). We consider a random selection of MS placements within a 200 m radius of the RRH. In order to verify that our selected urban scenarios do not signicantly deviate from those characterized by wellestablished models for wireless channels, we compared our path loss results with those predicted by such models. For the NLOS links between RRH and MS, we found a good match to the COST-Hata model [24] and to the alternative WINNER path-loss model for the F-NLOS case suggested in [22] using an operating frequency of 3.57 GHz. With respect to these models, our measured losses were slightly higher, 2 and 4 dB on average, with r.m.s. errors of 6 and 10 dB respectively. The measured losses for the street canyon links were far lower than those for NLOS links at the same distance, from 4 to 34 dB, with an average of 18 dB. They were however higher by about 8 dB on average than those predicted by the advanced LOS model for Type-F LOS scenarios (with both RS and MS below rooftops) [22]. For the collection of mobile locations that we considered, we also modeled empirical pathlosses for both LOS and NLOS links according to a singleslope log-distance model, with free-space losses at 1 m. The resulting slopes were 26 and 38 dB/log(m) respectively. We note that most of our LOS links were partially obstructed by relatively large trees. To evaluate the effect of choosing a RRH position, we considered two placements. One had an uncluttered path to two intersecting streets, while the other was in close proximity (5 m) but obstructed by construction. The rst placement is denoted here as Non-Obstructed RRH (N-RRH) and the second as Obstructed RRH (O-RRH). The N-RRH position provides LOS links to some of the chosen MS locations. For a setting such as this, where both antennas are below clutter, LOS links are described as being in a streetcanyon [26]. Correspondingly, we use this notation and refer to the NLOS links as non-street canyon. In the 200 m radius, half (15) of the N-RRH links are of the street canyon type while all O-RRH links are non-street canyon. When considering a 100 m radius all links to the N-RRH become of streetcanyon type. This classication (according to the RRH-MS link characteristics) is aimed at determining which users can be expected in practice to benet most from a given RRH placement.
 As an example of the achievable gains we present in Fig. 3 the CDF of the received power at the MS within the 200 m radius, for the case where the RRH transmits at 20 dBm for both RRH placements. All other parameters are as described in section II. As seen, the CDF for MRC and SC are virtually indistinguishable, the shift being less that 1dB. We observe that the benets resulting from the use of a RRH very much depend on its placement. For example, at 50% coverage, the BS will guarantee a power of at least -63dBm. Using an ORRH will only provide a slight improvement, guaranteeing the same power to 56% of users, while an N-RRH will increase this coverage to 76%. Repeating this calculation under the assumption of a RRH transmit power of 40 dBm results in coverage increases to 77% and 84% respectively. In what follows we summarize the power gains for various conditions illustrating the importance of proper placement of the RRH with respect to the intended users.
  
 Figure 3.	CDF of received power at MS for 200 m coverage radius and 20 dBm RRH power. RRH height was 3 m.
 A. RRH Gains as Function of Coverage Radius
 To evaluate the effect of coverage range we divided the locations where the power measurements were carried out into three groups with ranges of up to 200 m, 100 m and 60 m, referred to the RRH. Table I presents the power gain ? (as dened in (1)) obtained with a SC at three availability levels for two RRH transmit powers and the two placements described. We recall that for radii of 100 m or less the NRRH placement provides street-canyon-type links to all users, while only 50% of the users are in that condition for the 200 m radius. The consequence, clearly observed in the table, is that the gains available to 90% of the users increases sharply when the coverage radius is reduced to 100 m, i.e. when all of them are in a street canyon with respect to the RRH. Both Fig.
  
 Figure 4.	CDF of received power at MS street canyon positions for 200 m coverage radius and 20 dBm RRH power.
 3 and Table I illustrate the importance of proper positioning of the RRH. We further explore this below.
 B. RRH Gains for Street Canyon and Non-Street Canyon Links
 Based on our above observations we divided the results for all links with up to 200 m length into street canyon and nonstreet canyon types, rather than by distance as before. Table II presents the results for the power gain ? under this classication, using again the parameters previously detailed in section II. The N-RRH columns illustrate that very considerable power gains are available for street canyon links, even at low RRH powers. In contrast, the row corresponding to the non-street canyon links shows very modest gains regardless of the RRH position. The latter results were obtained considering only MS positions that are of non-street canyon type under both RRH placements, so that comparisons are based on a common set of data.
 To illustrate the gains achievable under the most favorable RRH and MS placements, we present in Fig. 4 the CDFs for received power considering all street canyon locations within the range of 200 m, at a RRH power of 20 dBm. When comparing this gure with Fig. 3, it becomes clear that the street canyon locations are those that contribute most signicantly to the observed increases in received power. For example the BS will provide at least -65 dBm to 50% of users. This same minimum power will be available to 96% of users if a RRH is included. Repeating this for an assumed 40 dBm transmit power increases coverage to virtually 100%.
 Finally we calculated correlation of fades for the BS-MS and RRH-MS links, for O-RRH and N-RRH as
  
 where PL(BS) and PL(RRH) represent respectively the path-losses, including the small-scale fades, from the BS and
 Table I
 SELECTION COMBINING GAIN FOR A LOW-ALTITUDE RRH.
  
 Table II
 SELECTION COMBINING GAIN, FOR STREET AND NON STREET CANYON.
  
 the RRH to a specic MS position. µBS = E[PL(BS)] and µRRH = E[PL(RRH)] are the statistical averages of these path-losses at the distance under consideration, obtained from a linear regression of path-loss vs. distance. sBS and sRRH are the standard deviations of the fades with respect to these averages.
 In both cases (O-RRH and N-RRH) these correlations were found to be below 0.01. When small-scale fades are averaged out, this increases to 0.27.
 IV. CONCLUSIONS
 Our empirical results suggest that a RRH allows important gains in received power even when placed as low as 3 m and operating at the relatively low transmit power of 20 dBm. However, these gains are basically associated with the existence of LOS (or, equivalently, street-canyon) links to users, and thus heavily depend on site-specic RRH and user placements. In all scenarios, the gains achievable using MRC are only marginally larger than those obtained by a SC, attesting to the fact that, in practice, the likelihood of the RRH and the base providing comparable powers is very small.
 Virtually no correlation (< 0.01) was observed between the fades of the RRH-user link and the base user link. This allows treating them as independent random variables when modeling a wireless system assisted by RRH units, using the appropriate statistical description of the individual links.
 
 ﻿This work offers a necessary and sufficient condition for a stationary and ergodic process to be-compressible in the sense proposed by Amini,Unser and Marvasti [“Compressibility of deterministic and random infinity sequences,” IEEE Trans. Signal Process., vol. 59, no. 11, pp. 5193–5201, 201 , Def. 6]. The condition reduces to check that the  -moment of the invariant distribution of the process is well defined, which contextualizes and extends the result presented by Gribonval, Cevher and Davies in [“Compressible distributions for high-dimensional statistics,” IEEE Trans. Inf. Theory, vol. 58, no. 8, pp. 5016–5034, 2012, Prop. 1]. Furthermore, for the scenario of non- -compressible ergodic sequences, we provide a closed-form expression for the best  -term relative approximation error (in the -norm sense) when only a fraction (rate) of the most significant sequence coefficients are kept as the sequencelength tends to infinity. We analyze basic properties of this rate-approximation error curve, which is again a function of the invariant measure of the process. Revisiting the case of i.i.d. sequences, we completely identify the family of-compressible processes, which reduces to look at a polynomial order decay (heavy-tail) property of the distribution. 
 Index Terms—Asymptotic analysis, best  -term approximation error analysis, compressed sensing, compressibility of infinite sequences, compressible priors, ergodic processes, heavy-tail distributions. 
 EFINING notions of compressibility for a stochastic process, meaning that with high probability realizations of the process can be well-approximated in some sense by its best  -term sparse version [3], has been a recent topic of active research [1], [2], [4]–[6]. Quantifying compressibility for random sequences and the identification of compressible and sparse distributions (priors) are relevant problems considering the recent development of the compressed sensing theory [7]–[9] and its applications. These results can play an important role in regression [10], signal reconstruction (for instance in the classical compressed sensing setting [2, Th. 2]), inference, and decision-making problems [11], [12]. One important case is defining such a compressibility notion for i.i.d. processes where the probability measure is equipped with a density function1 [1], [2]. In this context, realizations of the process are non-sparse (almost surely), and conventional ways of defining compressibility for finite dimensional signals, based on the power-law decay of the best -term approximation error (or sequences that belong to the weak- ball), are not applicable either, as shown in [1], [2]. 
 Motivated by this problem, Amini et al. [1] and Gribonval et al. [2] have introduced new definitions for compressible random sequences. These notions are not based on the typical absolute approximation error decay pattern of the signals, but on a relative -best -term approximation error behavior. In particular, Amini et al. [1] formally define the concept of  -compressible process (details in Section II below). This new definition provides a meaningful way of categorizing i.i.d. random sequences (and their distributions), in terms of the probability that almost all the  -relative energy of the process is concentrated in an arbitrarily small sub-dimension of the coordinate domain, as the block-length tends to infinity. Under this context, they provide two important results using the theory of order statistics [1]. First of all, [1, Theorem 3] shows that a concrete family of i.i.d. heavy-tail distributions is-compressible (including the generalized Pareto, Students’s and log-logistic), while on the other side, [1, Theorem 1] demonstrates that families with exponentially decaying tails (such as Gaussian, Laplace, generalized Gaussian) are not  -compressible. Therefore, it is interesting to ask about the compressibility of i.i.d processes not considered in that analysis. In this direction, we highlight the work of Gribonval et al. [2], which under an alternative notion of relative 
 -compressibility (involving almost sure convergences instead of convergence in measure, which was the criterion adopted in [1]) and a different analysis setting (fixed-rate instead of the variable rate used in [1]), elaborates an exact dichotomy between compressible and non-compressible i.i.d. sequences. This raises the question of whether it is possible to connect Amini et al. [1]  -compressibility with the more refined almost sure (a.s.) convergence analysis of the best -term relative approximation error in [2, Prop. 1], with the idea of completing the analysis of [1, Ths. 1 and 3]. 
 1053-587X © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 
 To address this question, we extend the analysis from i.i.d. sequences to stationary and ergodic processes. In this broader setting, the main result (Theorem 1) provides a necessary and sufficient condition for a stationary and ergodic process to be  -compressible (in the sense of Amini et al. [1, Def. 6]), for any arbitrary  . Furthermore, for the case of non  -compressible ergodic processes, we provide a closed-form expression for an achievable rate v/s  -approximation error function. The key element in the proof is the application of the ergodic theorem [14] and the derivation of intermediate almost sure convergence results (Lemma 2 and 3 in Section IV) that match and extend the approximation result presented by Gribonval et al. [2, Prop. 1] developed for the i.i.d. case. A corollary of Theorem 1 implies a necessary and sufficient condition to categorize i.i.d. random sequences in terms of  -compressibility, which completes the analysis presented in [1, Ths. 1 and 3]. In addition, for the class of non- -compressible ergodic sequences, we provide an analysis of its rate-approximation error curve demonstrating that is continuous, differentiable (Theorem 2) and is convex under some conditions (Theorem 
 3). Finally as an application, we revisit the interplay between  -compressible ergodic sequences and the performance of the classical Gaussian compressed sensing (GCS) setting [15], in the asymptotic regime when the block-length tends to infinity. Using the well-known  -instance optimality performance guarantee of the GCS scheme [3], [15], [16], we show (Theorem 4) that an arbitrarily small number of linear measurements (zerorate) is needed to achieve zero distortion, in an  -noise to signal ratio (NSR) sense. A preliminary version of this work was presented in [17]. The current version extends the presentation and analysis of the main result, provides further analysis of noncompressible ergodic sequences and explores connections with compressed sensing (CS). 
 The rest of the paper is organized as follows. Section II introduces some preliminary elements and definitions. Sections III and IV are devoted to the presentation of the main result on the characterization of compressible ergodic processes and its proof, respectively. Section V studies basic properties of the the rate-approximation error curve for non  -compressible processes. Finally, Section VI elaborates an interplay between 
 -compressibility and compressed sensing. Some of the proofs and derivations are presented in the Appendix sections. 
 denote the -norm of the best -term approximation of , where by definition . In addition, 
 denotes the best -term -approximation error of , in the sense that if	is the collection of -sparse signals, then	. For the analysis of infinite sequences, Amini et al. [1] and 
 Gribonval et al. [2] have proposed the following relative best -term -distortion indicator: 
 with the objective of extending notions of compressibility to sequences that have infinite -norm. 
 Definition 1: For a sequence , the rate-distortion pair is -achievablefor if,there isa sequence of positive integers such that and 
 Note that the use of the relative best -term -distortion in (4) allows the analysis of sequences with infinite -norm. 
 Definition 2: For a sequence and , we define its rate-distortion - approximation function by 
 Proposition 1: For all	such that then	. (The proof is presented in Appendix IV-A) 
 Hence, can be seen as the critical asymptotic rate of innovation of when a relative best -term -approximation error of magnitude is tolerated. 
 Alternatively,Amini, Unser and Marvasti [1] have introduced a notion of critical dimension for finite length signals, and from this, a notion of  -compressibility for infinite sequences. We revisit those notions here: 
 This notion of compressibility says that when the block-length tends to infinity, a negligible fraction of the coefficients is needed to represent with an arbitrary small -distortion in the sense of (3). Note that is signal dependent and a variable-rate sequence. In addition, it offers the critical number of terms needed to achieve a best -term approximation error smaller or equal to in the sense of (3). From this, it should be related with the critical rate (from a fixed-rate analysis) described in Definition 2. That relationship is presented in the following result result: 
 . In particular from Lemma 1, if is -compressible, then for all . We refer the interested reader to Amini et al. [1] for further discussion and examples of -compressible sequences. 
 B. Rate of Innovation vs. Distortion for Random Sequences Analogous notions of rate of innovation vs. best -term 
 -distortion and compressibility can be stated for the case of random sequences (or processes). Let be a random sequence with values in and characterized by its consistent family of finite-dimensional probabilities 
 [14], where and denotes the space of probability measures for the Borel measurable space . As a short-hand, we denote by the process distribution of 
 where the equality is by (6). Then in analogy with Definition 3, Amini et al. [1] proposed the following: 
 Definition 4: [1, Defs.5 and 6] Let be a random sequence (equipped with ). Then for any and 
 is the critical number of terms that makes the set -typical with respect to . The process (and , respectively) is said to be -compressible, if , , 
 Definition 5: Let	be a process characterized by	, and let us consider	,	and	. We say that the rate-distortion pair (r,d) is	-achievable for	with 
 Definition 6: The rate vs. best -term approximation error function of (in short the rate-approximation error function of ) with probability is given by : 
 A simple relationship between and the critical number of terms in (10) can be established in the asymptotic regime when   goes to infinity, showing that our fixed-rate concept is a weaker one. 
 In the next section, we will study the class of stationary and ergodic processes [14], where the best -term approximation properties measured in terms of will be characterized in closed-form. Furthermore, it will be shown for this class of random sequences that 
 Let be a stationary and ergodic process with distribution , where   denotes its marginal shift-invariant distribution [14]. For simplicity , we assume that where denotes the Lebesgue measure [14]. 
 Theorem 1: Let be a stationary and ergodic process with shift-invariant distribution such that  . Then for any , we have the following dichotomy: 
 ii)	If	: then	is not	-compressible. Furthermore, if we introduce the induced probability measure in	by: 
 1: Theorem 1 offers a necessary and sufficient condition for a stationary and ergodic process to be l-compressible in the sense elaborated in Definition 4. 
 achievable rate-distortion region for the process, given by the set of critical rate-distortion pairs: 
 This region depends solely on the shift invariant measure and its induced measure in (17). More details on the characterization of this region will be presented in Section V. 
 are achievable (see proof in Section IV and more details in Section V). This fact is used to derive a concrete analytical expression for in (18). Furthermore, from the characterization in (18) and (20), it can be shown that is a continuous and differentiable function with respect to (see Theorem 2 in Section V). 
 4: In both scenarios i) and ii), the critical ratefor a stationary and ergodic process is independent of . The reason is that asymptotically as goes to infinity, the characterization of implies to compute probabilities on events that belong to the tail -field of the process, which is known to be trivial (i.e., their events have zero or one probability) for the case of ergodic processes [14], [18], [19]. Therefore, we obtain almost sure convergence results that make independent of   the value of our object of interest (see Section IV for details). 
 5: A natural order among stationary and ergodic process can be established from Theorem 1. Proposition 3: If is -compressible for some , then is -compressible for all . 
 6: Revisiting the i.i.d. scenario , we want to highlight the results by Amini et al. [1] related to -compressibility in the sense of (11). In particular, [1, Theorem 1] says that if   is such that for some , 
 then the i.i.d. process is not -compressible. In contrast, [1, Theorem 3] says that if belongs to the domain of attraction of an -stable distribution [14, Chap. 9.11, pp. 
 207–213] with , then the process is  -compressible. First for , Theorem 1 provides a refined result, revealing a richer (indeed, the complete) family of i.i.d. distributions that are not -compressible. In fact, in addition to distributions that go to zero exponentially, and consequently, (Gaussian, Laplacian, Gamma, etc.), heavy tail distributions whose density function are tail lower and upper dominated by a power law decay of the form with are not  -compressible either5. On the other hand, concerning [1, Th. 3], it is simple to verify that any that is in the domain of attraction of an -stable law with [14, Ch.9] satisfies that 
 (see Appendix IV-E for details), and consequently, part i) of Theorem 1 covers this family of -compressible i.i.d. processes. 
 Therefore, stationary and ergodic processes equipped with a shift-invariant distribution that follows a Gaussian, generalized Gaussian, Laplacian and Gamma are not -compressible in the sense of Definition 4, for any . In addition, if is finitely supported, i.e., where , then its process is not -compressible for any . 
 Corollary 2: Let be a ergodic process with invariant distribution and density , . If decays as for some , then6 
 Therefore, for shift invariant distributions characterized by a power-tail behavior, which belong to the category of heavy tail distributions, a complete picture of the range in which its ergodic process is  -compressible is obtained. 
 8: For the proof of Theorem 1, we derive almost sure convergence results (see Lemma 2 and 3 in Section IV). 
 ,	. Furthermore, for	for some	, it follows that	,	. In the case when	: if	, then 
 sistent and extend the result by Gribonval et al. [2, Prop. 1], which for the i.i.d. case shows the same almost-sure convergence limit for the object . Their proof was based on the Wald’s lemma of order statistics (see details in [2, Th. 6]). In contrast, our proof is based on the use of the tail events in (19), some induced empirical distributions on those events, and the convergence of those empirical measures through the application of the ergodic theory (see Section IV for details). The idea adopted in our proof was to look at the empirical distributions of and as the objects of interest, instead of the partial 
 5A measure is tail lower and upper dominated by a no-negative function , if there exits and such that for any such that then . Here denotes the pdf of . 
 Fig. 1.	Graphical representation of the relationship between	,	and   in Theorem 1. Notice that	is the total area under the bottom curve. 
 sums of the ordered statistics considered in [1], [2]. That difference was essential to extend the mentioned almost sure convergence results (in Lemma 2 and 3) to the family of stationary and ergodic processes. 
 , respectively. The numerator in the last expression corresponds to the expected value of , for a random variable where . Similarly, from (18), the optimal rate 
 equals the expected value of . Thus, corresponds to the area under the “tail” of , which denotes the pdf of , depicted in Fig. 1 (top), while coincides with the area under the curve to the left of , coloured in Fig. 1 (below). This graphical representation allows for an intuitive interpretation of the relationship between and the compressibility of a given stationary ergodic process . In order for this process to be compressible, must be zero for every and for every . Equivalently, (and recalling from (16) that is a limit), it must be possible to achieve any while keeping an arbitrarily small fraction of the elements of the process, as . Such requirement is satisfied if and only if (i.e., if), which implies that no matter how large is chosen, the shaded area in Fig. 1 (bottom), being infinite, will yield a zero . 
 The second almost sure convergence is from the assumption that . Then, we can state the following: 
 Lemma 2: Let be a stationary and ergodic process with distribution and . Then for any and sequence such that , 
 quently, convergences almost surely to a distortion strictly less than , and then for all : 
 Hence from the definition of	in (10), we have that eventually in	, which implies that 
 The first inequality comes from Proposition 2 and the last equality from the fact that the function is continuous with respect to as . 
 Lemma 3: Let be a stationary and ergodic process with distribution and . Let us consider an arbitrary and such that , 
 and therefore eventually in . From this, for all , which concludes the result from Proposition 2. 
 Proof: To begin let us prove the fixed-rate result in (23). It is first important to concentrate in the case when and to show that 
 Let us take an arbitrary . From the zerorate assumption on , the definition of and the fact that 
 Performing the same steps for the sequence and defining accordingly , we have from (37) that 
 Equipped with this result, for an arbitrary let us consider such that . We know that there exists such that , and for this we consider 
 Furthermore, from definition of ordered sequences, it is simple to verify that (see (1)): 
 This is where the zero-rate result in (34) is used. In particular, if we consider	, from (40), (38) and the fact that is in 
 the last equality in (41) from definition of	in (35). Finally from (22b) and (22c), 
 let	be such that	. Let us consider an arbitrary where and consequently (note that and are mutually absolutely continuous). Again for this , we use the sets in (35), 
 The first equality follows from the continuity of the function with respect to as . Then from the fact that	, we have that, 
 , -a.s. Finally, proving that	-a.s. follows an equivalent symmetric argument and we omit it. 
 and	, we have that from (30) and (31). Let us fix an arbitrary . Considering that 
 , then eventually in , and therefore eventually, which implies from definition of that . Finally, the fact the event happens -almost surely concludes the result. 
 V. PROPERTIES OF THE RATE-APPROXIMATION ERROR CURVE FOR NON	-COMPRESSIBLE PROCESSES 
 For the family of non -compressible ergodic processes, in this section we study two tail functions that characterize the achievable rate-distortion region in (21). Let be stationary and ergodic with its invariant probability measure. Here we focus on the case where , then the measure in (17) is well-defined and by construction	, where the Radon-Nikodym (RN) derivative (or density) of	with respect to	is given by for all	. Furthermore, from the strict positivity of 
 achieves all the values in , and any distortion is achieved in with a given rate. 
 In summary, the rate-distortion approximation function is continuous, injective and achieves all the rates, in the sense that for any	there is only one	such that	. In addition, it is strictly decreasing and satisfies the following boundary conditions:	and	. Furthermore, it is simple to verify that	and 
 should be a non-increasing function as progress to 1 (considering that ), and consequently, 
 should present a convex dominating behavior  eventually as progresses to 1. The next section analyzes the convexity of more formally. 
 First in this section we show that is a convex function of . Then, we provide a necessary and sufficient condition for to be a convex function of . 
 Theorem 3: Let by a non -compressible stationary and ergodic sequence equipped with . Then 
 Remark 1: The condition in (48) is implicit and may be difficult to verify. A simple to check sufficient condition for the convexity of is the following:11 
 where . In particular, a non increasing pdf (i.e., almost everywhere in ) characterizes a convex rate-approximation error function. 
 We present few examples of rate-approximation error curves of non  -compressible i.i.d. processes. In particular following [1], we consider the Gaussian (exponentially decaying distribution), which is non  -compressible for any , from Corollary 1, and the family of Student’s -distribution with parameter 
 12, whose pdf goes to cero as . From Corollary 2, the i.i.d. process with a Student’s t-distribution ( ) is -compressible for any and non- -compressible for 
 , for all . For this we consider an estimation approach. Considering a sufficiently large set of i.i.d. realizations of (let say ), the law of large numbers [14] tells us that for any and , 
 12The pdf of a Student’s -distribution with	degrees of freedom is given by , where	is the gamma function. 
 Fig. 2.	Numerically estimated rate approximation error curves for several non -compressible i.i.d. processes and one	-compressible i.i.d. process. 
 the use of (17). Then, by sampling the space of thresholds and considering a sufficiently large , we can estimate with an arbitrary good precision the rate distortion region 
 . Following this path, Fig. 2 shows the estimated rate-approximation error curves for the Gaussian, and several Student’s -distribution for . We verify that some Student’s -distribution are -compressible (cases and ) and others are non -compressible (cases , and 
 ) as the Theorem 1 predicts. More interesting is to validate in all the cases of non compressible priors, that the curves have a convex behavior, which is justified from (49). Furthermore, the density with the exponentially decaying tail is less compressible than any prior with a power law decay, in the sense that for achieving a distortion the Gaussian i.i.d. process needs a higher rate. From these curves, as goes to infinity the i.i.d. process with a heavy-tail distribution approaches the approximation error behavior of the Gaussian law. 
 Fig. 3 shows the rate-approximation error curves for the Gaussian prior for different values of . It is interesting to observe the increasing monotonic behavior of as increases, for any fixed value of . Again all curves have a convex behavior. To contrast, Fig. 4 shows a set of curves for the Cauchy distribution (i.e., Student’s distribution with 
 We conclude this work analyzing compressible stationary and ergodic sequences, as characterized in Theorem 1, in terms of their ability to be represented with an arbitrary small proportion of linear measurements adopting for that the classical compressed sensing (CS) measurement and reconstruction setting. In particular, the focus is on  -compressible processes, as the standard Gaussian i.i.d. linear acquisition and 
 -minimization (sparsity promoting) decoder of CS [15], [16] offer a well-known  -instance optimality guarantee [3] (stated below in Lemma 4) that matches the modeling assumption of -compressible processes. 
 CS is a linear operator , that given a signal   generates a measurement vector . The case of interest is on the under-sampled regime, i.e., , where under sparse or compressible assumptions on , CS can offer perfect or near-optimal reconstruction by the solution of the following (linear programing) problem [16]: 
 Notably, the CS theory, based on the restricted isometry property (RIP), establishes sufficient conditions over (and implicitly over the number of measurements ) in order that 
 , when   for some . The next result, in its original form stated in [9], shows that random measurements offer a solution to that problem with a near optimal relationship between and [3] . 
 Lemma 4: ([15, Th. 5.2] and [16, Th. 1.2]) Let be a random matrix , , whose entries are driven by i.i.d realizations of a Gaussian distribution or a binary variable with uniform distribution over . For any arbitrary and , we have that: 
 with a probability,over the sensing sampling space ,at least equal to . Here is the solution of 
 (The proof of this result derives directly from [15, Th. 5.2] and [16, Th. 1.2]) 
 Here we formalize the reconstruction of infinite sequences using CS. For this, we consider a finite-length (or fixed-rate) approach, where the idea is to analyze consecutive finite-block versions of the sequence, i.e., to sense and reconstruct for any  , and study reconstruction performances in the limit when the block-length tends to infinity. 
 More precisely, let be a sequence of positive integers such that . From this sequence, we consider the family of Gaussian CS encoding-decoding pairs	where for any , is the random sensing matrix of generated by i.i.d. entries as mentioned in Lemma 4, and is the function from to that solves the -minimization problem in (52). Given a sequence and any finite block-length , we can apply the CS approach over   to recover	, which is a random reconstruction function of the matrix . In relation with the -relative approximation error introduced in (3), we consider as a fidelity indicator the -noise to signal ratio (NSR) given by: 
 More generally, if we have a process with distribution and a sequence of lengths with its associated Gaussian CS finite-block scheme	, we can also analyze the finite-block performance of the scheme by the object 
 tion of two independent random objects: the vector  and the random matrix . Therefore, it is important to consider the average NSR with respect to the statistics of the source (i.e., 
 Then the question we focus here is: for an -compressible process that satisfies (16), what is the minimum rate of measurements (i.e., , or more generally 
 with probability one with respect to the statistics of the sequence of random matrices	? 
 From Theorem 1 and the RIP-based -instance optimality result of the Gaussian CS setting in Lemma 4, we can state the following result: 
 Theorem 4: Let	be a stationary ergodic process. If is	-compressible, then for any sequence	such 
 1: This result states that in order to achieve zero distortion in the reconstruction for an  -compressible process, almost-surely in the NSR sense of (56), the CS scheme needs an arbitrary small number of measurements per sample. In other words, under the  -compressibility model assumption for the process, the minimum rate to achieve zero distortion is zero for the Gaussian CS scheme. Then, it is remarkable to validate that CS is able to achieve the same zero critical rate that it is obtained by the analysis of the pure oracle best- -term approximation error of -compressible process (see the result in Lemma 3). 
 2: This result shows that the lucid notion of -compressibility proposed by Amini et al. [1]  really translates in a meaningful performance result for the classical Gaussian CS (GCS) setting in the asymptotic regime when the blocklength goes to infinity. In other words, we can say that 
 -compressibility, meaning a sort of zero-rate of innovation in the process, implies zero-rate of measurements (per signal dimension) for perfect recovery (in the sense of NSR distortion) for the CS scheme. This result closes a gap not explored in [1] between their notion of  -compressibility and CS performance guarantee in the asymptotic regime. 
 3: Concerning compressibility of random sequences and CS performance guarantee, we want to highlight the work of Gribonval et al. [2] for the case of i.i.d. processes. They show in [2, Theorem 2] that if then 
 of Theorem 4 for the i.i.d. case [2, Remark 1]. Then, we want to give credit to this contribution to be the first result that offers a connection between notions of compressibility for i.i.d. processes (based on relative approximation errors) and the performance (in the asymptotic regime) of the classical GCS scheme. In this context, Theorem 4 can be seen as an extension of these results to the case of stationary and ergodic sequences and, in the technical side, an extension on the use of the-instance optimality property of the 
 -minimization decoder . On the other hand, focusing on the i.i.d. context, Theorems 4 and 1 offer a way to verify that Amini et al. [1]-compressible notion (variable rate in nature) has a connection with the results in Gribonval et al. [2,Th. 2 and Rem.1] in terms of what GCS can achieves for the case of  -compressible processes. 
 The main result of this work (Theorem 1) provides a connection between Gribonval et al. [2, Prop. 1] almost sure convergence result of relative approximation errors, and Amini et al. [1, Def. 6] notion of -compressibility for random sequences. More importantly, Theorem 1 offers new techniques to extend that connection (and, consequently, a dichotomy between being and non-being -compressible random sequences) to the family of stationary and ergodic processes. This extension is constructed over the almost sure convergence of the empirical distributions of and , respectively (see definitions in the statement of Theorem 1) to the true probabilities on the family of tail events(details in Section IV). 
 The idea of looking at specific empirical measures as the basic object of interest, in (22b) and (22c), instead of the statistics of the sum of the ordered sequence as considered in [2, Prop. 1] and [1], was essential to extend the analysis from the i.i.d. case to the case of stationary ergodic processes. 
 Finally, one can notice from the proof of Theorem 1 that this result does not rely on a stationary property, as it is essentially basedonanalmostsureconvergence(asymptoticinnature)over the family of indicator functions of the tail events  in (19). Then, we conjecture that the analysis of compressible priors can be extended over a family of random sequences with a specific ergodic property over the tail events, which is an interesting direction for future work. This observation leads us to put the attention on the general theory of (non-stationary) processes with ergodic properties [14], [18], [22]–[24]. 
 Point 2): (Achievability of all rates and distortions in ): Using Proposition 5 b) and c) and the hypothesis that 
 in . Then, adopting Proposition 5 e), we have that achieves all the rates and distortions in the range . 
 First, it is important to verify that the implicit characterization of presented in (18) and (20) offers a well-defined function. By contradiction, let assume that such characterization is not a function in the sense that for a given distortion 
 there are two values solution of (20) associated to two different rates . This last condition implies that from (45), which contradicts the fact that and are solutions of (20). 
 Moving to the continuity, let us fix an arbitrary and . Then by the achievability of the distortions, 
 such that . On the other hand, by the achievability of the rates, there exists , where 
 and (without loss of generality we assume that ). Then from monotonicity of we have that for any , 
 we have (by the monotonicity of the function ) that there exists such that , and consequently	, 
 First, it is clear that both functions and are differentiable by construction. In fact from (46), , 
 Furthermore, we can introduce the auxiliary function that is differentiable, and 
 Proof: Considering that , let be its pdf. In view of Theorem 1, for a fixed , there exists such that can be expressed as: 
 , the middle term in (63) is negative and increases with , proving the convexity for that case. For the case , from the right hand side of (63) convexity will hold, if and only if, 
 Proof: Let fix and . Assuming that is -compressible, from Theorem 1 we have that and, consequently, it follows that: 
 this inequality is valid with probability over sampling space of the random object . There- 
 20, which is valid for any arbitrary small and . In other words, if we define the set 
 , we need a slightly different argument. Under the assumption that , it is simple to verify that there exists a sequence of positive integers such that and, more importantly, it satisfies that 
 derives from the Borel-Cantelli Lemma [14] and the fact that as by hypothesis  . 
 Following similar steps than before, there exists such that we have that , and therefore, from Lemma 4, 
 with probability in . It is important to define the set	where Lemma 4 tells us that . Furthermore, from 
 with probability one with respect to the process distribution of . In other words, if we define the set 
 where it is simple to show that , by (72) and the definitions of and . Hence, the problem reduces to evaluate, 
 the last equality from the fact that . Then from the additivity and monotony of the measure , which concludes the result by the definition of . 
 and	. Consequently eventually in	,	, which implies that eventually . This concludes the result. 
 For the other inequality, we consider the nontrivial case when . We prove it by contradiction assuming that . Then from (5), there exists and a sequence such that 
 . Under the fact that , there exists such that , . Using this and the definition of in (6), , 
 This approach can be iterated a finite number of times (independent of the length 
 Proof: Using the arguments to prove Theorem 2 (Point 4), if we consider , then for any there exists   such that	. This last auxiliary function is diferentiable (with respect to ) and we have that: 
 with , it follows that is negative and increasing with , i.e., is a convex function of . 
 The family of stable laws is the class of non-degenerate probabilities that are limit (in distribution) of sequences of random objects of the form [14, Ch.9]: 
 where are i.i.d realizations of a random variable, and and are a sequences of real numbers. For the wellknown scenario when , the Central Limit Theorem tells us that the limit is a normal law. For the case  , we have the less known family of -stable laws, whose characteristic function is given by [14, Th.9.27]: 
 Definition 8: [14, Def. 9.33] The distribution	is said to be in the domain of attraction of an	-stable law with 
 , which we denote by	, if there exists and	such that:	(in distribution) and	follows the	-stable distribution in (81). 
 Proof: Let and . Without loss of generality let us assume that is slowly changing. Then it is simple to verify that , and such 
 ﻿ This paper with control system design subject to average data rate . By on , and a class of source , we establish lower and upper on the minimal average data rate to achieve a performance level . We also provide a specific source scheme , within the class , that is to achieve the desired performance level at average data below our upper bound . Our are based upon a recently framework to address control subject to average data rate . 
 The study of control subject to communication recently received much attention in the control community see , e .., the in the special issue . Within this framework , a key question to the trade between control and communication . This paper on the interplay between average data rate in per sample and stationary performance in a class of control . 
 When stability is the sole control objective , the of guarantee that , for a noisy plant model and subject to mild on the noise statistics , it is possible to find causal , and such that the resulting closed loop system is mean square stable , if and only if the average data rate is greater than the sum of the logarithm of the absolute value of the unstable plant . This result a fundamental separation line between what is achievable in over digital and what is not , when the problem of interest is mean square stability see also the thorough discussion in the survey paper . 
 When performance subject to average data rate are sought , there are relatively available . There exist lower on the mean square norm of the plant state that make explicit the fact that , as the average data rate the absolute minimum for stability , the performance becomes arbitrarily poor when are present , . This irrespective of how the coder , and controller are chosen . Unfortunately , it is unclear whether or not these are tight in general . 
 A more general performance approach been in , . In those works , for separation and certainty equivalence have been in the context of quadratic stochastic for fully with data rate in the feedback path . If the a specific recursive structure , then certainty equivalence and a quasi separation principle hold . This result is interesting , but does not give a computable characterization of the optimal . A similar drawback is by the in . In that work , performance related are expressed in of the so sequential rate distortion function , which is very difficult to compute in general . For fully first order , an expression for the sequential rate distortion function . However , it is not clear from the in whether or not the sequential rate distortion function is tight see Section in . 
 Other works related to the performance of control subject to data rate are in and . The first work on noiseless state estimation subject to data rate under three different criteria . The case most relevant to this work an asymptotic in time quadratic criterion to measure the state reconstruction error . For such a measure , it is shown in that the bound established in is sufficient to achieve any asymptotic distortion level . This is , however , at the expense of arbitrarily large estimation for any given finite time . This feature of the solution the in too optimistic . On the other hand , non linear stochastic control over noisy , and a functional i . e ., not explicit characterization of the optimal control is . 
 In this paper , we focus on . Our main contribution is a characterization of upper and lower on the minimal average data rate that one to attain a given performance level as measured by the stationary variance of the plant output . To that end , we focus on a specific class of source that , as special , the studied in , . We also provide a specific source scheme , within the class , that is to achieve the desired performance level at average data below our upper bound . Instrumental to our is the characterization of the minimal signal to noise ratio that a closed loop performance level in a related class of . , . 
 The remainder of this paper is organized as : Section the problem of interest in this paper , and the class of considered source . Section , for the class of source in Section , a relationship between average data and an internal . Section on the interplay between and closed loop performance . These are then used in Section to present our main contribution . Section . 
 Notation : the set of real , the set of strictly positive real , for the magnitude absolute value of . is the set of 
 all proper and real rational transfer , all stable transfer in , and U all the , all the strictly proper transfer in 
 minimum phase transfer in . L and L are defined as usual , and the associated are by and , respectively . 
 This paper on the of Figure . In that figure , is a plant , u is the control input , is the plant output , an input disturbance . The feedback path in Figure an error free digital channel , and thus quantization becomes mandatory . This task is carried out by an whose output correspond to the binary . These are then back into real by a . It is clear that the and also embody a controller for the plant . As will become clear as we proceed , a distinction between the controller task and the task is rather artificial in our setup . 
 Assumption The strictly proper , free of unstable hidden , and no or on the unit circle . The initial state of the plant , and the disturbance , are such that 
 , is jointly second order ; is stationary and spectral factor U . In this paper we focus on source i . e ., of the following type : 
 Definition The source scheme of Figure is said to be linear if and only if its output u are related via 
 a second order zero mean i . i .. sequence , is independent of and , are the transfer of with deterministic initial . 
 The class of linear the class of so independent and i . i .. in , . We acknowledge that it is a restricted class of . However , its simplicity one to actually address control system design subject to average data rate . 
 Any linear source scheme can be written as shown in Figure a , where E and are proper transfer . The design of E and is however non trivial . Indeed , it is easy to see that designing E and to an optimal control problem subject to sparsity . To the best of our knowledge , the only known sufficient see that allow one to pose such as convex are not satisfied in our case . 
 by the previous discussion , we introduce the alternative of a linear source scheme shown in Figure , where , E ,, are the transfer of with 
 deterministic initial . In the scheme of Figure , is redundant . However , as will become clear below , this over of linear source one to construct a convex optimization problem that is equivalent to that of designing and E in Figure a . 
 When the linear source scheme of Figure is used in the of Figure , the feedback system of Figure . For future reference we define 
 Remark In the remainder of this paper , when we refer to a linear source scheme , it must be understood that we refer to the specific architecture of Figure , in 
 In this section we make a connection between the average data rate across a linear source scheme , and the stationary second order of the Figure . In 
 order to do so , we first make the relationship between those and the channel in Figure explicit . 
 Without loss of generality , we assume that the link Figure is given by the scheme of Figure . In that figure , E ,, and are causal such that 
 where , are possibly nonlinear time deterministic , and SE , , SH correspond to side information that becomes available at time the side , side , and both at the and sides , respectively . The range of is assumed to be countable , and that of , to be a countable set of prefix free binary . The and are chosen so as to satisfy 
 for any , and any N . Condition explicit the fact that act as a transparent link between the output of E and the input Figure . Since we assume an error free digital channel , and , it that . 
 It is clear that , when one the scheme of Figure as the link a linear source scheme , one needs to focus on , such that the process 
 the in Definition . We also note that , since , the feedback link the side in Figure does not require a physical channel . To at the side , it to the side . 
 We denote the length of the symbol i by i , and define the average data rate across the considered source scheme as 
 The Di is such that , i N , there another deterministic . such that i . e ., is invertible upon knowledge of 
 Assumption a is by the sensible requirement that the Figure only past and present , and side information not related to the message being sent , to construct its current output . On the other hand , if , for some and , Assumption does not hold , then one can define another set of that achieve a the same statistics as in the original situation , but at the expense of a lower average data rate . Accordingly , if one at , then one can focus , without loss of , on and satisfying Assumption . 
 The main focus of this paper is on the minimal average data rate that one to attain a given performance level , as measured by the stationary variance of the plant output . With the above , we are now in a position to formally define the problem of interest as : Problem Consider the of Figure , suppose that Assumption , that the source scheme is linear , and that the scheme of Figure is used as the link . Denote by the minimum stationary variance is achievable in the of Figure when . Find , for a given performance level , , 
 where is the stationary variance of , and the optimization is carried out with respect to : 
 • All causal Definition , Assumption ., by , such that the noise see is as 
 Remark and the auxiliary definition of independent source scheme that , the in Definition , the considered in Problem ,,, E 
 In order to solve Problem , we will first establish a lower bound on the average data rate across a linear source scheme in of stationary second order of the see Figure . The existence of this bound Section , where we study the optimal design of linear source subject to . These are then used in Section to give both upper and lower on . 
 This section the of Section A in to show that , when a linear source scheme is used in the of Figure , the minimum average data rate across it subject to a performance constraint is bounded from below by a simple function of the minimum ratio between the stationary subject to the same performance constraint . To do so , we start by that the following : 
 Theorem Consider the of Figure where the source scheme is linear , and the link , given by the scheme of Figure . If ,,, E and and 
 where I the directed mutual information rate , , and is the variance of . Moreover , equality in the and , is the stationary power spectral density of inequality if and only . 
 Proof : The first inequality immediately from Theorem in . The second inequality from Part of Lemma . in , and from the proof of Theorem in see also Theorem . 
 Theorem a lower bound on the average data rate across a linear source scheme in of a simple function of the spectrum the equivalent noise variance . This key result , upon which the remainder of this paper is based upon , can be further simplified . Note that inequality 
 where is the stationary variance of , and is the of the linear source scheme . By finding the minimal subject to a performance constraint e .., an upper bound on the stationary variance of the plant output , one is also calculating an upper bound on the minimal value of the right hand side of , subject to the same performance constraint . If , in addition , the optimal solution to the former minimization problem is such that the gap between the left and right hand sides of the inequality in is arbitrarily small or can be made so without compromising , then , by virtue of Theorem , one would immediately get a lower bound on by the much simpler minimization problem . The following result that this is actually the case : 
 Lemma Consider the of Figure , where the source scheme is linear and a fixed noise source . Suppose that Assumption and define . If the choice ,,, E 
 H , C , F , E ,,, such that s , Fand , , then , for any arbitrarily that , f and , in addition , , there exist 
 Proof : The proof of this result goes along the of the proof of Theorem in . 
 by the discussion preceding Lemma , we will now focus on the following problem : 
 Problem Consider the of Figure where the source scheme is linear , and suppose that Assumption . Define , where pi is the unstable pole of . Find , for a given , , 
 Remark It from Theorem in that to the minimal compatible with mean square stability in the of Figure . 
 We note that Problem is concerned with the best achievable performance , as measured by , that is achievable when an upper on the . As shown in Lemma below , this problem is equivalent to the problem of finding the minimal achievable subject to an upper bound on . Our momentary change of focus is only due to technical see Remark below . The next lemma necessary for the ,,, E to belong to : 
 Lemma Consider the of Figure where the source scheme is linear , and suppose that Assumption . Define 
 and consider a factorization N , i . e ., consider . If ,,, EX ,, , , 
 Proof : It is clear from Figure in open loop . be stable to ensure the internal stability of the loop . On the other hand , a possibly non linear and discontinuous , accordingly , must be strictly proper for the architecture to be 
 well the same conclusion to ,,, E must contain as non minimum phase all the unstable , be an admissible one degree of freedom controller . Therefore , must belong to H , E , . If 
 Lemma . . in . Denote all the unstable we have from the Bode Integral Theorem that , for , E , i , . 
 To complete the proof , we note that is the closed loop transfer function from a disturbance at the input the output . By the , we thus conclude that must be as . 
 By virtue of Lemma , we are now ready to present a lower bound on the best achievable performance . To that end , we will make use of the following mild additional assumption : 
 If EH were identically zero at the optimum , then optimal performance would be by open loop . The where this are clearly of no interest in a control setting . 
 Theorem Consider the of Figure where the source scheme is linear , and suppose that and hold . Then , 
 , where the zero of . Moreover , the optimization problem on the 
 right hand side of is convex i . e . is a convex function of its , and is a convex set . 
 Proof : Under our we have that , for any ,,, E and , and exist and are given by 
 A simple contradiction based argument that , since Assumption , the constraint in is active at the optimum . Hence , at the optimum , 
 where the first equality upon for with , and the inequality from the inequality and the fact that . If one and . 
 that , then the first part of the result immediately from , and Lemma 
 To complete the proof we need to show that the optimization problem on the right hand side of 
 Lemma in convex . Since is an function of , is a convex function of its . The proof is thus ., and both convex , it from 
 Theorem that one can calculate a lower bound on the best achievable performance subject to an constraint by a convex optimization problem . Convexity , among other , that the problem of finding the bound and also and such that , for any , 
 Remark As an alternative to Problem , one can also consider the problem of finding the minimal subject to a performance constraint . By doing so , and by proceeding as in the proof of Theorem , one at an auxiliary optimization problem whose convexity we have not been able to prove yet . 
 We will next show that a solution of the optimization problem on the right hand side of actually a solution to Problem . To that end , we start by that the following : 
 . Equation by the proof of Lemma in , and by the definition of 
 . The that is an admissible one degree of freedom controller for . Thus , as all the unstable of . Accordingly , is stable , , and as the unstable plant only . Given the definition of , , we thus conclude that , and that , is also stable . Since , by assumption , is stable , we have that , L is analytic outside and on the unit circle and can thus be to any degree of accuracy 
 Theorem Consider the of Figure where the source scheme is linear , and recall the notation in Lemma . If and hold , then Moreover , for any > , there , , such that , satisfying the in Part of Lemma , and satisfying 
 Proof : Our first claim immediately from the second , which is proved below . 
 Since , E and , recall the definition of den , it that the choice of a well feedback loop . We next show that the choice of internal stability . To do so , we first note that the open loop transfer function u is given by HE . As such , there exist no unstable pole zero between . From the proof of Lemma , we know that is an admissible one degree of freedom controller for 
 and . Moreover , it is clear by construction see and that there are no unstable pole zero cancellation among any two transfer in the set , E , . The above imply that the closed loop is internally stable . 
 Thus , since , dis stable since f , it that there a , is admissible , the in Part of Lemma . On the other hand , and is stable . As a consequence of the above , such that s q 
 The fact that our choice of upon in , and that . 
 To complete the proof , it remains to show that the set of is such that , for any . By definition of , see , the choice for E in is such that there E L such that 
 and the numerator of the second term on the right hand side of can be written as 
 In , the first equality upon and , the inequality from the inequality , and the last equality from the fact that our choice of 
 , and from our choice and thus , . By the above and the use the definition of and 
 The result immediately from . Note that upon and , hence , to be chosen , once is chosen . 
 Theorem a characterization of the solution of Problem . In the next section , we will use this result to characterize on . 
 on the Minimal Average Data rate to Achieve a Given Performance Level 
 This section a characterization of the solution of Problem . In particular , we present both upper and lower on , and also a specific implementation of a linear source scheme that an average data rate that is to be below our upper bound . 
 Lemma Consider the of Figure where the source scheme is linear , and suppose that and hold . Define 
 Proof : By Problem one a set of such that and recall that Assumption that the constraint is active at the optimum . It is therefore immediate to see that , by definition , G is impossible . Assume now that and are the that minimize ,. Then , since the constraint in Problem is active at the optimum , we have that 
 Lemma that Problem is equivalent to the problem of finding in . That is , if some choice of the ,,, E and solve Problem and thus achieve a performance level and an , then the same optimize the subject to the constraint 
 Theorem Consider the setup and of Problem . If , in addition , Assumption and , then . Moreover , there a linear source scheme such that , while satisfying for any . 
 We now prove our second claim . Use Theorem to find ,,, E Ho , and a noise variance so such that and for the desired value of . Consider an entropy as the link , i . e ., pick E ,, and in Figure such that 
 where is a dither signal available at both the and sides accordingly , SE SH ,: i ; i to a uniform with step size , to the an entropy coder also loss less whose output symbol is chosen the conditional distribution of , given , and to the the entropy that is complementary to the entropy coder at the side . If is an i . i .. sequence , independent of ,, and uniformly distributed on 
 , , with so , then our claim from Corollary in . Theorem a lower bound on the minimal average data rate that is to achieve a given performance level , when linear source are employed to control . This lower bound is not tight , but the worst case gap is given by per sample 
 i . e ., . per sample . The first term of this gap to the divergence of the from , and due to the fact that generate uniform quantization noise 
 and not noise . The second term because practical entropy see proof of Theorem are not perfectly efficient , Chapter . A detailed discussion of these can be found in , . Since we constrain ourselves to a simple class of source , this gap inescapable but , in our view , it is a fair price to be given the simplicity of our approach . 
 A key aspect of our is that they are built upon the solution of an optimization problem . This is a key feature of our work , and one to easily provide average data rate in feedback , by standard control system design . 
 Remark If in Problem one the performance constraint , then the problem to the calculation of the minimal average data rate that is compatible with mean square stability , say 
 . By Theorem in , and proceeding as in the proof of Theorem , it that , for the setup and of the latter theorem , 
 where pi is unstable pole of . That is , independent source can achieve at average data that are at most per sample away from the absolute lower bound established in . 
 This paper studied control closed over noiseless digital . By on a class of source , we have established lower and upper on the minimal average data rate to achieve a performance level . Instrumental to our result was the characterization of the minimal that a given closed loop performance in a related control system . 
 Future work should focus on that include causal but otherwise unrestricted source . to the case are also under study by the . 
  
 ﻿This paper presents novel results on scalar feedback quantization (SFQ) with uniform quantizers. We focus on general SFQ configurations where reconstruction is via a linear combination of frame vectors. Using a deterministic approach, we derive two necessary and sufficient conditions for SFQ to be optimal, i.e., to produce, for every input, a quantized sequence that is a global minimizer of the 2-norm of the reconstruction error. The first optimality condition is related to the design of the feedback quantizer, and can always be achieved. The second condition depends only on the reconstruction vectors, and is given explicitly in terms of the Gram matrix of the reconstruction frame. As a by-product, we also show that the the first condition alone characterizes scalar feedback quantizers that yield the smallest MSE, when one models quantization noise as uncorrelated, identically distributed random variables. 
 In many signal processing applications, signals have to be represented by a series of numbers (samples), so that they can be processed, transmitted or stored in digital form. This paradigm requires sampling, quantization and reconstruction. 
 The quantization of the samples, namely the sequence {cj}Nj=1, N ? N, yields a sequence  whose elements are constrained to belong to a discrete set of scalars. We focus our attention on uniform quantization, and thus require that 
 The simplest and most common paradigm to recover the signal from the numbers is linear reconstruction. Here, one is able to recover the original signal, say a, via 
 In (2), {?j}Nj=1 is a set of vectors (a frame) in the reconstruction Hilbert space W (typically a subspace of 2 or of L2). Thus, the samples  are the frame expansion coefficients of a. Examples of linear reconstruction are the Shannon-Whittaker reconstruction formula, the reconstruction stage in filter-banks, and the inverse wavelet-transform. 
 Throughout this work, we will be concerned with the squared 2-norm of the reconstruction error, i.e., 
 for every c ? RN. Unfortunately, minimization of (3) subject to (1) is a non-convex optimization problem. Moreover, the complexity of solving this problem grows exponentially with the number of coefficients to be quantized. In addition, unless   forms an orthogonal set, one would need to “preview” the entire input sequence before being able to calculate any optimal quantized value for µj. This is incompatible with delay sensitive applications. 
 For the above reasons, in practice quantization is often accomplished via simpler sub-optimal methods that operate sequentially. The simplest of these correspond to scalar feedback (SF) quantizers. At the i-th iteration, these A/D converters obtain the output sample ui by simple scalar quantization of an auxiliary sequence, which is a linear combination of input and output samples, i.e., 
 In (5), the real scalars ai,j,ßi,j, i,j ? {1,2,...,N} are design parameters, and Q(·) is the nearest neighbour scalar quantization function 
 The above expressions can be used to describe many scalar quantization schemes, including PCM, DPCM and (multi-bit) Sigma-Delta (S?) converters [2]. The latter have been well studied in the context of shift-invariant reconstruction spaces (wherein reconstruction is done by LTI filters), and recently also for frame expansions (see, e.g., [3,4]). 
 Not surprisingly, for a given reconstruction frame, and in return for the above mentioned shortcomings, optimal vector quantization generally outperforms SFQ. However, it is not known under what conditions this performance gap exists. In this paper we derive those conditions. More precisely, we state necessary and sufficient conditions for SFQ to be optimal, i.e., to yield, for any input c, the quantized output sequence µ that minimizes D(c,µ) in (3). Our results extend the work documented in [5,6] to more general situations. 
 Notation We use bold lowercase letters, e.g. x, to denote both the sequence  and the column vector [x1 ··· xN]T, where the meaning is clear from the context. We also use bold letters to represent matrices (uppercase) and their corresponding column vectors (lowercase). For example, if G is a matrix, we use gi to refer to the i-th column of G, and gi,j to refer to the j-th element of gi. The null space and the Moore-Penrose pseudo-inverse of a matrix G are denoted respectively via N(G) and G†. The notation Gi refers to the sub-matrix obtained by removing the first i columns and i rows from G. Similarly, xj denotes the vector x without its first j elements. The symbol 0N denotes an N-length column vector of zeros. We use the short-hand notation G for the quadratic form xTGx. We write ”iff” as an abbreviation for ”if and only if”. Z corresponds to the integers, and we use ZN to denote the set of all N-length vectors with integer elements. We say a matrix or vector is integral iff all its elements are integers. 
 Here we will first present some facts regarding frames that will be used in our subsequent analysis . 
 A finite frame for a Hilbert space W is an ordered set of vectors   such that, for every w ? W, 
 where 2 denotes the set of square-summable sequences. The Gram matrix G ? RN×N of the frame  is defined element-wise via 
 It is easy to show from (5) that an SF quantizer cannot yield   for all c ? RN unless3 ai,j = di,j, ?i,j, where di,j is the Kronecker delta function. If the latter holds, then (5) can be written as follows: 
 where Q(v) = [Q(v1) ··· ,Q(vN)]T, F is the feedback matrix and n is the vector of quantization errors. In order for the above equations to be well defined, F needs to be lower strictly-triangular, i.e., lower triangular with all main diagonal elements equal to zero4. Notice also that F is the only degree of freedom in the design of an SF quantizer. 
 In order to determine D(c,u) for SF quantizers, it is convenient to define the noise shaping matrix 
 where IN denotes the N × N identity matrix. Clearly, S is constrained to be lower unit-triangular, i.e., lower triangular with all its main diagonal elements equal to 1. 
 Substituting (12) into (11) yields u = c + Sn. Using this, and substituting (10) and (7) into (3), the distortion achieved by SFQ can be written as 
 G ? RN×N, the distortion D(c,u) of an SF quantizer equals   for all c ? RN iff the following two conditions hold: 
 Notice that (i) describes a “matching” condition between the feedback matrix and the reconstruction frame. Thus, (i) can always be satisfied by a proper choice of F (which is given explicitly by (14)). On the other hand, condition (ii) depends only on the reconstruction frame, or more precisely, on its Gram matrix. 
 The proof of Theorem 1 will be given in Section 6, based on preliminary results given in Sections 4 and 5. The latter provide valuable insight into the SFQ problem, and stem from two alternative approaches: lattice quantization and dynamic programming. 
 In this section we use the fact that minimization of (3) subject to (1) is equivalent to a lattice quantization problem. To show this, we first note that any symmetric positive semidefinite matrix G ? RN×N can be decomposed as 
 where H ? RN×N is lower triangular (see., e.g., [8]). It then follows directly from (9) and (16) that 
 2. Thus, one can analyze the relationship between the images in W through ? of any group of sequences by looking at their images in 2 through H. In particular, 
 Since the quantization alphabet U is uniform (see (1)), the images through H of all the sequences µ ? UN constitute the reconstruction lattice 
 Accordingly, we say that H? is the generating matrix for L. Every lattice has a basic Voronoi cell, V0, associated with it, i.e., the region of points closer to the origin than to any other point in the lattice. More precisely, 
 . Similarly, we define the quantization cell around Hµ ? L of an SFQ converter as 
 where the hyper-cube ? : ? [- 2 , 2 ], 1 is the set containing all possible  quantization noise sequences. Thus, C(Hµ) is the set of all target points Hc for which an SF quantizer outputs the sequence µ. 
 With the above definitions, we can now prove the necessity of condition (i) of Theorem 1. 
 Lemma 1 For a reconstruction frame with gram matrix G, the distortion D(c,u) of an SF quantizer can equal  for all c ? RN only if condition (i) in Theorem 1 holds. 
 is that it has the minimum second moment among all the cells whose L-translations form a tessellation , see [1]. Thus, an SF quantizer is a candidate to be optimal only if its matrix F minimizes the second moment ofNC(0N). This second moment can be readily shown to be given by   trace   (I - F)TG(I - 
 f, since H is lower triangular and F is lower strictly-triangular. The fact that each trace term depends only on its corresponding column of F implies that the trace is minimized iff each fii minimizes i i i . 
 where ?i is an arbitrary vector in N(Hi) (and, thus, in N(Gi) as well). Substitution of the identity A† = (ATA)†AT into (21) yields (14), thus completing the proof. 
 Remark 1 If one models n as a vector of uncorrelated, uniformly distributed (u.u.d), random variables, one gets MSE =   trace trace{(I - F)TG(I - F)}. On the other hand, an SF quantizer whose feedback matrix satisfies (21) happens to characterize one of the noise shaping quantizers for frame expansions proposed in [3]. More precisely, condition (i) is satisfied by the variant in which the error associated with each quantized coefficient is projected onto all the coefficients ahead of the current iteration coefficient. Thus, Lemma 1 also shows that, using an u.u.d model for quantization errors, the latter scheme achieves the minimum MSE among all SF quantizers . 
 Sequential quantization methods, such as SFQ, decide upon the value of each output coefficient sequentially. Insight can be gained by analyzing them from a dynamic programming point of view. The key point is that each of the decisions contributes additively to the cost defined in (3), leaving, after each step, a sub-problem similar in form to the original one. In turn, each of these sub-problems is determined by the decisions already made. The following result allows us to formalize these observations 
 Proof 2 The result follows from direct algebraic manipulation, using the identity A†AA† = A† and from the fact that giiT = giiTGi†Gi, ?i ? {1,...,N} (which stems from G being positive semidefinite). 
 Recursive application of Lemma 2 to (17) allows one to split the total cost D(c,µ) as follows: 
 The summation on the right hand side of (23) represents the (irreducible) reconstruction error stemming from the first i - 1 decisions. The cost-to-go after decision i - 1 is the last term in (23). It has the same form as the original cost, but it contains the updated target vector tii. The latter can be regarded as a state vector which summarizes the effect of ci, and of previous decisions, on the costto-go. 
 Joint Sufficiency of (i) and (ii) It is well known in lattice theory that any two lattices L1 = M1ZN and L2 = M2ZN, with M1 and 
 M2 non-singular, are equal iff there exists an integral matrix T with det{T} = ±1 such that M1 = TM2 (see, e.g., [9]). On the other hand, if conditions (i) and (ii) hold, then there exists a lower strictlytriangular matrix ? ? N(H) such that S + ? is integral. Since S is lower unit-triangular, we have that det{S} = det{S+?} = 1, and thus ZN = (S + ?)ZN. It then follows that L = HZN = H(S + ?)ZN = HSZN. On the other hand, if condition (i) holds, then it follows directly from (21) that the product HS yields an orthogonal, lower triangular matrix. This in turn implies that L is a rectangular lattice. Moreover, it is easy to verify that the associated Voronoi cell V0 is given by the hyper-rectangle HSY, which is precisely the quantization cell of the SF quantizer, C(0N) (see (20)). Therefore (i) and (ii) guarantee that the corresponding SF quantizer is optimal. 
 Necessity of Conditions (i) and (ii) The necessity of (i) was shown in Lemma 1. Thus, it suffices to prove the necessity of (ii) assuming that (i) holds. If (i) holds, then the target vectors tj given in (24) can be written in terms of the feedback matrix F as follows 
 Now let us consider a vector c such that the target vector, at iteration i - 1, satisfies 
 for some ? ? UN-i+1 and some e ? (0,?). With the above target, an SF quantizer would choose µi-1 = Q(ti-1,i-1) = ?1 - ?, and thus. Then, from (23), the cost-to-go for the SF quantizer after i - 2 iterations can be split as 
 the minimum difference between the cost-to-go achievable by SFQ and that of the choice µi-1 = ? is 
 Usuch that (µi + fii?) ? N(Gi). As a consequence, the first term on the right hand side of (28) is strictly positive. It then follows that D(c,u) is strictly larger than , for sufficiently small values of e, completing the proof. 
 Lattice Quantization Interpretation It has been shown in the proof of Theorem 1 that (ii) is a sufficient condition for L to be rectangular and have a hyper-rectangular Voronoi cell. It is important to note that this can happen for a non-diagonal reconstruction Gram matrix, i.e., reconstruction vectors that are non-orthogonal, and even linearly dependent, since G is not required to be non-singular. It is also important to note that the converse does not necessarily hold, that is, a rectangular L does not ensure that condition (ii) is satisfied. More precisely, the fact that a lattice L = HZN is rectangular implies the existence of an integral matrix M with det{M} = ±1 such that HM is orthogonal. It doesn’t guarantee M to be also lower unit-triangular, as required by (ii). On the other hand, (i) alone implies HS is orthogonal, and thus C(0N) = HSY is hyperrectangular. For a uniformly distributed c, the MSE gap between such an SF quantizer and a lattice vector-quantizer is given by the difference between the second moments of C(0N) and V0. Although no closed from expressions are known for the second moment of V0 of arbitrary lattices, preliminary results suggest that it is possible to derive lower bounds for this gap from the non-integer part of the vectors mi defined in (14b). 
 Reconstruction by a Single LTI Filter By letting N ? 8 (and considering the distortion per sample D(c,µ)/N as the cost function), our results can be applied to cases where reconstruction is achieved using a discrete-time LTI filter, say R(z). Without loss of generality, we assume that limz?8 R(z) = 1. In this case, the reconstruction frame vectors take the form ?k = [0Tk-1 r(0)r(1)···]T, where r(·) is the impulse response of R(z). This setup turns F and H into infinite dimensional Toeplitz matrices, the first column of H being ?1. In turn, f1 can be seen as the impulse response of a filter F(z). It then follows that the orthogonality of the columns of HS stemming from (i) is equivalent to having 1-F(z) = R(z)-1. This corresponds to a whitening noiseshaping quantizer, which yields minimum MSE, in the alternative white quantization noise paradigm [2]. Similarly, an SFQ satisfying (i) also minimizes the MSE, see Remark 1. On the other hand, for this case, all the vectors mi (see (14b)) are equal to the impulse response (first sample removed) of F(z). Thus, (ii) translates into having the impulse response of 1 - R(z)-1 being integer-valued. Hence, the standard L-th order multi-bit S? converter is optimal for R(z) = (1 - z-1)-L. This extends the results reported in [6], obtained for U = {-1,1}. 
 We derived necessary and sufficient conditions that make scalar feedback quantization deterministically optimal, in the sense of generating, for any input, the quantized sequence that minimizes the 2-norm of the reconstruction error. The first condition, which depends only on the design of the scalar feedback quantizer, happens to characterize the best quantizer of this class, when a stochastic framework is adopted. The second condition depends only on the Gram matrix of the reconstruction frame, and can be satisfied for non-orthogonal, and even linearly dependent, reconstruction vectors. 
 ﻿ In this paper , we study a node asymmetric two way relay channel with relay private where two can communicate with each other only through a third node the relay , while the relay can itself exchange private with two other . Cut set upper bound and single sided genie upper bound are for this channel . To obtain an achievable rate , we use a superposition of lattice and random for at the , and successive interference cancelation for at the . It is shown that are within constant and . sec per user of single sided genie for restricted and non restricted , respectively . 
 I . INTRODUCTION 
 the potential to obtain higher coverage extension and throughput enhancement at lower cost . One of the important research in this area is two way relay channel where two can exchange information via a relay . It is commonly used as a building block in network . It was shown in , that by joint decode and forward and network , nearly optimal throughput can be accessible for the broadcast phase but it loss in as expressed in . In , it was shown that compress and forward scheme within half bit of the capacity region in the setting . 
 In another powerful method , compute and forward , the relay linear of the instead of each of them completely by knowing their own can extract the desired by linear . This method can exploit interference for gains as well as noise suppression . Based on this concept , in , an achievable scheme based on lattice was for the asymmetric , which was shown to achieve the cut set bound within bit . In , by lattice , the sum capacity of channel is within additive gap of sec and multiplicative gap of sec of single sided genie upper bound . In , by a superposition of lattice and random , the capacity region of two pair bidirectional relay network to within sec per user is . 
 In this paper , we consider an asymmetric full duplex with relay private where two user can communicate with each other through a third node the 
 . 
 , , r 
 m , r m , r 
 , , 
 Fig . : The Asymmetric . 
 relay . The relay its own private to be to two other , and also some of are to be used only at the relay . For simplicity , we use the term to refer to this model , in the sequel . There is no direct link between the . Forward and backward between the relay and are assumed to be asymmetric . We consider both restricted and non restricted for this channel . This situation the case where a base station between and between the backbone system and the . In , the deterministic capacity of was and was shown to be with single sided genie upper bound . Here we consider and obtain two upper for this channel based on the cut set upper bound , and the single sided genie upper bound . 
 Also we present an achievable rate for this channel based on the in and . In , lattice are used for that must be between the while superimposed random are used for that must be at the relay private . In , structured binning and successive interference cancelation are used at the . By examining the achievable rate region , we show that for all channel gains , it within and . sec per user of single sided genie upper bound for restricted and non restricted , respectively . 
 The paper is organized as : In section , system model and some are . The upper and achievable for are in and , respectively . In section , capacity gap calculation is . Some concluding are provided in section . 
 . SYSTEM MODEL AND 
 Consider as shown in Fig . . All are assumed to be full duplex , and there is a forward and a backward channel between each node and the relay . Let . ., , log , , ,, and Si i , i , i . e . excluding i . 
 Node i , to be to the Si , where positive real set . The are assumed to be independent . The of node i are into a an function . 
 , 
 where the symbol of , is a realization of a real random variable , satisfying the power constraint 
 . And , the previously 
 received at node i . This is non restricted which can result some dependency between the by different . In contrast , restricted the function 
 . 
 The received signal at node i at time given by the following . 
 , i , 
  
 X 
 , i 
 where the channel gain between i ,, i . is a realization of i . i .. noise i , with zero mean and unit variance . Each node i a function to decode Si as , 
 . All are shown by 
 a vector i ,, i and the sum rate is by Pi ,, i . The , or fi restricted , and define a code , for . A error if i ,, i . A said to be achievable if there a sequence of , with an average error probability that zero as . The set of all achievable rate the capacity the . The sum capacity is the maximum achievable sum rate given by . The degree of freedom is defined as 
  
 d lim lim , 
 . log . log where the ratio of available transmit noise power which is one . 
 Next , we review some on lattice that will be used in the sequel . For more see and the therein . 
 A lattice code is defined in of two and , which form a lattice partition , i . e ., . The lattice code is a lattice code which as and the region of as a shaping region . For , the set of coset is defined as . 
 In , Theorem , an achievable rate was for , where two lattice were designed , one for each source node . The theorem is as , 
 Theorem : For an , with power P P on the of the , and on the of the relay . The following region is , 
 , 
 , 
 where , and , denote of zero mean at the relay and and . The achievable rate region in Theorem is within bit of the cut set upper bound , regardless of channel . 
 Remark : Note that , in the , although the channel setting is broadcast , and achieve their point to point channel , second of the two in Theorem , without being affected by each other . This is because of the side information on the message at each node and the binning of message . For the complete proof see , Theorem . 
 . UPPER FOR 
 In this section , we present cut set upper bound and single sided genie upper bound for both non restricted and restricted . 
 A . Cut Set Upper For Non Restricted 
 The cut set provide upper from a its complement . The cut set for the are provided in the following theorem . 
 Theorem : The capacity region of non restricted is upper bounded by 
 R I X ; Yr X P 
 R I X ; Yr X P 
 I X , X ; Yr P 
 R I ; Y X P 
 R I ; Y X P 
 I ; Y , Y 
 Proof : It is very similar to what done in , e .. see , Appendix A . 
 Corollary : The sum capacity of non restricted is upper bounded by min P P P , 
 C P P and constraint satisfy . 
 Proof : By , the first term of is . By , , and , the second term of 
 is . By , is . 
 B . Single Sided Genie Upper For Non Restricted 
 According to the general in , Sec ., in the traditional cut set . . , the are divided into which represent the and , respectively . in each of these are assumed to fully by their side information . the traditional cut set bound to relay loose . In contrast , in the single sided genie bound , not all the in a set can share their side information . This method was used in the proof of the following theorem . 
 Theorem : The capacity region of is upper bounded by , 
 R P , R P 
 R P , R P 
 R , R P 
 R , R 
 Proof : Single sided genie approach is used to bound the last two sum as : To obtain the bound for R in , we assume that a genie 
 to node to node . To obtain the bound for R in , we assume that a genie to node and to node . To obtain the bound for R in , we assume that a genie to node and to node . To obtain the bound for 
 R in , we assume that a genie to node and to 
 node ,. 
 The Proof of some are in Appendix A . The whole strategy is similar to what was done in . The exist in the used side information which is from the between the nature of two . In channel , the are in the same symmetric situation around the relay and the relay have any private message . But in , the relay also as a sender node and is in a different situation in comparison to the other two . 
 Corollary : The sum capacity of is upper bounded by , 
 P and constraint satisfy . 
 Proof : By , the first term of is . Also by to , is . 
 C . Upper For Restricted 
 In this section , we consider restricted which the function instead of . In this model the transmit are independent which to a upper bound . Corollary and , and and are true for this model except that the term P should be with in , , , . 
 . THE ACHIEVABLE RATE FOR 
 In this section , we present an achievable rate for . In our scheme , we use , so it is an achievable rate for both restricted and non restricted . 
 The scheme the superposition of lattice and random for at the , and successive interference cancelation for at the . It can be considered as a mixture of the method for in , and the method for bidirectional relay network in . In strategy , each user its message into two , one for at the relay and one for at the other user . It random for the first part and lattice for the second part which is not necessary to be by the relay . 
 We should mention that there is a difference between this method and the one used in in which no private are assumed by the relay . In fact in , if we have e .., R R , user its message into a and a lattice , while user only a lattice . But we believe that by Theorem , it is not necessary to consider random for the part of the message of user with rate R R . All can be by lattice . 
 In the next step , the relay random code , and the linear equation of two lattice , with successive interference cancellation . Then , the relay its own private intended for each user and the linear equation of two lattice , each into a random , and the weighted superposition of them to two . The last step is at the , where each user first the undesired that have than the desired . Thus , those are and successively from the received signal one by one . 
 For the limited space of the paper , in the following , we only consider the case where the of relay private are greater than the rate of the other . 
 A . at the 
 According to the strategy , The transmit at the are given by , 
  
 where and are chosen from a random 
 of size , , and , for i , , respectively . and are lattice with sizes and respectively . 
 B . : at the Relay 
 The relay first the , , simultaneously , then the lattice point . It can be done successfully so long as , 
  
 C . at the relay 
 The relay , 
 from of size , for i , , respectively , with R , R . 
 D . : at the 
 We consider the case . Other case can be by the same interpretation . The at the can be done by successive interference cancelation as : at user : It can be done successfully so long as , 
  
  
  
 at user : First user , then by Theorem , be . It can be done successfully so long as , 
  
  
 In next section , we show that by choosing power coefficient appropriately any rate within a constant of the upper bound is achievable . 
 V . CAPACITY GAP CALCULATION FOR 
 In this section , we establish the capacity gap calculation for from single sided genie upper bound . Since the gap between the corresponding upper of restricted and non restricted is within sec , we only characterize the capacity gap calculation for restricted in the following lemma . The proof of this conjecture is shown based on the in section , via the following : 
 C P 
 , 
 Lemma : By the strategy , for any rate r , r , , satisfying 
 r P . , 
 r P . , 
 r , 
 r P . , 
 r P . , 
 r , 
 there a choice of power i ,, i such that of with i ,, i can be done with arbitrary small error probability for all channel gain . See the proof . 
 Note that if I , where I the unit vector , , then the rate the of Lemma . Thus , the sum capacity and the capacity of restricted can be within sec , and sec per user of single sided genie upper bound , respectively . By respect to , these are . sec , and . sec per user of single sided genie upper bound for non restricted , respectively . Note that each user i two corresponding , i . 
 . CONCLUSION 
 In this paper , we studied the capacity region of an two way relay channel with relay private with no direct link between . In this model the relay can exchange private with the in addition to the communication between two . We consider both restricted independent and non restricted dependent . We a cut set upper bound and a single sided genie for upper bound for restricted and non restricted . Then we extracted an achievable rate for this channel which was shown to be within constant and . sec per user of single sided genie are for restricted and non restricted , respectively . The scheme applied the same strategy in which is based on superposition of lattice and random for , and successive interference cancelation for . 
 APPENDIX A PROOF OF THEOREM 
 Proof : By inequality , and by considering that side information is available at user and 
 at user , we have , 
  
 where as . a from the fact that the are independent . and , 
  
 where from the fact that entropy . from the fact that given , i , and are independent . 
 Again , we consider genie side information and to and , respectively . By inequality and assuming independent , we have , 
  
 where as . the above two 
 and chain information in , 
  
 from and the fact that is a function of , , . e from and given are completely known . from and the fact that entropy . 
 OF LEMMA 
 Starting with for : From and , we have , r r a a , and , 
 , 
 From and , we have , 
  
 where , 
 , 
 a and ar , 
  
 Starting with for : It is obvious that is redundant since and log is an increasing function of . By working on the other , we have , 
 , 
 the above , we have , 
  
  
 ﻿This paper presents closed-form expressions for the space-frequency (SF) second-order statistics for the power gain of indoor wideband channels. These expressions hold for channel models in which multi-path components are clustered, both in their arrival angles and their arrival times. Assuming that the arrival times are independent of the angles of arrival, we derive a closed-form expression for the covariance between the channel power gains at two receiving antenna positions d meters apart, at any pair of frequencies ?1 and ?2. This expression reveals that increasing d yields the same SF covariance asymptotic reduction attainable by increasing |?1 - ?2|, only if, and that the covariance is symmetric in its dependence with ?1 -?2 and with ?1+?2. Thus, the channel power gain at higher frequency bands has less variation than at lower frequencies. Finally, we show that the variable part of the SF covariance decays approximately as 1/d. 
 The propagation of radio signals within indoor environments is a complex phenomenon, depending on the operating frequency, the type and location of the antennas and the presence of a usually large number of scatterers. Assuming linearity and that the channel conditions vary slowly, the channel behaviour is fully captured by its frequency response, H(j?), corresponding to the Fourier transform of its time-invariant channel impulse response h(t). 
 Due to multiple reflections in the surrounding scatterers, h(t) is typically composed of a sequence of short impulses with random arrival times and amplitudes, which tend to decrease with arrival time. It has been observed that, in indoor scenarios, both the arrival times and arrival angles of these multi-path components (MPCs) distribute in clusters (see [1]–[3] and [4]–[7], respectively). As a result, given any two MPCs known to belong to the same cluster, one cannot assume their times to be independent. The same is true for their angles of arrival and for their corresponding amplitudes. 
 The analysis of the space-frequency statistics of the indoor wireless channel has commonly been approached by studying the second-order statistics of the complex frequency response H(j?) or of its magnitude. The former case involves the complex correlation 
 This work was supported by the Chilean Research Council CONICYT under Projects 1095018, 1120468 and Basal project CCTVal (FB0821), and by the Research Direction of Universidad Tecnica Federico Santa Mar´ ´ia. 
 where H1(j?1) and H2(j?2) are evaluated at receiver (or transmitter) antenna positions x1 and x2, respectively, and where ()* denotes complex conjugation. This complex correlation, with ?1 = ?2, has been studied in [8], [9], empirically, and in [10], analytically. In both [8], [9] it is found that ?c decays with the separation d between x1 and x2 more rapidly when the operating frequency is increased. The analysis in [10] obtains closed-form expressions for ?1c,2(?,?) for non-isotropic three- and two-dimensional diffuse fields assuming independent scatterers, i.e., that the complex channel gain associated with MPCs from one direction are independent of those arriving from another direction. Under this assumption, it was shown in [10] that for the 2-dimensional case, 
 where c is the speed of light,  is the complex Fourier series coefficient of the normalized distribution function of scattered power P(f), Jm(·) is the m-th order Bessel function of the first kind, and f12 is the angle of the vector x2 - x1 with respect to a reference azimuth direction. 
 The second case (envelope correlation) is concerned with characterizing E[|H1(j?1)||H2(j?2)|] and the corresponding envelope correlation ?1e,2(?1,?2). The dependence of the latter expectation on the separation d has been studied in [9], [11], [12] from measured data and for ?1 = ?2. These studies reveal that, as expected, ?1e,2(?,?) decays as the separation d is increased, although it seems to approach a minimum, asymptotic value in the range 0.3–0.5, which is significantly larger than the lower asymptote of ?1c,2(?,?) [9]. 
 Our focus in this paper is on the channel power gain |H(j?)|2 and its second-order statistics, associated with E[|H1(j?1)|2 |H2(j?2)|2]. Considering the latter expectation (instead of that of the complex gains or their magnitudes) is relevant for two main reasons. First, the instantaneous channel power gain at a given frequency determines the narrowband associated capacity. Hence, characterizing in closedform the second-order SF statistics of |H(j?)|2 would allow for a better estimation of the performance of spatial diversity systems based upon, for example, maximum ratio combining with branches associated to different antenna positions and/or carrier frequencies. Second, in schemes based upon selection combining, as well as in multiple access channels subject to the power capture effect [13], the performance depends on the joint statistics of two or more power values, arising from different antennas and carrier frequencies. 
 In this paper, we find closed-form expressions for the second-order statistics of the channel power gain |H(j?)|2, associated with the SF-autocovariance 
 These expressions depend explicitly upon the frequencies ?1, ?2 (not necessarily equal) and the separation distance d. Our analysis is valid for an extended Saleh-Valenzuela model, with MPCs with clustered arrival times and angles, as in the models considered in [4], [5]. The obtained expressions reveal how c1,2(?1,?2) decays as d and |?1 - ?2| are increased, but predict a strictly positive lower asymptote, which is approached by letting |?1 - ?2| go to infinity, for any d, and by letting d ? 8, but only if . 
 In other words, there exists a SF-dependent additive term in c1,2(?1,?2) and, in the limit, increasing frequency or spatial separation asymptotically reduces this term to zero, yielding the same asymptotic autocovariance reduction. A second finding is that c1,2(?1,?2) behaves in exactly the same way with respect to ?2 - ?1 as it does with ?1 + ?2. The latter implies, by considering the case ?1 = ?2, that the power gain of narrowband channels with higher central frequency have smaller variability. On the other hand, the obtained expressions show that, beyond separations of a few central-frequency wavelengths, the SF-dependent component in c1,2(?1,?2) decays with d approximately as 1/d. 
 In the next section, we present the channel model and assumptions. The main results are derived in Section III, followed by conclusions. All proofs are presented in the Appendix. 
 As in [4], [5], the wireless indoor channels considered here are characterized by their impulse responses. Each impulse response consists of a sequence of multi-path components (MPCs) with random amplitudes, arriving at random times and angles, and where arrival times and angles are clustered. At a given reference location x0 in space, such random impulse response h0(t) can be written as 
 where ti,m is the random arrival time of the m-th (MPC) in the i-th cluster and ai,m is the corresponding (real valued) MPC amplitude. Our assumptions associated with the arrival times, amplitudes and angles of MPCs are described next. 
 where Ti = 0 is the random arrival time of the i-th cluster, and ti,m = 0 is the random delay of the m-th MPC in the i-th cluster relative to Ti, where T1 = T2 = ··· and ti,1 = ti,2 = ···, for all i. By definition, the cluster begins with its first ray, and thus 
 In the forthcoming analysis, it will be useful to describe the overall distribution of arrival times by their corresponding arrival density functions. This requires one to introduce the following preliminary notions and notation. For an infinite sequence of random arrival times , with x1 possibly 
 In this expression,  is a random variable representing the amplitude of the i-th cluster. The real-valued random variable ai,m = 0 is the amplitude of the m-th MPC within the i-th cluster relative to that cluster’s amplitude. The polarity factors {pi,m} take values from {-1,1} with equal probability [2]. 
 The cluster amplitudes   are independent given their arrival times  . As in [14], the dependency of the cluster amplitudes upon their arrival times can be characterized by the cluster mth-order moment delay profile functions 
 if T1 is random. Likewise, the MPC relative amplitudes   are independent given , and we define 
 where m ? {1,2}, for every i ? N. As we shall see, here we will only need to consider the orders m ? {1,,...,4}. 
