Let ?i,m be the angle of arrival, measured with respect to a reference vector d on the horizontal plane, of the MPC with arrival time ti,m, As in [4], [5], the angles of arrival of the MPCs within the k-th cluster can be decomposed as 
 In this expression, ?˜k is the angle of arrival of the k-th cluster, while, for each   are the angles of arrival of the 
 If the transmitter and receiver positions and the scattering environment are chosen at random, then the cluster angles {?˜k} will be uniformly distributed, as observed in [6], [7]. Likewise, the absolute angles  will also be uniformly distributed. Moreover, the measurements in [6], [7] reveal that, in indoor scenarios, the distribution of the difference between cluster angles and the angle of the first cluster in each location is also uniform. This suggests that, conditioned to a given location (room and building), cluster arrival angles can be assumed independent. In contrast, we shall not assume that the angles   are independent, since the MPCs within each cluster have been found to arrive from a relatively small range of angles [6]–[8]. Instead, we will assume that, for all k, the relative angles   are independent and identically distributed.  More precisely, we will work based upon the following assumption: 
 The cluster arrival angles {?˜k} are independent and uniformly distributed over [0,2p). 
 The MPC relative arrival angles {?¯i,m} are independent, identically distributed with probability density function f?¯ : [0,2p) ? [0,8). 
 Cluster arrival angles and MPC relative arrival angles are independent, i.e., {?¯k,m} ? {?˜k}. 
 In any given indoor scenario, the impulse response from transmitter to receiver will depend on their corresponding antenna positions. Suppose the impulse response (2) is obtained when the receiving antenna is located at a given point x0 on the horizontal plane. If we consider two additional receiver positions, x1  x0 + d/2, x2  x0 - d/2, then their MPC arrival times will increase or decrease, depending on their arrival angles. More specifically, if the separation   is small relative to the distance from the scatterers, then the arrival angles can be considered to be the same at x1 and x2. In such case, an arrival time tk at position x0 becomes tk - sk and tk + sk, at positions x1 and x2, 
 is the random delay increment, in seconds, for the k-th MPC produced by moving from x1 to x0 (or from x0 to x2) and c = 3 × 108 [m/s] is the speed of light. Thus, the channel frequency responses for receiver positions x1 and x2 are 
 For the above model and with only the assumptions given within it, we will next derive closed-form expressions for the second-order statistics of |H(j?)|2, over both space (separation) and frequency. 
 In this section we will derive a closed-form general expression for the space-frequency covariance c1,2(?1,?2) defined in (1). This result is formally stated as follows: 
 Theorem 1: Under the assumptions presented in Section II, let r and ? be two random variables distributed as 
 ?¯¯t(x,y)g2(x)g2(y), ?(x,y)  ?¯¯T(x,y)b2(x)b2(y) and where  denote, respectively, the Fourier transform and the L1 norm of the function f. 
 where (a) holds from the fact that ak = pkAkak and that the polarities {pk} are zero mean, i.i.d, and independent of all the other variables. On the other hand, 
 Since each amplitude ak = pkAkak and the polarities {pk} are i.i.d. with zero mean, it follows that all terms of (7) in which a given index is different from all the others will be zero. This leaves only the following index combinations: 
 Notice that letting   and   and returning to the double index notation (for clusters and MPCs within clusters), the function  can be written as, 
 From Assumption 1 we have that the random variable   is identically distributed for every k and for every 
 The sums corresponding to T0, T1 and the functions T3(·) and T4(·) have been characterized in [14, proof of Theorem 1]. Proceeding as in the latter proof, it is easy to show that these functions are as in (5) (please see [14] for details). Substituting (6), (8) and (9) and into (1), we obtain (4), completing the proof. 
 Theorem 1 reveals some interesting properties of the space-frequency covariance c1,2(?1,?2): 
 The difference T1 - T02 > 0 is the variance of , i.e., the variance of the impulse-response energy. This is the only term which does not depend upon ?1,?2. As already discussed in [14, Remark 3], under mild conditions on the impulse response statistics, the functions T3(x), T4(x) on the right-hand side of (4) vanish as x ? 8. The latter means that, in our case, the space-frequency covariance does not go to zero as the frequency separation grows unbounded. Interestingly, the expression for c1,2(?1,?2) here obtained shows that this covariance asymptote (given by the term T1 - T02) is the same regardless of the separation d. 
 The expectations of the complex exponentials involving d in (4) suggest that, as expected, increasing the separation between the two receiver antenna positions decreases the covariance of the channel power gain between any pair of frequencies such that 
 and  . Moreover, if the factors   E , is increased (which requires     ), then and E  should vanish as d 
 the resulting asymptotic covariance reduction would be exactly the same that would be obtained by letting |?1 - ?2| ? 8. 
 We now show that, as expected, the frequency-dependent terms in (4) do vanish as d ? 8, if O and T are non-zero. However, it turns out that the decay is not faster than 1/d. 
 Proposition 1: Suppose Assumption 1 holds and let the random variables r and ? distribute as in Theorem 1. Then, 
 are the complex Fourier series coefficients of f?¯(·).  It is well known that the Bessel functions {Jn(z)} are oscillating and that their envelopes decay as z-1/2. Hence, Proposition 1 reveals that the envelope of   E  decays as 1/d. The decay rate of E[ejOdr] with respect to d actually depends on the Fourier transform coefficients of f?¯. For instance, if ?¯ takes a deterministic value, then Cn(f?¯) = 1, 
 ?n, and we obtain E[e   (the last equality follows from Parseval’s theorem and noticing that {jnJn(z)} are the coefficients of the Fourier series of ejz cos(f), see (14) in the proof of Proposition 1 below). On the contrary, if ?¯has a uniformly-bounded probability density function, then, when d ? 8, E[ejOdr] goes to zero as 1/d or 1/d2, but not faster than that. These results are formally stated and proved in the following proposition: 
 Proposition 2: Suppose the conditions of Proposition 1 hold. Then, for every , there exists  such that 
 As a consequence of Proposition 2, we have that the spaceand-frequency dependent part of c1,2(?1,?2) (see (4)) decays with increasing d, asymptotically, as 1/d. Given the behaviour of the Bessel functions {Jn} and if the Fourier series coefficients of f?¯(·) decay sufficiently fast with n, then c1,2(?1,?2) will decay approximately as 1/d for d larger than a few wavelengths of the frequency difference O (the latter being given by 2pc/O), since |O| < T for ?1,?2 > 0 (see (4)). Likewise, for a fixed separation d, the two parts of this term decay as 1/O and 1/T, respectively, for relatively large values of O and T. 
 We have derived closed-form expressions for the spacefrequency (SF) second-order statistics of the power gain of indoor wideband channels. These expressions were obtained for the extended Saleh-Valenzuela channel model, wherein multi-path components arrival times and arrival angles are clustered. Our expressions reveal that the SF autocovariance c1,2(?1,?2), associated with two receiving antenna positions x1, x2 with channel power gains |H1(j?1)|2 and |H2(j?1)|2, respectively, is symmetric with respect to T = ?1 + ?2 and O = ?1 - ?2. Sine the autocovariance diminishes when O is increased, it follows that the variance of |H(j?)| also diminishes as ? grows. We have also shown that the SF-dependent part of c1,2(?1,?2) goes to zero as O and T go to infinity, which also happens if O and T are non-zero and we let d ? 8. In addition, we have demonstrated that the reduction of this SF-dependent term with d is approximately proportional to 1/d, for values of d larger than a few wavelengths of the frequency difference O. 
 Proof: [Proof of Proposition 1] From the distribution of r and ? (see the statement of Theorem 1), we can construct r and ? as 
 where ?˜ and ? are uniformly distributed over [0,2p), and ?¯1 and ?¯2 are independent and identically distributed according to the intra-cluster arrival-angle distribution with probability density function f?¯(·). From this, 
 which proves (10). In order to characterize E[ejrOd], we first recall the identity [15, p. 211] 
 Proof: [Proof of Proposition 2] For real-valued z, it holds that (see [16, section 9.2]) 
 , and because   is bounded. Rearranging terms in (16) leads directly to (13), completing the proof. 
 ﻿ In this paper , we study a node asymmetric two way relay channel with relay private where two can communicate with each other only through a third node the relay , while the relay can itself exchange private with two other . Cut set upper bound and single sided genie upper bound are for this channel . To obtain an achievable rate , we use a superposition of lattice and random for at the , and successive interference cancelation for at the . It is shown that are within constant and . sec per user of single sided genie for restricted and non restricted , respectively . 
 the potential to obtain higher coverage extension and throughput enhancement at lower cost . One of the important research in this area is two way relay channel where two can exchange information via a relay . It is commonly used as a building block in network . It was shown in , that by joint decode and forward and network , nearly optimal throughput can be accessible for the broadcast phase but it loss in as expressed in . In , it was shown that compress and forward scheme within half bit of the capacity region in the setting . 
 In another powerful method , compute and forward , the relay linear of the instead of each of them completely by knowing their own can extract the desired by linear . This method can exploit interference for gains as well as noise suppression . Based on this concept , in , an achievable scheme based on lattice was for the asymmetric , which was shown to achieve the cut set bound within bit . In , by lattice , the sum capacity of channel is within additive gap of sec and multiplicative gap of sec of single sided genie upper bound . In , by a superposition of lattice and random , the capacity region of two pair bidirectional relay network to within sec per user is . 
 In this paper , we consider an asymmetric full duplex with relay private where two user can communicate with each other through a third node the 
 relay . The relay its own private to be to two other , and also some of are to be used only at the relay . For simplicity , we use the term to refer to this model , in the sequel . There is no direct link between the . Forward and backward between the relay and are assumed to be asymmetric . We consider both restricted and non restricted for this channel . This situation the case where a base station between and between the backbone system and the . In , the deterministic capacity of was and was shown to be with single sided genie upper bound . Here we consider and obtain two upper for this channel based on the cut set upper bound , and the single sided genie upper bound . 
 Also we present an achievable rate for this channel based on the in and . In , lattice are used for that must be between the while superimposed random are used for that must be at the relay private . In , structured binning and successive interference cancelation are used at the . By examining the achievable rate region , we show that for all channel gains , it within and . sec per user of single sided genie upper bound for restricted and non restricted , respectively . 
 The paper is organized as : In section , system model and some are . The upper and achievable for are in and , respectively . In section , capacity gap calculation is . Some concluding are provided in section . 
 Consider as shown in Fig . . All are assumed to be full duplex , and there is a forward and a backward channel between each node and the relay . Let . ., , log , , ,, and Si i , i , i . e . excluding i . 
 Node i , to be to the Si , where positive real set . The are assumed to be independent . The of node i are into a an function . 
 where the symbol of , is a realization of a real random variable , satisfying the power constraint 
 received at node i . This is non restricted which can result some dependency between the by different . In contrast , restricted the function 
 The received signal at node i at time given by the following . 
 where the channel gain between i ,, i . is a realization of i . i .. noise i , with zero mean and unit variance . Each node i a function to decode Si as , 
 a vector i ,, i and the sum rate is by Pi ,, i . The , or fi restricted , and define a code , for . A error if i ,, i . A said to be achievable if there a sequence of , with an average error probability that zero as . The set of all achievable rate the capacity the . The sum capacity is the maximum achievable sum rate given by . The degree of freedom is defined as 
 . log . log where the ratio of available transmit noise power which is one . 
 Next , we review some on lattice that will be used in the sequel . For more see and the therein . 
 A lattice code is defined in of two and , which form a lattice partition , i . e ., . The lattice code is a lattice code which as and the region of as a shaping region . For , the set of coset is defined as . 
 In , Theorem , an achievable rate was for , where two lattice were designed , one for each source node . The theorem is as , 
 Theorem : For an , with power P P on the of the , and on the of the relay . The following region is , 
 where , and , denote of zero mean at the relay and and . The achievable rate region in Theorem is within bit of the cut set upper bound , regardless of channel . 
 Remark : Note that , in the , although the channel setting is broadcast , and achieve their point to point channel , second of the two in Theorem , without being affected by each other . This is because of the side information on the message at each node and the binning of message . For the complete proof see , Theorem . 
 In this section , we present cut set upper bound and single sided genie upper bound for both non restricted and restricted . 
 The cut set provide upper from a its complement . The cut set for the are provided in the following theorem . 
 Theorem : The capacity region of non restricted is upper bounded by 
 Corollary : The sum capacity of non restricted is upper bounded by min P P P , 
 C P P and constraint satisfy . 
 Proof : By , the first term of is . By , , and , the second term of 
 According to the general in , Sec ., in the traditional cut set . . , the are divided into which represent the and , respectively . in each of these are assumed to fully by their side information . the traditional cut set bound to relay loose . In contrast , in the single sided genie bound , not all the in a set can share their side information . This method was used in the proof of the following theorem . 
 Proof : Single sided genie approach is used to bound the last two sum as : To obtain the bound for R in , we assume that a genie 
 to node to node . To obtain the bound for R in , we assume that a genie to node and to node . To obtain the bound for R in , we assume that a genie to node and to node . To obtain the bound for 
 R in , we assume that a genie to node and to 
 The Proof of some are in Appendix A . The whole strategy is similar to what was done in . The exist in the used side information which is from the between the nature of two . In channel , the are in the same symmetric situation around the relay and the relay have any private message . But in , the relay also as a sender node and is in a different situation in comparison to the other two . 
 P and constraint satisfy . 
 Proof : By , the first term of is . Also by to , is . 
 In this section , we consider restricted which the function instead of . In this model the transmit are independent which to a upper bound . Corollary and , and and are true for this model except that the term P should be with in , , , . 
 In this section , we present an achievable rate for . In our scheme , we use , so it is an achievable rate for both restricted and non restricted . 
 The scheme the superposition of lattice and random for at the , and successive interference cancelation for at the . It can be considered as a mixture of the method for in , and the method for bidirectional relay network in . In strategy , each user its message into two , one for at the relay and one for at the other user . It random for the first part and lattice for the second part which is not necessary to be by the relay . 
 We should mention that there is a difference between this method and the one used in in which no private are assumed by the relay . In fact in , if we have e .., R R , user its message into a and a lattice , while user only a lattice . But we believe that by Theorem , it is not necessary to consider random for the part of the message of user with rate R R . All can be by lattice . 
 In the next step , the relay random code , and the linear equation of two lattice , with successive interference cancellation . Then , the relay its own private intended for each user and the linear equation of two lattice , each into a random , and the weighted superposition of them to two . The last step is at the , where each user first the undesired that have than the desired . Thus , those are and successively from the received signal one by one . 
 For the limited space of the paper , in the following , we only consider the case where the of relay private are greater than the rate of the other . 
 According to the strategy , The transmit at the are given by , 
 of size , , and , for i , , respectively . and are lattice with sizes and respectively . 
 The relay first the , , simultaneously , then the lattice point . It can be done successfully so long as , 
 from of size , for i , , respectively , with R , R . 
 We consider the case . Other case can be by the same interpretation . The at the can be done by successive interference cancelation as : at user : It can be done successfully so long as , 
 at user : First user , then by Theorem , be . It can be done successfully so long as , 
 In next section , we show that by choosing power coefficient appropriately any rate within a constant of the upper bound is achievable . 
 In this section , we establish the capacity gap calculation for from single sided genie upper bound . Since the gap between the corresponding upper of restricted and non restricted is within sec , we only characterize the capacity gap calculation for restricted in the following lemma . The proof of this conjecture is shown based on the in section , via the following : 
 Lemma : By the strategy , for any rate r , r , , satisfying 
 there a choice of power i ,, i such that of with i ,, i can be done with arbitrary small error probability for all channel gain . See the proof . 
 Note that if I , where I the unit vector , , then the rate the of Lemma . Thus , the sum capacity and the capacity of restricted can be within sec , and sec per user of single sided genie upper bound , respectively . By respect to , these are . sec , and . sec per user of single sided genie upper bound for non restricted , respectively . Note that each user i two corresponding , i . 
 In this paper , we studied the capacity region of an two way relay channel with relay private with no direct link between . In this model the relay can exchange private with the in addition to the communication between two . We consider both restricted independent and non restricted dependent . We a cut set upper bound and a single sided genie for upper bound for restricted and non restricted . Then we extracted an achievable rate for this channel which was shown to be within constant and . sec per user of single sided genie are for restricted and non restricted , respectively . The scheme applied the same strategy in which is based on superposition of lattice and random for , and successive interference cancelation for . 
 Proof : By inequality , and by considering that side information is available at user and 
 where as . a from the fact that the are independent . and , 
 where from the fact that entropy . from the fact that given , i , and are independent . 
 Again , we consider genie side information and to and , respectively . By inequality and assuming independent , we have , 
 from and the fact that is a function of , , . e from and given are completely known . from and the fact that entropy . 
 Starting with for : From and , we have , r r a a , and , 
 Starting with for : It is obvious that is redundant since and log is an increasing function of . By working on the other , we have , 
  
 ﻿ In this work , we deal with zero delay source of an unstable vector Auto Regressive AR source under average mean square error fidelity criterion . To begin with , we consider zero delay of a vector AR source in state space form . It turns out to be convenient to use an asymptotically stationary time domain scheme with feedback , which was recently in in the context of filtering theory via the Information Nonanticipative Rate Distortion Function . In this scheme , the feedback path a filter , which an estimate of the source . Our idea is then to encode the vector due to filtering via lattice quantization with subtractive dither and memoryless entropy . We show that the resulting system is stable and furthermore provide new to the . We then generalize our to vector AR of any order . With this , we are able to establish a approach with feedback , which by use of a single filter very close to the zero delay for vector AR of any order . Interestingly , for infinite dimensional vector , the with the Optimal Performance Theoretically Attainable by zero delay . 
 Zero delay of information in source is the reproduction of each source sample at the same time instant that the source sample is . Zero delay source is desirable in various , like for instance , signal and network control , . The class of zero delay is a subclass of the so causal source where the reproduction of the current source sample only on the present and past source but not on the future source . A preference for zero delay source over causal source from the fact that the latter does not exclude the possibility of long of , which for arbitrary end to end . Of course , every zero delay code needs to be causal , but the opposite is not true . 
 It is well known that zero delay and causal in with non causal cannot achieve the classical rate distortion function , i . e ., the optimal performance theoretically attainable by the class of . Indeed , an open problem in information theory is the gap between the by non causal , hereinafter by , and the by causal and , hereinafter by and , respectively . Notable where this gap is explicitly found are memoryless , stationary in high , and zero mean stationary scalar with average mean square error distortion . 
 Throughout the , the interest in information theoretic rate distortion that as tight as possible to the by causal or zero delay been very high . For example , the in the so entropy , hereinafter by , to demonstrate its utility in real time zero delay . Perhaps the most striking result inspired by , is the work of where the author the so sequential , hereinafter by , to investigate control related . A decade later , the in , the so information nonanticipative , by , in the context of filtering theory . The work of with the work of that under squared error distortion , for any AR source , the optimal reproduction distribution of the output , conditioned upon , X ,..., , Y ,..., , to i . e ., , , . the latter structural result , the in used an unstable AR source and a lower bound to . This bound can be by a feedback realization that was first in , where a filter a prediction of to the , which then via that act as the prediction error . Interestingly , it turns out that the filter in is from the noisy and thereby parallel a result of al . , on the of a stable , stationary , and scalar process by noisy prediction . Note that for stable stationary , the use of filter is not mandatory . In this paper , we show the following : 
 For the case of zero mean unstable vector fully AR with average fidelity , the feedback realization scheme of by the work in , that an upper bound to the can be by the due to a filter in a feedback loop , which the source based on its noisy . We provide a simple quantization strategy and assess the rate loss as to the achievable lower bound see Theorem . In addition , we show how to generalize our scheme to vector AR of any order see Theorem . 
 When the vector source is unstable , it is necessary to guarantee that the feedback realization is stable under quantization . We provide an upper bound to the due to finite dimensional quantization of the . We show that the rate loss is finite and directly linked to the space filling loss of the as well as the loss of the entropy coder due to zero delay . If the vector dimension of the source to infinity , it is possible to completely eliminate the rate loss and thereby show that in this limit , the with the see Section . 
 This paper is structured as . In Section we cast our problem . In and we derive upper and lower to the by zero delay and we discuss possible and related . In draw . 
 Notation : We let , , N , ,.... For N , we denote random and dimensional with , i . e ., and , respectively . 
 The transpose of a matrix or by . We denote by any term that is . For a square matrix with on the row and column , we denote by the matrix , i ,...,, on its diagonal and zero elsewhere . We denote the time index with and the dimension index with i . 
 In this paper we consider the zero delay source setting in Fig . . In this setting , the dimensional vector source is by the following linear time invariant state space model 
 N ; is the initial state , and the noise process is an i . i .. ; sequence , independent of x . 
 The system as . At every time step , ,..., the the source and a single binary from a alphabet set of at most a countable number of . Since the source is random , and its length are random . Upon , the an estimate of the source sample . We assume that both the and process information without delay and they are to have infinite memory of the past . 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable length binary . The countable set of all is time 
 Fig . : A zero delay source scenario binary . 
 to allow the binary representation to be an arbitrarily long sequence . There is no loss by to uniquely . The and are by of probability density as , : , ,... and , : 
 D , where is the distortion level , , yn The objective is to minimize the average length by 
 . These design are formally cast by the following optimization problem : 
 that A , is see , e .., to ensure that the limit in i . e ., . 
 In this section , we present a lower bound on the by zero delay the , . In general , the expression of the by zero delay given by is very hard to find and often are see , for example , , . 
 First , recall that since the by zero delay is a subset to the by causal see Section I then 
 Moreover , it is also known that for general the following hold see , e .., , equation . 
 Note that , inequality a is strict , in general , and becomes equality when the source is i . i .. or when the rate to infinity . In contrary , inequality is strict at high high resolution due to space filling loss and becomes equality at zero rate . Also , for unstable like the source model of , to be greater or equal to the sum of the unstable of matrix A , i . e ., whose are greater than one see e .., . 
 Since , the is a lower bound on the by zero delay to the classical , we can use this information measure to obtain on . 
 In nonanticipative rate distortion theory see e .., , , the mutual information can be written as 
 where E is the expectation with respect to the joint probability distribution 
 with , , yn , while is the marginal induced by the joint probability distribution , yn . We now state the definition of as given in , 
 where the in is taken with respect to the sequence of conditional probability density , : , ,...,. 
 If one by lim in , then an upper bound to is , defined as . 
 It is shown in that provided the limit in , and the source is stationary then na . The optimization problem of , in contrast to the one given in is convex see . In addition , due to the assumption of on the pair A , of . 
 , channel , via an asymptotically stationary feedback realization scheme in Fig . to find the optimal causal and zero delay information based filter . A detailed analysis on this scheme is provided in , Section . Next , we briefly explain the methodology in Fig . . 
 : N of : N based on the previous y ,..., defined by 
 The covariance matrix is by a unitary transformation E invertible matrix such that 
 at : Analogously , we introduce the error process : N defined by , and the scaling process : N defined by with , , ;, and 
 The fidelity criterion at not affected by the above of , : N , in the sense that the at both the and do not affect the form of the distortion function , that is , 
 basic of conditional entropy see , e .., . , it can be shown that 
 Fig . : Realization of the optimal distribution , see Section A . 
 In this section , we derive upper to the by zero delay a universal quantization scheme based on a subtractive dither with uniform scalar quantization on the feedback realization scheme in Fig . . Toward this end , we consider unstable vector AR source as in , and we quantize each time operating , with their being jointly entropy conditioned to the dither . Our approach is far from new in the literature . In fact , it is adopted in a variety of , e .., , , . to these works , our scheme with a purely information theoretic setup while at the same time it more general and . 
 Before we proceed , we state the definition of a scalar uniform with subtractive dither . A scalar function is defined as 
 where is the quantization step which is freely designed by the designer . A scalar universal uniform with subtractive dither is defined as 
 the realization of a uniformly distributed random the interval . 
 The execution of a common randomness both at the and the . In practice , the a synchronized pseudo random noise generator that can be used at both and end . 
 Next , we use use the asymptotically stationary feedback realization scheme in Fig . to design an efficient 
 We select the step size so that the covariance of the resulting quantization error . The does not quantize the state directly . Instead , it the deviation of from the linear estimate of . This method is known in least estimation theory as approach . As a result , we name the . 
 We consider the zero delay source setup in Fig . with the additional change of the parallel channel operating . This change is in Fig . . Note that , all matrices and adopted in Fig . still hold when the change is applied . 
 For each time step , the input to the , is a scaled estimation error defined as 
 Fig . : scalar uniform quantization by a dimensional channel operating . 
 Moreover , at is an valued random process . The parallel dimensional channel is operating , hence we can design the covariance the corresponding to the parallel in such a way , that for each , each diagonal entry of , i ,...,, i . e .,, V ,..., to correspond to a quantization step size i , i ,...,, such that 
 This into a input output transmission of parallel and independent . We apply to each component of at , i . e ., 
 and we let be the valued random process of dither whose individual , ,... are mutually independent and uniformly distributed random independent of the corresponding 
 source input at , i , , i . The output of the is given by , i i at , i , i , i ,...,. 
 Note that , ,..., , can take a countable number of possible . In addition , by construction see Fig . , the at : , ,... and : , ,... are not any more since by the change in Fig . , at : , ,... and : , ,... contain of the uniformly distributed process : , ,.... As a result , the filter in Fig . is no longer the least mean square estimator since the may no longer be . 
 For completeness , we illustrate in Fig . the relation between a and a scalar uniform additive noise channel , a result that was first pointed out in . 
 Entropy : In what , we apply joint entropy across the vector memoryless across the time , that is , at each time output of the is conditioned to the dither to generate a . The by the signal from . 
 Specifically , at every time step , we require that a message is into a , designed , Chapter . . For a random variable , the based on scheme give an instantaneous prefix free code with code length that the following bound 
 Fig . : An equivalent model to Fig . b based on scalar uniform additive noise channel . 
 In view of the assumption that the scalar uniform with subtractive dither memoryless entropy over time , the following theorem . 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating in Fig . . If the vector process : , ,... of the output is jointly entropy conditioned to the dither signal in a memoryless fashion for each , then the operational zero delay rate , , 
 the dimension of the state space representation given in , while the average distortion the end to end average the system . 
 Evidently , by combining the lower and upper to the by zero delay , we obtain 
 The derived in based on the scheme of Fig . hold for vector of any order . 
 In the next remark , we comment on and draw to in the literature . 
 Remark . For scalar AR , i . e ., , then to 
 This result was first in , Theorem but only for stable scalar AR . 
 Note that the zero delay rate distortion performance per dimension is given by the following expression 
 It is interesting to observe that if instead of scalar uniform quantization we quantize over a vector lattice by memoryless entropy conditioned to the dither , then the upper bound in becomes 
 where is the second moment of the lattice . If we take the average rate per dimension then becomes 
 Additionally , by assuming an infinite dimensional vector source , then by , Lemma and we obtain 
 As , vector quantization for infinite dimensional vector source , the term due to space filling loss and the loss due to entropy asymptotically goes to zero . 
 In this work , we considered zero delay source of an unstable vector AR source under average fidelity criterion . Based on a simple feedback realization scheme that the of a filter with a , we derived new to the by zero delay . We the performance of this scheme when lattice quantization . For infinite we that the is in fact the by zero delay . 
 In the realization scheme in Fig . , with the change of channel with parallel independently operating , the operational rate for equal to the conditional entropy where , ,..., ,, , i i at , i , i , i ,...,, i . e ., the entropy of the output conditioned on the value of the dither signal . This to the following analysis . 
 where a from , Theorem ; from the fact that the quantization noise is at see Fig . ; from the fact that the relative entropy x x , see , e .., , Theorem . . ; from the fact that at , with equality if and only if : , ,... becomes a distribution ; e from the fact that the differential entropy of a random vector with covariance , V ,..., is 
 where a from the structural of specific problem resulting in the realization of Fig . see , e .., the analysis in Section and , Remark . ; from the analysis in , Equation ; from the fact that E ,, are invertible matrices and as a result the information from is the same as from at to information lossless operation . 
 Since we are assuming joint memoryless entropy operating scalar uniform with subtractive dither , then by , for , ,...,, we obtain 
 Then , by first taking the per unit time limiting expression in and then the , we obtain 
 where a by and respectively , and na is the upper bound expression of for the unstable vector AR source model given by . 
 Finally , by assumption , i . e ., the source model given by is asymptotically stationary , hence as the source becomes stationary . this , we have na and the result . This the proof . 
 This can shown by the state of the source model given in as . Let denote a new state vector of the additional dynamics . Then the model of is as . 
 where the matrices A , A are the coefficient matrices of augmented dynamics . At this point , , can be written as 
 The state space representation of with the same on the augmented coefficient matrices A , can be used precisely as the feedback realization scheme of Fig . via a filter where the structure of the filter is the one derived in , Section . This the proof . 
  
 ﻿We analyze the behavior of the mean squared error (MSE) achievable by oversampled, uniform scalar quantization using feedback, pre- and post-filters of unrestricted order, when encoding wide-sense stationary discrete-time random sources having (possibly) unbounded support. Our results are based upon the use of subtractively dithered uniform scalar quantizers. We consider the number of quantization levels,  , to be given and fixed, which lends itself to fixed-rate encoding, and focus on the cases in which   is insufficient to avoid overload. In order to guarantee the stability of the closed-loop, we consider the use of a clipper before the scalar quantizer. Our results are valid for zero-mean sources having independent innovations whose moments satisfy some mild requirements, which are met by infinite-support distributions such as Gaussian and Laplacian. We show that, for fixed , the MSE can be made to decay with the oversampling ratio as when   tends to infinity, where . We note that the latter bound is asymptotic in but not in , and that it includes clipping errors. 
 T is well know that oversampling can reduce the magnitude of the reconstruction error that originates from quantizing the samples of an analog source, see., e.g., [1]–[3]. This reduction is exploited by analog-to-digital converters (ADCs) such as sigma-delta   modulators, which have been successfully utilized in audio and image quantization [1]. 
 , as , where is the oversampling ratio and denotes the order of the feedback filter (assumed fixed for all values of ). In their analysis, the authors of [2] utilized an additive noise model (ANM) [4], in which quantization errors are assumed to form a wide sense stationary (w.s.s.) random process, white and uncorrelated with the input samples. Also using the ANM, it was recently shown in [5] that by using different filters (of unrestricted order) for each value of  , the 
 MSE can be made to decay as  , where   denotes the signal-to-noise ratio of the scalar quantizer. The analysis in [2] and [5] restrict to the cases where the effect of quantizer overload errors is negligible, which cannot be guaranteed if   when the source has unbounded support unless infinitely many quantization levels are available. Indeed, an important body of literature related to oversampled quantization avoids overload errors either by careful design of the converters 
 or by simply assuming there exist enough quantization levels to avoid overload, see e.g., [3], [6] and the references therein. 
 Families of 1-bit (two-level)   converters in which the quantizeris guaranteedtoneveroverloadhavebeenfoundin[7], [8], by following a deterministic approach. The converters in [7] yield a continuous-time reconstruction error that can be uniformlybounded bya term proportional to  , where 
 is independent of  . In turn, the continuous-time reconstruction error with the converters constructed in [8] can be uniformly bounded as when . This leads immediately to an MSE that behaves as , when  . To the best of the author’s knowledge, the latter is the fastest decay rate of the reconstruction error with   available in the literature. 
 However, the results in [7] and [8] have not been extended to   modulators with more than two quantization levels, and rely upon the input samples being uniformly bounded. On the other hand, available results on the quantization of unbounded sources including the effects of overload errors do not consider oversampling, see, e.g., [9] and the references therein. 
 In this letter, we study the behaviour of the MSE with increasing oversampling ratio when the source is a (possibly unbounded) wide sense stationary (w.s.s.) band-limited process. Our analysis is based upon the use of a subtractively dithered uniform scalar quantizer (SDUSQ) [10], preceeded by a clipper, together with feedback, pre- and post-filters of unrestricted order (see Fig. 1). We focus on the cases in which the number of quantization levels,  , is insufficient to avoid quantizer overload. We show that, for this architecture, the 
 Assumption 1: The source process has independent innovations , with zero mean and symmetric probability density function (PDF). Moreover, there exists a constant such that the -th moments of each satisfy 
 This letter extends the work in [5] by taking account of clipping errors in the analysis. 
 Our results are related with the feedback quantization architecture shown in Fig. 1. In this scheme, the samples 
 form a zero-mean w.s.s. process, obtained from sampling a w.s.s. band-limited analog signal. For each oversampling ratio 
 In (2), , and is the square root of the PSD of the input process when . It is assumed that the input process has finite power, i.e., that	. For simplicity, we shall further restrict the analysis to the cases in which . Notice also from (2) that the total power of (in units of variance per sample), remains constant for all . 
 InFig.1, representsauniformscalarquantizer,withquantization interval and reconstruction levels.Thedither is a random process with i.i.d. samples independent of and uniformly distributed over the interval Adding dither to the input of the quantizer reduces the range for the input signal over which quantizer overload cannot occur. Since the dither is distributed over , this range is . It is well known that such a dither signal yields a quantization error process with i.i.d. samples which are also independent of the source [10], [11], provided 
 overload. Quantization error samples appear in the output as the stationary process	. In order to keep fromoverloading,weconsidertheuseofaclipperbefore , as shown in Fig. 1. The clipper limits the value of the input signal so that , if , or 
 thus ensuring stability, see [5]. The key point here is that, unlike overload errors, clipping errors, given by  , are not injected into the feedback loop. Instead, clipping errors appear in the output after being filtered by , to yield the process  . Unless the source 
 is a stationary process, one cannot guarantee that the samples of the clipping error will form a stationary, or even a w.s.s., random process. In order to quantify the contribution of clipping errors to the MSE for not-necessarily stationary sources, we define the average power of clipping errors in the output as 
 Two important parameters characterizing the conditions under which the combination of clipper and quantizer operate are the signal-to-noise ratio (SNR) 
 It follows from (5) and (6) that, if and are kept fixed, then   can only be increased at the expense of reducing the SNR at 
 For the scheme of Fig. 1, it was shown in [5] that the reconstruction MSE due to granular quantization errors only, which here corresponds to , can be upper bounded as 
 (11) i.e., on , for every , where satisfies (8). In (10) and (11), can be any bounded, nonzero gain. With the optimal filters in (10)–(11), relates to and the variance via [5] 
 Notice the upper bound on the MSE due to granular quantization errors in (7) decays exponentially with . However, the behaviour of the average power of clipping errors with increasing is unknown. Therefore, in view of (4), the exponential decay of given by (7) does not necessarily hold for , for sufficiently large. In the next section we find an upper bound to the total average power of the reconstruction error  , including clipping errors. 
 Proof: From one of Bernstein’s inequalities, given in [12, Sec. 5.5], we have that 
 every , the tightest bound for the first inequality in (14) is obtained with . Substituting this into 
 (14) yields	. The latter, together with the fact that leads directly to (13), completing the proof. 
 The following theorem provides an upper bound for applicable (but not restricted) to situations in which the source has unbounded support. 
 Theorem 1: Suppose there exists a scalar   such that   [see (2)]. Suppose that Assumption 
 DERPICH: A BOUND ON THE MSE OF OVERSAMPLED DITHERED QUANTIZATION WITH FEEDBACK	543 
 Thus, we have obtained an upper bound on the MSE due to clipping errors that grows linearly with and decays exponentially with (provided the product does not tend to zero as , see (16)). 
 The above upper bound for does not tend to zero with increasing unless one makes the loading factor grow with fast enough.Substituting and (6)into(5)weobtain 
 . From the latter, we have that	, where	. Thus, the term due to clipping errors in 
 (41) can be reduced only at the expense of having operate at a lower SNR. This, in turn, makes the term due to granular errors decay more slowly with increasing . 
 The optimal decay rate when is achieved by choosing and so as to make granular and clipping error terms decay at the same asymptotic rate. This is achieved if and only if and are chosen so that 
 equals 1. Before evaluating the above limit, note that from (16) we obtain since , being a random variable uniformly distributed over , has standard deviation and satisfies (1) with . Applying l’Hôpital’s rule to (43) twice and substituting by , we obtain that 
 We have studied the asymptotic behaviour of the reconstruction MSE of fixed rate dithered quantization with feedback as the oversampling ratio   tends to infinity, for w.s.s. sources having possibly unbounded support. It was shown that with the proper choice of filters and loading factor for each , the MSE can decrease with at least as fast as , where  does not depend on. 
 ﻿This work offers a necessary and sufficient condition for a stationary and ergodic process to be-compressible in the sense proposed by Amini,Unser and Marvasti [“Compressibility of deterministic and random infinity sequences,” IEEE Trans. Signal Process., vol. 59, no. 11, pp. 5193–5201, 201 , Def. 6]. The condition reduces to check that the  -moment of the invariant distribution of the process is well defined, which contextualizes and extends the result presented by Gribonval, Cevher and Davies in [“Compressible distributions for high-dimensional statistics,” IEEE Trans. Inf. Theory, vol. 58, no. 8, pp. 5016–5034, 2012, Prop. 1]. Furthermore, for the scenario of non- -compressible ergodic sequences, we provide a closed-form expression for the best  -term relative approximation error (in the -norm sense) when only a fraction (rate) of the most significant sequence coefficients are kept as the sequencelength tends to infinity. We analyze basic properties of this rate-approximation error curve, which is again a function of the invariant measure of the process. Revisiting the case of i.i.d. sequences, we completely identify the family of-compressible processes, which reduces to look at a polynomial order decay (heavy-tail) property of the distribution.
 Index Terms—Asymptotic analysis, best  -term approximation error analysis, compressed sensing, compressibility of infinite sequences, compressible priors, ergodic processes, heavy-tail distributions.
 I. INTRODUCTION
 EFINING notions of compressibility for a stochastic process, meaning that with high probability realizations of the process can be well-approximated in some sense by its best  -term sparse version [3], has been a recent topic of active research [1], [2], [4]–[6]. Quantifying compressibility for random sequences and the identification of compressible and sparse distributions (priors) are relevant problems considering the recent development of the compressed sensing theory [7]–[9] and its applications. These results can play an important role in regression [10], signal reconstruction (for instance in the classical compressed sensing setting [2, Th. 2]), inference, and decision-making problems [11], [12]. One important case is defining such a compressibility notion for i.i.d. processes where the probability measure is equipped with a density function1 [1], [2]. In this context, realizations of the process are non-sparse (almost surely), and conventional ways of defining compressibility for finite dimensional signals, based on the power-law decay of the best -term approximation error (or sequences that belong to the weak- ball), are not applicable either, as shown in [1], [2].
 Motivated by this problem, Amini et al. [1] and Gribonval et al. [2] have introduced new definitions for compressible random sequences. These notions are not based on the typical absolute approximation error decay pattern of the signals, but on a relative -best -term approximation error behavior. In particular, Amini et al. [1] formally define the concept of  -compressible process (details in Section II below). This new definition provides a meaningful way of categorizing i.i.d. random sequences (and their distributions), in terms of the probability that almost all the  -relative energy of the process is concentrated in an arbitrarily small sub-dimension of the coordinate domain, as the block-length tends to infinity. Under this context, they provide two important results using the theory of order statistics [1]. First of all, [1, Theorem 3] shows that a concrete family of i.i.d. heavy-tail distributions is-compressible (including the generalized Pareto, Students’s and log-logistic), while on the other side, [1, Theorem 1] demonstrates that families with exponentially decaying tails (such as Gaussian, Laplace, generalized Gaussian) are not  -compressible. Therefore, it is interesting to ask about the compressibility of i.i.d processes not considered in that analysis. In this direction, we highlight the work of Gribonval et al. [2], which under an alternative notion of relative
  -compressibility (involving almost sure convergences instead of convergence in measure, which was the criterion adopted in [1]) and a different analysis setting (fixed-rate instead of the variable rate used in [1]), elaborates an exact dichotomy between compressible and non-compressible i.i.d. sequences. This raises the question of whether it is possible to connect Amini et al. [1]  -compressibility with the more refined almost sure (a.s.) convergence analysis of the best -term relative approximation error in [2, Prop. 1], with the idea of completing the analysis of [1, Ths. 1 and 3].
 1053-587X © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
 To address this question, we extend the analysis from i.i.d. sequences to stationary and ergodic processes. In this broader setting, the main result (Theorem 1) provides a necessary and sufficient condition for a stationary and ergodic process to be  -compressible (in the sense of Amini et al. [1, Def. 6]), for any arbitrary  . Furthermore, for the case of non  -compressible ergodic processes, we provide a closed-form expression for an achievable rate v/s  -approximation error function. The key element in the proof is the application of the ergodic theorem [14] and the derivation of intermediate almost sure convergence results (Lemma 2 and 3 in Section IV) that match and extend the approximation result presented by Gribonval et al. [2, Prop. 1] developed for the i.i.d. case. A corollary of Theorem 1 implies a necessary and sufficient condition to categorize i.i.d. random sequences in terms of  -compressibility, which completes the analysis presented in [1, Ths. 1 and 3]. In addition, for the class of non- -compressible ergodic sequences, we provide an analysis of its rate-approximation error curve demonstrating that is continuous, differentiable (Theorem 2) and is convex under some conditions (Theorem
 3). Finally as an application, we revisit the interplay between  -compressible ergodic sequences and the performance of the classical Gaussian compressed sensing (GCS) setting [15], in the asymptotic regime when the block-length tends to infinity. Using the well-known  -instance optimality performance guarantee of the GCS scheme [3], [15], [16], we show (Theorem 4) that an arbitrarily small number of linear measurements (zerorate) is needed to achieve zero distortion, in an  -noise to signal ratio (NSR) sense. A preliminary version of this work was presented in [17]. The current version extends the presentation and analysis of the main result, provides further analysis of noncompressible ergodic sequences and explores connections with compressed sensing (CS).
 The rest of the paper is organized as follows. Section II introduces some preliminary elements and definitions. Sections III and IV are devoted to the presentation of the main result on the characterization of compressible ergodic processes and its proof, respectively. Section V studies basic properties of the the rate-approximation error curve for non  -compressible processes. Finally, Section VI elaborates an interplay between
  -compressibility and compressed sensing. Some of the proofs and derivations are presented in the Appendix sections.
 II. PRELIMINARIES AND BASIC DEFINITIONS
 For a finite dimensional vector in , let denote the ordered vector such that
 	. For some	and
 , let
 (1)
 denote the -norm of the best -term approximation of , where by definition . In addition,
 (2)
 denotes the best -term -approximation error of , in the sense that if	is the collection of -sparse signals, then	. For the analysis of infinite sequences, Amini et al. [1] and
 Gribonval et al. [2] have proposed the following relative best -term -distortion indicator:
 (3)
 with the objective of extending notions of compressibility to sequences that have infinite -norm.
 A. Rate of Innovation vs. Distortion for Infinite Sequences
 Definition 1: For a sequence , the rate-distortion pair is -achievablefor if,there isa sequence of positive integers such that and
 (4)
 where	is the finite-block version of length	of	.
 Note that the use of the relative best -term -distortion in (4) allows the analysis of sequences with infinite -norm.
 Definition 2: For a sequence and , we define its rate-distortion - approximation function by
 (5)
 for all	.
 A simple consequence of these definitions is the following result:
 Proposition 1: For all	such that then	. (The proof is presented in Appendix IV-A)
 Hence, can be seen as the critical asymptotic rate of innovation of when a relative best -term -approximation error of magnitude is tolerated.
 Alternatively,Amini, Unser and Marvasti [1] have introduced a notion of critical dimension for finite length signals, and from this, a notion of  -compressibility for infinite sequences. We revisit those notions here:
 	Definition 3: [1, Def. 4] For	and	, let us
 define
 (6)
 Then, a sequence	is called	-compressible if,
 (7)
 where	is the truncated finite-block vector of
 .
 This notion of compressibility says that when the block-length tends to infinity, a negligible fraction of the coefficients is needed to represent with an arbitrary small -distortion in the sense of (3). Note that is signal dependent and a variable-rate sequence. In addition, it offers the critical number of terms needed to achieve a best -term approximation error smaller or equal to in the sense of (3). From this, it should be related with the critical rate (from a fixed-rate analysis) described in Definition 2. That relationship is presented in the following result result:
  
 (8)
 (The proof is presented in Appendix IV-B)
 	A	corollary	of	this	result	implies	that	if	the	limit
 of	exists,	then
 . In particular from Lemma 1, if is -compressible, then for all . We refer the interested reader to Amini et al. [1] for further discussion and examples of -compressible sequences.
 B. Rate of Innovation vs. Distortion for Random Sequences Analogous notions of rate of innovation vs. best -term
  -distortion and compressibility can be stated for the case of random sequences (or processes). Let be a random sequence with values in and characterized by its consistent family of finite-dimensional probabilities
 [14], where and denotes the space of probability measures for the Borel measurable space . As a short-hand, we denote by the process distribution of
 .
 Let us define the following measurable set:
 (9)
 where the equality is by (6). Then in analogy with Definition 3, Amini et al. [1] proposed the following:
 Definition 4: [1, Defs.5 and 6] Let be a random sequence (equipped with ). Then for any and
 ,
 (10)
 is the critical number of terms that makes the set -typical with respect to . The process (and , respectively) is said to be -compressible, if , ,
 (11)
 Alternatively, we can consider the following fixed-rate notions:
 Definition 5: Let	be a process characterized by	, and let us consider	,	and	. We say that the rate-distortion pair (r,d) is	-achievable for	with
   probability, if there exists a sequence of positive integers
 such that	and
 (12)
 Definition 6: The rate vs. best -term approximation error function of (in short the rate-approximation error function of ) with probability is given by :
 (13)
 A simple relationship between and the critical number of terms in (10) can be established in the asymptotic regime when   goes to infinity, showing that our fixed-rate concept is a weaker one.
 Proposition 2: For any
 (14)
 (The proof is presented in Appendix IV-C)
 In the next section, we will study the class of stationary and ergodic processes [14], where the best -term approximation properties measured in terms of will be characterized in closed-form. Furthermore, it will be shown for this class of random sequences that
 for all and , refining the basic relationship presented in Proposition 2.
 III. ANALYSIS OF ERGODIC PROCESSES
 Let be a stationary and ergodic process with distribution , where   denotes its marginal shift-invariant distribution [14]. For simplicity , we assume that where denotes the Lebesgue measure [14].
 Then	is equipped with a probability density function (pdf) and
 .
 	For a measure	on	, a measurable function  
 is said to be integrable with respect
 to	if [14]
 (15)
 where	denotes the collection of	-integrable functions.
 We are in the position to state the main result:
 Theorem 1: Let be a stationary and ergodic process with shift-invariant distribution such that  . Then for any , we have the following dichotomy:
 i)	If	: then	is	-compressible,
 	i.e.,	,
 (16)
 ii)	If	: then	is not	-compressible. Furthermore, if we introduce the induced probability measure in	by:
 (17)
 	then	and
 (18)
 where
 (19)
 	and	is a solution of the identity:
 (20)
 The proof is presented in Section IV.
 A. Discussion and Interpretation of Theorem 1
 1: Theorem 1 offers a necessary and sufficient condition for a stationary and ergodic process to be l-compressible in the sense elaborated in Definition 4.
 2: In the case of non	-compressible processes, i.e., when
 , Theorem 1 offers what we call the
 achievable rate-distortion region for the process, given by the set of critical rate-distortion pairs:
 (21)
 This region depends solely on the shift invariant measure and its induced measure in (17). More details on the characterization of this region will be presented in Section V.
 3: Under the assumption that	and
 	, we have that any rate	and distortion
 are achievable (see proof in Section IV and more details in Section V). This fact is used to derive a concrete analytical expression for in (18). Furthermore, from the characterization in (18) and (20), it can be shown that is a continuous and differentiable function with respect to (see Theorem 2 in Section V).
 4: In both scenarios i) and ii), the critical ratefor a stationary and ergodic process is independent of . The reason is that asymptotically as goes to infinity, the characterization of implies to compute probabilities on events that belong to the tail -field of the process, which is known to be trivial (i.e., their events have zero or one probability) for the case of ergodic processes [14], [18], [19]. Therefore, we obtain almost sure convergence results that make independent of   the value of our object of interest (see Section IV for details).
 5: A natural order among stationary and ergodic process can be established from Theorem 1. Proposition 3: If is -compressible for some , then is -compressible for all .
 	Proof: If	, then	for all
 .
 Proposition 4: If is not -compressible for then is not -compressible for all .
 	Proof: If	, then	for all
 .
 6: Revisiting the i.i.d. scenario , we want to highlight the results by Amini et al. [1] related to -compressibility in the sense of (11). In particular, [1, Theorem 1] says that if   is such that for some ,
 then the i.i.d. process is not -compressible. In contrast, [1, Theorem 3] says that if belongs to the domain of attraction of an -stable distribution [14, Chap. 9.11, pp.
 207–213] with , then the process is  -compressible. First for , Theorem 1 provides a refined result, revealing a richer (indeed, the complete) family of i.i.d. distributions that are not -compressible. In fact, in addition to distributions that go to zero exponentially, and consequently, (Gaussian, Laplacian, Gamma, etc.), heavy tail distributions whose density function are tail lower and upper dominated by a power law decay of the form with are not  -compressible either5. On the other hand, concerning [1, Th. 3], it is simple to verify that any that is in the domain of attraction of an -stable law with [14, Ch.9] satisfies that
 (see Appendix IV-E for details), and consequently, part i) of Theorem 1 covers this family of -compressible i.i.d. processes.
 7: Complementing the previous point, from Theorem 1 we can state the following:
 	Corollary 1: Let	be a stationary and ergodic
 process, if	where	, then
 Therefore, stationary and ergodic processes equipped with a shift-invariant distribution that follows a Gaussian, generalized Gaussian, Laplacian and Gamma are not -compressible in the sense of Definition 4, for any . In addition, if is finitely supported, i.e., where , then its process is not -compressible for any .
 Corollary 2: Let be a ergodic process with invariant distribution and density , . If decays as for some , then6
 Therefore, for shift invariant distributions characterized by a power-tail behavior, which belong to the category of heavy tail distributions, a complete picture of the range in which its ergodic process is  -compressible is obtained.
 8: For the proof of Theorem 1, we derive almost sure convergence results (see Lemma 2 and 3 in Section IV).
 In the case when : if is such that for some , then
 ,	. Furthermore, for	for some	, it follows that	,	. In the case when	: if	, then
 These results are con-
 sistent and extend the result by Gribonval et al. [2, Prop. 1], which for the i.i.d. case shows the same almost-sure convergence limit for the object . Their proof was based on the Wald’s lemma of order statistics (see details in [2, Th. 6]). In contrast, our proof is based on the use of the tail events in (19), some induced empirical distributions on those events, and the convergence of those empirical measures through the application of the ergodic theory (see Section IV for details). The idea adopted in our proof was to look at the empirical distributions of and as the objects of interest, instead of the partial
 5A measure is tail lower and upper dominated by a no-negative function , if there exits and such that for any such that then . Here denotes the pdf of .
 6We say that	decays as	if there exists	and
 	and	, where: if	,
 then	,	; if	, then
 ,	; and otherwise, then	.
 Fig. 1.	Graphical representation of the relationship between	,	and   in Theorem 1. Notice that	is the total area under the bottom curve.
 sums of the ordered statistics considered in [1], [2]. That difference was essential to extend the mentioned almost sure convergence results (in Lemma 2 and 3) to the family of stationary and ergodic processes.
 9: Under the assumption that	, Theorem 1 shows another interesting dichotomy:
 	Corollary 3: If for some	and for some
 it holds that
 then the latter also holds for all	and for all
 . Likewise, if for some and some then for all and all
 .
 B. Graphical Interpretation of
 	Note from (20) and (17) that	and
 , respectively. The numerator in the last expression corresponds to the expected value of , for a random variable where . Similarly, from (18), the optimal rate
 equals the expected value of . Thus, corresponds to the area under the “tail” of , which denotes the pdf of , depicted in Fig. 1 (top), while coincides with the area under the curve to the left of , coloured in Fig. 1 (below). This graphical representation allows for an intuitive interpretation of the relationship between and the compressibility of a given stationary ergodic process . In order for this process to be compressible, must be zero for every and for every . Equivalently, (and recalling from (16) that is a limit), it must be possible to achieve any while keeping an arbitrarily small fraction of the elements of the process, as . Such requirement is satisfied if and only if (i.e., if), which implies that no matter how large is chosen, the shaded area in Fig. 1 (bottom), being infinite, will yield a zero .
 IV. PROOF OF THE MAIN RESULT
 Proof: Let us first consider the case when
 . For the rest, it is important to note that given that
 	, then for all   there exists	such that
 	, and for all	there exists   such that
 . 
 	For	, we can define:
 (22a)
 where from the ergodic theorem [14, Th. 6.28],	,
 (22b)
 (22c)
 The second almost sure convergence is from the assumption that . Then, we can state the following:
 Lemma 2: Let be a stationary and ergodic process with distribution and . Then for any and sequence such that ,
 we have that
 (23)
 In addition,	,
 (24)
 where is a solution of . (The proof is presented in Section IV-A)
 	In order to prove (18), let us fix	. Then there exists
 , such that (20) holds and from Lemma 2 if is such that , then , .
 	Let us consider an arbitrary	such that
 , then again from Lemma 2, if a sequence is such that , then
 	,	. Conse-
 quently, convergences almost surely to a distortion strictly less than , and then for all :
 (25)
 Hence from the definition of	in (10), we have that eventually in	, which implies that
 (26)
 This upper bound is valid for any	such that
 , then
 (27)
 The first inequality comes from Proposition 2 and the last equality from the fact that the function is continuous with respect to as .
 To derive a lower bound, let us consider an arbitrary
 	. We know that there exists	such that
 	. Again from Lemma 2, for all	such that
 , then	,
 .. Therefore,
 (28)
 This result implies that eventually in	,	and, consequently,
 (29)
 On the other hand from (28), we have that it is necessary that
 . This last inequality and (29) are valid for any
 	, then	and
 , which from (27) proves (18).
 	Moving to the case where	, we have that
 	, then from the ergodic theorem [14]	,
 (30)
 (31)
 Equation (31) comes from
 a.s., and
 .	In	other	words,	(31)	means	that
 	,	Furthermore,
 we have the following:
 Lemma 3: Let be a stationary and ergodic process with distribution and . Let us consider an arbitrary and such that ,
 then
 (32)
 (The proof is presented in Section IV-B)
 Let fix an arbitrary	and	where	, then from Lemma 3 we have that	:
 (33)
 Then for all	and	,
 and therefore eventually in . From this, for all , which concludes the result from Proposition 2.
 A. Proof of Lemma 2
 Proof: To begin let us prove the fixed-rate result in (23). It is first important to concentrate in the case when and to show that
 (34)
 For any	, let us define the sets
 (35)
 where from (22b) and (22c),	because
 (36)
 Letusfixan arbitrary and let and be such that , which implies that .
 Let us take an arbitrary . From the zerorate assumption on , the definition of and the fact that
 ,	such that	,	, which implies that	. Therefore considering that	,
 (37)
 Performing the same steps for the sequence and defining accordingly , we have from (37) that
 ,
 (38)
 Then from the sigma additivity of , , which proves the result in (34).
 Equipped with this result, for an arbitrary let us consider such that . We know that there exists such that , and for this we consider
 the sets	and	as defined in (35). Then for any
 it follows that
 (39)
 Furthermore, from definition of ordered sequences, it is simple to verify that (see (1)):
 (40)
 This is where the zero-rate result in (34) is used. In particular, if we consider	, from (40), (38) and the fact that is in
 (39), we have that
 (41)
 the last equality in (41) from definition of	in (35). Finally from (22b) and (22c),
 which proves (23).
 Concerning the fixed-distortion result in (24), for
 let	be such that	. Let us consider an arbitrary where and consequently (note that and are mutually absolutely continuous). Again for this , we use the sets in (35),
 where for all	, it follows that	, such that	,	.
 Considering , it follows by definition in (6) that:
 (42) then	.
 We can do the same for the countable collection:
 (43)
 where for any	,
 (44)
 The first equality follows from the continuity of the function with respect to as . Then from the fact that	, we have that,
 , -a.s. Finally, proving that	-a.s. follows an equivalent symmetric argument and we omit it.  
 B. Proof of Lemma 3
 Proof: For let us consider and , such that and such that . Considering the sets
 and	, we have that from (30) and (31). Let us fix an arbitrary . Considering that
 , then eventually in , and therefore eventually, which implies from definition of that . Finally, the fact the event happens -almost surely concludes the result.	 
 V. PROPERTIES OF THE RATE-APPROXIMATION ERROR CURVE FOR NON	-COMPRESSIBLE PROCESSES
 For the family of non -compressible ergodic processes, in this section we study two tail functions that characterize the achievable rate-distortion region in (21). Let be stationary and ergodic with its invariant probability measure. Here we focus on the case where , then the measure in (17) is well-defined and by construction	, where the Radon-Nikodym (RN) derivative (or density) of	with respect to	is given by for all	. Furthermore, from the strict positivity of
 	on	, it is clear that  , where
 and then these two measures a mutually absolutely continuous
 [14], i.e.,
  
 (45) This implies a close interplay between the tail-probability functions
 (46)
 that characterize	in
 (21). The following basic properties can be stated:
 Proposition 5:  
 a)	and	are left-continuous.
 b)	. Then, is continuous at	, if and only if,
 .
 c)	For any , is continuous at , if and only if, is continuous at .
 d)	For any pair	,	, if and only if,
 .
 e)	and
 .
 From these properties we can state the following:
 	Theorem 2: Assuming that	, then for any	:
 1)	is a continuous function with respect to
 .
 2)	there is	such that	and there exists	such that	. Then,
 the collection of critical rates
 achieves all the values in , and any distortion is achieved in with a given rate.
 3)	For	, then	.
 4)	is a differentiable function in	, and
 (47)
 where	and	denotes the inverse of the auxiliary function for	.
 The proof is presented in Appendix I.
 In summary, the rate-distortion approximation function is continuous, injective and achieves all the rates, in the sense that for any	there is only one	such that	. In addition, it is strictly decreasing and satisfies the following boundary conditions:	and	. Furthermore, it is simple to verify that	and
 . Then from (47), it seems that
 should be a non-increasing function as progress to 1 (considering that ), and consequently,
 should present a convex dominating behavior  eventually as progresses to 1. The next section analyzes the convexity of more formally.
 A. Convexity of
 First in this section we show that is a convex function of . Then, we provide a necessary and sufficient condition for to be a convex function of .
 Proposition 6: For every	,	is a convex function of	over	.
 (The proof is presented in Appendix IV-D)
 Theorem 3: Let by a non -compressible stationary and ergodic sequence equipped with . Then
 1)	if	,	is a convex function for all	.
 2)	Otherwise,	is convex, if and only if,
 (48)
 	where	is the pdf of	, with	, and
 .
 The proof is presented in Appendix II.
 Remark 1: The condition in (48) is implicit and may be difficult to verify. A simple to check sufficient condition for the convexity of is the following:11
 (49)
 where . In particular, a non increasing pdf (i.e., almost everywhere in ) characterizes a convex rate-approximation error function.
 B. Examples of Heavy Tail and Exponentially Decaying Tail
 Distributions
 We present few examples of rate-approximation error curves of non  -compressible i.i.d. processes. In particular following [1], we consider the Gaussian (exponentially decaying distribution), which is non  -compressible for any , from Corollary 1, and the family of Student’s -distribution with parameter
 12, whose pdf goes to cero as . From Corollary 2, the i.i.d. process with a Student’s t-distribution ( ) is -compressible for any and non- -compressible for
 .
 	To compute the rate-distortion function	,
 we use the fact that
 Then the problem reduces to compute	and
 , for all . For this we consider an estimation approach. Considering a sufficiently large set of i.i.d. realizations of (let say ), the law of large numbers [14] tells us that for any and ,
 (50)
 (51)
 11The proof is omitted for the sake of space.
 12The pdf of a Student’s -distribution with	degrees of freedom is given by , where	is the gamma function.
 Fig. 2.	Numerically estimated rate approximation error curves for several non -compressible i.i.d. processes and one	-compressible i.i.d. process.
 with probability one, assuming that	and
 the use of (17). Then, by sampling the space of thresholds and considering a sufficiently large , we can estimate with an arbitrary good precision the rate distortion region
 . Following this path, Fig. 2 shows the estimated rate-approximation error curves for the Gaussian, and several Student’s -distribution for . We verify that some Student’s -distribution are -compressible (cases and ) and others are non -compressible (cases , and
 ) as the Theorem 1 predicts. More interesting is to validate in all the cases of non compressible priors, that the curves have a convex behavior, which is justified from (49). Furthermore, the density with the exponentially decaying tail is less compressible than any prior with a power law decay, in the sense that for achieving a distortion the Gaussian i.i.d. process needs a higher rate. From these curves, as goes to infinity the i.i.d. process with a heavy-tail distribution approaches the approximation error behavior of the Gaussian law.
 Fig. 3 shows the rate-approximation error curves for the Gaussian prior for different values of . It is interesting to observe the increasing monotonic behavior of as increases, for any fixed value of . Again all curves have a convex behavior. To contrast, Fig. 4 shows a set of curves for the Cauchy distribution (i.e., Student’s distribution with
 ), where no clear monotonic pattern is observed as a function of .
 	VI.	-COMPRESSIBILITY AND COMPRESSED SENSING
 We conclude this work analyzing compressible stationary and ergodic sequences, as characterized in Theorem 1, in terms of their ability to be represented with an arbitrary small proportion of linear measurements adopting for that the classical compressed sensing (CS) measurement and reconstruction setting. In particular, the focus is on  -compressible processes, as the standard Gaussian i.i.d. linear acquisition and
 -minimization (sparsity promoting) decoder of CS [15], [16] offer a well-known  -instance optimality guarantee [3] (stated below in Lemma 4) that matches the modeling assumption of -compressible processes.
  
 A. Compressed Sensing in a Nutshell
 In the finite dimensional setting, the analysis phase of the
 CS is a linear operator , that given a signal   generates a measurement vector . The case of interest is on the under-sampled regime, i.e., , where under sparse or compressible assumptions on , CS can offer perfect or near-optimal reconstruction by the solution of the following (linear programing) problem [16]:
 (52)
 Notably, the CS theory, based on the restricted isometry property (RIP), establishes sufficient conditions over (and implicitly over the number of measurements ) in order that  
  , when   for some . The next result, in its original form stated in [9], shows that random measurements offer a solution to that problem with a near optimal relationship between and [3] .
 Lemma 4: ([15, Th. 5.2] and [16, Th. 1.2]) Let be a random matrix , , whose entries are driven by i.i.d realizations of a Gaussian distribution or a binary variable with uniform distribution over . For any arbitrary and , we have that:
 (53)
 if
 (54)
 with a probability,over the sensing sampling space ,at least equal to . Here is the solution of
 (52) and, , and are positive universal constants independent of and . 
 (The proof of this result derives directly from [15, Th. 5.2] and [16, Th. 1.2])
 B. Zero-Rate Reconstruction for  -Compressible Processes
 Here we formalize the reconstruction of infinite sequences using CS. For this, we consider a finite-length (or fixed-rate) approach, where the idea is to analyze consecutive finite-block versions of the sequence, i.e., to sense and reconstruct for any  , and study reconstruction performances in the limit when the block-length tends to infinity.
 More precisely, let be a sequence of positive integers such that . From this sequence, we consider the family of Gaussian CS encoding-decoding pairs	where for any , is the random sensing matrix of generated by i.i.d. entries as mentioned in Lemma 4, and is the function from to that solves the -minimization problem in (52). Given a sequence and any finite block-length , we can apply the CS approach over   to recover	, which is a random reconstruction function of the matrix . In relation with the -relative approximation error introduced in (3), we consider as a fidelity indicator the -noise to signal ratio (NSR) given by:
 (55)
 More generally, if we have a process with distribution and a sequence of lengths with its associated Gaussian CS finite-block scheme	, we can also analyze the finite-block performance of the scheme by the object
 , which is a random variable func-
 tion of two independent random objects: the vector  and the random matrix . Therefore, it is important to consider the average NSR with respect to the statistics of the source (i.e.,
 (56)
 Then the question we focus here is: for an -compressible process that satisfies (16), what is the minimum rate of measurements (i.e., , or more generally
 ) of the classical Gaussian CS scheme that
 ensures that:
 (57)
 with probability one with respect to the statistics of the sequence of random matrices	?
 From Theorem 1 and the RIP-based -instance optimality result of the Gaussian CS setting in Lemma 4, we can state the following result:
 Theorem 4: Let	be a stationary ergodic process. If is	-compressible, then for any sequence	such
 that	, it follows that
 (58)
 almost-surely	with	respect	to	the	statistics	of and the joint statistics of	and
 , respectively.
 The proof is presented in Appendix III.
 C. Discussion and Interpretation of Theorem 4
 1: This result states that in order to achieve zero distortion in the reconstruction for an  -compressible process, almost-surely in the NSR sense of (56), the CS scheme needs an arbitrary small number of measurements per sample. In other words, under the  -compressibility model assumption for the process, the minimum rate to achieve zero distortion is zero for the Gaussian CS scheme. Then, it is remarkable to validate that CS is able to achieve the same zero critical rate that it is obtained by the analysis of the pure oracle best- -term approximation error of -compressible process (see the result in Lemma 3).
 2: This result shows that the lucid notion of -compressibility proposed by Amini et al. [1]  really translates in a meaningful performance result for the classical Gaussian CS (GCS) setting in the asymptotic regime when the blocklength goes to infinity. In other words, we can say that
  -compressibility, meaning a sort of zero-rate of innovation in the process, implies zero-rate of measurements (per signal dimension) for perfect recovery (in the sense of NSR distortion) for the CS scheme. This result closes a gap not explored in [1] between their notion of  -compressibility and CS performance guarantee in the asymptotic regime.
 3: Concerning compressibility of random sequences and CS performance guarantee, we want to highlight the work of Gribonval et al. [2] for the case of i.i.d. processes. They show in [2, Theorem 2] that if then
 (for distribu-
 tion equipped with a pdf) and they enunciate a version
 of Theorem 4 for the i.i.d. case [2, Remark 1]. Then, we want to give credit to this contribution to be the first result that offers a connection between notions of compressibility for i.i.d. processes (based on relative approximation errors) and the performance (in the asymptotic regime) of the classical GCS scheme. In this context, Theorem 4 can be seen as an extension of these results to the case of stationary and ergodic sequences and, in the technical side, an extension on the use of the-instance optimality property of the
  -minimization decoder . On the other hand, focusing on the i.i.d. context, Theorems 4 and 1 offer a way to verify that Amini et al. [1]-compressible notion (variable rate in nature) has a connection with the results in Gribonval et al. [2,Th. 2 and Rem.1] in terms of what GCS can achieves for the case of  -compressible processes.
 VII. DISCUSSION AND FINAL REMARKS
 The main result of this work (Theorem 1) provides a connection between Gribonval et al. [2, Prop. 1] almost sure convergence result of relative approximation errors, and Amini et al. [1, Def. 6] notion of -compressibility for random sequences. More importantly, Theorem 1 offers new techniques to extend that connection (and, consequently, a dichotomy between being and non-being -compressible random sequences) to the family of stationary and ergodic processes. This extension is constructed over the almost sure convergence of the empirical distributions of and , respectively (see definitions in the statement of Theorem 1) to the true probabilities on the family of tail events(details in Section IV).
 The idea of looking at specific empirical measures as the basic object of interest, in (22b) and (22c), instead of the statistics of the sum of the ordered sequence as considered in [2, Prop. 1] and [1], was essential to extend the analysis from the i.i.d. case to the case of stationary ergodic processes.
 Finally, one can notice from the proof of Theorem 1 that this result does not rely on a stationary property, as it is essentially basedonanalmostsureconvergence(asymptoticinnature)over the family of indicator functions of the tail events  in (19). Then, we conjecture that the analysis of compressible priors can be extended over a family of random sequences with a specific ergodic property over the tail events, which is an interesting direction for future work. This observation leads us to put the attention on the general theory of (non-stationary) processes with ergodic properties [14], [18], [22]–[24].
 APPENDIX I
 PROOF OF THEOREM 2
 Proof: We first verify the second point to then move to the rest of the points.
 Point 2): (Achievability of all rates and distortions in ): Using Proposition 5 b) and c) and the hypothesis that
 , it follows that	and	are continuous functions
 in . Then, adopting Proposition 5 e), we have that achieves all the rates and distortions in the range .
 	Point 1): (Continuity of	):
 First, it is important to verify that the implicit characterization of presented in (18) and (20) offers a well-defined function. By contradiction, let assume that such characterization is not a function in the sense that for a given distortion  
 there are two values solution of (20) associated to two different rates . This last condition implies that from (45), which contradicts the fact that and are solutions of (20).
 Moving to the continuity, let us fix an arbitrary and . Then by the achievability of the distortions,
 such that . On the other hand, by the achievability of the rates, there exists , where
 and (without loss of generality we assume that ). Then from monotonicity of we have that for any ,
 .  At this point, we can obtain the distortions
 , where the strict in-
 equalities that relate them follow from the fact that by construc-
 tion	and (45). Then, we can define
 , where for any
 we have (by the monotonicity of the function ) that there exists such that , and consequently	,
 the last set of equalities from (18) and (20). As
 and	are arbitrary numbers, this proves the continuity of
 .
 	Point 3): (Strict monotonicity of	):
 	Let fix	. By definition (13), we have that
 . Furthermore, from the characteri-
 zation given in (18) and (20), we have that there exists
 such that and . This implies that , and consequently from (45) we have that	.
 	Point 4: (Differentiability of	):
 First, it is clear that both functions and are differentiable by construction. In fact from (46), ,
 (59)
 where	and denote the pdf of	and	, respectively.
 Furthermore, we can introduce the auxiliary function that is differentiable, and
 (60)
 Finally, for a fix	there exists	such that
 and, consequently, 
 (61)
 the first equality by the characterization of
 and (20), and the third using that and  .
 APPENDIX II
 PROOF OF THEOREM 3
 Proof: Considering that , let be its pdf. In view of Theorem 1, for a fixed , there exists such that can be expressed as:
 (62)
 where	, for which we introduce the short-hands	and
 . Then using (61),
 (63)
 By construction	is non-decreasing with	. Hence, when
 , the middle term in (63) is negative and increases with , proving the convexity for that case. For the case , from the right hand side of (63) convexity will hold, if and only if,
 	for all	. To check that, it is
 useful first to note that
 (64)
 Then
 (65)
 where	follows from (64). Hence, convexity will hold, if and only if,	for all	.	 
 APPENDIX III
 PROOF OF THEOREM 4
 Proof: Let fix and . Assuming that is -compressible, from Theorem 1 we have that and, consequently, it follows that:
 (66)
  in (10). Again
 this inequality is valid with probability over sampling space of the random object . There-
 fore taking the limit when	goes to infinity, we have that
 (69)
 with	probability	one	with	respect	to	distribution	of
 20, which is valid for any arbitrary small and . In other words, if we define the set
 ,
 then	,	and	. Finally,
 	from sigma additivity of	, which
 implies that
 (70)
 with	probability	one	with	respect	to	distribution	of
 .
 For the almost-sure convergence result on the sequence
 , we need a slightly different argument. Under the assumption that , it is simple to verify that there exists a sequence of positive integers such that and, more importantly, it satisfies that
 (71)
 20The	almost	sure	convergence	with	respect	to	the	statistics	of
 derives from the Borel-Cantelli Lemma [14] and the fact that as by hypothesis  .
 Following similar steps than before, there exists such that we have that , and therefore, from Lemma 4,
 (72)
 with probability in . It is important to define the set	where Lemma 4 tells us that . Furthermore, from
 Lemma 3, we have that
 (73)
 with probability one with respect to the process distribution of . In other words, if we define the set
 , we have that
 from (73). Finally, we are interested in the set
 where it is simple to show that , by (72) and the definitions of and . Hence, the problem reduces to evaluate,
 (74)
 the last equality from the fact that . Then from the additivity and monotony of the measure , which concludes the result by the definition of .
 APPENDIX IV
 COMPLEMENTARY RESULTS A. Proposition 1
 Proof: From the definition of and the hypothesison ,it followsthat where
 and	. Consequently eventually in	,	, which implies that eventually . This concludes the result.
 B. Lemma 1
 	Proof: As a short-hand, let	for all	.
 By definition	for all	, and consequently,
 . Then from (5) and Def. 1, it follows that	.
 For the other inequality, we consider the nontrivial case when . We prove it by contradiction assuming that . Then from (5), there exists and a sequence such that
 . Under the fact that , there exists such that , . Using this and the definition of in (6), ,
 (75)
 where, consequently,
 . Therefore,
 (76)
  
 (77)
 This approach can be iterated a finite number of times (independent of the length
 (78)
 which contradicts (75).	 
 C. Proposition 2
 	Proof: Fixing	and	, let us define
 	,	. Then by (10),
 , where from (13) it follows that
 .
 D. Proof of Proposition 6
 Proof: Using the arguments to prove Theorem 2 (Point 4), if we consider , then for any there exists   such that	. This last auxiliary function is diferentiable (with respect to ) and we have that:
 (79)
 the last equality by considering that
 	. Since	is non-decreasing
 with , it follows that is negative and increasing with , i.e., is a convex function of .
 E. Analysis of the Domain of Attraction of
  -Stable Distributions
 The family of stable laws is the class of non-degenerate probabilities that are limit (in distribution) of sequences of random objects of the form [14, Ch.9]:
 	 	(80)
 where are i.i.d realizations of a random variable, and and are a sequences of real numbers. For the wellknown scenario when , the Central Limit Theorem tells us that the limit is a normal law. For the case  , we have the less known family of -stable laws, whose characteristic function is given by [14, Th.9.27]:
 (81)
 being the exponent of the law, and , and constants.
 Definition 8: [14, Def. 9.33] The distribution	is said to be in the domain of attraction of an	-stable law with
 , which we denote by	, if there exists and	such that:	(in distribution) and	follows the	-stable distribution in (81).
 The collection is non-empty and is characterized by the following result:
 Theorem 5: [14, Th.9.34 and Prop. 9.39] Let and let us define :
 (82)
 on	. Then	belongs to	, if there exists	and with	such that:
 i)	,
 ii)	implies that	is slowly changing , and
 iii)	implies that	is slowly changing.
 Then we can state the following:
 	Proposition 7: If	for some	, then
 .
 Proof: Let and . Without loss of generality let us assume that is slowly changing. Then it is simple to verify that , and such
 that	. Therefore
 .
 ﻿ There are two major which underly the design of an to digital converter ; namely when to sample and how to represent the amplitude of each sample . In the majority of past work , these two have typically been separately . Recently we have a novel algorithm which moving horizon optimization to determine both when and how to sample . Our work gave a heuristic description of the algorithm and , via , that a performance gain was in both the bit rate and the distortion level . This seemingly paradoxical result is due to the interpolative inherent in . The goal of the current paper is to give theoretical support to the algorithm . In particular , we provide on the probability that beneficial interpolation for the particular case of horizon length with flat , unity gain error weighting filter . 
 INTRODUCTION 
 The two fundamental that arise when an to digital converter are when to sample and how to represent the amplitude of each sample . Several deal with the first question quantization effects , see , e .., , and rise to irregular and or periodic non uniform sampling . Other deal with each value . In particular , conversion been in a wide range of , see , e .., , . Recently , conversion been extended to incorporate stemming from finite set constrained predictive control , see . 
 work by the present , in , moving horizon optimization to address the dual question of when to sample and how to quantize . The scheme , which we in , to be marked as mute whenever it is desirable to replace them by a linearly value derived from their . Notice that this procedure implicitly to signal dependent adaptive non uniform sampling . 
 Our work , gave an introduction to the algorithm and , via , that distortion in the conversion process can be reduced , when to , such as straight quantization , conversion and its step . Additionally , we that , in general , the number of for the digital representation of the signal can also be reduced . This seemingly paradoxical result from the interpolation inherent in the algorithm . 
 In the current paper , we will give theoretical support to the algorithm by examining the under which beneficial interpolation . In particular , a bound on the probability that interpolation is derived for the special case of horizon optimization . 
 REVIEW OF THE A CONVERTER 
 The goal behind is to perform quantization so as to minimize both bit rate and distortion . It is based on the idea of linear interpolation between . can mark a sample as mute . Later , mute , are reconstructed by linearly between . This concept is in Fig . . 
 Fig . a and show the reconstruction of an signal from a uniformly discrete time signal via straight quantization . The dashed line in Fig . a to reconstruction zero order hold . The benefit of 
  
 Fig . . Reconstruction of an signal from via : a straight quantization ; linear interpolation 
 linear interpolation is clear in Fig . , where the for the at times , , were by linear interpolation of of the . It can be that the reconstructed signal in Fig . b better the original signal . The algorithm on this observation and distortion by virtual quantization between the original every time a sample is . Additionally , data compression due to the fact that mute do not require to represent their value thereby reducing the average bit rate . 
 A . Interpolation and Optimal Conversion 
 Consider a scalar discrete time signal with underlying sampling rate . The purpose of to digital conversion is to obtain a representation of ak , i . e ., a discrete time signal . Each value is restricted to belong to a given finite set of , s , s ,..., . 
 In the algorithm , is to be assigned as mute . This condition is by the symbol . Thus , the on can be expressed as : 
 where U , 
 U s , s ,..., , 
 and nU the of U . 
 In order to incorporate mute into the conversion problem , we will make use of an interpolator whose output is defined as : 
  
 that for to a vacant sample non mute sample before non mute sample after 
 We can see that is a discrete time signal formed of all of and the linearly mute of . Notice that is not constrained to belong to quantization process : 
 ak 
 can be designed by a frequency weighted measure of the error , i . e .: 
  
 the total number of of ak . In , is the error , defined as : 
  
 and is a linear time invariant filter . 
 B . Feasible Sub Optimal Conversion : The Algorithm 
 Finite Horizon Formulation : To ensure that the are feasible , it is necessary to restrict the number of decision as well as the number of future of ak considered in the optimization . Thus , at sampling will replace the infinite horizon cost function by the following quadratic cost function defined over a shorter optimization horizon of length : 
 , 
 a fixed design parameter . 
 The finite horizon cost in is determined by only a finite constrained . These decision are grouped into the vector 
  
 Note that the last element of , that is , , must not take the value mute sample , because , if it did , then the calculation of the corresponding value of the sample would require the value of the successive value , namely , which outside and ahead of the horizon . 
 Moving Horizon Approach : Minimization of the step cost in rise to the : 
  
 Given , a feasible sequence over the optimization horizon . If one was to use the standard moving horizon concept as , e .., in , then only the first element would be . At the next optimization step the whole procedure would be repeated again with the horizon by . While this is in principle possible , the inclusion of mute to additional that are in . In general , the optimization method may give rise to several consecutive mute . In that case , if the horizon one step , the decision for the next element would have to consider the backwards propagation effect , which is not taken into account in the finite horizon cost . This would lead to performance degradation . To overcome this problem , the following modification to the standard moving horizon approach : 
 The converted output is assigned to the sequence 
 , ,..., , 
 such that , i . e ., is non mute . The horizon will then be by . The above procedure that , at every optimization step , the precursor is non mute . 
 Remark : Notice that the use of mute rise to a non uniform sampling pattern , and that the average sampling rate will be lower than . 
 Remark : The non interpolative such as straight quantization , conversion and the Converter of in a more general setting . Since the conversion process is carried in an optimal manner , see , the will , in general , give lower distortion than these other conversion . In the sequel , we will analyze the produced by the algorithm . 
 EFFECT OF THE PROBABILITY OF INTERPOLATION ON DATA COMPRESSION 
 In it was shown by simulation that lower quantization noise than traditional to digital conversion , and at the same time it less . Clearly , these gains depend on a high proportion of mute , where : 
  
 here , I the number of been converted . 
 Notice that in the case of horizon length , at most one out of every two can be marked as mute , limiting the value . . We also define the probability of a sample as : 
 Pint 
 The proportion of mute ,, is related to Pint by : 
  
 In it was also shown that the Bit Ratio , defined as the total number of by the output sequence from , namely R , divided by the total number of by traditional quantization , is given by : 
  
 As a consequence , the data compression of on what proportion of are marked as mute and on the size of the constrained set nU , see . 
 FOR INTERPOLATION TO OCCUR 
 Towards the goal of for Pint see , in this section we will elucidate the under which the interpolation rather than quantization . The following are used : 
 : 
 , the first difference of ak ak , the second difference of ak . 
 Q , a standard nearest scalar , with output set s ,..., . 
 , the quantization error . 
 In the sequel , we will restrict our analysis to the case , in which , , and where si si 
 , i ,..., nU for a given and fixed quantization step . This to 
  
 We will also assume that input signal and output set are scaled such that no overload , i . e . 
 Given the above , a sufficient condition for interpolation is : 
  
 where to the value stemming from , i . e .: 
  
 We note that , in general , and could be mute 
 or . However , restricted the horizon length to , the of the algorithm a 
 Fig . . Amplitude Band 
 imply that see . , if a given sample ak is a candidate for interpolation , then the previous converted sample result is non mute . Similarly , if the sample ak was to be marked as mute , then the conversion result for the sample just after it , namely , would be non mute . As a consequence , when considering the possible interpolation of ak , the expression for becomes 
  
 Substituting into , the condition for interpolation becomes : 
  
 EMBELLISHMENT OF THE CONDITION FOR INTERPOLATION 
 Probabilistic analysis of is non trivial since its left hand side a sum of correlated continuous random . For our purpose , it is convenient to first assign to each quantization level an integer number in ascending order see Fig . and then define the Amplitude Band Number .. Region for a sample ak as 
  
 or , equivalently 
 i ak si 
 This integer quantity the number of the quantization level for sample ak . As an illustration , for the example in Fig . , , and . Given the above , the ak can be decomposed into : 
  
 Thus , to : 
  
 where 
  
 is an integer number that could be as a coarse second difference of ak . 
 Since ¨ is integer valued and , by assumption we only need to distinguish that fulfill , namely : 
  
 It can be seen from that the for ¨ depend on the of ak , ak and ak . For the purpose of probabilistic analysis , it is more convenient to express it in of : 
  
 These can be through careful examination of the given in , and Fig . . For sake of simplicity , are . 
 Substituting and into that can be written as : 
  
 that , equation 
 could be numerically . Alternatively , it is possible to devise a graphical method to represent and solve , as in Fig . . In this figure , the range of of which satisfy are shadowed . The horizontal axis is , the value for the first difference see , as by the vertical marked . The vertical axis is a . The horizontal horizontal of mark yielding the same result for see . Thus , a given value for the of a that in yield either , or in order to satisfy the for interpolation of sample ak . 
 The diagonal line ... in Fig . entirely inside the solution area , and the condition . If , this line to the case a a , that is , a ¨ . Since , by assumption we can conclude , in accordance to intuition , that in order to satisfy the condition for interpolation , a ¨ must be as close to zero as possible . In general , a given value of a ¨ will satisfy the condition of interpolation depending on the value of . It can also be seen from figure that the of the 
  
 Fig . . Graphical solution to equation . Shaded represent the of satisfying it . 
  
 a a 
 Fig . . Range of a ¨ for interpolation as a function of for 
 . 
 interval for a are periodical along the diagonal line . 
 Additionally , if , the difference between each of these periodic and the diagonal line the for a ¨. The for a ¨ then depend upon . Fig . the effect of quantization error in the range for a ¨ that the condition for interpolation , for three different of a . In particular , it can be from and that , if a ¨ , then 
 will not be met regardless of . 
 PROBABILITY OF INTERPOLATION 
 We can utilize the framework established in analyze Pint see . Towards this goal , we interpret , a and a ¨ as random which we denote by , a and ¨ a respectively . Assuming that is uniformly distributed on 
  
 Fig . . Graph of conditional probability function and . 
 the interval and statistically independent from a and a ¨ , it is possible to obtain see Fig . , for a given value of a and a ¨, the set of to which must belong in order to satisfy . Since is uniformly distributed , the total length of these , which we will denote here as a ,¨ a , is proportional to the probability that the condition for interpolation is satisfied . Thus , the conditional probability of interpolation given that a and ¨ a is given by : 
  
 Note that the function , a random variable a ,¨ a . If the joint probability density function for a and ¨ a is fa ,¨ a ,, then Pint can be calculated via : 
  
  
 Note that the and in the first integral arise from the fact that , , as previously noted . Function , can be numerically or graphically . Fig . , y for three different of . 
 A LOWER BOUND FOR THE PROBABILITY OF INTERPOLATION 
 Based on the analysis on section , the probability of interpolation for a given signal depend on the statistical of the first and second of the signal . Clearly , if fa ,¨ a , is known , the probability of interpolation can be precisely from . However , in practice it is desirable to estimate the probability of interpolation a , based on coarse information about the signal . We present next a lower bound for Pint for the case where only the second moment of ¨ a , that is , E ¨ a , is known . 
 THEOREM : For each y , , and provided E ¨ a , a lower bound for the probability of interpolation can be by 
  
 where . 
 PROOF : Since , , fa ,¨ a , , , , we can write : 
  
  
 Fig . . Statistical frequency of interpolation for 
  
 where ¨ a is the probability density function of ¨ a . Equation then to 
 Pint y ¨ a y 
 Application of inequality to then the result . 
 REMARK : Clearly , the above theorem on the choice of y . Since , by construction , y is monotonically decreasing , the derivative of the right hand side of is zero only at the global maximum . Thus , the value for the bound can be found numerically by y . 
 SIMULATION STUDY 
 were by the algorithm to two quality music each of length , . We grouped the into , for of a and of a ¨, and calculated the statistical of interpolation within each category . According to the law of large , the greater the number of in each category , the closer the statistical frequency will approach the actual value of the probability of interpolation for each category . More specifically , for a category defined as ak such that a and a ¨ , it can be that for highly the statistical frequency will approach the conditional probability of interpolation , as , . 
 show the for quantization . In these , the dashed line the function , for the corresponding value of see Fig . . 
 It can be seen that statistical frequency ,. The in the of the from the dashed line are from the fact that in some the number of was rather small . This is particularly true for of a or a ¨ . Despite that , it is important to 
  
 Fig . . Statistical frequency of interpolation for 
  
 verify that the for which no interpolation is indeed no at all except an isolated bar on the extreme right in Fig . , from a category of one sample whose existence is by the fact that that category with a not strictly equal to zero . Similarly , for the where , probability of interpolation equal to , all were in deed . 
 To illustrate the derived in section , we converted different audio , each one with and quantization . For each signal , the value for E ¨ a was from the entire signal . Fig . the empirical of interpolation . Furthermore , it the lower from by choosing which . and y which . . It can be from this figure that the lower , although weak , are confirmed by . In particular , as by the , the empirical probability of interpolation as E ¨ a . 
  
 Fig . . Empirical of lower . 
  
 This paper a recently algorithm which simultaneously reduced bit and lower distortion . The algorithm moving horizon optimization together with interpolation . In this paper , we give a lower bound on the probability that interpolation can be beneficially used for the particular case of horizon length and flat error weighting filter . Simulation have also been which the validity of the lower bound in practical . 
  
  
 ﻿We obtain the maximum average data rates achievable over block-fading channels when the receiver has perfect channel state information (CSI), and only an entropy-constrained quantized approximation of this CSI is available at the transmitter. We assume channel gains in consecutive blocks are independent and identically distributed and consider a short term power constraint. Our analysis is valid for a wide variety of channel fading statistics, including Rician and Nakagami-m fading. For this situation, the problem translates into designing an optimal entropy-constrained quantizer to convey approximated CSI to the transmitter and to define a rate-adaptation policy for the latter so as to maximize average downlink data rate. A numerical procedure is presented which yields the thresholds and reconstruction points of the optimal quantizer, together with the associated maximum average downlink rates, by finding the roots of a small set of scalar functions of two scalar arguments. Utilizing this procedure, it is found that achieving the maximum downlink average capacity C requires, in some cases, time sharing between two regimes. In addition, it is found that, for an uplink entropy constraint H <¯ log2(L), a quantizer with more than L cells provides only a small capacity increase, especially at high SNRs.
 Index Terms—Channel state information feedback, Information rates, fading channels, quantization, radio communication.
 INTRODUCTION
 I
 T is well known that the achievable data rates for reliable communication over a fading wireless channel depend on the availability of channel state information (CSI) at the transmitter and receiving end [1], [2]. For single-input singleoutput (SISO) flat fading channels, the CSI consists of channel gain and phase. If perfect CSI is available at the transmitter (perfect CSIT) and at the receiver (perfect CSIR), the channel is slowly fading and the transmission is subject to a long-term average power constraint, then the average capacity is achieved by adapting rate and power to the channel gain in a time waterfilling fashion [3], [4]. By contrast, if an instantaneous (per block) maximum power constraint is imposed, the fades are ergodic and the transmission blocks are long enough so that the fade statistics over each block converge to their ensemble statistics, then the ergodic channel capacity is achievable without CSIT [3], [5]. Else, if the fading is so slow that channel gain can be regarded as constant within each block (which corresponds to a block-fading scenario) then CSIT is beneficial. In this case, with perfect CSIT and per-block power constraint, the capacity is achieved by transmitting at maximum power, with only the data rate being adapted to the channel gain in each transmission block [3].
 If perfect CSIR is available and the receiver feeds back this CSI via an uplink with limited information throughput, then only imperfect CSI will be available at the transmitter. In a block-fading situation, the uncertainty at the transmitter about the true channel gain in each block implies a trade-off between throughput and reliability: the larger the data-rate chosen by the transmitter, the higher the probability of exceeding the channel capacity during the transmission block [6]. This poses the problem of encoding the CSI at the receiver and decoding it at the transmitter (i.e., choosing rate and power) in a rate-distortion optimal fashion, where the distortion is some measure of the decrease in downlink throughput, as in [3], [7] or the increase in error probability, as in [8].
  2013 IEEE
 The capacity of memory-less block-fading SISO channels with long-term power constrained downlink transmission and fixed-rate constrained CSI feedback was studied in [9]. A similar situation was considered in [10], assuming a multilayer downlink coding scheme in which data-blocks are decoded perfectly or totally lost if transmission data-rate is, respectively, below or above the channel capacity during the block. The idea in [10] was to design a quantizer with a fixed number of quantization cells so as to maximize the expected downlink rate, i.e., the expected number (or long term average) of successfully decoded bits. Also for a constraint in the number of CSI quantization cells, [11] studied the maximization of downlink throughput considering a noisy feedback channel. There exist also numerous results related to downlink throughput maximization problems for multipleinput multiple-output (MIMO) wireless channels (see, e.g. [2], [12]–[14] and the references therein). Although not directly related to the SISO problem (which is the focus of this work), it is worth mentioning that, in all the MIMO results in [2], [12]–[14] and the references therein, the only constraint on the quantizer (where there is a quantizer) is its cardinality. In [15], the maximum SISO downlink average throughput under a long-term power constraint and for a fixed number of quantization cells is analyzed. The performance of zerooutage schemes (referred to as MASA schemes) was compared against that of average reliable throughput schemes (referred to as ART schemes), which allow for outages to occur. It is shown in [15] that, in some regimes, when the additional feedback load of the ART policies (associated with informing the transmitter of a previous outage) is counted in, MASA schemes outperform ART schemes. In that context, the feedback load refers to the entropy of the messages (quantized CSI plus ACKs and NACKs) that are sent to the transmitter. However, in [15] this entropy is evaluated a posteriori, i.e., after the quantizers have been optimized without considering uplink entropy as a constraint.
 Thus, in all these papers, the design of optimal CSI quantizers has been addressed only considering a constraint on the number of quantization intervals (or cells). However, if one addresses the question “what is the maximum throughput that can be attained if there is a constraint on the amount of information that can be sent to the transmitter for representing the CSI?”, then it is more appropriate to consider an entropy constraint (instead of a cardinality constraint) for the CSI quantizer. On the other hand, the entropy of the quantized output, say H, is a lower bound to the average number of bits required to represent this output. At the same time, by using Huffman coding, it is possible to find prefix-free bitwords for each quantized CSI outcome with an average length not greater than H + 1 bits per CSI realization. Moreover, in a situation in which K i.i.d. CSI realizations are quantized at a time (which would happen, for example, in an OFDM system with K independently fading carriers), joint entropy coding would yield bitwords with an average length of K times H plus, at most, 1 bit. Since, in general, having fewer information bits to feed back for each CSI realization requires less average power, bandwidth or time, the latter benefits can be directly associated with a low entropy. This constitutes a practical motivation for considering entropy, instead of cardinality, as a constraint for the CSI quantizer. However, to the best of the authors’ knowledge, there are no available results on average downlink throughput maximization in which the quantizer utilized to encode CSI for the transmitter is to be designed subject to a constraint on the entropy of its output.
 With the motivations stated in the previous paragraph, in this paper we study the problem of finding entropy-constrained quantizers with any given number of quantization intervals, for encoding block channel gains for the transmitter, that yield the largest average downlink data rate. In our setup, the downlink channel is assumed to experience i.i.d. block fading, with associated gains and phases perfectly known to the receiver. We consider a wide family of fading statistics, general enough to include Ricean and Nakagami-m fading channels with one or more degrees of freedom. As in [10], the uplink over which quantized CSI is fed back is an errorfree, zero-delay channel. To solve this problem, we propose a numerical method which yields the optimal quantization thresholds and reconstruction points for any given number of quantization intervals and average channel signal-to-noise ratio (SNR). The optimization problem is partly similar to the quantizer design problems addressed in [16]–[19] because of the common entropy constraint. However, as we shall see, since the distortion measure in this case is the decrease in average downlink rate (not mean squared error), the resulting situation is vastly different from the one encountered in standard entropy-constrained quantization. The CSI entropyconstrained coding problem turns out to be non-convex, and our analysis reveals that it has, in general, several local optima. Its Lagrangian formulation, for L = N +1 quantization cells, leads to a system of 3N + 2 non-linear equations in 3N + 2 unknowns, each of which taking values over the non-negative real numbers. Since each of these equations must be solved numerically, and due to the high-dimensionality of the search space, direct solution of this system of equations constitutes a high numerical complexity task. The numerical procedure introduced in this paper greatly reduces this complexity by turning the problem into finding the roots of a small set of scalar functions of two scalar arguments, only one of which having unbounded support. The evaluation of each of these functions involves solving N line-search problems with respect to monotonic functions. By applying this procedure, it is found that, in general, the maximum average downlink capacity C for a given uplink entropy H¯ is a non-concave function. Since in our formulation time sharing between two regimes yields an average capacity and entropy equal to the weighted averages of the capacity/entropy values of each regime, the region of all achievable C, H¯ pairs is given by the convex hull of the C v/s H¯ curve. On the other hand, it is found that if H¯ is log2 of the number of available quantization cells, then arranging the thresholds so as to obtain equiprobable cells is nearly optimal. Our results also allow one to find the gain in downlink average throughput of using an optimal entropy-constrained quantizer instead of a cardinality constrained optimal quantizer. For instance, when the average downlink SNR is 0 dB, then an entropy-coded quantizer with 3 levels and an average rate of 1 bit per CSI realization yields an 8% increase in average downlink throughput over an optimal fixed-rate quantizer with two levels (i.e., requiring the same average rate). The performance of the latter fixed-rate quantizer corresponds with the one found in [10]. It is also found that for any given maximum uplink entropy constraint H <¯ log2(L), the increase in maximum downlink capacity obtained by using a quantizer with more than L cells is relatively small. Moreover, our analysis also suggests that, for any given H¯, the maximum average downlink capacity is achieved using a quantizer with a finite number of cells. This contrasts with what is obtained also for an exponentially distributed source but with MSE as the distortion measure, wherein the optimal quantizer turns out to be uniform with infinitely-many levels [16].
 In the following section we present a precise model description, introduce some notation, and formally state the problem of interest. To illustrate some of the properties of this problem and its solutions, we first analyze the case N = 1 (two quantization cells), which can be solved explicitly, in Section III. Then we extend the analysis to the case N > 1 in Section IV, where we introduce the numerical procedure to solve the problem in its generality. Section V shows and analyzes the results obtained with this procedure for the case N = 2 and N = 3 under Rayleigh fading. Finally, Section VI draws conclusions.
 Fig. 1. Transmitter and receiver connected by downlink and uplink channels.
 PROBLEM FORMULATION
 We consider a block-fading downlink additive white Gaussian noise (AWGN) channel, a transmitter, a receiver and an error-free, zero-delay uplink channel, as depicted in Fig. 1. In the transmitter, the binary message sequence W is mapped into consecutive blocks of K symbols. During each block b ? N, a real-valued sequence   is transmitted over the downlink channel. The random block-channel gain magnitude for the b-th block, , is assumed constant within each block. Channel gains in consecutive blocks are i.i.d. according to a probability density function (PDF) satisfying the following:
 Assumption 1: The PDF of the the channel gain’s squared magnitude, fg, has the form
 	fg(u) = K1 e-K2u ß(u),	(1)
 for appropriate constants K1,K2 > 0, where the differentiable function  is such that the ratio  dßß(u(u)/du) is non increasing with respect to u over [0,8). 
 The structure of the PDF of g in (1) is fairly general. For example, if channel gain magnitude is Ricean distributed, then the PDF of g has the form
 	  ,	(2)
 where I0(·) is the modified Bessel function of the first kind of order zero. From direct comparison with (1), we obtain, for this case,, K2 = 1/(2s2) and ß(u) =  . It can be verified (numerically) that the latter form of ß(·) satisfies the conditions required by Assumption 1. Likewise, if channel gain magnitudes are governed by a Nakagami-m distribution, then the PDF of g takes the form
 	mm	m
 	fg(u) =  G(	m u -1 e-m? u	(3)
 m)?
 and we have K1 = G(mm)m?m, K2 = m/? and ß(u) = um-1. If m = 1, it is easy to verify that ß(u) also satisfies the
 conditions required by Assumption 1.1
 Returning to Fig. 1, the real-valued random process nb[k], k ? {1,...,K}, is AWGN with sample variance N0. Thus, if  were the samples, taken at Nyquist frequency, of continuous-time AWGN band-limited to B [Hz], then the
 1The necessity of the condition upon ß(·) in Assumption 1 will become evident in Lemma 1 (Section III), in which it allows us to prove the convexity of a function playing a key role in the problem under study.
 two-sided PSD of the latter would be N0. On the other hand, the information-bearing signal xb[k] is subject to a per-block power constraint of the form
 	 ,	(4)
 where M > 0. With this constraint, if the block-length K is large, then the maximum achievable data-rate during any given block b can be well approximated by Shannon’s capacity formula [20] as Cb = ln(1 + ? gb) nats/s/Hz, where
 	 	(5)
 is the mean SNR at the receiver for a channel power gain g with unit mean value.
 At the other end of the downlink channel, the receiver is assumed to acquire a perfect estimation of gb prior to (or at the beginning of) the b-th transmission block. This channel power gain is instantaneously quantized and entropy coded, with the resulting bits being sent over a zero-delay, error-free uplink channel. These assumptions about the feedback channel have been considered before in [7]–[10], [21]. The zero-delay condition can be expected to be a good approximation when the time spent to feed the quantized CSI back to the transmitter is much shorter than the duration of a downlink frame. In turn, it is possible to have an almost error-free feedback channel if the feedback SNR is sufficiently large and/or strong forward error correction is employed for the CSI bits. And naturally, if in a given situation these latter conditions are not present, then our results would provide upper bounds to achievable performance.
 As foreshadowed in the Introduction, it is possible to translate a small entropy of the quantized CSI into using less average power, bandwidth or time to convey this CSI to the transmitter. At this point, it is perhaps worth noting that if only a single CSI realization is quantized and fed back at the beginning of each downlink block, then attaining these benefits may require one to match the channel coding and modulation scheme in the feedback link to the variable bitword lengths coming out of the entropy coder. For instance, placing an “off-the-shelf” channel coder and modulator in the feedback channel would yield an uplink that conveys only sequences of fixed-length data blocks. Such choice which would entail significant inefficiencies when transmitting variable-length bitwords, in comparison to sending fixedlength bitwords. However, in this scenario wherein a single CSI realization is quantized and fed back at a time, the Vi
  
 	µi	ui	µi+1
 Fig. 2. Illustration of the i-th quantization cell.
 feedback channel coding and modulation can be chosen so as to handle variable-length bit words (or the associated unequal probability outcomes of the quantizer) as efficiently as it is possible for fixed-rate quantizers. This can be done, e.g., by employing variable-length error-correcting codes [22] or joint source-channel coding (see, e.g., [23]–[25] and the references therein). Although the design of such coders and modulators is beyond the scope of this work, we illustrate this fact with an example (presenting a simple scheme similar in spirit to [26]), which can be found in Section VII-A in the Appendix.
 Upon receiving the quantized CSI, the transmitter chooses a transmission data-rate rb from a discrete set of data-rates. To define the quantizer and its reconstruction values, let N +
 1 be the number of quantization intervals (or cells), and let   denote the thresholds set, where
 	0 = µ0 = µ1 = ··· = µN < µN+1 = 8.	(6)
 Define also the quantization cells Vi  [µi,µi+1), i = 0,...,N.  As in [10], whenever the channel power gain gb falls within cell Vi, the transmitter outputs a codeword   satisfying (4), belonging to the i-th codebook amongst N + 1 codebooks, one for each cell. This codebook is capacity-achieving for some nominal channel power gain ui associated with the cell Vi, i.e.,
 g ,
 (7)
 ?i ? {0,...,N}.
 Thus, the power-gain levels  can be seen as the set of reconstruction points (or codepoints) of the quantizer, as represented in Fig. 2.
 Since in each block the transmitter sends information over the downlink using a capacity-achieving code for a nominal channel gain ui, all the transmitted bits are correctly decoded if gb = ui. Else, if gb < ui, then ri is not supported by the channel, and the receiver declares an outage, discarding all the information received during the b-th block. From this, it is clear that a necessary condition for a set of reconstruction values to be optimal is
 	ui ? Vi,	?i = 0,...,N.	(8)
 Let the random variable ub, taking values in   with probabilities Pr{ub = ui} = Pr{gb ? Vi}, denote the output of the quantizer for the b-th block. As already mentioned in Section I, we focus on quantizers that satisfy an entropy constraint, which we now formally state as
 H(ub) =  
 	nats/block,	?b ? N,
 with H¯ = 0 representing the maximum entropy allowed for the quantizer’s output.
 We are interested on finding the quantizer (i.e., the thresholds   and reconstruction points  ) satisfying the entropy constraint (9) and maximizing the average datarate in the downlink channel, defined as the average number of correctly decoded bits. The average number of correctly decoded bits (sometimes referred to as average “goodput” or average reliable throughput) is also the the objective function in [7], [10], [12]. It is a reasonable figure of merit if one assumes forward error correction followed by interleaving has been applied to the data being sent over the downlink, so that, with high probability, downlink blocks lost due to outage do not cause irrecoverable errors. Otherwise, and if all data is to be decoded correctly, ACKs and NACKs would have to be sent back over the uplink to request retransmission of lost data. As we shall see from the examples in Section V, at least for Rayleigh fading and for ? equal 0 dB and 30 dB, the optimal entropy-constrained CSI quantizers are such that only the first cell (V0) allows for outage events. The latter suggests that if sending ACKs and NACKs is necessary, then it would mean adding at most Pr{gb ? V0} bits per CSI realization to the uplink datarate. (The extra bit-rate is upper bounded by Pr{gb ? V0} bits/block because only when gb ? V0 it becomes necessary to send an ACK or NACK during block b + 1, which means at most 1-extra bit every time gb ? V0.)
 Since fades in consecutive blocks are i.i.d. and ergodic, averages over a large number of blocks converge to ensemble averages. Thus, for notation simplicity, in the following we drop the frame and channel-use indexes b and k. With this, the quantizer design problem can be stated as finding the thresholds   and codepoints   satisfying (6) and (8), that maximize the average data rate in the downlink channel without exceeding the entropy constraint in the uplink. More precisely, combining (6), (7), (8) and (9), we state the optimization problem in canonical form as minimize: 
 (10a)
 subject to:(10b)
 (10c)
 with µ0 = 0 and µN+1 = 8, and where  is the cumulative distribution function (CDF) of g.
 This optimization problem is difficult to solve primarily because the entropy constraint (10c) is non-convex. As we shall see, this leads to the existence of several local solutions, which, in principle, requires one to run an optimization program several times with a potentially large number of different starting values. In the following sections we solve this optimization problem, first explicitly for the case N = 1, and then numerically by means of Lagrangian optimization and a novel procedure which greatly reduces the overall complexity of the task.
 PROBLEM SOLUTION FOR TWO CELLS
 We now address the optimization problem stated in (10) for the case N = 1, corresponding to two quantization cells. In this case, (10) reduces to: minimize: µ1,{ui}i1=0
 subject to:  
 (11c) (11d)
 (11e)
 This problem can be solved explicitly without using Lagrange multipliers by noticing that the entropy associated with the two cells depends only on the threshold µ1. Supposing µ1 is given, the optimal value of u0 is found by differentiating the objective function J with respect to it and equating to zero:
 ?J
   = 0
 	?u0	??	(12)
 1 + ?u0
 Fg(µ1) =   ln(1 + ?u0)fg(u0) + Fg(u0)
 ?
 We see from this equation that for every u0 ? R+ there exists a unique µ1 > u0 for which u0 minimizes J. On the other hand, in order to determine the optimal value of u1 given µ1, we notice that this value has to minimize the term ln(1 + ?u1)(F(u1) - 1) in (11a). Although, in general, such value cannot be found explicitly, the following lemma guarantees that it is unique and that it can easily be found numerically: Lemma 1: Let f be a PDF satisfying Assumption 1. Define u
 Then, for any µ > 0, the function
 	 	(13)
 is convex for all u ? [0,µ].
 The proof of this lemma, which will play a key role later in Section IV, can be found in the Appendix, at the end of this document.
 It turns out that the value of u which minimizes ?(u,µ)  ln(1 + ?u)[F(u) - F(µ)] increases monotoni- cally with µ. More precisely, define the function U(µ)
  . Then, under Assumption 1, it
 holds that
 	 .	(14)
 To prove this claim, suppose that  .
 This means that
  
 for any ? = 0, with the inequality being a consequence of the fact that F(·) is non-decreasing. Recalling from Lemma 1 that ?(u,µ) is convex in u, we conclude that  becomes zero at a single value of u greater than or equal to  . This proves that   holds for all µ ? (0,8).
 The latter result implies that the optimal value for u0 must belong to the interval (0,?(?)), where .
 Also, by applying Lemma 1 with µ = 8, it is readily found that the unique value of u1 that minimizes J in (11a) also equals ?(?). The convexity granted by Lemma 1 means that the latter function can be easily obtained numerically by line search. Moreover, Lemma 1 implies that sgn(?J/?u1) = sgn(u1-?(?)), which leads to the conclusion that the optimal value of u1 given µ1 is
 	u1 = max{µ1 , ?(?)}.	(15)
 It then follows that, if u0 is part of an optimal quantizer for some entropy constraint, then the optimal µ1 can be explicitly obtained from u0 by using (12), and then the optimal u1 can be directly derived from µ1 using (15). In this manner, by increasing u0 from 0 to ?(?) and evaluating µ1 and u1 with the latter equations, one generates a family of quantizers containing the optimal quantizer for every value of H¯.
 Fig. 3 (top) shows the curve of downlink capacity versus uplink entropy obtained by the method described in the pre-
 vious paragraph, for two quantization cells, under an average SNR ? = 1 (0 dB, left) and ? = 1000 (30 dB, right). More precisely, for every , the corresponding µ1 was calculated using (12). Then, for this value of µ1, the optimal codepoint u1 was determined using (15). In this way, for each value of u0, a different quantizer was obtained. The pair {downlink capacity, output entropy} associated with each quantizer yielded a single point in the top two graphs in Fig. 3. In this example, channel gain magnitudes distribute Rayleigh, so gb has exponential PDF, chosen to yield unitmean, i.e., fg(u) = e-u, ?u ? [0,8). Notice that, for every value of H <¯ 2, there exist two solutions to (12) and (15), corresponding to different local constrained minimizers of J in (11). There is one quantizer associated with each of these two solutions. One of these yielded the upper section of the curve in each of the plots shown in 3-top, and the other one yielded the lower section. Of course, the optimal quantizers are those responsible for the upper part of their respective curves, i.e., those which yield the maximum downlink capacity for a given uplink entropy constraint. As expected, in general, downlink capacity grows when the entropy of the quantized CSI available to the transmitter (uplink entropy) is increased.
  
  
 Fig. 3. Solutions to (12) and (15) (two quantization cells) for Rayleigh fading with unit-mean channel power gain. Top: Downlink capacity v/s uplink entropy under an average SNR ? = 1 (0 dB, left) and ? =1000 (30 dB, right). Bottom: Thresholds and code-points for the optimal solution as a function of uplink entropy under an average SNR ? =1 (0 dB, left) and ? =1000 (30 dB, right).
 Note that the maximum downlink capacity for ? = 1 (SNR = 0 dB), 0.52347 bits/s/Hz, occurs at an entropy of 0.85644 bits/block, not coinciding with maximum uplink entropy, which, for a two-cell quantizer is 1 bit/block. This is expectable for a quantizer with a fixed number of cells, since there is no reason why all cells being equally likely (the only situation in which the entropy is maximized) should yield the largest downlink capacity. Also, the maximum downlink capacities for a two-cell quantizer at 0 dB and 30 dB SNRs coincide with what was obtained in [10], where average downlink capacity was maximized without an entropy constraint on the quantized output. At an average SNR of 30 dB, Fig. 3 (top right) shows that the maximum downlink capacity occurs closer to maximum uplink entropy, and that equallylikely cells, corresponding to this maximum entropy, are near optimal. Interestingly, the maximum capacity curve at this SNR shows a breakpoint at an entropy of about 0.5 bits/block (where the curve crosses itself in Fig. 3 top right), which makes it non concave. For this case, this implies that better performance for entropies below   bits/block can be achieved by doing time-sharing between two quantizers: one with a single cell and zero entropy, and another with two cells and an entropy of about 0.88 bits/block. By choosing one regime more frequently than the other, it is possible to achieve a downlink capacity and an uplink entropy equal, respectively, to the weighted averages of the capacities and entropies of both regimes. In this manner, all capacity/entropy points within the convex hull of the capacity v/s entropy curves can be achieved.
 Fig. 3 (bottom) shows the evolution of thresholds and codepoints for the optimal solution, as the entropy of the quantized output varies. For ? = 1 (0 dB), on the left, u1 = µ1 at all entropies. We see also that higher capacities are achieved by bringing thresholds and code-points closer together. For the 30 dB case, shown in Fig. 3 (bottom right), the breakpoint in capacities coincides with a change in the arrangement of thresholds and code-points in the quantizer. Except in a neighborhood to the right of this breakpoint, we find that the codepoint u1 coincides with its left-boundary threshold µ1.
 PROBLEM SOLUTION FOR MORE THAN TWO QUANTIZATION CELLS
 A. Preliminaries
 The straightforward approach presented in the previous section cannot be extended directly to the case in which there are more than two quantization cells. Lagrangian optimization can be utilized instead, which, as we shall see, leads to a system of non-linear equations that must be solved numerically. The non convexity of the optimization problem (10) and the existence of several local minimizers satisfying the constraints imposes the need to solve this system of non-linear equations possibly many times with different initial values. However, we will introduce an algorithm, in the same spirit as the strategy illustrated in the previous section, that allows one to simplify the Lagrangian optimization problem to a sequence of simple line search problems, each with a single solution.
 Before stating the Lagrangian associated with (10), we note from (10a) that, at the optimum, the inequality constraints stated in (10e) are not active. Similarly, constraint (10d) for the case i = 0 is not active since increasing u0 above µ0 = 0 would raise the average downlink data rate without increasing the entropy of the quantized output.
 Taking the above observations into account, the Lagrangian associated with (10) adopts the following form
  
 with Fg(µ0) = 0, Fg(µN+1) = 1, and where  are
 Lagrange multipliers. Differentiating L with respect to u0 and equating to zero gives
  
 (17a)
 Notice that (17a) is identical to (12), which implies that u0 is a solution to (17a) only if
 	u0 ? (0, ?(?)].	(17b)
 Differentiating L with respect to the other codepoint values and equating to zero yields
 	?L	-?	-
 ?uj = 1 + ?uj (Fg(µj+1) Fg(uj)) (17c) + ln(1 + ?uj)fg(uj) - ?j = 0, 1 = j = N
 Hence, we obtain N+1 non-linear equations, which must hold simultaneously.
 On the other hand, differentiating the Lagrangian with respect to the thresholds we obtain
  
 yielding N additional equations to be satisfied simultaneously.
 Finally, the Karush-Kuhn-Tucker (KKT) conditions [27], [28] provide another set of N + 1 equations,
  
 plus the requirement that
 ? = 0		(17h)
 ?j = 0,	?j = 1,...,N,	(17i)
 all to be satisfied simultaneously.
 Although it is possible to solve this system of 3N + 2 non-linear equations by standard numerical algorithms, the existence of numerous local minima requires one to apply these algorithms repeatedly, each time with different initial values. This shortcoming is worsened by the fact that the vector of initial values (Lagrange multipliers plus threshold and code-point values) lies in a (3N + 2)-dimensional space, which implies a large number of initial guesses is required to obtain a “reasonably good” coverage of the search space. In the following section we will show how these inconveniences can be avoided by using an approach similar to the one presented in Section III. More precisely, we derive a method which, for this problem, allows one to find all local minima in a systematic, sequential manner, greatly reducing the number of required computations.
 B. An Efficient Algorithm for More than Two Cells
 In this section we exploit the recursive nature of (17) to reduce its induced system of non-linear equations into a sequence of line-search problems over bounded intervals, in a spirit similar to the one behind the approach followed in Section III.
 To begin with, recall that the KKT conditions imply that, for every constrained (local) minimizer of (10a), the multiplier ?j > 0 only if uj = µj (i.e., only if the associated constraint is active). The next corollary of Lemma 1 provides an easyto-verify condition for uj = µj to hold in such a minimizer:
 Corollary 1: Let fg be a PDF satisfying Assumption 1.
 Suppose   are a solution to optimization problem (10). Then uj = µj holds for some j ? {1,...,N} if and only if
  Proof: The result follows directly from Lemma 1, since it implies the convexity of the function -ln(1 + ?u)(Fg(µj+1) - Fg(u)), and from (10a), upon recalling that the entropy of the quantized output does not depend on the choice of codepoint values.	 
 Corollary 1 will allow us to find uj and µj+1 from ?, uj-1, µj and µj-1 in a simple manner. For this purpose, define, for  ,...,N where
 	 	(19c)
 is the complementary CDF of g evaluated at µj. With these functions, the combined conditions (17c), (17d), (17g) and (17i) can be written in the following equivalent form
 ?j = ?j(uj,?j+1) = sj(?j+1) = 0, j = 1,...,N (20a) ?j(µj - uj) = 0, j = 1,...,N. (20b)
 	?j(0)+1	?j	?j(0)+1	?j
 Fig. 4. Plots of ?j(µj,?j+1) and sj(?j+1) (defined in (19)), as functions of ?j+1. Left: a case in which  . Right: a case in which
  .
 Figure 4 shows a (qualitative) description of ?j(µj,?j+1) and sj(?j+1) as functions of ?j+1. It can be seen that both functions are monotonically increasing, the first one being affine, the second one convex.
 A look at Fig. 4 immediately suggests that, for any given j, and depending on the values of the parameters ?, ?, uj-1, µj-1 and µj, there will be, in general, more than one pair of values of uj and µj+1 satisfying (20). Let us find out which solutions actually exists by first considering the conditions under which the constraint uj = µj is active or inactive:
 • Inactive Constraint: In this case, uj > µj, which implies ?j = 0 (see (20b)). In view of (20), this is equivalent to having
 	0 = sj(?j+1) = ?j(uj,?j+1).	(21)
 The first equality of (21) is satisfied by a unique value of the argument of s(·), say  , shown in Fig. 4.
 From (19b) this value is given explicitly by
 	 .	(22)
 From the definition of ?j+1, this quantity must be nonnegative. On the other hand, Corollary 1 implies that uj > µj if and only if
 	 ,	(23)
 situation exemplified and illustrated in Fig. 4-right. In addition, Lemma 1 states that, if this inequality is satisfied, then there exists a unique uj satisfying the second equality of (21). Thus, there exists a solution to (20) for which the constraint uj = µj is inactive if and only if   and (23) holds. In this case, we say solution (0) exists for j.
 • Active Constraint: In this case, uj = µj, and ?j can be positive. By looking at (20), a solution satisfying this condition exists if and only if there is ?j+1 for which
 (a)
 	?(µj,?j+1) = s(?j+1) = 0,	(24)
 which correspond to intersections of the plots of ?(µj,·) and s(·) occurring on the first quadrant in Fig. 4. Since, with respect to ?j+1, the function ?j(µj,?j+1) is affine and increasing, and the function s(?j+1) is convex and monotonically increasing, it follows that (24) is satisfied for either none, one or two values of ?j+1. Indeed, these properties imply that two solutions to equality (a) in (24) will exist if and only if
 	 ,	(25)
 where
 	 	(26)
 is the unique value of x at which ??(µj,x)/?x = ?s(x)?x, see Fig. 4. It is also easy to show that if (25) holds, these two solutions, say  and , will lie at opposite sides of . Also, it is straightforward to verify that both solutions are not larger than ?j. Therefore,
  . Returning to our original
 problem, each solution to equality (a) in (24) will also be a solution to (20) if and only if it is non-negative and it yields a non-negative value for s(·). This allows one to discard solution   or if
  . On the converse, if these two conditions do not hold, solution   will be non-negative and yield
  , i.e., it will be a valid solution to (20). In this case, it is easy to verify that
  
 a situation which is illustrated in Fig. 4-right.
 Making use of all these all these conditions, and letting
  , with i = 0,1,2, be the threshold value associated with ?j+1, we can devise the following procedure to find the solutions to (20) for a single, given j:
 Procedure 1: Suppose ? = 0, and that uj-1, µj-1 and µj are given, with 0 = µj-1 = uj-1 = µj. Then, in order to find the solutions to (20) for j,
 Calculate  and ?jc+1 explicitly using (22), (26), and (27), respectively.
 If  
 a) Find  by solving equality (a) in (24) with respect to ?j+1 by line search over  ,
 set , in which case solution
 (2) exists.
 b) If   and   and
  , find   solving equality (a) in (24) by line search over   (unless , in which case  ). Set uj = µj, and
  . In this case, solution (1) exists.
 If 	, find uj solving  
 (c.f. (21)) by line search over   and set ?j+1 =
  . In this case, solution (0) exists.
 If j < N discard solutions in which ?j+1 < 0. 
 The last step, where solutions yielding ?j+1 < 0 are discarded whenever j < N, responds to the fact that ?j+1 is a complementary CDF value. This requirement is dropped only if one is calculating the last threshold (i.e., when j = N), to allow a higher-level routine to iteratively adjust ? so that ?N+1 = 0. (Such a routine is exemplified by Procedure 3 below.) Of course, unless j = N, solution (0) can be discarded before doing the corresponding line search if  . The same applies for solution (1) if .
 In each of the line searches mentioned in Procedure 1, there exists a single solution over the corresponding search interval, since, in all cases, the involved function is monotonic within it. This makes each step straightforward to execute. Notice also that for every j = 1, and depending on the values of uj-1, µj-1 and µj, there are between zero and three pairs of values for uj, µj+1, namely solutions (0), (1) and (2), which satisfy (20) for that j.
 The above procedure can be applied sequentially to find all pairs uj, µj+1, for j = 1,2,...,N, that satisfy (20). More precisely, one can first choose values for u0 ? (0,?(?)] and  , with which one can calculate µ1 explicitly from (17a). Then, setting j = 1, Procedure 1 can be carried out to find at most three pairs of values for u1, µ2 satisfying (20) for j = 1. Each solution can be considered a branch in a tree structure. Increasing j by one and repeating the procedure for each emerging branch, until j = N, yields the complete tree of valid solutions, each of which is associated with a path in the tree. Thus, for each choice of u0 and ?, there can be at most 3N different paths, each associated with a sequence of codepoints and thresholds satisfying (20) for all j = 1,2,...,N. However, we shall see in the following that the number of valid paths obtained in practice is much smaller than 3N.
 Following our notation for solutions adopted in Procedure 1, we label each solution path in the tree using the numbers of the solution types associated with its segments, thus referring to a sequence . For a given choice
 of u0 and ?, we define the set of valid solution paths as
 O(u0,?)
  is a valid solution path for 
 It is important to note that there is a one-to-one relationship between every solution   satisfying (20) for a given choice of u0,? and every path in O(u0,?). Indeed, the path associated with any such solution can be easily determined by analyzing each of its code-point/threshold pairs using the the reasoning that led to Procedure 1.
 Now, suppose one wants to know whether a given path  is associated with a valid solution to (20) and then find this solution. Instead of applying Procedure 1 to find the entire tree of valid paths and then checking if  is one of them, one can apply the following algorithm, derived directly from Procedure 1, for this purpose:
 Procedure 2: Let u0 ? (0,?(?)], ? = 0 and the corresponding µ1 be given by (17a). Let  be a path.
 Then the following steps can be taken to determine whether   and find its associated thresholds and code-
 points:
 Set j = 1
 Calculate  explicitly using (22), (26), and (27), respectively.
 If 	:
 a) If   and : Set
   and calculate uj solving 
 0 by line search over  .
 b) Else, declare  and quit.
 Else if :
 If  
 and  : Calculate  solving equality (a) in (24) by line search over  .
 If   and 
 solution  . Else, declare  
 O(?,u0) and quit.
 Else, declare  and quit.
 Else (if 
 If  : Calculate   solving equality (a) in (24) by line search over  .
 If   and  , output solution . Else, declare   and quit.
 Else, declare  and quit.
 If j = N, declare  and quit. Else, set j = j + 1 and go to step 2).
 With the above procedure, the resulting values of all thresholds and codepoints are a function of  . For convenience, we denote this function by , defined
 as
  
 	  †	from Procedure 2	, if 
 , otherwise
 where we have chosen † as a special symbol to indicate that the path  does not yield a valid solution. For future use, we also define
 	  from 	(29)
 Although being a solution to (20) is a necessary condition for code-points and thresholds to be a solution to (17), two additional conditions must be satisfied for sufficiency. The first one is the entropy constraint, expressed in the KKT equation (17f). The second is the construction condition µN+1 = 8 or, equivalently, ?j+1 = 0. For a given path , u0 and ?, this condition can be expressed as .
 Therefore,
  
 (30)
 is the set of all paths , values for the first codepoint u0 and Lagrange multiplier ? associated with solutions to (17) for some entropy constraint, while S    corresponds to the set of all
 such solutions. Finally, by defining   and
  , respectively, as the entropy and capacity associated with , the original optimization problem (10) can be stated as
 	 	(31)
 Thus, we have reduced the problem from solving a set of (3N + 2) non-linear equations, over a (3N + 2)-dimensional space, into a moderate number of searches (one per valid path), over two dimensions, requiring the evaluation of a scalar function that takes N line searches to compute.
 The following is a procedure that can be utilized to solve (31)
 Procedure 3:
 Define a grid of values of u0 over (0,?(?)), say U, and a grid of values for ? ? [0,?max], say ?, for some ?max.
 For each u0 in U,
 For each ? in ?, run Procedure 1 recursively to find all valid paths.
 Detect pairs of consecutive values of ? between which a sign change of ?N+1 occurs for some valid path. In each of the intervals formed by such pairs, find the value of ? for which ?N+1 = 0 by line search, running Procedure 2 for the corresponding path.
 Calculate  and  for each of the   values found in the previous step. Select the combination that yields the largest capacity with an entropy not greater than H¯.
 In the following section we present an example in which Procedure 3 was utilized to find the solution to (31) for N = 3.
 EXAMPLE
 In this section we apply Procedure 3 to find the set G (see (30)), for the case N = 3, i.e., for quantizers having four cells, assuming vg  is Rayleigh fading with g being unitmean. The latter set contains the paths , values for u0 and ? that characterize a solution to (17), i.e., quantizer thresholds and code-points that yield a local maximum (or minimum) downlink capacity for some fixed maximum uplink entropy constraint H¯. The obtained results are presented in Fig. 5, for ? = 1 (SNR=0 dB), on the left, and for ? = 1000 (SNR=30 dB), on the right.
 The two top graphics in this figure correspond to downlink capacity v/s uplink entropy plots for all solutions to (17). It can be seen that, although for each SNR there exist several such solutions, for each entropy constraint the number of these solutions is significantly smaller than 3N = 27. For the 0 dB case, the absolute maximum downlink capacity is 0.6466 bits/s/Hz, attained with an uplink entropy of 1.842 bits/block, by a solution with associated path . The curve that yields this maximum is ended at that point and, to the eye, it seems as if there was a missing segment which would connect it to the curve that reaches the right boundary of the plot, at an entropy of 2 bits/block. The absence of this segment can be attributed to the fact that in the Lagrangian formulation of the problem, the uplink entropy is an inequality (and not an equality) constraint.
 The solution yielding the maximum downlink capacity for a given uplink entropy is given by the highest curve in each of the capacity v/s entropy plots. Interestingly, for entropies somewhat below log2(3) and log2(2) bits/block, the highestcapacity curve coincides with the optimal solution with 3 and 2 quantization cells, respectively. This can be seen by noticing from the bottom plots in Fig. 5 that a few bits/block below these entropy values, the optimal solution is such that one, two or three thresholds, respectively, tend to infinity, effectively leaving three, two and then one cell, as the entropy is decreased.  Such behaviour suggests that for a finite uplink entropy constraint, the maximum downlink capacity over all quantizers is achieved with a quantizer having a finite number of cells.
 As already observed for the same SNR and two cells, the region of achievable downlink average capacities is given by the convex hull formed by all the curves in the capacity v/s entropy plots. Unlike what is observed for ? = 1, since the composite maximum capacity curve for ? = 1000 (Fig. 5 top-right) is not concave, achieving the maximum downlink capacity for some entropy constraint values requires timesharing between two regimes.
 A. Comparison	with	Fixed-Rate,	Cardinality-Constrained Quantization
 The point of maximum capacity for each number of quantization levels corresponds to the solution obtained when a quantizer with that number of levels is optimized for maximum average downlink throughput without an entropy constraint. Therefore, those peak points are the solutions found in [10] for single-layer coding, where the quantizer was optimized under a cardinality constraint only. From this fact, we can conclude from Fig. 5 (top left) that at SNR = 0 dB, and for the same average uplink rate of an optimal fixed-rate quantizer from [10] with two levels (that is, 1 bit per CSI realization), which achieves 0.52347 bits/s/Hz, an optimal entropy-coded quantizer with 4 quantization cells yields (approximately) 0.56 bits/s/Hz. This represents an increase of roughly 8% in downlink average throughput for the same average uplink rate. The corresponding increase with respect to a fixed-rate quantizer with 3 cells goes from 0.6 bits/s/Hz (at a fixed
  
  
 Fig. 5. Downlink capacity v/s quantized output entropy for the solutions to (17), for Rayleigh fading with unit-mean channel power gain, using up to 4 quantization cells (N = 3). Left: Average SNR ? = 1 (0 dB). Right: Average SNR ? = 1000 (30 dB).
 rate of log3 = 1.585 bits/CSI realization) to (approximately)
 0.63 bits/s/Hz (using an optimal entropy-coded quantizer with 4 cells). For an SNR of 30 dB, Fig. 5 (top right) reveals that an optimal entropy-constrained quantizer provides smaller gains over fixed-rate optimal quantizers. Notice also that fixedrate quantizers can only operate at a limited set of uplink rates (given by log2 N bits/CSI realization), Thus, variablerate entropy-coded quantization is the only scheme which allows one to send quantized CSI using other average uplink rates (for example, rates below 1 bit/CSI realization).
 CONCLUSIONS
 We have proposed a numerical procedure to find the maximum downlink average capacity over block-fading channels, under a fixed per-block power constraint, when the receiver has perfect CSI and an error-less, delay-free, entropyconstrained uplink channel is available to convey quantized CSI to the transmitter. This procedure, which has a smaller numerical complexity than trying to directly solve the Lagrangian equations associated with the problem, also yields the quantizer thresholds and codepoints that achieve the optimal solution. Our results are valid for a broad class of channel fade distributions, including Nakagami and Ricean fading. We have applied the procedure proposed here to find optimal quantizers having 2 and 4 quantization cells. The obtained results revealed that, for a given number of quantization cells, say L, maximum capacity is achieved at an uplink entropy slightly below log2(L) bits/block. Furthermore, our results show that for any uplink entropy below log2(L) bits/block, there is little to be gained (in average downlink capacity) by using more than L quantization cells or intervals. This suggests that for any finite uplink entropy, the optimal quantizer has a finite number of quantization intervals. Our analysis also revealed that for high average SNRs, achieving the maximum average downlink capacity requires time sharing between two regimes with different uplink entropies and associated capacities.
 As a final remark, we would like to mention that, after several attempts, the authors have found that the results and strategies developed here for a short-term power constraint do not seem to be applicable for the long-term power constrained version of the problem. Indeed, the latter problem appears to be significantly harder to solve than both the one addressed in this paper and the long-term problem without the uplink entropy constraint.
 APPENDIX
 Efficient Channel Coding for the Output of an EntropyCoded Quantizer
 Here we provide an example to illustrate how a discrete random source with small entropy (which can be associated with the variable-length bitwords coming out of an entropy coder) can be efficiently transmitted using a matched channel coder. The latter coder is able to transmit the low-entropy source using less power and with a smaller message error probability than what is obtained when transmitting equiprobable symbols using 4-QAM modulation and maximum likelihood decoding. For this purpose, suppose we have two quantizers, each with four quantization cells. The first quantizer is not entropy coded, and each of its outcomes, represented using two bits, has equal probability. For simplicity, suppose that this quantizer is followed by a rate 1/2 error-correcting channel coder and that 4-QAM is utilized. Assume each symbol in the 4-QAM constellation has unit energy and that there is a memoryless channel in the uplink, with complex circularly symmetric white Gaussian noise with variance sn2. Therefore, each quantized outcome (or message) is mapped into channelsymbol sequences of length two (that is, two consecutive 4QAM symbols are sent for each message), which yields an average energy of 2. At the other end, the decoding is done by by picking the most likely transmitted symbol sequence given the received signal. For this scheme, it can be found (either analytically or via simulations) that at an SNR of 10 dB, the message error probability is approximately 0.9 × 10-3.
 For the entropy-coded quantizer, suppose that its four possible outcomes, say m1, m2, m3, m4, have probabilities 1/2, 1/4, 1/8 and 1/8, respectively. An entropy coder for this quantizer (for example, a Huffman coder) would output 1, 2, 3 and 3 bits, respectively, for each of its outcomes. In order to send these four outcomes (or messages) over the feedback channel, consider a time-varying digital modulator generating symbols from the constellations shown in Fig 6 (top). Each of these symbols (except symbol o) has unit energy.
 An outcome from the entropy-coded quantizer is fed back to the transmitter by sending a sequence of three channel symbols, each coming from the first, second and third constellation, as specified in the table of Fig 6. Notice that messages with higher probability are mapped to symbol sequences with smaller total energy, the latter being proportional to the length of the bitwords yielded by a Huffman entropy coder (i.e., proportional to -log2(pmi), where pmi is the probability of the i-th message or quantizer’s outcome). This corresponds to the sequence-length (or energy) distribution that minimizes average energy, the latter energy being in this case equal to 7/4. Therefore, the average energy required by the channel coding scheme for the entropy-coded quantizer is just 7/8 of the mean energy associated with the fixed-rate quantizer. On top of this power reduction gain, if this “variable-rate” scheme matched to the entropy-coded quantizer is combined with a maximum-likelihood sequence decoder at an SNR of 10 dB, then each message is decoded with a message error probability not greater than 0.6 × 10-3. This shows that if the outcomes of the CSI quantizer have uneven probabilities (which amounts to a smaller entropy than with equi-probable outcomes), then it is possible to transmit the quantized CSI using less power and with a smaller probability of error than when transmitting the outcomes of a fixed-rate quantizer.
 Proof of Lemma 1
 Proof: We have that
  
 Differentiating again,
  
 (33)
 where we use f as a short-hand notation for df/du. For every u such that f(u) = 0, it readily follows from the structure of  ·) (see (1)) that f (u) = 0 and therefore (33) immediately yields d2a(u)/du2 = 0. The same holds for u = 0. Thus, it is only left to consider values of u > 0 and such that f(u) > 0. If da(u)/du = 0, then, from (32),
 	 .	(34)
 Since we are only considering the cases in which f(u) > 0, (34) implies F(µ) - F(u) > 0. This allows one to
 substitute (34) into (33), obtaining
 Thus, for every u > 0 at which , the following
 implication holds
  
 On the other hand, if da(u)/du > 0, then
 	 .	(36)
 Since we are considering values of u such that f(u) > 0, we can substitute (36) into (33), obtaining
  (37)
 It then follows that (35) also holds if da(u)/du > 0. We therefore conclude that, irrespective of the sign of da(u)/du,
  
 Now, from the definition of f, we have that
  
  
 Fig. 6. Top: Three consecutive constellations of a digital modulator adapted for transmitting the outcomes of an entropy-coded quantizer with 4 quantization cells. Symbols a,b,c,d have unit energy. Bottom: A mapping between quantization outcomes (messages m1 to m4) and channel symbol sequences. The
 error probabilities refer to message errors, and the associated SNR is 1/sn2.
 On the other hand,
 (40)
 (41)
 Therefore,
  
 (42)
 Since   is a non-increasing function, it follows that the RHS of (42) is positive. Therefore, we obtain from (39) that a(u) is also convex for all u ? (0,µ] such that f(u) > 0. This completes the proof.	 
 
 ﻿ The performance of a noisy linear time invariant plant , over a noiseless digital channel with transmission delay , is in this paper . The channel the single measurement output of the plant to its single control input through a causal , but otherwise arbitrary , coder controller pair . An approach is to analyze the minimal average data rate to attain the quadratic performance when the channel a known constant delay on the data . This average data rate is shown to be lower bounded by the directed information rate across a set of and an additive white noise channel . It is that the presence of time delay in the channel the data rate to achieve a certain level of performance . The applicability of the is through a numerical example . 
 Rate constrained are generally studied from two of view ; control theory and information theory . In the former case , classical nonlinear control are employed . In the latter case , the key idea is extending information theoretic to the case of closed loop control . In both , stability analysis of linear is well studied see , e .., , as early and , as recent . 
 However , system performance from an information theoretic viewpoint are less abundant in the literature . Fundamental are in . In this work , for a discrete time plant , the well known Bode integral is extended to the case of causal rate limited arbitrary feedback . Along the of , research in , on the minimum data rate which is to attain a quadratic performance level in with delay free . For the lower bound , that the rate constrained optimization to find desired data over causal but otherwise arbitrary coder controller , is reduced to a convex constrained optimization problem over an auxiliary feedback loop closed through an channel . In , , it is furthermore shown that the directed information from the plant output and to the 
 control input a lower bound on the rate for any policy , and that it to use linear , when the initial state and external are jointly and the plant is linear . These were used by , to establish a general framework for the problem of control for fully observable multiple input multiple output . 
 In this paper , we address output feedback control of an comprised of a noisy plant and a causal controller set connected through a noiseless digital channel with a constant transmission delay . More specifically , the problem is the on minimal average data rate to guarantee that the steady state variance of an error signal does not become than a certain value . by its such as simplicity and practical appeal , we use the approach in , to gain outer and build upon . However , the main departure of this work from is considering a channel which is not delay free . So , as the first contribution , we fundamental information of the system under the delay assumption . Secondly , we characterize the trade off among performance , delay , and minimal desired average data rate . It is shown through a numerical example that greater transmission delay greater minimal average data rate to guarantee the considered quadratic level of performance . Simulation that by a simple scalar uniform in the architecture that the lower bound , the quadratic performance is by operational average data at most . away from the lower bound . 
 The outline of this work is as . Section the notation and some . Then the problem of interest is in Section . Section is to the lower bound characterization . An illustrative numerical simulation is provided in section . Finally , Section the paper . 
 The set of real is subset as the set of strictly positive real . the set of natural , based upon which N is defined . Furthermore , is the time index and for random considered in this paper , N . Magnitude and H norm of a signal are by . and . k , respectively . The set U is defined as the set of all proper and real rational stable transfer with that are 
 stable and proper as well . E the expectation operator and log for the natural logarithm . The entry of the i th row and th column is by i ,. Moreover , min and represent the and magnitude , respectively . 
 All random and in this paper are assumed to be vector valued , unless otherwise stated . A random process is said to be asymptotically wide sense stationary if it and E E E hold , where is a constant . the corresponding steady state covariance matrix upon which the steady state variance of is defined as , trace . The covariance matrix for a scalar random sequence is defined as . Considering 
 , as two square matrices , the and are asymptotically equivalent if and only if the following for finite : 
 The structure considered in this work can be found in Fig . an plant with u as control input sensor output . Moreover , there is a disturbance by and the output signal , with , , upon which the desired performance is . The plant the following transfer function matrix description : 
 , , and for G , G , G and G , respectively . The input alphabet of the channel is by A and is defined as a countable set of binary . Due to the delay , the output of the channel to A . The average data rate across the channel is as : 
 where i the length of the i th binary word i . The channel input is provided by the E based on the following dynamics : 
 in which e is the side information at the with an arbitrary possibly nonlinear or time deterministic . It should be noted that is a shorthand for . On the side , we have 
 is assumed to be an arbitrary deterministic , like , and the side information available at the at time . It should be that E Fig . are possibly time or nonlinear causal . 
 Assumption . : The , proper and free of unstable hidden . Moreover , the open loop transfer function from u single input single output and strictly proper . The disturbance signal ,, is a zero mean white noise with identity covariance matrix I and jointly with x , the initial condition , finite differential entropy . 
 Assumption . : Each of e and is jointly independent of x ,. So regarding the dynamics of the system , I u ; for . Moreover , upon knowledge of and di , the is invertible . It that there a deterministic 
 Now , suppose that Assumption . . Let denote the steady state variance of the all u for and u independent of x and . 
 for any , , where the search is to be restricted to with and with which satisfy Assumption . and make the of Fig . strongly asymptotically wide sense stationary this notion of stability is defined in . Moreover , the steady state variance of . The optimization problem in is feasible if , see Appendix A for the proof . 
 Theorem . : For the feedback loop in Fig . and satisfying . and . , the following : 
 see for the definition . Moreover , as defined in , the directed information rate across the forward channel u with delay . The proof can be found in . 
 Now , a lower bound can be derived on the directed information across the scheme of Fig . . 
 ,, u , form a jointly second order set of and that . and . hold . Moreover , take and uG into account as the u where ,, uG , are jointly with the same first and second order cross as ,, u ,. 
 where a from a slight modification in , Lemma . , from , Lemma . , form the chain based upon b in , Theorem . , is a consequence of Assumption . and being a deterministic function of , and wi , and e from a in , Theorem . and Assumption . . This the proof . 
 In what we will relate the directed information from to uG to their corresponding power spectral : 
 Lemma . : u as jointly . Moreover , suppose that u is with where . Then the following 
 Proof : and joint ness of u , in mind and based on , Theorem . with a little modification , we can conclude that is and as well . We start by the following : 
 where from the definition of mutual information and from the definition of , from , Property and , and i from , Property . So the directed information rate can therefore be as : 
 where from the chain rule of the differential entropy and being independent of . Since the process u is with for some , , Lemma . will approve the validity of the leftmost term in . The rightmost term is self explanatory because is and . 
 It from Theorem . , Lemma . and Lemma . that the rate performance pair by any scheme which the , is attainable with a lower rate by a scheme comprised of and an noise source . Such a scheme is in Fig . . 
 The of Fig . is defined under the same Assumption . as the main system in Fig . except for one thing ; the arbitrary are by . Moreover , the communication channel is a channel with noiseless one sample feedback . The dynamics of this auxiliary scheme can be as : 
 in which is the with zero mean and variance . This noise is assumed to be independent of x ,. Additionally , we suppose that the initial state of ,, and the delay are deterministic . 
 Theorem . : For the in Fig . and satisfying . and . , is lower bounded as if , : 
 where the feasible set for the optimization problem u is the noise with rendering the feedback loop of Fig . internally stable and well when . In these , and denote the steady state variance of z and the steady state power spectral density of u in Fig . , respectively . 
 Proof : Since , there is at least one pair , say and , satisfying Assumption . , rendering the of Fig . and , and where and 
 can be based on Theorem . , if the in Lemma . and Lemma . are met . It should be noted that the steady state power spectral density of . A scheme comprised of linear with a unit gain noisy channel and generate and which satisfy those and keep within , . Such a scheme is as : 
 where a noise with zero mean and independent of . Regarding the causality and linearity of , can be written as : 
 According to the causality in , joint ness of , and transitivity of asymptotic equivalence for and sum of the matrices noted in , the and are asymptotically equivalent to of lower triangular matrices . Moreover , the internally stable and well . With all of this in mind , by setting a concatenation of linear with the steady state behaviour of in and considering a variance for equal to , the system of Fig . will be well and internally stable where and are . Then according to Lemma . , the following can be : 
 Lemma . : Consider the loop of Fig . with fixed . Define as : 
 in which the steady state power spectral density of . Then for any , upon the existence of the pair , B , J making the system of Fig . internally stable and well , there exist another pair , comprised of 
 the filter J and B , which the feedback loop of Fig . internally stable and well , the the steady state power spectral density of z and the following : 
 Proof : It is well known that the system of Fig . is well and internally stable if and only if the transfer ,, , to z , y ,, u in Fig . to . By Ti and , we refer to the set such that , i , . Moreover , and represent of By in the situation where . Now , consider the following set of : 
 in which d the relative degree of J and U is chosen in such a way that . Consequently , J is and T can be written as : 
 So regarding the definition of d and of , T if and only if T . Moreover , based on the same argument , B , J would give the same power spectral density for z as for the case where B , J is . Let d ,..., represent the of lying on the unit circle . Now for , we define the following : 
 Hence , U and can be for every , . By following the same procedure as for the proof of , Theorem . , the existence of , will be shown in such a way that for any , setting will give a pair B , J that . 
 Corollary . : If . and . hold for the of Fig . and , , then 
 where the optimization is done over all filter , and the noise variance making the system in Fig . internally stable and well . 
 Proof : According to the feasibility of finding u see for the proof , there exist the triplet , with a proper filter and , that for the system of Fig . . Furthermore , based upon the definition of and in and , the following can be derived for any : 
 So regarding Lemma . , since there exist a filter and a proper one making the feedback loop of Fig . internally stable and well and keeping intact , the following can be : 
 To characterize , we will mostly use of linear and some on H optimization with input delay . Consider the structure of Fig . , where except for shifting the delay block to the plant model , which to 
 the same as Fig . hold . The of Fig . is internally stable and well if and only if the transfer function Ta from ,, , to za , ya , ra , in Fig . is a member of . It can be easily shown that Ta . So the feedback of Fig . and Fig . are equivalent in the sense of internal stability and well ness . Moreover , the and output variance of the in Fig . can be stated in of H as : 
 . Likewise , the following for the structure of Fig . : 
 and . So every triplet that can the while making the system output satisfy for the of Fig . , can do the same for the the system of interest , in Fig . , and vice . In other , the in Fig . and Fig . are equivalent regarding the optimization problem in as well . This problem is studied for such feedback as auxiliary system of Fig . in . Consequently , it can be that the problem of finding is equivalent to an constrained optimal control problem which was proved to be convex . As another result , being a monotonically decreasing be . All in all , the interplay between the desired performance , the average data rate and the time delay is through 
 Consider the the following transfer function representation for the of Fig . : 
 where x , Assumption . . the of the previous section , we simulate the lower bound on in regarding five different of delay , , , , and for each , over a range of . Fig . the behaviour of the lower bound with respect . Additionally , it the operational when scalar uniform for and . First , as , in is a monotonically decreasing function of . Secondly and more importantly , when see . So greater delay worse best performance . The most significant 
 outcome is associated with the behaviour of in with respect to delay . It can be from Fig . that for a fixed , is increasing in . Therefore , a delay in the channel an increase in the data rate to achieve a quadratic level of performance . The greater delay , the higher rate to be spent in order to get a certain level of performance . Indeed , this finding the delay free of . Another observation is the convergence of the data to the minimum rate for as . As in Fig . , high are to attain the ideal non performance . Along the of , , we now simply replace the in the independent scheme in Fig . , by a uniform scalar in order to assess the operational performance by a simple scheme . It is interesting to note that the operational average data rate in Fig . is at most around . away from the derived lower bound at all performance . 
 In this paper , rate constrained control noisy , causal but otherwise arbitrary control and digital noiseless communication with time delay , have been studied . For such , a certain level of performance is attainable if and only if the average data rate does not fall below a minimal value . A lower bound on this rate been . Through a numerical example , it been that the channel time delay the average data rate to achieve a level of performance . Moreover , by a simple scalar , operational average data fairly close around . to the lower bound have been . 
 G , x Assumption . . Regarding the of x the fact , it can be from some in 
 in which the variance of is the set of all proper which render the system of Fig . internally stable and well . The considered that finding is feasible . The feasibility of finding u in and in from the feasibility of . 
  
 ﻿ In this paper we review the previously for successive compute and forward and discuss about their incompleteness through some . Then we present a comprehensive formulation for step successive interference cancellation problem through the asymmetric successive forward strategy . It is shown that the generalized formulation the previous . 
 I . INTRODUCTION 
 Interference management in relay is one of the most important in wireless . Among the various for relay forward , compress and forward , amplify and forward , and compute and forward see and therein , the later one interesting attention for the problem of interference in noisy relay , especially when it is more helpful to decode linear of rather than individually them . In , several have been for integer non integer linear of in the with equal unequal power , and with symmetric asymmetric channel gains . 
 In , a compute and forward strategy been for relay with equal power and asymmetric channel gains based on lattice . The receiver given sufficiently many linear , can decode integer linear of and solve for its desired . To obtain higher , can recover those linear which are closer to the channel fading . This strategy simultaneously protection against noise and the opportunity to exploit interference for gains . On the other hand , in , a scheme been for relay with unequal power and symmetric channel gains based on lattice and compute an achievable cast rate within a constant gap to the capacity . There is one point here : in , the assumed equal power and stated that unequal power can be incorporated by scaling the channel appropriately . Here with an example we show that , when one does this , the of a smaller rate region than that found in . 
 In successive strategy been for the relay network with the same assumption as in . In this method after a linear combination of , the receiver can combine it with its channel observation to obtain a new effective channel which is better for the next targeted linear combination . 
 In an asymmetric been for with unequal power and asymmetric channel gains based on lattice . In , relay with unequal power and asymmetric channel gains been considered as a special case of the model in . They also consider successful recovery of the individual at the final destination in addition to the relay recovery . 
 In and a asymmetric been . This method scaling to decode integer linear of . The use of scaling in fact is equivalent to non integer linear of lattice and it different to have different , so by appropriately those , different on the boundary of the rate region can be . Also , in the idea of non integer linear of lattice been . Their method can be considered as a special case of the of and as well . 
 In this correspondence we first give a brief review of the previous works about . Moreover we discuss about their incompleteness of some through some . Then we present our generalized formulation for the successive interference cancellation problem in asymmetric relay through the asymmetric successive forward strategy . More specifically , we extend the method of to step and also we take advantage of scaling to reach to a comprehensive formulation for the asymmetric strategy . It is shown that the generalized formulation the previous . 
 . 
 The paper is organized as : In Section the system model is defined . In Section the of are and . In Section our main , the extended asymmetric successive compute approach for general relay is . Some concluding are provided in Section 
 V . 
 Through the paper we use the following : the 
 set of the real up to the nearest integer value . are shown with . 
 . THE SYSTEM MODEL 
 In this paper we consider a real relay network . It is the same network as been used in , but here we consider the assumption of unequal power and asymmetric channel gains . Each relay in the network , indexed by ;, a noisy linear combination of the through 
 the channel , 
 L 
 y , 
  
 where are the channel gains and is i . i .. noise , , In . Let denote the vector of channel gains from the relay . For all , each channel input length sequence subject to the average power constraint , i . e . 
 The channel gains are assumed to be known and constant . In this document we only consider communication between the transmitter and the . 
 . REVIEW OF PREVIOUS WORKS 
 In this section we review and compare the works of , and discuss about their . 
 A . Review of and 
 In and , achievable rate have been for real valued based on the successive compute and forward strategy . The combination of those is in the following theorem . 
 Theorem . For the defined system model with equal equation gain am , , where non zero am am , , with ;, the following computation are achievable 
  
 for user , where 
 , 
 N am , 
 As in , the interpretation of is that the relay can first target a linear combination , am , that is easy to decode and then use it to create a better effective channel for the second linear combination 
 B . Review of 
 In , a class of real with one receiver power P and symmetric channel gains been considered . Channel gains have been assumed to be one . An achievable rate region been based on the compute and forward strategy . It is in the following theorem , 
 Theorem . Any rate less than 
 , 
 is achievable for user , with bounded error probability which goes to zero infinity . 
 C . Comparison of and 
 In equal power have been to the in the network . It been declared in that in the case of unequal power , channel gains can be scaled in order to get equal power . Next by the aid of two , we show that the that can be by the method of with the scaling of channel gains are smaller than those by the method of . 
 Example : Consider a network with , , unequal power constraint P P , and channel gains h h . Theorem the following achievable rate for : 
  
 If we do channel gain scaling as , we get to equal power constraint P for both . Substituting these along with a which 
 N am in Theorem 
  
 is than , e .., assume i in , then from the assumption P P , it is that . 
 With another example , we show that the rate region of with equal power and asymmetric channel gains , is smaller than the one from by scaling the power accordingly while leaving unit channel gains . 
 Example : Consider a network with , , P P , and h h . Substituting them in Theorem along with a , 
 . 
 If we do scaling in order to obtain symmetric channel gains equal to , the scaled power will be P h and P h . With this , Theorem the following rate for user i , 
  
 since h h h subject to h h . So as before , it is seen that in a higher rate R in comparison with 
 . 
 Thus , the method of , scaling channel gains with power , smaller rate region than that in in the case of unequal power . 
 In and , an asymmetric compute and forward is which the problem by appropriately fine and course for each user . Also in , by the aid of scaling , the equation been which can reach to the rate of for the case of unequal power . 
 D . Review of and 
 In a network been considered which a receiver with which an dimensional channel , the channel gain between th transmitter and the receiver , is i . i .. white noise with distribution , , to the power constraint . Let h and P . The theorem is as , 
 Theorem . For the defined system model and equation coefficient am , with ; where am am , the following computation rate is 
 ;, 
  
 In , a special form of been for the 
 relay network model as considered in as , 
 , 
 for user , where 
 , 
 vector of power of the as P , P . 
 In Section we state the extension of to step asymmetric successive compute and forward by the same method as used in and also we consider scaling as done in and . 
 E . Review of and 
 In and by real scaling and the resulting non integer linear combination of , one additional flexibility for the individual message of different . Corresponding of interest from and can be summed up in the following theorem , 
 Theorem . Let real . For the defined system model in Section with equal power equation coefficient am , where am am , the computation rate is achievable if 
  
 for all where a : . 
 The main idea behind this result on the observation that the lattice do not have to lie in the lattice which is used for at the . 
 As an example in , the two way relay channel been considered where two have unequal power P and P and the channel gain vector to the relay is unity . The relay power constraint . All are with unit variance . They obtain the following achievable rate region for the , where the relay is assumed to decode a linear combination of user of the form where , 
  
 where for any positive , , : is equal to , 
  
 Remark : and can be by substituting 
 , , and for all in . As it is seen , the previously problem for unequal power is here by the aid of , but it is not obviously a straightforward substitution of channel vector and in the equation of the rate region . In the next section we propose a straightforward formulation which this problem . 
 In , the step successive interference cancellation strategy been for more than one linear combination of in multiple access . The method is based on the matrix algebra consider general equation gain which is different from the method of which is based on successive interference cancellation and consider orthogonal equation gain . 
 F . Review of 
 In a network with relay and been considered . They used the idea the non integer linear combination of instead of scaling and they have critical on the rate region where the rate of one user the cut set bound . For the method of is actually a special case of and by considering . 
 . ASYMMETRIC SUCCESSIVE COMPUTE AND FORWARD 
 In this section we propose a straightforward and comprehensive equation for step asymmetric successive compute and forward strategy . We assume unequal power and asymmetric channel gains . Our main result for step asymmetric successive compute and forward strategy is in the following theorem : 
 Theorem . For the system model with unequal power constraint P , consider non zero equation coefficient ami , where ami ami , ami , and i ;, and let real . The computation rate R is achievable where 
  
 for all is equal to 
 , 
 for all i , and 
  
 used in , , and . See Appendix A . 
 Corollary . For , i . e ., single step of integer linear combination of , the rate of 
 to 
  
 for all . 
 Remark : For P P ... , we , and 
 . Substituting these in . 
 Remark : One can see that by scaling channel gains with power , i . e ., substituting with , and also substituting with in , is . But in spite of the formal equivalence of and in the case of unequal power , there is a difference between the conceptual basis of these . As in the proof of Theorem in Appendix , in the strategy of , the of different have equal independent of what the of are , but in the strategy of , the transmit their own real power . 
 Corollary . For , i . e ., step interference cancelation , the rate of to 
  
 where 
 , N am , am 
 . 
 Remark : Equal power and asymmetric channel By considering equal power P P , to , and also by considering for all , and am , 
 to , and . 
 Remark : Unequal power and symmetric channel Assume for all , and a I , then . 
 Substituting them in and . 
 As it is seen from the above , our formulation is capable of both unequal power and asymmetric channel without the aid of scaling . 
 In the following corollary we obtain a more reduced form for and when and , e .., the network with one relay and less than three . 
 Corollary . For the network with , , the rate of Theorem for step successive interference cancelation , i . e . 
 N , to , 
  
 for where 
 . 
 Proof : See Appendix . 
 Remark : For , P P , i . e ., equal power , a a , a and a b , b , and to , . 
 V . CONCLUSION 
 In this paper we studied the asymmetric step successive compute and forward scheme for interference management in relay with unequal power and asymmetric channel gains . We first the previous for this scheme and about their through some . Then we a comprehensive formulation for asymmetric successive compute and forward scheme by taking advantage of all the of the previous works together . It is shown that the approach the previously . 
 APPENDIX A PROOF OF THEOREM 
 Proof : Let ,..., which are simultaneously good and the lattice among them . The equivalent noise variance at the , which will be defined later in the proof , determine the order of this lattice chain . noise variance to a lattice . Additionally , are such that which are simultaneously good with second , where is a positive number . At each transmitter , the is , where the region of , and the number of the relay which in lower achievable among the which should decode the message of transmitter . The transmitter , 
 x , 
 where is a random vector uniformly distributed in which is as a dither . is independent uniformly distributed in , so it average all 
 some real number am , the 
 L 
 y 
  
 where 
 , 
 is independent of m . 
 The receiver to retrieve from ym instead of , for all . Note that there is a one to one between , so 
 is equivalent to . For , the receiver consider lattice which the lattice point ym in in which , i . e ., 
 . The probability of error is given by 
 , 
 so the lattice of transmitter , is successful following relation , 
 , 
 for all , where the variance of the effective noise m and it is as : 
  
 where 
  
 is chosen to minimize via the following computation , 
  
 am 
 Substituting in , 
  
 Substituting in for . 
 The Nth step Successive Interference Cancellation : 
 In the following , we obtain the for the successive of other possible integer of . To decode the integer sum with coefficient vector ami , where i ;, the relay the projection of onto ym from ym as : 
  
 where 
 . 
 Then the relay a new effective channel observation 
 L i 
 y 
 , 
 where 
 . 
 At last the relay the linear combination with coefficient vector ami by 
 . 
 By the same as shown in and substituting with with , and am with ami , it can be shown that the following rate is achievable for all 
 , 
 where , is equal to 
  
 The optimum for to get the global minimum are as , 
 . 
 Plugging back these in and . 
 OF COROLLARY 
 First we introduce the following lemma which will be used in the proof of Corollary . 
 Lemma . The following identity is satisfied for arbitrary dimensional real a a , a an , b , b and c , c , with . 
 Proof : We prove the equivalent form of which is , 
  
 By substituting the of the a ,, the right hand side of , we have 
 The number of in 
 is n . The number of in 
  
 i . The number of 
 . So the total number of in the first and second line of are n n n which is equal to and is zero for . 
 The number of in 
 is n , the number of in i i is n n when i is not satisfied , so the number of in is n n n which is equal to zero for . 
 Proof of Corollary : 
 Proof : We must show that and to . It is easy to see that is exactly the same as the first term of . Now we simplify to obtain the second term of 
 . 
 According to Lemma , to , 
 . 
 Substituting in the second term of . 
  
  
 ﻿ We characterize the rate distortion function for zero mean stationary under the fidelity criterion and subject to the additional constraint that the distortion is uncorrelated to the input . The solution is given by two coupled through a single scalar parameter . This a structure similar to the well known water filling solution without the uncorrelated distortion restriction . Our fully characterize the unique statistics of the optimal distortion . We also show that , for all positive , the minimum achievable rate subject to the constraint is strictly than that given by the un constrained rate distortion function . This gap with the distortion and to infinity and zero , respectively , as the distortion to zero and infinity . 
 Introduction 
 Many source have the property that the end to end reconstruction error is uncorrelated with the source . We refer to such as uncorrelated distortion . As an example , consider a typical transform coder , as in Fig . . Here , a random vector is first by an analysis transform to yield U . Then U is , yielding the vector , U . The input signal is finally by , where is the synthesis transform , . If the quantization error is 
 T Y 
  
 T 
 X U 
 Figure : Transform coder . 
 uncorrelated to U , and if I , then it is easy to show uncorrelated to , thus yielding a coder . 
 More generally , any quantization scheme satisfying the following two a coder : a The error by the is uncorrelated to its input ; The linear if any before and after the perfect reconstruction in the absence of quantization . Property a is satisfied in many , e .. in high resolution or when a with dither either subtractive or non subtractive is employed . On the other hand , the condition Property is often sometimes implicitly in the design of filter , transform , , and feedback , . Thus , any source coder , for example , quantization , is a coder . The rate distortion performance of any coder can be to the underlying rate distortion function of the source , for a given distortion metric . One may question whether such a comparison is , in fact , fair . After all , the additional constraint that the end to end distortion is uncorrelated with the source is not upon . With this in mind , let denote the rate distortion function with the additional constraint that the end to end distortion is uncorrelated to the source . A formal definition of is given in Section . Clearly ,. However , to the best of the knowledge , the problem of not been formally before . Therefore , such as in which if any , and how can be , appear to be unanswered . 
 In this paper , we not only give conclusive to the above , but more importantly , we completely characterize for the quadratic case as a lower bound for the rate achievable under the uncorrelated distortion constraint . We show , in Section , that can be through a single scalar variable a . This is a result which the conventional water filling that describe . We characterize the unique optimal statistics that the reconstruction to have in order to achieve , for a given source . In particular , we show be . In addition , we recast the in a transform sense . More precisely , we show that if the quantization are , independent both mutually and from the source , then the Transform is optimal among all perfect reconstruction , at all . A comparative analysis between and is then in Section . There we show that is convex and monotonically decreasing in , and that , , converging in the limit as . 
 Furthermore , we show that , which is different from the well known result . 
 It is worth that our are not tied to any particular source architecture , but are general in the sense that any scheme in which the end to end distortion is uncorrelated with the source can do no better than . 
 Notation We use to represent random , a subscript when to one of its , i . e ., Xi is the i th element of the random vector . The expectation operator is by E . bold are used for matrices . The positive definite square root of a positive definite by . We write and for the determinant and the trace of a matrix , respectively . The probability density function 
 and covariance matrix of a random column respectively by and , where is the transpose of . We write , for the cross covariance matrix between two . The spectrum of a ... random function is by , 
 ,. The differential entropy and the differential entropy per dimension of an length random , respectively , by . a random process , the differential entropy rate of . 
 We use to refer , respectively , to the mutual information and the mutual information per dimension between two . random , the mutual information rate . We write a . e . for almost everywhere . 
 Rate Distortion Function with Uncorrelated Distortion 
 We begin by the definition of the quadratic rate distortion function under the constraint that the end to end distortion be uncorrelated with the source . Then , in Section . , we characterize this function for random , the case of stationary to Section . . 
 Definition . The uncorrelated quadratic rate distortion function for a random vector source is defined as 
 R ;, 
 Y : , , 
 an length random vector . 
 R for Random Vector 
 We now present one of the main of this paper , namely that , for vector , is given by two linked through a single scalar parameter . This the water filling that describe . The proof of this result , which is in Theorem , use of the following lemma . 
 Lemma . Let Let and be two random with zero mean and the same covariance matrix , i . e ., , and the same matrix with respect to , that is If jointly , and distribution , then 
 I ; I ; . 
 If furthermore , then equality is in , jointly . 
 Proof . Define . Then 
 I ; I ; 
 y y 
  
  
 where is the relative entropy or distance between the two probability . The equality a from the fact that log is a quadratic form , and from the fact that , The inequality from the 
 fact that , with equality . Thus , equality is y 
 . The proof is by that Z 
 for all . 
 Remark . We note that the above Lemma Lemma . in , by the requirement that and , to the requirement , 
 We are now in a position to present the main result of this section : 
 Theorem . Let the source be a zero mean random vector with positive definite covariance matrix Then 
 For any positive , 
 , 
 where the scalar parameter a is such that 
 . 
 For each , the value of a that is unique . 
 Let . Then I ; 
  
 K , 
 and where a . 
 Proof . Let U denote the set of all length random uncorrelated to , and define the 
 U as 
  
 With the above , can be written as where a directly from Lemma and where since the definition of see , that both and exist . 
 We now prove , by contradiction , that the minimizer of log I in , namely , is such that . For this purpose , suppose that , and let be the of be a random vector with covariance matrix 
 K . We then have that , and that 
 a strictly increasing function . Thus cannot be a minimizer of unless . 
 The minimizer of log subject to can be found a variational approach . More precisely , the covariance matrix of the minimizer must necessarily be such that the derivative of the 
  
 with respect to is zero at , for some , which is equivalent to the condition that the matrix differential , . the fact that log , for any positive definite matrix , the necessary condition be a minimizer the form 
 Z , 
 K Z , 
 Z . 
  
 The fact that needs to be positive definite that and that it is infeasible to have a negative sign before the square root in . This , together with the change of variable , directly to , with a . On the other hand , the value of a must be such that the equality constraint is satisfied . From , and Lemma see appendix , this requirement is equivalent to , which . 
 Similarly , is by substituting into and then Lemma , which : 
 . 
 The uniqueness of a is easily by that the right hand side of is monotonically increasing with a . Since a is unique , it from that the covariance matrix of I ; is unique , the proof . 
 Transform Realization of : Closer examination of Lemma , when used in , that , for a source , can be by the transform architecture shown in Fig . . More precisely , an end to end distortion the optimal covariance matrix given by is by choosing the , where ,..., i . e ., the transform for , and by a random vector of quantization E with Interestingly , here 
 E is not a scaled identity matrix , as is usually the case in transform . This discrepancy from the approximation E cE , commonly used to link the variance of to the bit rate at which each th transform coefficient is . In this expression , is a constant that on the of the source and on the type of . The well known optimal bit allocation , e .., in , is based upon this formula , and thus the total bit rate . On the other hand , the optimal quantization by Theorem need to be , their being such that the end to end mutual information is . Thus , the difference in the optimal for in each case is due to the fact that . 
 R for Stationary Random 
 The function defined in can be extended to random as : 
 Definition . The uncorrelated quadratic rate distortion function for a random defined as 
 R min I ;, 
 Y : E , limN N , limN 
 a random process . 
 The function for stationary random can be derived from the in Section . , by to random a covariance matrix , and then . More precisely , we have the following result : 
 Theorem . Let the a stationary random process with spectrum such that , a . e . on ,. Then 
 For any 
  
 where the scalar parameter a is such that 
  
  
 For each , the value of a that is unique . 
 E , N limN , limN N . 
 Then is a stationary random process with spectrum 
 , a . e . on ,. 
 Proof . Define , from the random 
 Y YN ,. It is known that ,, where and are 
 the of and , respectively see e .. , Theorem . . . This result , together with Lemma , in the Appendix , and the fact that , a . e . on ,, that , for all . We can then apply Theorem to each , , 
 N 
  
 R N log a vaN , , 
 k 
 where a 
  
 D a , , 
 N 
 and where the set of of . From , the optimal distortion covariance matrix for is 
 K . 
 Direct application of Lemma see the Appendix to and 
  
 wherein is the only scalar that 
 . 
 Similarly , Lemma to we obtain . This the proof . 
 Remark . It is interesting to note that the the optimal feedback derived in achieve an end to end distortion whose spectrum is given precisely by . Furthermore , it is easy to show that such would achieve the function if the noise due to the scalar quantization within the feedback loop were white noise uncorrelated with the input . 
 Comparison with 
 The next theorem that strict and convexity with , but from in the asymptotic limit of large . 
 Theorem . For any random vector stationary random process with positive definite covariance matrix monotonically decreasing and convex . In addition ,. 
 Proof . We present here only the proof for the case of random . The proof for 
 stationary proceeds in an analogous fashion . : We have that 
 , provided that and exist and that the latter derivative is non zero . From , we obtain 
 . 
 On the other hand , from , 
 , 
 and thus 
 , 
 proving that is a strictly decreasing function since a . Convexity : The fact that a monotonically with increasing , together with , imply that D D , and thus is convex . : It is clear from that and . Since , as can be seen from , monotonically with increasing a , a , , it that a , and that a . 
 Similarly , it from and the respect to a that , for fixed , a and a . We then have that and 
 D , the proof . 
 We next show for all , converging asymptotically as . 
 Theorem . For any random vector stationary random process with positive definite covariance matrix with , a . e . on ,, the following 
 i , ; . 
 Proof . We present here only the proof for the case of random . The proof for stationary proceeds in an analogous fashion . Recall that for a random covariance matrix with one that , with equality . As a consequence , 
  
 Equality a is by substituting and into the right hand side of . The validity of then directly by taking the limit of the right hand side of as a . In order to prove i , we will show that , where the function is the inverse of and is distortion rate function . For this purpose , consider the random vector Y , where is a symmetric , positive definite matrix , as in Theorem . Notice that are not uncorrelated unless I . The mutual information per dimension between and 
 h WY 
 positive definite . We next show that for any and corresponding , choosing an optimal distortion is strictly smaller than 
 D . It is easy to show that is by choosing , where W 
 is the Wiener filter matrix for . From this equation , we define the function 
 , the distortion associated with , with the covariance matrix in when . Since , we obtain , from Lemma , and after some algebraic manipulation , that 
 , 
 where the last inequality from the fact that . Finally , the fact that both and are monotonically decreasing , together with , i . This the proof . 
 The summation term in exactly the rate loss for all 
 . this range , it can be shown , sum inequality , 
 that a , which that monotonically with increasing . On the other hand , for all , the ratio can be lower bounded by . It can be shown that this bound with a and thus well , tending to as a , which is in agreement with Theorem . 
 Concluding 
 In this work we have completely , the quadratic rate distortion function subject to the constraint that the end to end distortion be uncorrelated with the source . We have further proved that this function convexity and with rate distortion function , but is positively bounded away from the latter , converging to only in the limit as the distortion to zero . We that the constraint the distortion to unboundedly grow as the rate to zero . We also the of for random and stationary random through transform and feedback quantization . 
 Appendix 
 Lemma from Corollary . . in . Let 
 A , 
 with , and where :, and , denote the th column and the th row 
 Q , respectively . If is analytic in a around each , for ,...,, then 
 f A . 
 Lemma Theorem . . in . Let A be an infinite matrix with entry ak on the th diagonal . Then the of A are in the interval , the essential and , respectively , of the function . Moreover , if finite and is any continuous function of 
 ,, then 
  
 where the are the of the sub matrix A of A centered about the main diagonal of A . 
  
 ﻿By focusing on a class of source coding schemes built around entropy coded dithered quantizers, we develop a framework to deal with average data-rate constraints in a tractable manner that combines ideas from both information and control theories. We focus on a situation where a noisy linear system has been designed assuming transparent feedback and, due to implementation constraints, a source coding scheme has to be deployed in the feedback path. We give a closed form expression for the minimal average data-rate required to achieve a given performance level, and also study the interplay between stability and average data-rates for the considered architecture. 
 Consider the networked control system (NCS) of Figure 1, where P is a given proper real rational transfer function, d is an exogenous signal, e is a signal related to closed loop performance, y can be measured, u is a manipulable input, and the channel is a noiseless digital channel. Partition P as 
 where P22 is SISO and strictly proper, and both P12 and P21 are non-zero. The initial state xo of P is a second order random variable and d is a second order wss process with PSD Sd > 0. 
 If no constraints but causality are imposed on the source encoder and decoder, then mild conditions guarantee that the NCS of Figure 1 is mean square stable (see definition in Section III) if and only if the average data-rate R of the coding scheme (i.e., across the channel) satisfies [1] 
 where pi is the ith unstable pole of P. When performance guarantees are sought, then much less is known. A causal rate-distortion inspired approach was followed in [2], but no achievable rate regions where established there. 
 Theorem 1: Consider a causal source coding scheme inside a feedback loop, as in Figure 1. Under suitable assumptions (see [3]), R = I8(y ? u), where I8(y ? u) denotes the directed mutual information rate defined by Massey [5]. 
 Definition 1: The source coding scheme of Figure 1 is said to be independent iff the assumptions of Theorem 1 hold and the noise sequenceobeys n = Oq, where q is a second order zero-mean i.i.d. sequence, q is independent of (d,xo), and O is a stable and stably and causally invertible filter with deterministic initial state (see Figure 2(a)). 
 Any independent source coding scheme can be written as shown in Figure 2(b), where v and w are auxiliary signals, q is as in Definition 1, and A and F are auxiliary stable filters, and both A and 1-F are stably and causally invertible. Moreover: Theorem 2: If an independent coding schemes is written as in Figure 2(b), then I8(y ? u) = I8(v ? w). 
 The previous fact allows one to focus on I8(v ? w) instead of I8(y ? u). It is possible to establish upper bounds on I8(v ? w) that depend only on second order properties of w and v: 
 Theorem 3: If in the NCS of Figure 1 the source coding scheme is independent, then1 
 where Sw is the stationary PSD of  is the stationary variance of v, and  is the variance of q. Equality holds in the first inequality iff (d,xo,q) is Gaussian, whereas equality holds in the second inequality of iff  is constant a.e. 
 An additional key fact of independent source coding schemes is stated next (see also [7]): 
 Theorem 4: For any given independent source coding scheme, there exists another independent source coding scheme, with the same noise color O and the same directed mutual information rate across it, such that the gap between the left and right hand sides of the second inequality of Theorem 3 can be made arbitrarily small. 
 Theorem 5: Consider the system of Figure 2(b) with A and F as before. If an ECDQ is used as the link between v and w, ? < 8 and the dither dh is i.i.d., independent of (xo,d), and uniformly distributed on (-?/2,?/2), then the system of Figure 2(b) becomes an independent source coding scheme and the entropy coder inside the ECDQ can be chosen so that 
 If the entropy coder inside the ECDQ is memoryless, then it can be chosen so that 
 Definition 2: A system x(k + 1) = Mxx(k) + Mww(k), k ? N0, x(k) ? Rn, x(0) = xo, where xo is a second order random variable, Mx and Mw are constant matrices, and w is a second order wss process is said to be mean square stable (MSS) iff there exist finite µ ? Rn and a finite and positive semi-definite M ? Rn×n such that limk?8 E {x(k)} = µ, 
 The previous definition applies to the NCS of Figure 1, when the source coding scheme is independent. 
 Theorem 6: Consider the setup and assumptions of Theorem 5. Then, irrespective of whether the entropy coder inside 
 the ECDQ has memory or not, the minimal average data-rate compatible with MSS, say RMSS, satisfies (cf. (1)) 
 Theorem 7: Consider the setup and assumptions of Theorem 5. If Dinf < D < 8, then, irrespective of whether the entropy coder inside the ECDQ has memory or not, the minimal average data-rate that allows one to attain a performance level se2 = D, say RD, obeys 
 The bound on RD provided by Theorem 6 is, by construction, achievable. To our knowledge, our result corresponds to the first closed form bound on the achievable average data-rate needed to attain a given performance level. 
 We note that, if (xo,d) is Gaussian, then the bound on RD given in (2) is tight up to   nats/sample. 
 ﻿ By on a class of source built around entropy , we develop a framework to deal with average data rate in a tractable manner that from both information and control . We focus on a situation where a noisy linear system been designed assuming transparent feedback and , due to implementation , a source scheme to be in the feedback path . We give a closed form expression for the minimal average data rate to achieve a given performance level , and also study the interplay between stability and average data for the considered architecture . 
 Consider the control system of Figure , a given proper real rational transfer function , is an exogenous signal , e is a signal related to closed loop performance , can be measured , u is a manipulable input , and the channel is a noiseless digital channel . 
 where P is and strictly proper , and both P and P are non zero . The initial state a second order random variable a second order process with . 
 If no but causality are on the source and , then mild guarantee that the of Figure is mean square stable see definition in Section if and only if the average data the scheme i . e ., across the channel 
 where pi is the unstable pole of . When performance are sought , then much less is known . A causal rate distortion inspired approach was in , but no achievable rate where established there . 
 Theorem : Consider a causal source scheme inside a feedback loop , as in Figure . Under suitable see , I u , where I u the directed mutual information rate defined by . 
 Definition : The source scheme of Figure is said to be independent the of Theorem hold and the noise , a second order zero mean i . i .. sequence , is independent of and O is a stable and stably and causally invertible filter with deterministic initial state see Figure a . 
 Any independent source scheme can be written as shown in Figure , auxiliary , is as in Definition , and A auxiliary stable , and both A and are stably and causally invertible . Moreover : Theorem : If an independent is written as in Figure , then I u I . 
 The previous fact one to focus on I instead of I u . It is possible to establish upper on I that depend only on second order : 
 Theorem : If in the of Figure the source scheme is independent , then 
 where is the stationary of is the stationary variance of , and is the variance of . Equality in the first inequality is , whereas equality in the second inequality of is constant a . e . 
 An additional key fact of independent source is stated next see also : 
 Theorem : For any given independent source scheme , there another independent source scheme , with the same noise color O and the same directed mutual information rate across it , such that the gap between the left and right hand sides of the second inequality of Theorem can be made arbitrarily small . 
 Theorem : Consider the system of Figure with A before . If an is used as the link , and the dither is i . i .., independent of ,, and uniformly distributed on , , then the system of Figure becomes an independent source scheme and the entropy coder inside the can be chosen so that 
 If the entropy coder inside the is memoryless , then it can be chosen so that 
 Definition : A system , N , , , where is a second order random variable , and are constant matrices , a second order process is said to be mean square stable there exist finite and a finite and positive semi definite such that E , 
 The previous definition to the of Figure , when the source scheme is independent . 
 Theorem : Consider the setup and of Theorem . Then , irrespective of whether the entropy coder inside 
 the memory or not , the minimal average data rate compatible with , say , . 
 Theorem : Consider the setup and of Theorem . If , then , irrespective of whether the entropy coder inside the memory or not , the minimal average data rate that one to attain a performance level se , say , 
 The bound on provided by Theorem is , by construction , achievable . To our knowledge , our result to the first closed form bound on the achievable average data rate to attain a given performance level . 
 We note that , if , is , then the bound on given in is tight up to sample . 
  
 ﻿This paper presents novel results on the optimal design of Noise-Shaping Differential Pulse-Coded Modulation coders. The main contribution resides in the derivation of explicit analytic formulas for the optimal filters and the minimum achievable frequency weighted reconstruction error. A novel aspect in the analysis is the fact that we account for fed-back quantization noise and that we make no restrictions on the order of the filters deployed.
 I. INTRODUCTION
 Analog-to-Digital converters which utilize a scalar quantizer and linear, time invariant filters in a feedback loop have been extensively employed as a source coding method since the concept was first introduced in the ’s. The generalized form of this architecture, which we denote Noise Shaping Differential Pulse Code Modulation  NS-DPCM, can be represented as in Fig. . The filters in a NS-DPCM system allow one to account for the correlation between consecutive input samples, and to spectrally shape the quantization noise in the output, so as to minimize the frequency weighted mean square reconstruction error FWMSE. Special cases of the NS-DPCM architecture include ?-Modulators, DPCM converters [], and noise-shaping converters, such as one and multi-bit Sigma-Delta modulators []. NS-DPCM converters are extensively used in the context of audio compression [], digital image half-toning [] and oversampled A/D conversion [].
 Provided that the input power spectral density PSD, frequency weighting error criterion, and scalar quantizer characteristics are known, the design of an NS-DPCM converter that achieves minimum FWMSE amounts to finding the corresponding optimal filters. This has been an intense area of research for at least  years. However, available to date results on optimal filter design for NS-DPCM encoders have been obtained assuming either fixed, finite order filters [], [], []– [], negligible fed back quantization noise [], [], or have relied upon heuristic design methods [], []. Since optimal performance can, in general, only be attained by arbitrary order filters designed accounting for fed back quantization noise, an exact characterization of the optimal performance and filters for NS-DPCM converters has remained an open problem. In this paper we derive an explicit analytic expression for the optimal performance and filter frequency responses for NS-DPCM converters. We characterize the scalar quantizer via its signal-to-noise ratio, and adopt a white quantization noise model []. The performance bound obtained corresponds to the minimum FWMSE that can be achieved by an NS-DPCM encoder-decoder with any linear, time-invariant filters. A key departure from [] which, to the best of our knowledge, gives the only currently available explicit analytic solutions to the problem, is that we account for fed back quantization noise. This allows us to derive exact expressions.
 Our results show that an optimal NS-DPCM converter exhibits several interesting properties. These include a spectrally flat frequency weighted error spectrum, and a white signal at the input of the scalar quantizer. We also show that, for AR Gaussian sources, the rate-distortion efficiency with the optimal filters depends only on how efficient the embedded scalar quantizer is at quantizing nearly Gaussian samples.
 Notation and Preliminaries
 We use standard vector space notation for signals. For example, x is used to denote {xk}k?Z. We also use z as the argument of the z-transform. Given two square integrable complex valued functions f? and g? defined over [-p,p], we adopt the inner product where * denotes complex conjugation. We denote the usual
 -norm as  is a transfer function, then we use the short hand notation F to refer to the associated frequency response Fej?, ? ? [-p,p]. If I is a set, then we will write “a.e. on I” almost everywhere on I as a short hand notation for “everywhere on I except at most on a zero Lebesgue measure set of points”.
 We use sx to denote the variance of a given wide sense stationary w.s.s. random process x, having PSD Sxej?. Note that, where
 v
 ?x is a frequency response satisfying |?x| , Sx , ?? ?
 [-p,p]. For a given function f : [-p,p] ? C, we define ?f ,
 provided this integral converges. This
 allows one to describe the Kolmogorov’s minimal prediction error variance [] of a w.s.s. process x via ?x , ?Sx = ??x. The spectral flatness measure of a w.s.s. process x is denoted . It is easy to show that  = ?x = , and that ?x =  if and only if Sxej? is constant a.e. on [-p,p].
 II. NS-DPCM MODEL
 As foreshadowed in the introduction, we consider the general form of an NS-DPCM architecture shown in Figure . In our model, the input sequence x is assumed to be a zero mean, w.s.s. random process, with known PSD Sx = |?x| satisfying Sxej? > ,a.e. on [-p,p]. The element denoted by Q describes a scalar quantizer, with given and known characteristics. For each input vk, k ? Z, it outputs wk and generates the quantization error nk , wk-vk. The three discrete-time filters Az, Bz and Fz in Fig.  are design choices.
 To asses performance, we introduce the delay-compensated frequency weighted error
 ² , Pzx˜ - z-tx,  where t = . The error weighting filter Pz models the impact that reconstruction errors have on each frequency. Thus, it is application dependent.
 ¯ In this paper, we restrict attention to the cases in which¯ ¯Pej?¯ > , ?? ? [-p,p], i.e., Pz has no zeros on the unit circle. Additionally, we require:
 Constraint : Az,Bz, Fz and Pz are stable. In addition, Fz is strictly causal i.e., limz?8 Fz = .
 The first part in the above constraint is required in order to avoid unbounded signals in the NS-DPCM converter. The additional requirement on Fz is needed for the feedback loop in Fig.  to be well defined see, e.g., [, Chap. ].
 Since the NS-DPCM architecture embeds a nonlinear element a scalar quantizer within a feedback loop, exact analysis of quantization errors is, in general, a formidable task []. This has motivated the widespread use of an additive noise model for quantization errors []–[], []–[]. This model allows one to study the converter via linear analysis tools. It is usually formulated as follows:
 Assumption : The quantization errors are i.i.d. random variables, uncorrelated with the input signal.
 In order not to limit our subsequent analysis to a specific type of scalar quantizer, the following is also assumed:
 Assumption :	The probability density function PDF of v is not affected by the filters in the converter other than via its second moment  .
 Under Assumption , any given type of scalar quantizer with a fixed number of quantization levels leads to quantization errors whose variance is proportional to the variance of its input. This can be stated as
 ,
 where ? is the signal-to-noise ratio of the scalar quantizer not to be confused with that of the NS-DPCM encoder-decoder system. ? depends on the number of quantization levels, the PDF of the signal being quantized and the companding characteristics of the scalar quantizer itself .
 III. FORMULATION OF THE OPTIMIZATION PROBLEM
 Our ultimate goal is to find the frequency responses of the filters A, B, and F that minimize the variance of ² under Assumptions  and , and for given and known ?x, P and ?. The quantity s² so obtained will constitute the achievable lower bound on the FWMSE for the NS-DPCM converter.
 Towards the above goal, we first derive an expression that relates the decision variables to the error measure that we wish to minimize. From equation, Assumption , and recalling we have where sn is the variance of the quantization error, and is a delay compensated version of AB, the frequency response from x to x˜. The first term on the right hand side of  corresponds to the variance of the frequency weighted quantization error in ². The second term in  accounts for the frequency weighted linear distortion introduced by the filters in the encoder-decoder pair .
 The variance is related to via . From Assumption , the latter is given by . Combining this result with  gives
 When substituted into , this yields
 The above expression relates the filters Az,Bz,Fz, and the quantizer signal-to-noise ratio ?, to the FWMSE. Minimization of this cost functional will yield expressions for the optimal filters and performance.
 For comparison, we note that the cost functional , together with Assumptions  and , is also part of the analysis in [], [] and [], wherein equivalent optimization problems are addressed  .
 Finally, we note that, since must be positive,  implies
 Constraint :	kFk < ?.	 IV. OPTIMAL NS-DPCM
 In this section we derive explicit analytic expressions for the optimal filters and the associated optimal performance for the NS-DPCM scheme, subject to a mild restriction. The analysis is based on a set of equations that the optimal filters must necessarily satisfy. To facilitate the flow of ideas, all proofs are given in the Appendix.
 Minimization of  is simplified by noting that, for stable and strictly causal Fz, it holds that kFk = k - Fk - . Substitution of this equality into  yields
 We then have the following result:
 Lemma : For given frequency responses F and W, the
 optimal Az satisfies
 Notice that the cost functional in  involves only two unknown functions, namely W and F. This makes it simpler to work with than the functional in .
 The optimization problem can be further simplified by writing the optimal W in terms of | - F|. Unfortunately, the relationship between F and the optimal W, for the general case, can only be stated implicitly, as shown next.
 Lemma :	For a given frequency response F, the optimal
 W satisfies
 
 a.e. on [-p,p].
 Remark : Notice that, from , W is a positive, symmetric and real valued function of ?. It then follows from  that the product of the optimal filters Az, Bz must exhibit linearly decreasing phase.
 In general, the presence of |W| in the inner product on the right hand side of  makes it difficult, if not impossible, to express the optimal W explicitly in terms of F. However, under specific conditions on ?, ?xP and | - F|, an analytical explicit solution to  can be obtained, as follows:
 Lemma : Provided , a.e. on [-p,p], then, for a given frequency response F, the optimal W satisfies , a.e. on [-p,p].
 To summarize our results so far, we have shown that, provided  holds, F determines the optimal W through .
 These two, in turn, determine the optimal A and B via  and , respectively. We can now state the main result of this paper:
 Theorem :If a.e. on [-p,p], then the minimum achievable frequency weighted reconstruction MSE of an NS-DPCM converter is
 This minimum is achieved when the filters F, A and B satisfy:
 is an arbitrary constant.	
 V. DISCUSSION
 The results stated in Theorem  have very interesting consequences. Some of these consequences are discussed below.
 Optimality of Scalar Quantization Without Feedback:It is easy to verify from the results in Theorem  that scalar quantization without feedback is optimal if and only if |?xP| is constant. In particular, it follows from  that if |?xP| = , a.e. on [-p,p], then the optimal NS-DPCM converter reduces to a PCM converter with a fully whitening pre-filter and a post-filter satisfying |A| = ?|?x|- and |B| = |A|- ?/? + .
 Comparison with []: The minimum FWMSE for an NS-DPCM system derived by Noll in [], neglecting fed back quantization noise, is. Perhaps surprisingly, Theorem  shows that the optimal performance is slightly better compare with . Moreover, the corresponding optimal filters AN, BN and FN derived in []
 respectively. Substituting these expressions into  actually yields an FWMSE , where is the spectral flatness
 measure of. It then follows that for any finite ?, and that.
 Total Frequency Weighted Distortion is White: It follows from  and  that, in an optimized NS-DPCM system, the PSDs of frequency weighted quantization noise and linear distortion are, respectively
 From the above, one can see that when the condition of
 Theorem  holds, the noise shaping effected by an optimal NS-DPCM system is not “complete”, i.e., frequency weighted quantization noise is not white. However, the PSD of the total frequency weighted error is white, since
 Relation with the Reverse Water-Filling Paradigm:The parametric Rate-Distortion formula for a Gaussian w.s.s. process and FWMSE as the distortion measure is given by the well known reverse water-filling paradigm see, e.g., [].
 For, it predicts total frequency weighted distortion to be equally distributed over frequency. It also predicts the input signal to appear at the output with PSD
 , i.e., less significant spectral components of x suffer higher attenuation. Interestingly,  is equivalent to Sx |P| = s?², a.e. on [-p,p]. Furthermore, S² is flat, as discussed in c above, and  yields Sx |W| |P| = Sx |P| - s?², in full agreement with the above prediction.
 Output of the Scalar Quantizer is White: It can be seen from a that, unless |?xP| is constant, the optimal A is not a full whitening filter for ?x. Interestingly, however, it is straightforward to verify that the optimal filters in  render a sequence w see Fig.  with flat PSD. More precisely, Sw , |?x| |A| + sn | - F| = ?, where ? is the same arbitrary constant that appears in a. A remarkable implication is that the quantized output of the optimized NS-DPCM converter can be efficiently translated into bits by means of a first-order entropy coder.
 Rate-Distortion Analysis: The rate-distortion efficiency of any source encoding scheme with quadratic error as distortion measure can be established by comparing its
 against the upper bound derived by O’Neal in []. For the case s² = min? Sxej?Pej?, and restricting to Gaussian inputs, this bound [, eq. ] can be written as 
 SNRmaxdB , R - log?x - log?P ,  where R denotes the bit-rate in bits per sample, ?x is the spectral flatness measure of x and ?P is the minimum variance associated with P see Section I. Notice that R is Shannon’s upper bound [] for the SNR in decibels of encoding a Gaussian memoryless source.
 On the other hand, under the conditions of Theorem  and using , the best achievable SNR of an NS-DPCM system is given by:
 In decibels, this ratio is
 By comparing   and , we see that the SNR of the NS-DPCM converter optimized via Theorem  departs from the information-theoretic upper bound  as follows:
 SNRdB-SNRmaxdB = log?+-R ˜ log?-R.
 The difference ?SNR , log?-R for of Gaussian sources has long been known for a variety of scalar memoryless quantizer types see, e.g. [] and the references therein. Assuming v to be Gaussian in the optimized NS-DPCM converter, ?SNR can be approximated by -., -., -. and -. for a uniform quantizer with entropy coding E.C., non-uniform quantizer with E.C., non-uniform quantizer optimized for MSE without E.C., and uniform quantization without E.C. and a loading factor of , respectively  see [].
 VI. CONCLUSIONS This paper has derived explicit analytic expressions for the best achievable performance and optimal filters for noiseshaping DPCM encoders. These expressions, which we believe to be novel, were found by accounting for fed back quantization noise in the optimization. The results presented in this paper simplify the analysis and design of NS-DPCM converters, and provide valuable insight into the trade-offs inherent in linear feedback quantizers.
 VII. APPENDIX
 A. Preliminary Result
 Lemma : Let g ? L be a given function such that g? >
 , ?? ? [-p,p] and e is finite. Then arg min , f?B+
 is the set of non-negative log-integral functions.
 Proof: SinceRln· is a monotonically increasing function,¡R ¢ minimization of fg is equivalent to minimizing ln fg . From Jensen’s inequality and the constraint f ? B+, we obtain
 Equality is obtained in a if and only if f = ?g-,a.e. on [-p,p], for some ? > . Inequality b becomes equality if and only if. This completes the proof.
 This last equation leads directly to , completing the proof.
 Thus,
 Substituting this into  yields . Notice that  guarantees that the denominator on the right hand side of  is strictly positive. The proof is completed by noting that substitution of  into  gives the inequality , thus validating our initial supposition.
 Proof: [Theorem ] Suppose the optimal F is such that  in Lemma  holds. Then, one can substitute  into  to obtain
 . After some algebra, this becomes
 Requiring Fz to be stable and strictly causal from Con-¯ ¯ straint  is equivalent to requiring the function ¯ - Fej?¯ to belong to the set of non-negative log-integral functions defined in , see, e.g. [, Theorem ..] and []. Then, it follows from Lemma  that the optimal | - F| is as in a. Substitution of the latter into  yields . It also follows from a that the inequality in Lemma  is equivalent to the condition required by the theorem. This validates our initial supposition. Notice also that the latter inequality also guarantees that k-Fk < ?+, as required by Condition . Finally, substituting a into ,  and  yields the remaining equalities of . This completes the proof.
 ﻿ This paper novel on the optimal design of Noise Shaping Differential Pulse Modulation . The main contribution in the derivation of explicit analytic for the optimal and the minimum achievable frequency weighted reconstruction error . A novel aspect in the analysis is the fact that we account for fed back quantization noise and that we make no on the order of the . 
 to Digital which utilize a scalar and linear , time invariant in a feedback loop have been extensively employed as a source method since the concept was first in the . The generalized form of this architecture , which we denote Noise Shaping Differential Pulse Code Modulation , can be as in Fig The in a system allow one to account for the correlation between consecutive input , and to spectrally shape the quantization noise in the output , so as to minimize the frequency weighted mean square reconstruction error . Special of the architecture include and noise shaping , such as one and bit Sigma Delta . are extensively used in the context of audio compression , digital image half and A conversion . 
 Provided that the input power spectral density , frequency weighting error criterion , and scalar are known , the design of an converter that minimum to finding the corresponding optimal . This been an intense area of research for at least . However , available to date on optimal filter design for have been assuming either fixed , finite order , negligible fed back quantization noise or have upon heuristic design Since optimal performance can , in general , only be by arbitrary order designed accounting for fed back quantization noise , an exact characterization of the optimal performance and for an open problem . In this paper we derive an explicit analytic expression for the optimal performance and filter frequency for . We characterize the scalar via its signal to noise ratio , and adopt a white quantization noise model . The performance bound to the minimum that can be by an with any linear , time invariant . A key departure from which , to the best of our knowledge , the only currently available explicit analytic to the problem , is that we account for fed back quantization noise . This us to derive exact . 
 Our show that an optimal converter several interesting . These include a spectrally flat frequency weighted error spectrum , and a white signal at the input of the scalar . We also show that , for AR , the rate distortion efficiency with the optimal only on how efficient the scalar is at nearly . 
 We use standard vector space notation for . For example , is used to denote . We also the argument of the transform . Given two square integrable complex valued and defined over ,, we adopt the inner product where complex conjugation . We denote the usual 
 norm as is a transfer function , then we use the short hand refer to the associated frequency response , ,. If I is a set , then we will write a . e . on I almost everywhere on I as a short hand notation for everywhere on I except at most on a zero measure set of . 
 We use to denote the variance of a given wide sense stationary ... random process Note that , where 
 one to describe the minimal prediction error variance of a ... , . The spectral flatness measure of a ... . It is easy to show that , and that if and only if is constant a . e . on ,. 
 As in the introduction , we consider the general form of an architecture shown in Figure . In our model , the input assumed to be a zero mean ,... random process , with known satisfying , a . e . on ,. The element scalar , with given and known . For each input ,, it and the quantization error , . The three discrete time , and in Fig . are design . 
 To performance , we introduce the delay frequency weighted error 
 , , where . The error weighting filter the impact that reconstruction have on each frequency . Thus , it is application dependent . 
 In this paper , we restrict attention to the in which , ,, i . e ., no on the unit circle . Additionally , we require : 
 Constraint : and are stable . In addition , is strictly causal i . e ., . 
 The first part in the above constraint is in order to avoid unbounded in the converter . The additional requirement on is for the feedback loop in Fig . to be well defined see , e .. Chap 
 Since the architecture a nonlinear element a scalar within a feedback loop , exact analysis of quantization is , in general , a formidable task . This the widespread use of an additive noise model for quantization This model one to study the converter via linear analysis . It is usually as : 
 Assumption : The quantization are i . i .. random , uncorrelated with the input signal . 
 In order not to limit our subsequent analysis to a specific type of scalar , the following is also assumed : 
 Assumption : The probability density function not affected by the in the converter other than via its second moment . 
 Under Assumption , any given type of scalar with a fixed number of quantization to quantization whose variance is proportional to the variance of its input . This can be stated as 
 where is the signal to noise ratio of the scalar not to be confused with that of the system . on the number of quantization , the of the signal being and the of the scalar itself . 
 Our ultimate goal is to find the frequency of the A ,, minimize the variance of under and , and for given and known , and . The will constitute the achievable lower bound on the for the converter . 
 Towards the above goal , we first derive an expression that the decision to the error measure that we wish to minimize . From equation , Assumption , and we have where is the variance of the quantization error , and is a delay version of , the frequency response . The first term on the right hand side of to the variance of the frequency weighted quantization error in . The second term in for the frequency weighted linear distortion by the in the pair . 
 The variance is related to via . From Assumption , the latter is given by . Combining this result with 
 The above expression the , and the signal to noise ratio , to the . Minimization of this cost functional will yield for the optimal and performance . 
 For comparison , we note that the cost functional , together with and , is also part of the analysis in , and , wherein equivalent optimization are . 
 In this section we derive explicit analytic for the optimal and the associated optimal performance for the scheme , subject to a mild restriction . The analysis is based on a set of that the optimal must necessarily satisfy . To facilitate the flow of , all are given in the Appendix . 
 Minimization of is simplified by that , for stable and strictly causal , it that . Substitution of this equality into 
 Notice that the cost functional in only two unknown , . This it simpler to work with than the functional in . 
 The optimization problem can be further simplified by writing the of . Unfortunately , the relationship the optimal , for the general case , can only be stated implicitly , as shown next . 
 Remark : Notice that , from , is a positive , symmetric and real valued function of . It then from that the product of the optimal , must exhibit linearly decreasing phase . 
 In general , the presence of in the inner product on the right hand side of it difficult , if not impossible , to express the in of . However , under specific on , and , an analytical explicit solution to can be , as : 
 Lemma : Provided , a . e . on ,, then , for a given frequency response , the optimal , a . e . on ,. 
 To summarize our so far , we have shown that , provided , the . 
 These two , in turn , determine the optimal A and , respectively . We can now state the main result of this paper : 
 Theorem : If a . e . on ,, then the minimum achievable frequency weighted reconstruction of an converter is 
 The stated in Theorem have very interesting . Some of these are below . 
 of Scalar Quantization Without Feedback : It is easy to verify from the in Theorem that scalar quantization without feedback is optimal if and only if is constant . In particular , it from that if , a . e . on ,, then the optimal converter to a converter with a fully whitening filter and a post filter satisfying A and A . 
 Comparison with : The minimum for an system derived by Noll in , fed back quantization noise , is . Perhaps surprisingly , Theorem that the optimal performance is slightly better compare with . Moreover , the corresponding optimal AN , and derived in 
 respectively . Substituting these into actually an , where is the spectral flatness 
 Total Frequency Weighted Distortion is White : It from and that , in an system , the of frequency weighted quantization noise and linear distortion are , respectively 
 Theorem , the noise shaping by an optimal system is not complete , i . e ., frequency weighted quantization noise is not white . However , the of the total frequency weighted error is white , since 
 Relation with the Reverse Water Filling Paradigm : The parametric Rate Distortion formula for a ... process and as the distortion measure is given by the well known reverse water filling paradigm see , e .. 
 For , it total frequency weighted distortion to be equally distributed over frequency . It also the input signal to appear at the output with 
 , i . e ., less significant spectral higher attenuation . Interestingly , is equivalent to , a . e . on ,. Furthermore , is flat , as , and , in full agreement with the above prediction . 
 Output of the Scalar is White : It can be seen from a that , unless is constant , the optimal A is not a full whitening filter for . Interestingly , however , it is straightforward to verify that the optimal in render a Fig . with flat . More precisely A , where is the same arbitrary constant that in a . A remarkable implication is that the output of the converter can be efficiently into by of a first order entropy coder . 
 Rate Distortion Analysis : The rate distortion efficiency of any source scheme with quadratic error as distortion measure can be established by its 
 against the upper bound derived by O Neal in . For the case min , and to , this bound can be written as 
 , log log , bit rate in per sample , is the spectral flatness measure is the minimum variance associated Section I . Notice upper bound for the in of a memoryless source . 
 On the other hand , under the of Theorem and , the best achievable of an system is given by : 
 By and , we see that the of the converter via Theorem from the information theoretic upper bound as : 
 The difference , log for of long been known for a variety of scalar memoryless see , e .. and the therein . be in the converter , can be by . and . for a uniform with entropy E .., non uniform with E .., non uniform for without E .., and uniform quantization without E .. and a loading factor of , respectively see . 
 . This paper derived explicit analytic for the best achievable performance and optimal for . These , which we believe to be novel , were found by accounting for fed back quantization noise in the optimization . The in this paper simplify the analysis and design of , and provide valuable insight into the trade inherent in linear feedback . 
 Proof : is a monotonically increasing function ,¡ minimization of is equivalent to . From inequality and the constraint , we obtain 
 Equality is in a if and only if , a . e . on ,, for some . equality if and only if . This the proof . 
 Substituting this into . Notice that that the denominator on the right hand side of is strictly positive . The proof is by that substitution of into the inequality , thus our initial supposition . 
 Proof : Theorem Suppose the such that in Lemma . Then , one can substitute into to obtain 
 to be stable and strictly causal from Con straint is equivalent to the function to belong to the set of non negative log integral defined in , see , e . Theorem .. and . Then , it from Lemma that the optimal is as in a . Substitution of the latter into . It also from a that the inequality in Lemma is equivalent to the condition by the theorem . This our initial supposition . Notice also that the latter inequality also that , as by Condition . Finally , substituting a into , and the of . This the proof . 
  
 AR source subject to an average mean squared error fidelity criterion . Toward this end , we consider the nonanticipative rate distortion function which is a lower bound to the causal and zero delay rate distortion function . We use the realization scheme with feedback in to model the corresponding optimal of the , when considering vector AR subject to an average distortion . We give on the vector AR source to ensure asymptotic of the realization scheme bounded performance . Then , we encode the vector due to filtering via lattice quantization with subtractive dither and memoryless entropy . This scheme a tight upper bound to the zero delay . We extend this result to vector AR of any finite order . Further , we show that for infinite dimensional vector AR of any finite order , the with the zero delay . Our theoretical framework is with a simulation example . 
 Zero delay source is desirable in various real time , such as , in signal and control . Zero delay form a subclass of causal source see , namely , where the source on the source in a causal manner . However , zero delay source to causal source allow the reproduction of each source sample at the same time instant that the source sample is . Unfortunately , causal source does not exclude the possibility of long of , which may cost arbitrary end to end . 
 Zero delay and causal in to cannot achieve the classical rate distortion function . Indeed , an open problem in information theory is the gap between the optimal performance theoretically attainable by non causal , and the by causal and zero delay , hereinafter by and , respectively . Notable where this gap is explicitly found are memoryless , stationary in high , and zero mean stationary scalar with average mean squared error distortion . 
 Throughout the , the interest in zero delay is growing , thus , further research on the fundamental of the by zero delay . Unfortunately , it turns out that is very hard to compute and for this reason there been a turn in of classical that perform as tight as possible to 
 In this paper , we derive a tight upper bound to zero delay source for vector AR subject to an average distortion . We consider nonanticipative rate distortion function see , e .., , , , which a lower bound to to the classical see , e .., . Then , we employ the feedback realization scheme in , Fig , that to the optimal test channel of for vector AR and average distortion . Further , we give to asymptotically stabilize the performance of the specific scheme . By standard entropy , on the of the feedback realization scheme , we derive the tight upper bound . In addition , we show how to generalize our scheme to vector AR of any finite order . If the vector dimension of the Gauss AR source to infinity , we show that the with the . We demonstrate our with a numerical example . Notation : We let , , N , ,.... 
 We denote a sequence of its realization by , , ,...,. The distribution of by . The conditional distribution . For a square . The transpose of a with 
 on the row and column , we denote by the matrix , i ,...,, on its diagonal and zero elsewhere . 
 In this paper we consider the zero delay source setting in Fig . . In this setting , the dimensional vector source is by the following linear time invariant state space model 
 N ; is the initial state , and the noise process is an i . i .. ; sequence , independent of x . We allow A to have outside the unit circle which that can be unstable . 
 The system as . At every time step , the the source and a single binary from a set of of at most a countable number of . Since the source is random , and its length are . Upon , the an estimate of the source sample . We assume that both the and process information without delay and they are to have infinite memory of the past . 
 The analysis of the noiseless digital channel is restricted to the class of instantaneous variable length binary . The countable set of all is to allow the binary representation to be an arbitrarily long sequence . The and are by of conditional probability as , : N and , : N , respectively . At , we assume , x x and , z z . 
 . These design are formally cast by the following optimization problem : 
 In this section , we give the definition of of vector AR subject to an average distortion . 
 , the reconstruction distribution by , and the joint distribution by . 
 The marginal on , , is induced by the joint distribution We assume that at , , x x . 
 Given the previous , we introduce the mutual information between and yn as 
 where E is the expectation with respect to the joint distribution 
 Definition . with average distortion The finite time is defined by 
 The optimization problem of Definition , in contrast to the one given in is convex see e .., . In addition , for the source model and the average distortion , then , by , , the optimal test channel corresponding to is of the form 
 P , N , where at , , x x , and the corresponding joint process , : N is jointly . 
 The in , Theorem the optimal test channel of with the feedback realization scheme in , Fig that to a realization of the form : 
 where is a scaling matrix ; is an independent noise process with ; , independent of x ; the with 
 with ;, and . Moreover , are given by the following filter 
 Fig . : Asymptotically stationary feedback realization scheme corresponding to . 
 , and is an orthogonal matrix . It is easy to verify that the following hold : 
 where I the identity matrix . By substituting in , we can also observe that , 
 The realization scheme of becomes asymptotically stationary stable if one of the following two hold : A is stable , i . e ., its have magnitude less than one ; the limit of the covariance matrix , i . e ., with . This that with , and 
 Next , we briefly discuss the resulting asymptotically stationary realization scheme in Fig . . 
 or , we ensure that and it is unique . The error covariance matrix is by an orthogonal matrix E invertible matrix such 
 that E E . To facilitate the computation , we introduce the scaling process , where 
 at : Analogously , we introduce the process : N defined by d and the scaling process : N defined by , with 
 asymptotic of and , respectively . The fidelity criterion at not affected by the above of , : N , in the sense that the at both the and do not affect the form of the squared error distortion function , that is , 
 The steady state of b is . The end to end distortion of the scheme in Fig . is 
 trace Hence , following , Theorem , the per unit time asymptotic limit of subject to the total distortion can be expressed as . 
 Clearly , the optimization problem in , is a minimization problem and can be , for instance , Tucker , Chapter 
 . . or semidefinite . A way of is in . However , to that work , our realization scheme is with feedback to take into account the effect of unstable in the dynamical system . 
 In this section , we derive an upper bound to the zero delay a uniform scalar on the feedback realization scheme of Fig . . The scheme was in and since then it been used in several see , e .., , , under various realization . However , it never been for the realization scheme in this work . Here , we consider the vector AR source of , and we quantize each time operating , with their being jointly entropy conditioned to the dither . We extend our when vector quantization showing that at infinite dimensional , the space filling loss due to compression and the entropy , i . e ., and coincide . 
 Next , we use the asymptotically stationary feedback realization scheme in Fig . to design an efficient , pair . 
 We select the step size so that the covariance of the resulting quantization error . The does not quantize the state directly . Instead , it the deviation of from the linear estimate of . This method is known in least estimation theory as 
 Fig . : Scalar quantization by a dimensional channel operating . 
 approach and , therefore , the is as an . We consider the zero delay source setup in Fig . with the additional change of the parallel operating . This is in Fig . . Note that , all matrices and adopted in Fig . still hold when the replacement is applied . 
 For each time step , the input to the , is a scaled estimation error defined as 
 Moreover , at is an valued random process . The parallel dimensional channel is operating , hence we can design the covariance matrix of the corresponding to the parallel in such a way , that for each , each diagonal entry of , i ,...,, i . e ., , to correspond to a quantization step size i , i ,...,, such that 
 This a input output transmission of parallel and independent . We apply to each component of at , i . e ., 
 and we let be the valued random process of dither whose individual , ,... are mutually independent and uniformly distributed , i independent of the corresponding source input at , i , , i . The output of the is given by 
 Note that , ,..., , can take a countable number of possible . In addition , by construction see Fig . , the at : , ,... and : , ,... are not 
 any more since by the change in Fig . , at : , ,... and : , ,... contain of the uniformly distributed process : , ,.... As a result , the filter in Fig . is no longer the least mean square estimator . 
 Entropy : In what , we apply joint entropy across the vector memoryless across the time , that is , at each time output of the is conditioned to the dither to generate a . The by the signal from . Specifically , at every time step , we require that a message is into a , designed , Chapter . . For a , the based on scheme give an instantaneous prefix free code with code length that the E . 
 Since the memoryless entropy over time , the following theorem . 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating in Fig . . If the vector process : , ,... of the output is jointly entropy conditioned to the dither signal in a memoryless fashion for each , then the operational zero delay rate 
 the dimension of the state space representation given in , while the average distortion the end to end average the system . 
 The previous main result combined with the lower bound on zero delay , to the following corollary . 
 Consider the realization of the zero delay source scheme in Fig . with the change of channel with parallel independently operating as in Fig . . Then , for vector stable or unstable AR the following hold 
 Proof . This is the fact that , and Theorem . 
 The derived in Corollary based on the realization scheme of Fig . hold for vector of any order . 
 Proof . This is shown by the state of the model of . For see , Appendix . 
 For stationary stable scalar valued AR , our upper bound in Theorem with the bound in , Theorem . However , the upper bound in is a realization scheme with four instead of only one that we use in our scheme . In addition , our result into account unstable too . 
 Compare to , we use based on a different realization setup that into different lower and upper . 
 Example . We consider a two dimensional unstable AR source as : 
 where R , the parameter matrix A is unstable because one of its , by i A , magnitude greater than one , the pair A , is and ; I . By we plot the theoretical attainable lower and upper to the . This is in Fig . . As from theory , source sample . 
 It is interesting to observe that if instead of uniform scalar quantization we quantize over a lattice vector by memoryless entropy conditioned to the dither , then the upper bound in becomes 
 where is the second moment of the lattice . If we take the average rate per dimension and assume an infinite dimensional vector source , then by , Lemma , , and the due to space filling loss and the loss due to entropy in asymptotically goes to zero . the latter , and the fact that , we obtain 
 We considered zero delay source of a vector AR source under distortion . Based on a feedback realization scheme that the of a filter with a , we derived an upper bound to the . We the performance of this scheme when lattice quantization . For infinite we that the coincide with the zero delay . An illustrative example is to support our . 
 As an ongoing research , we will apply the scheme based on to find the actual operational corresponding to the zero delay . Moreover , we will examine similar for fixed length rate . 
  
 ﻿ This work a formal connection between density estimation with a data rate constraint and the joint objective of fixed rate universal source and model identification by in TIT , , , . an equivalent learning formulation , 
 we derive a necessary and sufficient condition over the class of for the of the joint objective . The learning framework used here is the skeleton estimator , a rate constrained learning scheme that achievable for the joint and modeling problem by its learning to the specific of the problem . The with the skeleton estimator significantly extend the context where universal source and model identification can be , for that move from the known case of parametric collection of with some smoothness and to the rich family of non parametric L totally bounded . In addition , in the parametric case we are able to remove one of the that constrain the applicability of the original result similar in of the distortion redundancy and per letter rate overhead . 
 : fixed rate source ; joint and modeling ; universal source ; learning with rate ; the skeleton estimator ; L totally bounded classes 
 Universal source a long history in information theory and statistics . seminal work the variable length lossless problem and important information for performance analysis , . In this lossless setting , it is well understood that the entropy the minimum achievable rate in per sample to code a stationary and memoryless source when the probability model of the source is available . When the probability of the source is not known but to a family of the so universal source problem , the focus of the problem is to characterize the penalty or redundancy in per sample that an and pair will experience due to the lack of knowledge about the probability . In the lossless case , a seminal result that the least worst case redundancy over or the solution of the problem for is determined by the information radius of . 
 Building on this connection between least worse case redundancy and information radius of , there are numerous important for lossless , . In particular , it is known that the information radius sub linearly with the block length for the family of finite alphabet stationary and memoryless , which the existence of a universal source code that entropy as the block length goes to a large value for every distribution in . 
 However universality is not possible for the family of alphabet stationary and memoryless because the information radius of this family is unbounded , , . More recent on lossless over countable infinite have at the analysis to specific of with some tail bounded to achieve universality and also at weak of the lossless source setting . 
 In the fixed rate source problem , assuming first that the probability of a memoryless source is known , the performance limit of the problem is given by the distortion rate function , . Consequently , the universal source problem to compare the distortion of a scheme satisfying a fixed rate constraint with the distortion rate function assuming that the designer only that . The literature on this problem is rich , , with a first result dating back to who the existence of weakly fixed rate universal source code for the class of stationary under certain about the source , the alphabet , and the distortion measure . More refined were in , one of which established necessary and sufficient to achieve weakly universality for the class of stationary and ergodic . To provide a more specific analysis of universal source , Linder al . a scheme with a distortion redundancy that goes to zero as O for the case of independent and identically distributed i . i .. bound . Later Linder al . previous showing a fixed rate 
 construction with a distortion redundancy that as O log and O log finite alphabet i . i .. and bounded infinite alphabet i . i . respectively . Similar convergence were a nearest neighbor vector quantization approach in . 
 It is also understood that universal variable length lossless source is connected with the problem of distribution estimation , , as there is a one to one correspondence between prefix free and finite entropy discrete in the finite and countable alphabet case , , . Building on this one to one correspondence in the lossless case , al . , Theorem that the redundancy in per sample of a given code upper the divergence between the true distribution of the source and the distribution derived from the code . Therefore , the existence of a universal lossless source code existence of a universal distribution free in estimator of the distribution in direct information divergence . This that lossless not only a lossless representation of the data , but it a consistent error free estimator of the distribution at the receiver . 
 The connection between and distribution estimation that is evident in the lossless case is not , however , present in the fixed rate source problem . As in , a fixed rate source code does not offer a direct map with a probability distribution model for the source . In light of this gap between and and by some in adaptive control , where it is relevant to both compress data in a way and identify the distribution of the source at the receiver , , the joint objective of fixed rate universal source and model i . e ., distribution identification in . 
 Inspired by construction in , , a new setting for the problem of fixed rate universal compression of continuous memoryless based on the idea of a two stage joint and model or distribution identification framework . In this context , he a two stage scheme to consider two : fixed rate universal source and source distribution model identification . The first objective of the scheme is to transmit the data in the classical distortion rate sense , while the second objective is to learn and transmit a description version of the source distribution model , . Taking from statistical learning , splitting the data into training and testing . The training data is used in the first stage of the process to construct a estimation of the source distribution and encode it the first stage . Then in a second stage of the process , the first stage are used to pick a with the distribution fixed rate source code to encode the test data the second stage . In this joint and modeling setting , the existence of a zero rate consistent estimator of the density in total variation is sufficient to show the existence of a weakly universal fixed rate source scheme Theorem . , the distortion rate function , , , , for any given rate . This result is for a wide class of single letter bounded distortion and for a family of source : indexed over a bounded finite dimensional space , i . e ., a parametric collection with some smoothness and Theorem . . 
 It is important to highlight that the joint and modeling in did not degrade the performance of the source objective . In fact by the analysis to the source objective alone , the joint and modeling framework in the same state of the art performance as conventional two stage universal source or universal vector , , in of distortion redundancy and per letter rate 
 overhead O and O log , respectively as the block a large number . Importantly , the first stage of this joint and modeling scheme are used to achieve model identification at the receiver with arbitrary precision in total variation with a rate of convergence 
 of O to infinity , with no extra cost in per letter with conventional fixed rate source . 
 This work formally the interplay between density estimation under a data rate constraint and the joint fixed rate universal source and modeling problem with training data or memory in . The first main result Theorem a connection between zero rate density estimation and a universal joint and modeling scheme that optimal source in a distortion rate sense and lossless model identification . This result is for the general family of bounded single letter . Remarkably , this connection that the construction of a joint and modeling scheme to the construction of a zero rate density estimator . From this result , the second main result Theorem a necessary and sufficient condition for the existence of a weakly universal joint and modeling scheme . For the part of this result , we used the skeleton estimator as our learning framework . this learning framework we extend the parametric context in to the rich non parametric scenario of L totally bounded . 
 Furthermore , the parametric case studied in , by the skeleton estimator we are able to remove some of the that limit the applicability of the original result . We show that the skeleton estimator the best performance in in of the distortion redundancy and per letter rate overhead , in particular of convergence to 
 zero of O and O log , respectively , as the block length to infinity . To obtain this , our result the finite and dimension assumption considered in . On the other hand , when the finite dimension assumption is added in the analysis , 
 the skeleton learning scheme a convergence rate of O for the distortion redundancy as the sample length goes to infinity . Finally , the skeleton framework is in the parametric case as its minimum distance decision is carried out on a finite number of and the oracle e skeleton or the e covering in total variation of Chapter can be by a practical uniform covering of the compact index set Theorem . Finally , it is worth that a preliminary version of this work in the context of density estimation under a data rate constraint was in . 
 The rest of the paper is organized as : Section the setting of the joint and modeling with training data . Section the with zero rate density estimation . Section the main joint and modeling result Theorem and the skeleton estimator . Finally , Section a special case where the are indexed by finite dimensional bounded space the parametric context . A summary of the is in and . Finally , the are in Section . 
 The fixed rate and modeling problem in is in this section . This joint and modeling problem will be the main focus of this work . In addition , and used in the rest of the paper will be . 
 Let be a separable and complete subset of where is the sigma field . Let be the collection of probability on ,, with the sigma field restricted to , and let denote the set of probability absolutely continuous with respect to the measure . For any its probability density function . The total variational distance in is given by to avoid any confusion , a set then its . 
 For to , if we define the set for the pair , by 
 Let : be an i . i .. stochastic process or stationary and memoryless source , where Xi in and a distribution in : . is in general an index set for . The problem of source of a finite block of the process X .., to find a or code from to , where is a finite set . Given a constraint on , the design is to make as close as possible to in average for that a distortion function . The standard problem the knowledge of for finding the optimal code for any finite block , , , as well as for the fundamental performance of this task to infinity , , , . 
 A more realistic scenario is the universal source problem , where the source distribution is unknown and a scheme needs to be designed for the family . Here we focus on a specific learning variation of this task by in , where in addition to the data that needs to be compressed and with respect to a fidelity criterion , 
 we have a finite number of i . i . following the same distribution and that can be used to estimate in the process more of this approach in Section . . This additional data can be as memory , training data , or side information about available at the because it is data that is not to be compressed and . The existence of this memory from the standard zero memory setting considered in universal source . However , this information can be seen as a realistic assumption in the context of a sequential block by block of an infinite sequence , where the data is partitioned into of the same finite length and compressed sequentially block by block . Then in a given stage of this sequential process , the data from previous are available at the lossless for the process the current block . 
 More specifically following the fixed rate block and modeling setting by in , we consider an block scheme with finite memory , where there is a distinction between the data Z .., that is available as side information to estimate the source distribution training data and the data that needs to be and source or test data , under the important assumption that both data are i . i .. of the same unknown probability . A systematic exposition of this setting and its connection with the classical setting of zero memory block is in Section . Formally , let us define an , block code by the pair 
 Then given a set of training and a finite block of the source is the composition of : a function , that to an element in a finite set conditioned on the training data or memory by , and a function that a 
 ,. In this context , into the reproduction the reproduction space . As a short hand , we denote , : that we the 
 , , the reconstruction of by , and its memory for simplicity , the dependency of or , on the memory will be implicit in the rest of the exposition .. 
 The rate of , in per letter is given by . In general , it is not possible to recover from given the constraint on , and thus a single letter distortion measure : is used to quantify the block discrepancy by 
 Finally considering and , the average distortion per letter of , given is 
 which is a function of and hence the average distortion per letter of , is 
 In universal source the performance of a code , is over a collection of and is point wise with the best code that can be assuming that is known . For this analysis , we need the following : 
 Definition . For a finite block distribution , the order operational distortion rate function of at 
 In this context , the operational distortion rate function , is given by 
 The celebrated source theorem a single letter theoretical characterization for in also known as the . A nice exposition of this celebrated result can be found in , , . 
 It is worth that the operational distortion rate function in is equivalent to the classical order operational distortion rate function given by : such that R 
 Lemma . . Then , a nonzero memory side information at the does not help in the minimization of the distortion when is known . 
 For the rest of the exposition , we will concentrate on the simple case studied in where i . e ., the block length is equal to the memory of the code . To be precise about the meaning of universality in this context , we resort to some standard : 
 Definition . A scheme ,: is weakly universal for the rate , 
 the first term is the order distortion redundancy , which is the discrepancy that can be exclusively to the goodness of the scheme . The second term in , i . e ., 
 , to do with how fast to the as the block length 
 to infinity see further in Section and therein . From this observation , 
 Definition . A scheme ,: is strongly finite block universal for the 
 Note that if ,: is strongly universal then it is strongly finite block universal , but the converse result is not true in general . The missing condition to make these two criteria equivalent is the uniform convergence of to in the class . More discussion about this point in Section . 
 by the work of , a two stage block code with finite memory training data , with the objective of doing both fixed rate source , and identification of the source distribution at the receiver . More precisely , given and the training and the source data , respectively , an , joint and modeling rule is given by 
 where finite set of , in two . In the first stage , the pair , in to do density estimation and finite rate quantization by , and an density in . At the end , the first stage 
 the index , the second stage of ,, by , and the source data by 
 In summary , the outcome of the whole process is the concatenation of the that represent first stage , and the that represent , second stage . The process , on the other hand , the first stage to recover and then the second stage to recover . see Figure in which this process is . The rate in per letter of , is 
 Figure . Illustration of two stage joint source and modeling scheme . Top figure the process and the bottom figure the respective process . 
 Based on this two stage scheme , we could simultaneously achieve source and density estimation modeling at the . This new joint and modeling objective the introduction of the following definition : 
 Definition . A joint and modeling scheme ,: in is strongly universal for a class of distribution : at the rate , if 
 Consequently , if ,: is strongly universal for , it that infinity , density estimation is at the in total and , from the source perspective , ,: is strongly finite block universal the sense of Definition . For the rest of the paper , the strongly universality of Definition will be the main and modeling objective . 
 This section a connection between the objective of joint and modeling declared in Definition and a problem of zero rate density estimation . 
 . . Density Estimation with a Rate Constraint Let us first introduce the problem of rate constrained density estimation . Let : be an indexed collection of as in Section . . 
 Definition . An , learning rule of a pair of ,, with : 
 The composition of these two : the rate constrained learning rule in the : , where log its description complexity in per training sample . 
 Definition . The rate is achievable for , if a learning scheme , : such that 
 where Z , Z . in the left hand side of to i . i .. driven by . In this case , we say that is an rate uniformly consistent scheme or estimator for the class . 
 Proposition . If for a given , ,: is strongly universal for the the rate Definition , then its induced finite description learning scheme from the first stage in , i . e ., , : , is a zero rate uniformly consistent estimator for Definition . 
 Interestingly , the existence of a zero rate uniformly consistent scheme also sufficient to achieve the joint and modeling Definition if some mild are adopted from the work in . This is stated in the following result : 
 : can be expressed by , , where , is a bounded metric in with and 
 for all , for all , and for all , there a , block code , say , that the order operational in . 
 Then the existence of a learning scheme , : that is zero rate uniformly consistent there a joint and modeling scheme ,: that is strongly universal rate Definition . 
 Remark . The construction for ,: at any rate in Section . the zero rate density estimation scheme : that : 
 , where is a constant . It is worth that these two summarize the result in Theorem and , importantly , these two are independent of . 
 Remark . An important consequence of the in and is the fact that a learning scheme : with specific of convergence for E , and to infinity a joint and modeling scheme that a uniform rate of convergence to zero over of the overhead in distortion by and a uniform rate of convergence to zero of the overhead in rate by . This observation will be used in all the achievable in and , where , consequently , the problem to determine and for E , and . 
 From the connection with zero rate density estimation in Section , here we present a set of new for the joint and modeling problem of Section . . In these , the general i and stated in Theorem are assumed . 
 Definition . Let be a class of . We say L totally bounded if for every e , there is a finite set of : i .., that , 
 Definition . totally bounded , let Ne denote the positive integer that the condition in . Ne is the e covering number e log Ne is the e entropy of . 
 Definition . An e covering Ge that Ge Ne is an e skeleton of . 
 Theorem . There is a strongly universal joint and modeling scheme any rate if , and only if , is L totally bounded . 
 The part of the proof of Theorem on the adoption of the skeleton estimator with its minimum distance learning principle in , which is a zero rate uniformly consistent density estimator for Definition . Furthermore , Theorem can be saying that the construction ,: derived from the skeleton estimator that is a short hand for the process distribution of by under the i . i .. assumption . 
 Knowing specific for e log Ne , the skeleton estimator can be its design parameter appropriately . In particular , the sequence en see in Section . is selected as the solution of the optimal balance between estimation and approximation see 
 in Section . , which is given by e Chapter . . The of this analysis are in Section . and Chapter . By doing so , an zero rate skeleton scheme , with concrete rate of convergence for , and 
 R pen , can be . From and , these imply specific performance for the induced joint and modeling scheme . To illustrate , we present three interesting below . 
 Let : with be the class of which are a convex combination of .., , i . e ., A . is 
 L totally bounded with e being O log e Chapter . . From the optimal sequence 
 is O , which the following finite rate performance bound Chapter . : 
 universal non negative constant . The rate in per O log . 
 the collection of with support on , , monotonically decreasing per and bounded by a constant . This class is known to be L totally bounded , and furthermore e Lemma . , with the only on . From being O n 
 is optimal please see in , with the following performance bound , 
 the class of defined on the bounded support , , continuous integer greater than zero and satisfying that : for a constant . This class is L totally bounded with e being O er Chapter . . From , the optimal sequence , where , o is O n and the rate in per O n . 
 Notably , the last two are fully non parametric , where e is a polynomial function of 
 e . non parametric of L totally bounded of , where e is even exponentially in e , are in . and . and its . 
 Looking at the distortion redundancy bound in , totally bounded the rate 
 convergence that could be with the skeleton estimator in Theorem is O see Section . and the estimation error bound in . In this section , more specific density 
 are studied to achieve this best rate O v for density estimation and distortion redundancy from . We follow the path by in , who of with a finite and dimension the so classes , . Let us first introduce some : 
 Definition . Let : be an indexed collection of . The class for such a collection is given by 
 the class AT a finite dimension A in , and 
 the entropy of associated with the sequence en strictly sub linearly , i . e ., log N is o , then there is a zero rate density estimator scheme , : that 
 where is the skeleton estimator in with en . Furthermore , is also a zero rate strongly consistent density estimator where F 
 From Definition , log Ne is inversely proportional to e . In fact , depending of , log Ne can go from being O log e , passing from being polynomial in e , to being O e see a number of in Chapter and its . Then the role of in the statement of Theorem is to bound how fast Ne should tend to infinity as e goes to zero , to guarantee a zero rate in the skeleton learning scheme . It is simple to show that Ne being O e e with , is sufficient to achieve that log N is o . This is a condition satisfied by a rich collection of L totally bounded classes in . Concrete are in Chapter . 
 The so far are of theoretical interest because they rely on the skeleton estimator that is from the skeleton covering of see Definition , which is unknown in practice . Moving towards making the zero rate skeleton learning scheme of practical interest , we revisit the important parametric scenario in which , the index set of , is a compact set in a finite dimensional space . Interestingly , in this context we can consider a practical covering by the uniform partition of the parameter space , as used in . Unlike , 
 where a minimum distance estimate is first found and then , here we first quantize the then find the minimum distance estimate among a finite collection of i . e ., over a number of in . Some will be . 
 Definition . Let : with . Let IF : be the index function to . IF is said to be locally uniformly , if there and , such , 
 where the ball of radius with respect to the norm in centered at . 
 The following lemma L totally bounded under some parametric . 
 Lemma . Let : with . bounded such that 
 bounded . Furthermore and the is O IF : for this family . is locally uniformly Definition , L totally 
 It is important to note that the e covering in the proof of Lemma to derive an upper bound for Ne is practical see Appendix . This the possibility of a practical skeleton estimator , which is the focus of the following result . 
 Under the of Lemma , let , e ,, e denote the learning rule of with the minimum distance principle in with parameter e see in Section . , where instead of the e skeleton Ge of in Definition , the see Appendix e covering the proof of Lemma is used . This practical e covering is by e by definition , Ne e O , this last part from Lemma .. With this , let , en , denote our practical learning scheme indexed by the precision en . We are in a position to integrate Theorem and Lemma to state the following : 
 Theorem . Under the of Lemma , the practical skeleton estimator en with en that 
 When , that the finite dimension assumption of Theorem is satisfied by the class of mixture in Section . . and a rich collection of exponential of the form : with , where is a reference density , hi : i .., is a set of arbitrary real valued , is a normalization constant see in Section , a compact subset of see in Section . 
 We summarize the of the zero rate density estimation approach adopted for the problem of joint fixed rate source and modeling of continuous memoryless . 
 Proposition and Theorem formalize the interplay between the two stage joint fixed rate and modeling objective and the problem of zero rate uniformly consistent in total variation density estimation . 
 Theorem a necessary and sufficient condition on a family of for the existence of a strongly joint and modeling scheme both source and model identification Definition . The result is for the rich non parametric collection of L totally bounded . 
 For the modeling stage , we propose the skeleton estimator , which first the data and then the minimum distance decision on this finite set of density . This is a practical solution in the sense that the inference minimization is carried out over a finite set . 
 By combinatorial regularity on the family of : , 
 the skeleton scheme O rate of convergence in the order distortion redundancy , and the same rate in the total variational distance for the modeling part Theorem . • Finally , for a relevant parametric setting , a practical skeleton based joint is that a rate of for the order distortion redundancy Theorem . This rate is slightly better than the O in under the same rate overhead of O log . Furthermore , Theorem the finite dimension assumption over the class AT considered in Theorem . , while the same performance 
 in of order distortion redundancy O , uniform risk to learn the 
 Concerning the last parametric result , we note that the result in can be by the adoption of Dudley entropy bound , which would yield the same asymptotic rate in this work for the order distortion redundancy . 
 A final remark is that under the bounded distortion metric assumption of Theorem condition i , Linder al . Theorem that , and for every such that , there is a constant such that 
 where is a sequence that to zero o uniformly in . This result a rate of convergence of the order operational distortion rate function to the as the block length to infinity . In view of , we can adopt this result in and , to say that the average distortion of the respective joint and modeling at rate , i . e ., ,, to the as O point wise . Therefore in the process of 
 This work the problem of fixed rate universal source and model identification with training data in from a learning perspective . Remarkably , we found that the problem is equivalent to the problem of density estimation of the source distribution with some concrete but non conventional operational data rate in per sample . This learning problem can be seen as the task of and the distribution of with a zero rate in per sample , while a consistent estimation in total of the distribution after the process . From our perspective , the rate constraint density estimation problem is interesting in itself and can have relevant in other such as distributed learning and sensor network . 
 Importantly for the joint and modeling problem , the connection with density estimation a context for the use of the skeleton estimator by in . We highlight two important from its use . First , we extend about universality from the parametric context in to the rich non parametric family of L totally bounded , . This result significantly the where the joint model and objective can be . We this with some in Section . and many more can be found in the literature of density estimation , . 
 Second , in the parametric case studied in , we were able to remove some of the and obtain not only the same performance result in of rate of convergence of the order distortion redundancy but also slightly better convergence . Therefore , the Skeleton estimator , though essentially a non parametric learning scheme , is shown to be instrumental in enriching the applicability of the joint and modeling framework . 
 Proof . The fact that is uniformly consistent directly from Definition . On the other hand , the rate of is . From the definition of , it is simple to show from the strict of that in order for limn , i , it is that e for any e . Then , from , and since log , lim , that limn . 
 Proof . The proof upon the in Theorem . ,. . Let us consider an arbitrary and let , : be the zero rate learning scheme of the assumption let us construct the joint and modeling rule of : 
 Concerning the first stage of ,: , it is induced directly from the of . 
 For the second stage , , the pair , is picked such that ,, which 
 is the optimal block code that from the hypothesis in , with , 
 short hand for the reproduction induced from the first stage pair and satisfying the rate constraint , i . e ., . From construction and the fact that zero rate , 
 then ,: the rate condition . On the other hand , based on the assumption that is zero rate uniformly consistent , it that 
 where . Then ,: the modeling objective . Concerning the objective , we use the following key result : 
 Lemma Lemma . . two probability in ,. Let , be a zero memory block coder with the nearest neighbor property i . e ., is nearest neighbor if ,, with the reproduction of .. If we denote the performance 
 where the product measure with , , and the condition i of Theorem and is bounded by , then 
 Furthermore , the inequality can be extended for the order operational in , i . e ., 
 For the first equality we use . The inequality in is from the definition in and , and the equality in is from the construction of which is operational optimal for the distribution at rate . Finally , is from . 
 Concluding , , is random a measurable function of and dominated by 
 V . Hence taking the value with respect to on both sides of this inequality 
 Proof . Let us first assume L totally bounded and prove the direct part of the statement . 
 We adopt the skeleton estimate by and extended by al . , 
 a complete presentation can be found in Chapter . For any arbitrary e , let us consider the 
 o e e skeleton Ge .., Ne of . We use ie i as short hand for the i th in 
 to represent the index set of Ge . Let us consider the class of Ge given by 
 Hence , given i . i .. X .., with Xi , let us propose the pair , e e associated with Ae by , 
 is the well known skeleton estimate . is the minimum distance approximation of with of Ge , , the measure in the right hand side of that is reminiscent of the total variational distance in . In order to choose a sequence en , we consider the following performance bound . 
 Equation is valid for any e and , consequently , it a trade off between an approximation error term and an estimation error term . The approximation error is Ge 
 which is bounded by the definition of Ge . For the estimation error , on the other hand , the use of inequality to obtain that Theorem . , 
 in , it that This last expression is distribution free and it is valid if the approximation fidelity e is a chosen function of . Consequently , for any sequence en , 
 Chapter . , which is well defined and to zero infinity . Consequently from , limn . Then the learning scheme 
 O by construction . To conclude the argument of this part i . e ., the construction of the second stage of a joint modeling scheme , we adopt the result and the construction in the proof of Theorem see Remark for . This result that there is a strongly universal joint and modeling scheme rate . 
 For the other implication the converse part of the statement , let us fix and assume that we have a joint modeling scheme that is strongly universal Definition rate . Then from Proposition , we have a learning scheme , : such that limn and 
 For the learning rule of length , we have its reproduction that we denote by . Let us define the minimum distance oracle solution in by 
 From , we have that limn , . In other , e , there 
 N e , such that for all e , , e uniformly for every . This that e there e , such that for any arbitrary e , Be , where by 
 . totally bounded , which the proof . construction T 
 with the class of the skeleton Gen . It is clear that e , Ae AT . Then by 
 . Here is where we use the assumption that AT finite dimension , which from Theorem . that 
 for some constant . Substituting this result in , the argument by en , a solution which the intended rate of convergence for , o . Finally , the rate of the learning rule is e , which to 
 For the almost sure convergence part if e , it is sufficient to show that the second term in 
 the right hand side of is O almost surely . From the fact that AT finite dimension Definition A , and from the classical inequality Corollary . and Theorem . and Chapter . , it that 
 the proof . As an is o , this result the almost sure to zero of , as 
 Finally , similar , it is possible to show that , is o almost 
 Proof . First note in a compact set , consequently , the finite covering property of a compact set , i . e ., e , there a finite covering such that , e 
 On the other hand , from the locally uniformly assumption on IF :, there and such that , , , . Then , by considering , it by construction of that 
 where :, is the ball centered at , induced from the total variational distance , and the last inequality from the condition . Hence , from , e 
 e o there e min e ,,..., e , such that i , which the result . 
 For the final part , let , be the uniform that characterize the condition of 
 from IF Definition Ne is upper bounded by . Without loss of generality , let us assume the critical regime e , which is the covering number of me hence , 
 we will work with a uniform partition of to find a bound for e . Let e me , then a product type partition , where in each we have uniform length , we have the e covering . The number of is O , which is O as a function of e e e . 
 To clarify the constructive nature of the e covering used to prove this result , an algorithm with the basic of the construction of this practical covering is in Appendix . 
 Proof . Let e be the e covering induced from the uniform partition Lemma . 
 From this we can construct the minimum distance estimate in the class of e with index set e , i . e ., A e , which , from , 
 The latter upper bound is asymptotically dominated by from the fact that is O log Lemma , which the made in . 
 Concerning part , the in the proof of Theorem , we can obtain that e , 
 From this point , the proof from the of Theorem and the fact that is O log . 
 Author : Conceptualization ,.. Silva and .. ; Methodology ,.. Silva and .. ; 
 Formal Analysis ,.. Silva and .. ; Investigation and ,.. Silva and .. ; 
 Writing Original Draft Preparation ,.. Silva and .. ; Writing .. Silva ,.. ; Project Administration ,.. Silva ; Acquisition ,.. Silva . 
 : The work is by from and , Chile and the Advanced Center for Electrical and Electronic Engineering ACE , Basal Project . In addition ,.. Silva support from Project , Chile . 
 : We want to thank the anonymous for their constructive that were instrumental to improve the technical content and organization of this work . We thank for and proofreading all this material and for Figure . 
 First , we show that the zero rate skeleton estimate en , en en : in and is also strongly consistent . 
 Proof . Let us consider the skeleton estimate , where the sequence was chosen by the rule 
 V . As by construction , we just need to concentrate on the estimation error term . inequality , 
 where from the lemma , , the estimation error to zero almost surely . 
 Finally considering the inequality in , we have that , which the argument . 
 Let be a collection of measurable , and x .., be a sequence . Then we define by , the number of different in 
 The shatter coefficient is an indicator of the richness dichotomize a finite sequence of in the space , where by definition n . 
 Definition A . The first time in the index where is strictly less than n is the and 
 dimension of . finite dimension then it is a class ; otherwise if n , then the class is said to have an infinite dimension . 
 Appendix . Pseudo Algorithm to Implement the Practical e Covering in Lemma 
 Under the parametric of Lemma , we recognize four structural that characterize : the dimension of the space that , associated with the assumption that ,, and , the associated with the locally assumption of IF . Given these four ,,, and e , there is a constructive e covering in the proof of Lemma that can be in the following : 
 In each of the k , the interval , is partitioned uniformly with sub e 
 of length e m . This a scalar quantization of , with e per . 
 A product partition of , is made with the scalar of the previous step . 
 From the proof of Lemma , this is a e covering . Let us denote this set by i , i .., . 
 From the proof of Lemma , the covering the previous step an e covering the indexing function IF , i . e ., by IF i : i .., . 
  
 ﻿This paper deals with the design of feedback quantizers to encode plant output measurements in networked control systems with data-rate constrained channels. Starting form a nominal design made under the assumption of transparent communication links, we show how to design a feedback quantizer so as to systematically reduce the impact of quantization on closed loop performance. To obtain our results, we model quantization errors as additive white noise with a signal-to-noise ratio constraint. As a byproduct, we obtain a simple characterization of the minimal quantizer signalto-noise ratio that allows one to design a feedback quantizers that guarantees stability. This bound depends only upon the plant and controller unstable poles. If the plant is strongly stabilizable, then the bound is consistent with the absolute minimal data-rate for stabilization obtained in previous work.
 1	Introduction
 Standard control theory deals with situations where the communication links between plant and controller can be regarded as transparent (see, e.g., [22,55]). There exist, however, cases where the links in a control system are far from being transparent and may become bottlenecks in the achievable performance. Control systems where this happens are collectively referred to as Networked Control Systems (NCS’s) (see, e.g., [1,2,27,28] and the many references therein). Clearly, unless the channel characteristics are explicitly taken into account at the design stage, the performance of an NCS may be far from optimal and sometimes completely unsatisfactory. The main issues that need to be considered when dealing with NCS’s include data-rate constraints (i.e., quantization), data loss and random delays. A unifying framework for the treatment of the general NCS design problem does not exist. Nevertheless, there has been significant progress in the study of specific situations that focus on subproblems. For example, data-rate constraints have been studied in [40,42,45,48,60] and design strategies to deal with quantization have been proposed in, e.g., [21,66]. The issue of data loss has been studied in [33,49,51], among many others, and delays have been considered in, e.g., [32,43,58,62].
 In this paper we focus on linear time invariant (LTI) plant models, and concentrate on the effects of quantization on closed loop performance. Within this framework, a key result obtained in [40] relates the minimal data-rate which is necessary and sufficient to achieve stabilization of an unstable plant, to its poles in very simple way. This bound has been linked to information theoretic concepts where it has been given an interpretation akin to that of entropy (see, e.g., [37,41,48]). Furthermore, [7,8] established that the minimal data-rate for stabilization is sometimes consistent with minimal signalto-noise ratio requirements in standard one-degree-of-freedom control architectures that employ LTI controllers. More precisely, [7,8] showed that, if the plant is defined in discrete time, has relative degree one and is minimum phase, then a Gaussian memoryless channel having a signal-to-noise ratio equal to the lower bound derived in [7,8] would exhibit a channel capacity equal to the data-rate bound in [40]. 
 The results discussed above give absolute lower limits on the admissible channel data-rate which cannot be by-passed by any control law. It seems, however, quite difficult to obtain practical design guidelines from considerations such as those in [40,41,48]. This has motivated some researchers to move towards a simplified treatment of quantization. For example, [21] models quantization as a sector bound uncertainty and employs standard robust control tools. On the other hand, [66] uses a simple white noise model for quantization errors. The latter model for quantization has close connections to the signal processing literature, where it has been successfully used to design high performance quantization schemes (see, e.g., [4,25,30,36,50]).
 In the present work we assume that a controller has already been designed under the assumption of transparent communication links. However, we subsequently extend the set-up by assuming that the control loop has to be implemented using a bit-rate limited channel in the plant to controller communication link. Thus, the plant output measurements have to be quantized prior to transmission. To that end, we borrow ideas from the signal processing literature and employ a feedback quantizer to encode the plant output (see, e.g., [30,50]). Using a fixed signal-to-noise ratio additive noise model for quantization errors, we show how to design the feedback quantizer so as to systematically reduce the impact of quantization on closed loop performance, as measured by the tracking error variance. We show via simulations that our approach gives very good results even for bit rates as low as one bit per sample. We also study stability properties for this linear model. As a byproduct, we obtain a simple characterization of the minimal quantizer signal-to-noise ratio that allows one to design a feedback coding system that guarantees stability. This result is expressed in terms of the plant and controller unstable poles only. For stable controllers, and regardless of the plant zeros or relative degree, our results suggest a minimal data-rate for stabilization that is consistent  with the bound in [40].
 The idea of designing coding schemes to embellish given controller designs is not new. For example, our previous work documented in [23] considers a coding scheme that turns out to be a special case of the one considered here. On the other hand, [53] considers the same coding architecture as the one studied in this paper, but the design procedure in [53] assumes that quantization effects are relatively small. The methodology used in the current paper does not require this assumption. Also, the stability analysis included in the current paper goes beyond the results of [23,53]. Another related line of work has been developed in [9,10,34]. The latter work presents a precise deterministic stability analysis when the coding system is constrained to be a ?-modulator (or variations thereof;  see, e.g., [30]), but does not address performance issues. Another recent publication closely related to the current paper is [38]. In that work, the authors propose a coding architecture similar to the one in this paper, but restrict the quantizers to have infinitely many levels and a prespecified quantization step. The latter assumptions are not needed here. Interestingly, the optimal coder in [38] (which focuses on minimizing a time domain functional) turns out to have a structure that is a special case of the architecture considered here.
 The remainder of this paper is organized as follows: Section 2 presents the notation employed in the paper. Section 3 describes the NCS architecture of interest and derives a linear model that is suitable for analysis and synthesis using linear system theoretical tools. Section 4 studies stability properties of the linear model, while Section 5 presents the proposed design procedure. Section 6 documents a simulation study. Concluding remarks are included in Section 7.
 2	Notation
 We use standard vector space notation for signals, i.e., x denotes {x(k)}k?N0. We also use z as both the argument of the z-transform and as the forward shift operator, where the meaning is clear from the context. Given any matrix X, (X)H and (X)T denote conjugate transposition and transposition, respectively. Given any complex scalar x, |x| and ¯x denotes magnitude and complex conjugation, respectively.
 The set of all discrete time real rational transfer functions is denoted by R. We define six subsets of R as follows: Rp contains all proper transfer functions, Rsp contains all strictly proper transfer functions, RH8 contains all stable and proper transfer functions, U8 contains all matrices in RH8 that have inverses in RH8, RH2 contains all stable and strictly proper transfer functions, RH?2 contains all transfer functions that have only poles outside the unit circle and are either proper or improper. For any A(z) ? R we define A(z)~ , A(z-1)T. We say that A(z) ? R is unitary if and only if A(z)~A(z) = I. We also define {A(z)}|z=8 , A(8) = limz?8 A(z).
 Every A(z) ? R with no poles on the unit circle belongs to L2 in which case we define the 2-norm of A(z) via (see, e.g., [39])
  , trace .
 For each such A(z), we can always find   and A2(z) ? RH2 such that A(z) = A?(z) +
 A2(z) and, accordingly,  (see, e.g., [39]).
 Any biproper n × 1 transfer matrix A(z) ? RH8 admits an inner-outer factorization of the form
 A(z) = Ai(z)Ao(z),
 where Ai(z) ? RH8 is unitary (i.e., Ai(z) is inner) and Ao(z) ? RH8 is a biproper scalar transfer function, that has no zeros in |z| > 1 (i.e., Ao(z) is scalar and outer). Moreover, if A(z) has no zeros on the unit circle, then Ao(z) ? U8 (see, e.g., [20]).
 Given any wide sense stationary (wss) process x, we denote its power spectral density by Sx(ej?), its variance by sx2 and its standard deviation by sx. We note that if, in addition, x has an always positive rational spectrum, then we can always find a spectral factor ?x(z) ? U8 such that Sx(ej?) = ?x(ej?)?x(ej?)H, ?? ? [-p,p]. We also recall the well known fact that  (see, e.g., [56]).
 3	Coding for Networked Control Systems
 In this paper, we will consider the NCS architecture depicted in Figure 1. In that figure, G(z) is the SISO plant model, C(z) is a SISO controller, y is the plant output, r is the reference signal, do
  
 Figure 1: Considered networked control architecture.
 models output disturbances and dm corresponds to measurement noise. Unlike standard non-networked situations (see, e.g, [22,55]), the feedback path in Figure 1 comprises a communication channel and a (source) coding/decoding system (C and D).   The main focus of the current paper lies in designing this coding system, having performance in mind. To that end, we utilize as the performance assessment quantity the stationary variance of the tracking error e, defined via
 	e , r - y.	(1)
 We next describe the assumptions that underly our subsequent analysis.
 3.1	The nominal design
 Since our aim is to design coding systems, we will assume that the controller C(z) in Figure 1 has been already designed assuming transparent communication links.  The control loop formed by C(z) and G(z) when transparent communication links are in place (i.e., when ˆym = ym in Figure 1) will be referred to as the nominal loop (or nominal design).
 For future reference we note that, in the nominal loop,
 	e = Twe(z)w,	(2)
 	£	¤T
 where w , r	do	dm	,
 	£	¤
 	Twe(z) , S(z)	-S(z)	T(z) ,	(3)
 and S(z) and T(z) are defined via
 	 .	(4)
 The transfer function S(z) is the nominal loop sensitivity function and T(z) is the nominal loop complementary sensitivity function (see [22]).
 In the sequel, we will assume the following:
 Assumption 1 (Plant and nominal design) The plant model belongs to Rsp, whilst the controller, C(z), belongs to Rp, is non-zero and is such that the nominal loop is stable and well posed (in the standard sense; see, e.g., [22,55]).
 The assumption that the nominal loop is stable and well defined is, of course, sensible in our context where the coding system is designed a-posteriori. We assume that G(z) is strictly proper for simplicity. In principle, this can be removed at the expense of additional technical care. On the other hand, the assumption of C(z) being non-zero discards non interesting situations, where the nominal loop is such that G(z) is left in open loop (i.e., uncontrolled).
 We end this section with a description of the exogenous signals r,do and dm.
 Assumption 2 (Signals) The signals r, do and dm are mutually independent scalar zero mean wss processes, each having a rational power spectral density that, if not identically zero, admits a spectral factor in U8.
 We note that Assumption 2 is standard (see, e.g, [56]).
 3.2	The coding system
 In this paper, we will focus on error free bit-rate limited channels. As a consequence, the input to the channel, i.e., h (see Figure 1), must be quantized prior to transmission. To that end, we will consider a standard feedback quantizer as depicted in Figure 2 (also known as a noise shaping quantizer; see, e.g., [30,50]). In that figure, A(z),B(z) and F(z) are filters in Rp that need to be designed and Q denotes a uniform quantizer (see, e.g., [25,30]), i.e., 
 	Q(v(k)) , sat  ,	(5)
 where V is the quantizer dynamic range, ? , 2V (L - 1)-1 and L is the number quantization levels.
 We recall that a quantizer is said to be overloaded if and only if the absolute value of its input is greater than its dynamic range, i.e., |v(k)| > V for some k ? N0. If the quantizer does not overload, then the quantization noise, defined via
 	q , h - v,	(6)
 is such that   for every k.
 As already mentioned in Section 3.1, we are interested in designing coding systems for pre-specified nominal designs. In this setting, it is natural to employ coding systems that, in the absence of channel artifacts, have unit transfer function. That is, we will utilize coding systems that achieve perfect reconstruction. In our case, the channel is assumed to be error-free and hence hˆ = h. As a consequence, it is straightforward to see from Figure 2 that 
 	yˆm = B(z)A(z)ym + B(z)(1 - F(z))q,	(7)
  
 Figure 2: Considered coding and decoding system.
 It follows from (7) that perfect reconstruction is tantamount to having B(z) = A(z)-1 for every z. On the other hand, in order to have a properly defined feedback loop around the quantizer it is necessary to have a strictly proper F(z) (see, e.g., Chapter 4 in [44]). We summarize the previous discussion as follows:
 Constraint 1 (Structural constraints on the feedback quantizer) The feedback quantizer filters are such that B(z) = A(z)-1 and F(z) ? Rsp.
 Quantization is a deterministic non-linear operation and hence, the exact analysis of quantized systems is difficult (see, e.g., [16,26,40,47]). It has thus become standard, particularly in the signal processing literature (see, e.g., [5,25,30,36,50,63]), to approximate quantization noise by an additive white noise source uncorrelated with the input of the quantizer. Here, we adopt this paradigm and assume the following:
 Assumption 3 (Quantization noise model) The quantization noise signal q (defined in (6)) is a sequence of i.i.d. random variables uniformly distributed in  , and uncorrelated with w.
 Note that we do not assume that the quantization noise is uncorrelated with v, which is certainly not the case since the quantization noise is fed-back to the input of the quantizer and, moreover, the coding system is inside the main feedback control loop. Instead, we adopt a milder assumption that requires only uncorrelatedness with the exogenous signals contained in w. We stress that the previous model is valid only if ? is small enough, the quantizer does not overload and v has a smooth probability density (see, e.g., [4]). These conditions usually do not hold in the case of quantizers that are embedded in feedback loops (see, e.g., the discussion regarding stand alone feedback quantizers in [24]). Nevertheless, one can make use of dithered quantizers (see, e.g., [25,65]) to render the model in Assumption 3 exact provided no overload occurs. Despite the above points, we will see in the simulation study included in Section 6, that, even if one employs a non-dithered uniform quantizer with as few as 2 levels, the predictions made using the simple model summarized in Assumption 3 are surprisingly accurate (see also simulation studies in [17,23,53]).
 In order to guarantee that the quantizer does not overload, in principle one needs to consider infinite quantization levels (or assume that the quantizer input is deterministically bounded, which is seldom the case in a stochastic framework). In practice, it is standard to choose a dynamic range such that the probability of overload is negligible (see, e.g, [30]). Indeed, if v is wss and ß is any positive real, then one can always find a finite a such that choosing V = asv guarantees that the probability of overload is less than ß; a is called the quantizer loading factor.  With such a choice for the overloading factor, it is immediate to see that
 	 ,	(8)
 where we have used the fact that, according to Assumption 3, . This justifies the following additional assumption:
 Assumption 4 (Fixed signal-to-noise ratio) For a fixed number of quantization levels, the variance of the quantization noise is proportional to the variance of the signal being quantized, i.e., the quantizer has a fixed signal-to-noise ratio ? defined via
 	 .	(9)
 Assumption 4 is a key constraint. As mentioned before, it allows one to guarantee that the quantizer dynamic range is always properly scaled. In addition, it has a regularizing effect on the optimization based design of the coding system. Indeed, if this constraint were not in place (i.e., if q were assumed to have some prescribed statistics), then it would be optimal to choose F(z) = 0 and A(z)-1 = ² with ² ? 0. This is, of course, not a sensible choice since A(z) and sv2 grow unbounded when ² ? 0.
 Remark 1 We would like to stress that, in some situations, quantizer overload may become the dominant quantization effect in feedback schemes. Indeed, quantizer overload may trigger limit cycle oscillations that are, of course, not predicted by the linear model for quantization introduced above (see, e.g., [19,44,47]). As implied by Assumptions 3 and 4, we assume in this paper that quantizer overload is infrequent enough and, accordingly, that it has no significative effect on overall closed loop performance. (Careful design of the quantizer loading factor may act as a safeguard against quantizer overload.)
 Considering the model for quantization described above, together with the nominal loop description in Section 3.1, it is easy to derive the linear model shown in Figure 3 for the considered NCS. (Note that we have made the perfect reconstruction constraint explicit.) In Figure 3, q satisfies Assumptions 3 and 4, and r,do,dm satisfy Assumption 2. We will refer to this model as the linear model. It will be the basis of the remainder of this paper.
 4	Mean Square Stability
 In this section we study stability properties of the linear model for the considered NCS derived in Section 3. In particular, we characterize all filters F(z) and A(z) that lead to stable linear models (in an appropriate sense) for a given quantizer signal-to-noise ratio ?. As a byproduct, we characterize the minimal quantizer signal to noise ratio ? that allows one to find F(z) and A(z) such that the resulting linear model is stable.
 We begin by noting that, if x is a n-dimensional vector that contains the states of C(z), G(z), A(z), A(z)-1 and F(z) (see Figure 3), then the evolution of x can be described by a linear state space model:
 x(k + 1) = Ax(k) + Bww(k) + Bqq(k),	x(0) , xo,	(10a)
 v(k) = Cvx(k) + Dwvw(k) + Dqvq(k),	(10b)
  
 Figure 3: Linear model for considered networked situation.
 where A,Bw,Bq,Cv,Dwv and Dqv are matrices of appropriate dimensions that depend on the particular realizations of C(z),G(z),A(z),A(z)-1 and F(z). Next, since we are considering a stochastic system, we need an appropriate notion of stability:
 Definition 1 (Mean Square Stability [12,13,31]) The linear system in (10) is Mean Square Stable (MSS ) if and only if there exist a finite µx ? Rn and a finite Rx ? Rn×n, Rx = 0,  both not dependent on the initial state xo, such that 
 	n	o
 lim µx(k) = µx,	lim E	(x(k) - µx(k))(x(k) - µx(k))H	= Rx,	(11) k?8	k?8
 where µx(k) , E {x(k)}.
 The next theorem gives necessary and sufficient conditions for MSS in our case:
 Theorem 1 (Conditions for Mean Square Stability) If Assumptions 1-4 hold, and xo is an independent random variable with finite mean and finite variance matrix, then the linear model in Figure 3 is MSS if and only if A(z) ? U8, F(z) ? RH2 and
 	  .	(12)
 Proof: Define Rw as the variance matrix of w. Since the spectral factor of w, ?w(z), belongs to RH8, we lose no generality if we restrict attention to the case where ?w(z)?w(z)~ = Rw = 0, for every z (i.e., if we assume that w is white noise). The general case employs the same arguments, but requires an augmented description of the system that has additional stable modes.
 Consider the state space description of the system under study given by (10). Standard results allow one to conclude (see, e.g., Chapter 4 in [56]) that under our working assumptions
 µx(k) =AkE {xo}			(13)
 	£	¤	£
 	Rx(k + 1,k + 1) = ARx(k,k)AH + Bw	Bq Rwq Bw
 	£	¤T
 where Rwq is the variance matrix of the vector w(k)	q(k)	and	¤H Bq	,	(14)
 	n	o
 H
 	Rx(k,k) , E	(x(k) - µx(k))(x(k) - µx(k))
 Moreover, we also have that	.		(15)
 µv(k) = Cvµx(k)			(16)
   ,		(17)
 where µv(k) and Rv(k,k) are defined as µx(k) and Rx(k,k), but considering v instead of x.
 • (?) If the NCS is MSS, then both µx and Rx = 0 are finite and unique. Therefore, (13) implies that A must be stable. On the other hand, we also see from (17) that limk?8 Rv(k,k), i.e., the stationary variance of v, say sv2, must be positive semi-definite, finite and unique.
 Since the nominal loop is stable, a simple calculation shows that A being stable implies that both A(z) and A(z)-1 must be stable, and moreover, that F(z) is stable. Of course, both A(z) and A(z)-1 must be proper and, on the other hand, F(z) is constrained to be strictly proper (recall Constraint 1). Therefore, it follows that A(z) ? U8 and F(z) ? RH2.
 If A is stable, then it is easy to see that the stationary variance of v satisfies (see also Section
 5.1)
 	  ,	(18)
 where Twym(z) is defined in (30), and where we have used the fact that w is uncorrelated with q and is such that ?w(z) = Rw. Using the definition of ? in (18) yields
 	 .	(19)
 Therefore, we conclude that, provided A is stable,   being positive semi-definite, finite and unique is equivalent to (12).
 • (?) Since the nominal loop is stable, F(z) ? RH2 and A(z) ? U8, we have that A is stable. Therefore, it follows from (13) that µx is finite, unique and well defined.
 If (12) holds, then the facts deduced when proving the sufficiency part of this theorem imply that sv2 is positive semi-definite, unique and finite. Using the definition of ?, it then follows that sq2 is positive semi-definite, finite and unique. Therefore, Rwq is positive semi-definite, unique and finite and, since A is guaranteed to be stable (see above), then we have that the Lyapunov equation that describes the limiting value of Rx(k,k) in (14) admits a finite, unique and positive semi-definite solution (see, e.g., Section 21.1 in [67]). Therefore, we have proven that Rx is as required. This completes the proof.
 ¤¤¤
 The condition for MSS given in Theorem 1 is deceivingly simple. This is due to the fact that the nominal loop is assumed stable and we are focusing on coding systems that achieve perfect reconstruction (recall Section 3.2). It is relevant to note that (12) does not depend on A(z). Therefore, one can easily characterize the greatest lower bound on ? that allows one to guarantee MSS:
 Theorem 2 (Minimal signal-to-noise ratio for MSS) If Assumptions 1-4 hold, and xo is an independent random variable with finite mean and finite variance matrix, then there exist filters A(z) and F(z) that allow one to guarantee MSS if and only if
 	 ,	(20)
 where {pi}i?{1,···,n} denotes the set of unstable poles of G(z)C(z).
 Proof: It suffices to compute inf  (see Theorem 1). To that end, we employ the techniques described in detail in, e.g., [8,39,64]. We first note that F(z) ? RH2 ? Q(z) , zF(z) ? RH8. Define
 	 ,	(21)
 where {pi}i?{1,···,n+p } denotes the set of non-minimum phase zeros of S(z) that lie strictly outside the unit circle (i.e., the unstable poles of G(z)C(z) outside the unit circle). It is clear that , is unitary, and is such that the transfer function ?S(z)S(z) belongs to RH8, is biproper and has as non-minimum phase zeros the zeros on the unit circle of S(z) (i.e., the poles on the unit circle of
 G(z)C(z)). It thus follows that
 	¯¯	¯¯
 ||T(z) + S(z)F(z)||22 = ¯¯1 - S(z) + S(z)z-1Q(z)¯¯22
 ¯¯¯¯?S(z) - ?S(z)S(z) + ?S(z)S(z)z-1Q(z)¯¯¯¯22
 =
 = ||?S(z) - ?S(0)||22 + ||z?S(0) - z?S(z)S(z) + ?S(z)S(z)Q(z)||22 = ||?S(z) - ?S(0)||22 + ||?S(0) - ?S(8)||22 +
 	||z (?S(z)S(z) - ?S(8)) - ?S(z)S(z)Q(z)||22 ,	(22)
 where we have used orthogonal decompositions in L2, the fact that both ?S(z) and z are unitary, the fact that Assumption 1 implies S(8) = 1, and basic properties of the 2-norm. By construction, z (?S(z)S(z) - ?S(8)) ? RH8 and ?S(z)S(z) is invertible in RH8 except for zeros on the unit circle. Elementary results (see, e.g., Chapter 6 in [64]) allow one to conclude from (22) that
 	 .	(23)
 Use of the Residue Theorem and some simple algebra yields the desired result.	¤¤¤
 Theorem 2 states a precise condition that the quantizer signal-to-noise ratio ? has to satisfy in order to be able to find a coding system that, when inserted in the feedback path of a stable nominal loop, guarantees the MSS of the resulting linear model. The bound on ? depends only on the unstable poles of G(z)C(z), i.e., on the unstable poles of the plant and controller. If the plant model is strongly stabilizable (i.e., can be stabilized using a stable controller; see, e.g., [18]), then employing a stable controller in the nominal loop allows one to find a feedback coder capable of stabilizing the resulting linear model if and only if
 	 ,	(24)
 where {pGi}i?{1,···,nG} denotes the set of unstable poles of G(z). We note that the same conclusion applies if the controller is stable except for poles on the unit circle (e.g., controllers with integral action).
 If we fix F(z) = 0, then ? must satisfy , which is a fixed constraint in our framework. If it were possible to redesign the controller under the constraint F(z) = 0, then one can use the results in [8] to establish that the admissible signal-to-noise ratio must satisfy
 	 ,	(25)
 where ?G is non-negative and depends on the non-minimum phase zeros and on the relative degree of the plant model G(z) (?G = 0 if and only if G(z) is minimum phase and has relative degree equal to one). We thus conclude that the inclusion of the proposed coding system allows one to reduce the requirements on the quantizer signal-to-noise ratio, at least for strong stabilizable plants (regardless of the plant zeros or relative degree). This reduction may be very significative if, e.g., the plant has high relative degree. This is an important indication of the benefits that coding brings to networked control situations. A question that remains open, however, is whether or not there exist different coding architectures that allow one to recover (24) for any plant.
 Remark 2 (Relationship to prior work) In [8] it is proved that (25) is the minimal signal-to-noise ratio that allows one to find one-degree-of-freedom controllers that stabilize a given LTI plant model over an additive noise channel with a power constraint. In a second step, the authors show that a Gaussian memoryless channel, with a signal-to-noise ratio ? that satisfies (25), would have a capacity Cap (see, e.g., [14]) that satisfies
 	 ,	(26)
 where Rinf is the minimal data-rate which is necessary and sufficient to stabilize an LTI system over an error-free bit-rate limited channel [40]. Equality in (26) is achieved if and only if the plant is minimum phase and has a relative degree equal to one. These results suggest that signal-to-noise ratio requirements in LTI one degree-of-freedom control loops are, for a restricted class of plants, consistent with the minimal data-rate requirements of [40]. If the plant has non-minimum phase zeros, or has a relative degree larger than one, then ?G > 0 (see (25)). It thus follows that, in these situations, data-rate requirements suggested by signal-to-noise ratio considerations may be more demanding than those in [40].
 Our results can be applied to the channel model in [8] as well, provided error free feedback (with a unit delay) is available from the channel output to the channel input.  When doing so, it turns out that (24) is consistent (in the sense described above) with the minimal data-rate derived in [40]. Our results holds even if the plant model has arbitrary relative degree and arbitrary zeros, as long as G(z) is strongly stabilizable. We thus conclude that, within the LTI framework, the use of feedback coding is key to achieve (24) and, accordingly, key to make signal-to-noise ratio requirements consistent with the results in [40]. We stress that the issue of existence of feedback from the channel output to the channel input is inconsequential to the set-up used in [40] because the channel is error free, as in our case. (Note that the assumption of channel feedback has been explicitly made for NCS’s with stochastic channels (see, e.g., [3,59,61]) again recovering the results in [40]. If channel feedback is removed from the analysis of [59,61] then the minimal data-rates for stabilization obtained do not necessarily coincide with those in [40] (see Section VI in [60])).
 5	Design for Performance
 In this section we go beyond stability and focus on how to actually design a feedback coding system that minimizes the impact that the communication channel has on closed loop performance, as measured by the steady state variance of the tracking error.
 5.1	Problem definition
 The purpose of this section is to define the performance goals of interest in a precise way. To that end, we consider the linear model in Figure 3. Straightforward analysis reveals that the tracking error obeys
 	e = Twe(z)w + T(z)A(z)-1(1 - F(z))q.	(27)
 Therefore, if the linear model is MSS and Assumptions 2 and 3 hold, then the stationary variance of e exists and is given by
 	  ,	(28)
 where ?w(z) is a spectral factor of the power spectral density of w. Since Assumption 4 holds, sq2 is not a given constant; indeed, it depends on the variance of v. Proceeding as above (and using the same assumptions), it follows from Figure 3 that
 v = A(z)Twym(z)w - (T(z) + S(z)F(z))q,
 where	(29)
 	£	¤
 	Twym(z) , T(z)	S(z)	-S(z) ,	(30)
 and, accordingly,
 	  .	(31)
 Using (9) and (31) in (28), it follows that
 (32)
 	.	(33)
 We note that MSS of the linear model guarantees that 	0 and, consequently,
  0, as expected.
 We note that, since C(z) is assumed to be given, the choice of the coding parameters (i.e., A(z) and F(z)) affects only the second term in (33), which we denote as
 	 .	(34)
 Accordingly, we can state the problem of interest as follows:
 Problem 1 (Main problem) Given a fixed ? ? (?inf,8), a controller C(z) and a plant G(z) that satisfy Assumption 1, and exogenous signals satisfying Assumption 2, find Jopt defined via
 	Jopt ,	inf	J(A(z),F(z))	(35)
 A(z)?U8
 F(z)?RH2
 ||T(z)+S(z)F(z)||22<?
 and filters A(z) and F(z) that achieve Jopt (or approximate Jopt arbitrarily well).
 We note that all constraints in the formulation of Problem 1 stem from MSS considerations, as discussed in Theorems 1 and 2. We will use the term admissible A(z) (resp. admissible F(z)) to refer to a filter A(z) (resp. F(z)) that satisfies the constraints in Problem 1.
 Problem 1 is non-trivial. Indeed, the much simpler problem of designing A(z) and F(z) so as to minimize the steady state variance of ˆym - ym, when G(z) = C(z) = 0 and dm = r = 0 has been only recently solved exactly (see [17]). This is quite surprising given the fact that feedback quantizers have been studied extensively (see, e.g., [30,50,57]). Unfortunately, the technique employed in [17] does not seem to yield an explicit characterization of the solution in the present situation. Instead of pursuing that line of reasoning here, we will derive an iterative approach that is guaranteed to yield performance that is arbitrarily close to optimum.
 Before describing the proposed design procedure, we note that the following holds:
 Fact 1 (Asymptotic behavior of Jopt) Assume that the conditions of Problem 1 hold. Then:
 1.	If ? ? ?inf, then Jopt ? 8 (unless all exogenous signals have zero spectral density, in which case J(A(z)F(z)) = 0 for every admissible A(z) and F(z)).
 2.	If ? ? 8, then Jopt ? 0.
 Proof:
 1.	By definition of Jopt, we have that . Thus Jopt ? 8 unless either	 = 0. Since A(z) ? U8 and
 Assumption 1 holds, the result follows upon noting that inf 
 2.	Fix A(z) ? U8 and F(z) ? RH2. In these conditions, ? ? 8 ? J ? 0 and hence, Jopt ? 0.
 ¤¤¤
 As a consequence of Fact 1, we will omit from our subsequent presentation an explicit analysis of the cases ? ? 8 or ? ? ?inf. (The reader can easily verify that the results below are consistent with Fact 1 by letting ? ? 8 or ? ? ?inf.)
 5.2	Choosing A(z)
 We begin by showing how to choose A(z), when an admissible F(z) is given. To that end we define, for any given admissible F(z), 
 Jopt1 (F(z)) , inf	J(A(z),F(z)),
 A(z)?U8	(36)
 	 , arg	inf	J(A(z),F(z)).	(37)
 A(z)?U8
 The next theorem characterizes both Jopt1 (F(z)) and 
 Theorem 3 (Optimal A(z) for a given F(z)) Assume that the conditions of Problem 1 hold and consider a fixed admissible F(z). If ?w(z) is not identically zero, then:
 1.	The minimal value of J is given by
  
 2.	The corresponding optimal A(z) satisfies
 	 ,	(39)
 where a is any arbitrary positive real.
 Proof: The definition of the 2-norm allows one to conclude that, for every X(z) ? R n L2, the following identities hold:
 	  .	(40)
 Both (38) and (39) follow using (40) and the Cauchy Schwartz inequality in (34) (note that (39) is always well defined if Assumption 1 holds, and ?w(z) is not identically zero). To complete the proof we note that (39) is a condition on the magnitude of the infimal filter A(z). Thus,  can always be approximated, to any desired degree of accuracy, by a rational filter in U8 as required. ¤¤¤
 Remark 3 Of course, assuming that ?w(z) is not identically zero does not hinder the generality of Theorem 3 (see Part 1 in Fact 1).
 The characterization of ) given by Theorem 3, although explicit, is usually not satisfied by any transfer function in U8. This is due to the fact that, except in very special cases, the 4th root of the right hand side in (39) is irrational. Nevertheless, as mentioned in the proof of Theorem 3, it is always possible to find a filter in U8 that achieves a performance that is as close as desired to
 Jopt1 (F(z)).  In practice, it is usually enough to consider reasonably low order filters to approximate  ) (see also [23]).
 5.3	Choosing F(z)
 In this section we address the problem of choosing F(z) when an admissible A(z) is given. Consistent with the notation introduced before,
 	Jopt2 (A(z)) ,	inf	J(A(z),F(z))
 F(z)?RH2
 ||T(z)+S(z)F(z)||22<?
 denotes the minimal value of J when A(z) ? U8 is fixed. We also define	(41)
 	 , arg	inf	J(A(z),F(z)).	(42)
 F(z)?RH2
 ||T(z)+S(z)F(z)||22<?
 We begin by noting that Jopt2 (A(z)) can be written in a simpler form as follows:
 Fact 2 (Equivalent formulation for Jopt2 (A(z))) Assume that the conditions of Problem 1 hold and consider a fixed A(z) ? U8. Then,
 	 ,	(43)
 where
 	  .	(44)
 Proof: Using the definition of J and the fact that A(z) is fixed, it is immediate to see that
 	 .	(45)
 Define a new real variable, M, constrained to belong to [?inf,?). With this definition, elementary optimization results (see, e.g., Section 4.1.3 in [6]) allow one to write (45) as
 	 ,	(46)
 where we have used the fact that, by definition of ?inf, J2(F(z)) = ?inf for any F(z) ? RH2. The result is now immediate.	¤¤¤
 Fact 2 is key to derive the main result in this section. Namely, a one parameter characterization for Jopt2 (A(z)) and the corresponding optimal F(z). Towards that goal, we begin by considering an auxiliary problem. Define the functional
 L² , ²J1(F(z)) + (1 - ²)J2(F(z)),
 where ² ? [0,1], and	(47)
 	F²(z) , arg	inf	L².	(48)
 F(z)?RH2 We have the following characterization of F²(z):
 Lemma 1 (Solution to auxiliary problem) Consider L² defined in (47) and suppose that Assumption 1 holds.
 1.	If ² ? (0,1), then the infimum in (48) is achievable in RH2 and
 F²(z) = 1 - P²,o(8)P²,o(z)-1,
 where P²,o(z) ? U8 is an outer factor of	(49)
 	 	(50)
 and
 	 ,	(51)
 where m is the relative degree of G(z)C(z), {ci}i?{1,···,n+c } (resp. {pi}i?{1,···,n+p }) is the set of non-minimum phase zeros (resp. unstable poles) of G(z)C(z) that lie strictly outside the unit circle.
 2.	If ² = 0, then
 F0(z) = 1 - (?S(z)S(z))-1 ?S(8)
 and, if ² = 1, then	(52)
 	 .	(53)
 3.	If ² = 0 (resp. ² = 1), then the infimum in (48) is achievable in RH2, if and only if G(z)C(z) has no poles (resp. zeros) on the unit circle.
 Proof:
 1. We will proceed as in the proof of Theorem 2 (see also [11]). As before, we define Q(z) ? RH8 via F(z) , z-1Q(z). It is easy to see from (22) that
 	³	´
 L² = (1 - ²) ?inf + ||z (?S(z)S(z) - ?S(8)) - ?S(z)S(z)Q(z)||22 +
 	¯¯¯¯T(z)A(z)-1 - T(z)A(z)-1z-1Q(z)¯¯¯¯22	(54)
 ²
 Moreover, using the same procedure as in the aforementioned proof, it is also clear that
 	¯¯¯¯T(z)A(z)-1 - T(z)A(z)-1z-1Q(z)¯¯¯¯22	¯¯¯¯z?T(z)T(z)A(z)-1 - z?T(z)T(z)A(z)-1Q(z)¯¯¯¯22
 =
 	= ©? (z)T(z)A(z)-1ª¯¯2	+
 	T	z=8
 ¯¯¯¯z ¡? (z)T(z)A(z)-1 - ©?T(z)T(z)A(z)-1ª¯¯z=8¢ - ?T(z)T(z)A(z)-1Q(z)¯¯¯¯22 , (55) T where we have used the fact that the relative degree and non-minimum phase zeros of T(z) are the relative degree and non-minimum phase zeros of G(z)C(z), that ?T(z) and z are unitary, and that, since A(z) ? U8, ?T(z) is such that ?T(z)T(z)A(z)-1 belongs to RH8, is biproper and has as non minimum phase zeros the zeros on the unit circle of T(z) (i.e., the zeros on the unit circle of G(z)C(z)). From (54) and (55) it follows that
 	 ,	(56)
 where
 	  ,	(57)
 P²(z) (defined in (50)) belongs to RH8, is biproper, and
 	 .	(58)
 Define the unitary matrix
 	  ,	(59)
 where P²,i(z) is an inner factor of P²(z) and P²,o(z) is the corresponding outer factor. We note that, since Assumption 1 holds, P²(z) has no zeros on the unit circle for ² ? (0,1). Thus, for those values of ², P²,o(z) ? U8.
 Pre-multiplying the argument of  in (57) by f(z) it is immediate to see that
 	  .	(60)
 A straightforward calculation shows that	
 P²,i(z)~W(z) = zP²,o(z) - zP²,i(z)~P²(8).
 Therefore, orthogonal decompositions as those employed before allow one to write
  	(61)
   .	(62)
 Since P²,o(z) ? U8 the result follows.
 2.	If ² ? {0,1}, then (54) and (55) yield immediately the results.
 3.	The result follows upon noting that, by definition of ?T(z) and ?S(z), (?S(z)S(z))-1 (resp. (?T(z)T(z)A(z)-1)-1) belongs to RH8 if and only if G(z)C(z) has no poles on the unit circle (resp. zeros on the unit circle).
 ¤¤¤
 The characterization of F²(z) given in Lemma 1 plays an essential role in our subsequent discussion. It is worth mentioning that the only critical step when calculating F²(z) is the inner-outer factorization of P²(z). Since P²(z) has no zeros at infinity (i.e., P²(z) is biproper), this factorization can be made with the aid of standard algorithms (see, e.g., [20,46]).
 The next theorem provides a characterization of the optimal F(z) in terms of F²(z).
 Theorem 4 (Optimal F(z) for a fixed A(z)) Assume that the conditions of Problem 1 hold and consider a fixed A(z) ? U8. Then,
 (63)
 	,	(64)
 where 
 ²* , arg min	.	(65) ²	²	2	²
 In (65), ²ˆ is defined as follows: If there does not exist ² ? (0,1] such that J2(F²?(z)) = ?, then ²ˆ= 1. Otherwise, ²ˆ= ²?, where ²? is the unique real in (0,1] such that J2(F²?(z)) = ?.
 Proof: We will use the alternative formulation for J in Fact 2.
 1. We first show how to solve an auxiliary problem related to the inner optimization problem in
 (43). Consider the problem:
 	inf	J1(F(z)).	(66)
 F(z)?RH2
 J2(F(z))=M
 The well-known KKT conditions for this problem (see, e.g., [6,35]) allow one to conclude that the optimal F(z), say Faux(z), (if it exists) is a critical point of ?1J1(F(z))+?2J2(F(z)), where ?1 + ?2 > 0, ?1,?2 = 0 and, moreover, ?2(J2(Faux(z)) - M) = 0. It is immediate to see that this is equivalent to saying that Faux(z) is a critical point of L² (see (47)), with ² ? [0,1] and (1 - ²)(J2(Faux(z)) - M) = 0.
 L² is a strictly convex functional (and so are J1 and J2). Hence, it has an unique critical point given by F²(z) (see (48)). Moreover, the set of points in the J1 versus J2 plane defined by F²(z), when ² ranges from zero to one, is the set of Pareto optimal points of the multi-objective problem of minimizing simultaneously J1 and J2 (see, e.g., [6,15]). This set is a (strictly) convex and decreasing function when J1 is seen as function of J2. Clearly, the Pareto optimal point corresponding to ² = 1 (resp. ² = 0) is such that J1 is minimum (resp. J2 is minimum). Thus, by definition of Pareto optimal point, the minimum of J1, when J2 = M is achieved when J2 = M, provided J2(F1(z)) = M. If J2(F1(z)) < M, then the minimum J1 is achieved when J2 = J2(F1(z)). As a consequence, the optimal solution of the auxiliary problem is given by Faux(z) = F²M(z), where, provided J2(F1(z)) = M, ²M belongs to [0,1] and is such that J2(F²M(z)) = M (note that convexity ensures that, in this case, ²M is unique). On the other hand, if J2(F1(z)) < M, then ²M = 1.
 2. We next show how to exploit the above reasoning to prove the result. We first note that, since F1(z) optimizes J1, it is of no use to consider values of M such that J2(F1(z)) < M (note also that M ? ?inf is optimal for the first optimization problem in (43) and, accordingly, constraining J2(F1(z)) to be greater than or equal to M does not impede the minimization of J). Thus, Fopt(z) satisfies
 	Fopt(z) = arg	 inf	,	(67)
 Using Part 1 it follows that Fopt(z) = F²M*(z) with M* such that
 	M* = arg	 inf	,	(68)
 M=J2(F1(z))
 where ²M is guaranteed to exist in [0,1] (and to be unique). A key feature of this problem is that the Pareto optimal points of the auxiliary problem considered in Part 1 do not depend on M. Thus, varying M in [?inf,min{?,J2(F1(z))}] is equivalent to just varying ²M in [0,min{1,²?}] (if ²? [defined in the body of this Theorem] does not exist, then pick ²? = 1). It should be clear that the structure of the problem is such that M* ? (?inf,min{?,J2(F1(z))}). Thus, it suffices to consider ²M ? (0,min{1,²?}). As a consequence, the result follows.
 ¤¤¤
 Theorem 4 provides a one parameter characterization of the optimal F(z) and the corresponding minimal cost Jopt2 (A(z)), for any admissible A(z). The scalar parameter ²* can be found using any standard line search procedure and, as such, its calculation embodies no additional difficulties. This is reinforced by the fact that the search for ²* is made over (0,1) (and that ²* actually exists in (0,1)) which is precisely the range of values of ² for which F²(z) is always defined in RH2 (see Lemma 1).
 5.4	Design procedure and final remarks
 In this section we show how to use the results in Sections 5.2 and 5.3 to design a feedback coding system in an iterative fashion. Of course, one can always choose to fix one of the coder filters (trivial choices are A(z) = 1 or F(z) = 0) and then use Theorem 3 or 4 to design the free parameter. Obviously, this choice will limit the achievable performance. To exploit the full potential of a feedback coding system we suggest that one uses an iterative algorithm such as the following:
 Algorithm 1 (Iterative design procedure) For a given plant and controller satisfying Assumption 1, and a given ? > ?inf, proceed as follows:
 •	Initialization:
 –	Pick a tolerance ? > 0, a transfer function A0(z) ? U8 and a transfer function F0(z) ? RH2 that is admissible.
 –	Set A(z) = A0(z), F(z) = F0(z). Set V (0) = J(A(z),F(z)), fix A(z) (or, alternatively, fix F(z)) and set k = 0.
 •	Repeat ((k + 1)th iteration):
 –	Set k = k + 1.
 –	If at the (k - 1)th iteration A(z) was fixed, then use Theorem 4 to obtain . Set   and V (k) = J(A(z),F(z)). Fix F(z).
 –	If at the (k - 1)th iteration F(z) was fixed, then use Theorem 3 to obtain  . Set   and V (k) = J(A(z),F(z)). Fix A(z).
 •	Until: |V (k) - V (k - 1)|V (k - 1)-1 < ?.
 It should be clear that it is not certain that Algorithm 1 will converge to the global minimum of J. Nevertheless, it is easy to see that, by definition of  and , the algorithm reduces the value of J at each iteration. Therefore, Algorithm 1 converges, necessarily, to a local minimum. Thus, we suggest to use multiple starting points so as to find the global minimum. A procedure for getting a good starting point is mentioned below.
 In general,  = 1 and  = 0. Thus, fixing A(z) or F(z) and optimally choosing the other filter, will obviously provide a coding system that enhances closed loop performance when compared with a non-coded networked situation.  It is also clear that the use of Algorithm 1 allows one to design coding systems that will always outperform coding systems that have been designed using the guidelines in our earlier work described in [23]. This is a consequence of the fact that [23] constrains F(z) to be identically zero.
 A more interesting discussion arises if one compares the results in this paper with the results in [53].
 In the latter work, it is assumed that ? is sufficiently high so as to be able to approximate J in (34) by
 	 .	(69)
 In order to minimize J8 it suffices to choose A(z) so as to minimizeand, in
 ¯¯
 a second stage, to choose F(z) as the minimizer of [53]. A problem with the above approach is that deciding, a priori, which ?’s are high enough seems to be impossible. In particular, since the procedure in [53] does not take the constraint  explicitly into account, the proposed choice for F(z) may be not admissible or may be such that J(A(z),F(z)) ? 8. (Needless to say, this drawback is explicitly avoided in the current paper.) It is also clear that choosing A(z) as in [53] and, then, using Theorem 4 to choose F(z) will always lead to a feedback coder that achieves a tracking error variance that is lower than the one achieved by the filters proposed in [53]. Of course, if the filters suggested in [53] are feasible, then they may provide a good starting point for Algorithm 1.
 6	Design Example
 This section documents a design study that illustrates the results in this paper. We consider a very simple case that, nevertheless, will allow us to present the main features of our proposal.
 We consider a nominal loop with plant and controller given by
 	 .	(70)
 The measurement noise and output disturbance are assumed zero, whilst the reference is considered to have a power spectral density with spectral factor
 	 .	(71)
 The quantizer loading factor is fixed at 4 in all cases,  and the number of quantization levels, L , 2b, ranges between L = 21 and L = 28. Since we do not consider the use of channel coding schemes (e.g., entropy coding; see [9,14]), b corresponds to the rate at which data is sent through the channel (in
 [bit/sample]).
 Figure 4 shows the steady state tracking error variance se2 (see (33)) as a function of the number of iterations in Algorithm 1 for two representative values of the quantizer signal-to-noise ratio: ? = 1.6875 and ? = 9.1875, which correspond to b = 2 and b = 3, respectively. Cases 1 and 2 refer to iterations that start with A0(z) = 1 and F0(z) = 0. In Case 1 we initially fixed A(z), whereas in Case 2 we start fixing F(z). Case 3 refers to iterations that start with the choices suggested in [53]. We note that ? has to be greater than 4.42 in order for the proposal in [53] to be admissible. (Accordingly, we omitted
  
 Figure 4: Tracking error as function of the number of iterations in Algorithm 1 (see text for details).
 Case 3 in Figure 4 when ? = 1.6875.) It can be seen that rapid convergence of Algorithm 1 occurs and, more interestingly, that the limiting performance does not depend on the order in which the filters are calculated or on the initial condition. Thus, local minima related issues do not seem to play a role in this example.
 In Figure 4 we have identified three points. The first of these (point (1)) refers to the performance achieved without coding (F(z) = 0 and A(z) = 1). The second (point (2)) refers to the performance achieved when employing the optimal coding system proposed in [23]. The third (point (3)) refers to the performance achieved using the approximately optimal filters described in [53].
 The results show that coding is, indeed, necessary to achieve the best possible loop performance. (Compare point (1) with, e.g., the value of se2 for 10 iterations.) It is also possible to see that use of Algorithm 1 yields coding systems that perform better than our previous proposals in [23,53], which is consistent with the discussion at the end of Section 5.4. (Compare points (2) and (3) with the limiting value for se2.) It is also interesting to mention that, for b > 4, the performance provided by the filters in [53] is substantially closer to the limiting value of se2 than the case shown in Figure 4. This suggest, as mentioned before, that the filters in [53], when feasible, provide good starting points for the iterative procedure proposed here.
 We end this section by studying the behavior of the tracking error variance as a function of the channel bit rate b. The results are presented in Figure 5, where “Nominal performance” refers to the performance achieved by the nominal loop (without quantization), “No coding (empirical)” refers to simulated results  when no coding is employed (i.e., when A(z) = 1 and F(z) = 0), “Opt. coding (empirical)” refers to simulated results obtained with the filters suggested by Algorithm 1 (after 10 iterations), and “Opt. coding (analytical)” refers to the corresponding predictions made using the simplified noise model for quantization. One can see that, as expected, the effects of quantization vanish as b ? 8. Interestingly, the predictions made using our model turn out to be very accurate for every bit rate: indeed, for b = 3 the relative errors are of less than 1% and, for b ? {1,2}, the relative errors are around 8%. (We note that F(z) = 0 turns out to be non-admissible for b = 1. Accordingly,
  
 Figure 5: Tracking error as function of the channel bit rate.
 we have omitted the non coded results for b = 1.)
 7	Conclusions
 This paper has presented a methodology to design feedback quantizers that encode plant output measurements in a networked control situation employing data-rate limited channels. Using a fixed signal-to-noise ratio additive noise model for quantization, we have shown how to iteratively design the parameters of a feedback coding system so as to minimize the impact of quantization on the closed loop tracking error. Our results show that feedback quantization schemes are beneficial when compared to simpler schemes documented in the literature. An interesting by-product of our results lies in the characterization of the smallest quantizer signal-to-noise ratio compatible with stabilization. We have shown that, for a given quantizer signal-to-noise ratio, the class of plants that are stabilizable when feedback coding is employed is significatively larger than the class of plants that are stabilizable when no coding is used. This result opens the door to investigating other LTI control and feedback coding architectures and the associated signal-to-noise ratio requirements.
 A very interesting extension of the present work lies in addressing multiple-input multiple-output problems. In that case, it is worth exploring how networked architectures may help overcoming the well-known performance limitations that arise when constraining the structure of the controller (see, e.g., [29,52]). A second immediate extension lies in the problem of joint controller and coder design. The study of how to apply similar ideas to the case of channels prone to data loss is also interesting (see preliminary work in [54]).
 A	Appendix
 Consider a set X and a function J defined on X ? Xˆ (and extensible to Xˆ). If infX?X J(X) exists and infX?X J(X) is achievable in X, i.e., if ? X* ? X such that J(X*) = infX?X J(X), then Xopt , arginfX?X J(X) = X*. On the contrary, if @ X* ? X such that J(X*) = infX?X J(X), then Xopt defined as above should be understood as Xopt = limn?8 Xn, where {Xn}n?N is a sequence in X (whose limit belongs to Xˆ) such that limn?8 J(Xn) ? infX?X J(X). Therefore, if we write Xopt = Xˆ and Xˆ 6? X, it is implicit that one can find a sequence {Xn}n?N as above. In these cases, it is clear that one can always pick an X ? X such that J(X) is as close to infX?X J(X) as desired.
 ﻿The characterization of `p-compressible random sequences is revisited and extended to the case of stationary and ergodic processes. The main result of this work offers a simpleto-check necessary and sufficient condition for a stationary and ergodic sequence to be `p-compressible in the sense proposed by Amini, Unser and Marvasti [1, Def. 6]. Furthermore, for non `p-compressible random sequences, we provide a closed-form expression for the best k-term relative approximation error given a rate of coefficients as the block-length tends to infinity. 
 Defining notions of compressibility for a stochastic process, meaning that every realization can be well-approximated in some sense by its best k-term sparse version [2], has been a topic of recent interest [1], [3], [4]. Quantifying compressibility for random sequences can play an important role in regression, reconstruction (for instance in the classical compressed sensing analysis and reconstruction setting [3, Th. 2]), inference and decision-making problems. One important case is defining such a compressibility notion for i.i.d. processes where the probability is equipped with a density function. In this context, every realization of the process is clearly non-sparse (almost-surely), and conventional ways of defining compressibility for finite dimensional signals, based on the power-law decay of the best k-term approximation error (or sequences that belong to the weak-`p ball), are not applicable either, as shown in [1], [3]. 
 Motivated by this, Amini et al. [1] and Gribonval et al. [3] have recently introduced new definitions for compressible random sequences. These notions are not based on the typical absolute approximation error decay pattern of the signals, but on a relative `p-best k-term approximation error behavior. In particular, Amini et al. [1] formally define the concept of `p-compressible process (details in Section II below). This new definition provides a meaningful way of categorizing i.i.d. random sequences (or distributions) in terms of the probability that almost all the `p-relative energy of the process is concentrated in an arbitrarily small sub-dimension of the coordinate (innovation) domain, as the block-length tends to infinity. Under this umbrella, they provide two important results using the theory of order statistics [1]. On the one hand, [1, Theorem 3] shows that a concrete family of i.i.d. heavytail distributions is `p-compressible (including the generalized Pareto, students‘s t and log-logistic), while on the other side, 
 [1, Theorem 1] demonstrates that families with exponentially decaying tails (such as Gaussian, Laplace, Generalized Gaussian) are not `p-compressible. Therefore, it is interesting to ask about the compressibility of i.i.d processes not considered in this analysis. In this direction, we highlight the work of Gribonval et al. [3], which under an alternative notion of relative `p-compressibility (involving almost sure convergence instead of convergence in measure criterion adopted in [1]) and a different analysis setting (fixed-rate instead of the variable rate used in [1]), elaborates an exact dichotomy between compressible and non-compressible i.i.d. sequences. Therefore, it is an interesting direction to connect Amini et al. [1] `pcompressibility with the more refined almost sure convergence analysis of the `p best k-term relative approximation error in [3, Prop. 1], with the idea of completing the analysis of [1, Ths. 1 and 3]. 
 To address this question, as well as to extend the analysis to more general random sequences, this work precisely categorizes all `p-compressible random sequences, in the sense of Amini et al.[1], within the family of stationary and ergodic processes [5]. Our main result (Theorem 1) offers a necessary and sufficient condition for an ergodic process to be `p-compressible, for any arbitrary p > 0. Furthermore, for the case of non `p-compressible ergodic processes, we provide a closed-form expression for an achievable rate v/s `p-approximation error function. The key element in the proof is the application of the ergodic theorem [5] and the derivation of intermediate almost-sure convergence results (Lemma 1 and 2 in Section IV) that match the approximation result presented by Gribonval et al. [3, Prop. 1] developed for the i.i.d. case. A corollary of Theorem 1 implies a necessary and sufficient condition to categorize i.i.d. random sequences in terms of `pcompressibility, which completes the analysis presented in [1, Ths. 1 and 3]. 
 For a finite-dimensional vector xn = (x1,..,xn) in Rn, let (xn,1,..,xn,n) ? Rn denote the ordered vector such that 
 denote the `p-norm of the best k-term approximation of xn, where by definition ||xn||`p = ?p(n,xn). In addition, 
 denotes the best k-term `p-approximation error of xn, in the sense that if   is the collection of k-sparse signals, then sp(k,xn) is the solution of minx˜n?Skn ||xn - x˜n||`p. For the analysis of infinite sequences, Amini et al. [1] and Gribonval et al. [3] proposed the following relative best k-term `p-distortion indicator: 
 From this, Amini et al. [1] introduced a notion of critical dimension (for a finite-length signal) and a notion of `pcompressibility for infinite sequences. 
 Definition 1: [1, Def. 4] For xn ? Rn and d ? (0,1), let us define ?p(d,xn) = min{k ? {1,..,n} : s˜p(k,xn) = d}. (4) 
 Let X1,..,Xn,.. be a random sequence with values in (R,B(R)) and characterized by its consistent family of finite-dimensional probabilities {µn ? P(Rn) : n = 1} [5], where Xn = (X1,..,Xn) ~ µn and P(Rn) denotes the space of probability measures for the Borel measurable space (Rn,B(Rn)). As a short-hand, we denote by P = 
 {µn : n = 1} the process distribution of (Xn)n?N. Let us define the measurable set An,kd	= 
 where the last equality is by (4). Then in analogy with Definition 1, Amini et al. [1] proposed the following: 
 is the critical number of terms with which the set typical with respect to µn. With this, the process (Xn)n?N (and P, respectively) is said to be `p-compressible, if , ?d ? (0,1), 
 We say that the rate-distortion pair (r,d) is `p-achievable for (Xn) with  probability, if there exists a sequence of positive integers (kn) such that  and 
 Definition 4: The rate-distortion approximation function of (Xn), with  probability is given by: 
 In the next section, we will study the class of stationary and ergodic processes [5], where the best k-term approximation properties measured in terms of the function  in (10) will be characterized in closed-form. Furthermore, it will be shown for this class of random sequences that 
 Let (Xn)n?N be a stationary and ergodic sequence (process) with distribution P = {µn : n = 1}, where we denote by µ ? 
 P(R) its shift-invariant distribution [5]. For simplicity , we assume that  where ? denotes de Lebesgue measure 
 [5]. Then µ is equipped with a probability density function (pdf) and dµ(x) = dµ d?(x)d?(x). 
 For a measure v on (R,B(R)), we say that a measurable function f : (R,B(R)) -? (R,B(R)) is integrable with respect to v if [5] 
 THEOREM 1: Let (Xn)n?N be a stationary and ergodic process with marginal shift-invariant distribution µ ? P(R) such that , and let us consider an arbitrary p > 0. Then we have the following dichotomy: 
 ii)	If fp(x) ? L1(µ): then (Xn)n?N is not `p-compressible. Furthermore, if we introduce the induced probability measure in (R,B(R)) by: 
 1)	Theorem 1 offers a necessary and sufficient condition for a stationary and ergodic process to be `p-compressible in the sense elaborated in Definition 2. 
 2)	In the case of non `p-compressible processes, i.e., when fp(x) ? L1(µ), Theorem 1 offers, what we call, the achievable rate-distortion region for the process, given by the set of critical rate-distortion pairs: 
 This region depends solely on the shift invariant measure µ ? P(R) and its induced measure vp ? P(R) in (13). 
 3)	In both scenarios i) and ii), the critical rate  for a stationary and ergodic processes is independent of . The justification is that asymptotically as n goes to infinity, the characterization of  implies to compute probabilities on events that belong to the tail s-field of the process, which is known to be trivial (i.e., their events have zero or one probability) for the case of ergodic processes [5]. Therefore, we obtain P-almost-sure convergence results that make irrelevant the role of  in the characterization of  (see Section 
 4)	A natural order among stationary and ergodic process can established from Theorem 1. 
 Proposition 1: If (Xn)n?N is `p-compressible for some p > 0, then (Xn)n?N is `q-compressible for all q = p. Proof: If fp(x) ?/ L1(µ), then fq(x) ?/ L1(µ) for all q = p . 
 Proposition 2: If (Xn)n?N is not `p-compressible for p > 0 then (Xn)n?N is not `q-compressible for all q = p. Proof: If fp(x) ? L1(µ), then fq(x) ? L1(µ) for all q = p. 
 5)	For the emblematic i.i.d. scenario , we want to highlight in more detail the results by Amini et al. [1] related to `1compressibility in the sense of (8). [1, Theorem 1] says that if µ is such that for some ? < 0, EX~µ(e?·X) < 8 then the i.i.d. process is not `1-compressible. On the other hand, [1, Theorem 3] says that if µ belongs to the domain of attraction of a-stable distribution [5, Chap. 9.11, pp. 207-213] with a ? (0,1), then the process is `1-compressible. First for p = 1, Theorem 1 provides a refined result, revealing a richer (indeed, the complete) family of distributions that are not `1-compressible. In fact, in addition to distributions that go to zero exponentially and consequently f1(x) ? L1(µ) (Gaussian, Laplacian, Gamma, etc.), heavy tail distributions whose density functions are lower dominated by a power-law decay of the form   with q > 1 (for instances, student‘s t with q degrees of freedom), are not `1-compressible either3. On the other hand, concerning [1, Th. 3], it is simple to verify that any µ that is in the domain of attraction of an a-stable law with a < 1 [5, Ch.9] satisfies that EX~µ(|X|) = 8 and consequently, part i) of Theorem 1 covers this family of `1-compressible i.i.d. processes. 
 Corollary 1: Let (Xn)n?N be a stationary and ergodic process. If ?? > 0 where EX1~µ(e-?X) < 8, then (Xn)n?N is not `p-compressible for any p > 0. 
 Corollary 2: Let (Xn)n?N be an ergodic process with invariant distribution   and density fµ(x) =  dµd?(x), ?x ? R. If fµ(x) decays as |x|-(t+1) for some t > 04, then (Xn)n?N. is lp-compressible, if and only if, p = t. 
 7)	For the proof of Theorem 1, we derive almost-sure convergence results (see Lemma 1 and 2 in Section IV). In the case when fp(x) ? L1(µ): if (kn) is such that 
 for some t > 0, it follows that  , P-a.s.. In the case when , then limn?8 s˜p(kn,Xn) = 0, P - a.s. These results are consistent and extend the result by Gribonval et al. [3, Prop. 1], which for the i.i.d. case shows the same almostsure convergence limit for the object s˜p(kn,Xn). Their proof was based on the Wald‘s lemma of order statistics (see details in [3, Th. 6]), while ours is based on the application of the ergodic theorem. 
 then the latter also holds for all  and for all d ? (0,1). Likewise, if for some , for some r ? (0,1) and for all d ? (0,1) the pair (r,d) is `p-achievable for (Xn) with  probability, then all pairs (¯r,d) with r¯ ? (0,1) and d ? (0,1) are `p-achievable for   probability. 
 Proof: Considering  as a short-hand, by definition in (7)  , where from (10) it follows that  . 
 3µ is lower dominated by g(x), if there exits x0 > 0 and C0 > 0 such that for any x such that |x| > xo then fµ(x) = C0 · g(x). 
 Let us first consider the case when fp(x) ? L1(µ). For the rest, it is important to note that given that , then for all r ? (0,1) there exists t > 0 such that µ(Bt) = r, and for all 
 d ? (0,1) there exists t > 0 such that pp 1 - vp(Bt) = d. 5 For (X1,..,Xn) ~ µn, we can define: 
 The second almost sure convergence is from the assumption that fp(x) ? L1(µ). Then, we can state the following: 
 In order to prove (14), let us fix d ? (0,1). Then there exists t, such that (15) holds and from Lemma 1 if (kn)n is such that  , then limn?8 s˜p(kn,Xn) = d, P - a.s.. 
 Let us consider an arbitrary t < t˜ such that µ(Bt˜) > µ(Bt), then again from Lemma 1, if a sequence (k˜n) is such that knn ? µ(Bt˜), then limn?8 s˜p(k˜n,Xn) = 
 pp 1 - vp(Bt˜) < d, P - a.s.. Consequently, s˜p(k˜n,Xn) convergences almost surely to a distortion strictly less than d, and then for all : 
 Hence from the definition of  in (7), we have that   eventually in n, which implies that 
 5In general, the achievability condition on t > 0 for the rate (i.e., ?r ? (0,1) ?t > 0 such that µ(Bt) = r) and the distortion (?d ? (0,1) ?t > 0 
 The first inequality from Proposition 3 and the last equality from the fact that the function fµ(t) = µ(Bt) is continuous as . 
 To derive a lower bound, let us consider an arbitrary r˜ ? (0,µ(Bt)). We know that there exists Bt˜ with t > t˜ such that µ(Bt˜) = r˜. Again from Lemma 3, for all (k¯n) such that 
 On the other hand, from (26), we have that it is necessary that . This last inequality and (27) are valid for any r˜ ? (0,µ(Bt)), then   and µ(Bt) = 
 In	other	words,	(29)	means	that limn?8 s˜p(n(Xn,Bt),Xn) = 0, P - a.s. Furthermore, we have the following: 
 kn eventually in n. From this,  for all r > 0, which concludes the result from Proposition 3. 
 The work of J. F. Silva is supported by awards of CONICYT Fondecyt Grant 1140840 and the CONICYT Basal Grant AMTC. The work of M. S. Derpich is supported by CONICYT Fondecyt Grant 1140384, and UTFSM internal research grant 
 such that pp 1 - vp(Bt) = d for some t > 0. Let us take an arbitrary (xn)n ? At n Bt. From the zero-rate assumption on (kn) and the definition of At, ?N > 0 such that ?n = N, kn < n(xn,Bt), which implies that s˜p(n(xn,Bt),xn) < s˜p(kn,xn). Therefore considering that (xn) ? Bt, 
 Doing the same process for the collection  , we have that ?(xn) ? Tm=1(A m n B m), 
 Then from the sigma additivity P(Tm=1 Atm n Btm) = 1, which proves the result in (32). 
 Equipped with this result, for an arbitrary r ? (0,1) let us consider (kn) such that limn kn/n = r. We know that there exists to > 0 such that r = µ(Bto), and for this to we consider Ato and Bto as defined in (33). Then for any (xn) ? AtonBto it follows that 
 Furthermore, from definition of the ordered sequence, it is simple to verify that (see Eq.(1)): 
 if we consider (xn)n?N ? (Ato n Bto) n Tm=1 Atm n Btm, from (38), (36) and the fact that |kn - n(xn,Bto)| is o(1) in 
 the last equality in (39) from definition of Bto in (33). Finally from (19) and (20), 
 let t > 0 be such that pp 1 - vp(Bt) = d. Let us consider an arbitrary t > t¯ where µ(Bt¯) < µ(Bt) and consequently vp(Bt¯) < vp(Bt) (µ and vp are mutually absolutely continuous). Again for this t¯, we use the sets in (33), where for all  , it follows that   such that  . 
 The first equality follows from the continuity of the function fµ(t) = µ(Bt) with respect to  . Then from the fact that P(Ttm?C Atm n Btm) = 1, we have that, µ(Bt) = liminfn?8 ?p(d,Xn)/n, P-a.s. Finally, proving that limsupn?8 ?p(d,Xn)/n = µ(Bt) P-a.s. follows an equivalent symmetric argument and we omit it. 
 Proof: For r > 0 let us consider r¯ ? (0,r) and t > 0, such that µ(Bt) = r¯ and (kn) such that  . Considering the sets   and 
 that P(At n Bt) = 1 from (28) and (29). Let us fix an arbitrary (xn) ? At n Bt. Considering that r < r¯ , then   eventually in n, and therefore   eventually, which implies 
 from definition of Bt that limn?8 s˜p(kn,xn) = 0. The fact the event At n Bt happens P-almost surely concludes the result. 
 ﻿ In this work we present an empirical study of the added propagation that may be associated with providing fixed wireless service from near ground base to in a suburban environment . We present for various of , classified according to the existence of in the propagation path and the choice of outdoor outdoor or outdoor indoor service . Our indicate that while on average the additional path associated with lowering the base antenna are relatively small , the variance of these will increase at near ground level , particularly in links . This as a result that the power margin for high availability of a near ground base antenna may be quite significant . 
 Index Channel modeling , fading , fixed wireless service , near ground propagation , path loss . 
 I . INTRODUCTION 
 T 
 HE explosive growth in wireless demand an intense interest in diverse deployment . New , eager to participate in this very attractive market are considering various , suburban residential where electricity , telephone , cable are being from underground , without the use of . This the need to also evaluate the effectiveness of providing wireless service from outdoor near ground bases . We validate the applicability of well known path loss and small scale fading , which will be useful when designing such . To the best of our knowledge , such have not been in the open literature for this type of environment . Most with near ground bases less than height deal with urban or correspond to sensor in open or forested , as will be in detail below . Suburban have received less attention . Furthermore , as stated in , there is a lack of on the characterization of outdoor indoor wireless links , which are important for voice data transmission and wireless local area . The statistical description of the variation in the channel model , associated with lowering the base station will be useful for a system designer , considering that most currently available data to service provided by bases that are at least at lamp post height , where well established propagation apply . The scenario that is the objective of our study quite significantly from those that have been in the literature before . Therefore the characterization of this new propagation scenario cannot be based on currently known . 
 A thorough bibliographical search of work related to the subject led us to the relevant into . Each category with one specific aspect covered in our study : outdoor indoor propagation , near ground propagation , the effect of vegetation , and path loss . We discuss this classification in what , only those that specifically relate them to our work . 
 Outdoor Indoor Propagation : Various such as and therein have this subject for that in some are similar to ours . However in contrast to our study , in all of them the was at of at least . , while in some , the altitude of the subscriber unit was varied , to not lower than . . We note that due to the considerable difference between the scattering surrounding an outdoor and those in proximity of an indoor , lowering the altitude of the latter cannot be to have the same effect as that of lowering the . However some are similar to those in our work , such as a range of fade for the lower altitude . 
  
 Near Ground Propagation : This topic been in various , many of which are centered on modeling electromagnetic field propagation , such as . Here the scope is sensor in indoor with both at less than a height and link below . The analytical and empirical address very specific propagation , which can be analytically from electromagnetic and do not lend themselves to the derivation of statistical . Conversely , our study a wide range of of such complexity that they cannot be analytically and instead require statistical such as those in , . Propagation for sensor in an office environment are in , again a setting too far removed from ours to extrapolate valid . for outdoor sensor network propagation with both at low altitude over diverse of open have been in and . In and path for urban low altitude bases are in the context of military . The effect of lowering the subscriber terminal from an altitude of to is in , however this scenario is very different from our application . Nevertheless , among the is the reduction of the factor with antenna altitude , a result to be with that in our work . 
 Effect of Vegetation : Another element that the signal attenuation between base and user is vegetation . Theoretical the effect of these have been in . The effect of vegetation on frequency dispersion was in based on various simulation . Only some of the available theoretical for the attenuation by vegetation have been through empirical data . None of them have been for the residential studied here , but some relate to certain present in our study . In military are considered with bases at exceeding . and links that can involve forested . In this work a decrease in antenna height is also to reduce the factor . Empirical in , , which involve various of vegetation , show excess path between and exceeding . 
 Path Loss : With regard to the statistical description of path loss , single and slope log distance that consider shadow and small scale fading have typically been used . This is also the approach chosen for this work . In the case of outdoor propagation , the two ray model is often considered as a reference , . The presence of a break point leading to the dual slope path loss model been extensively , , . The use of these and the effect of lowering the antenna in a microcellular setting is in , however the considered range between . and . and were carried out on streets rather than in outdoor indoor . 
 The provide a background with regard to methodology and previous theoretical and empirical on modeling propagation links that may involve a terminal at low altitude . As , they do not specifically address the problem that is the objective of our study , namely to draw regarding the effect of providing suburban outdoor outdoor or outdoor indoor residential wireless service from low altitude bases , in contrast to doing so from a conventional height . We here consider both outdoor outdoor and outdoor indoor links , with and without the presence of vegetation . These will be useful when fixed wireless service to suburban , which may also be as a few adjacent in an system . In such of course , the relation between coherence for the specific environment and the transmission will also need to be considered in order to take into account possible fade reduction through frequency diversity . 
 We found that the additional propagation resulting from lowering the base antenna from . to . was not than at the median level , but may grow to . when availability is . We also quantify penetration when serving indoor from the outdoor antenna , and power by vegetation within the link . The latter are relevant when the lowering of the base antenna such an obstruction to the link . The measurement campaign was carried at . a transmitter and a power measuring receiver . The choice of frequency is based on the fact that it been used for fixed wireless service such as . A total of different links were tested in residential urban . For each link , the distance between base and subscriber ranged from to . Two large scale propagation were used as a comparison basis to our empirical model : equation and the two ray model . 
 The remainder of this paper is organized as : Section the measurement hardware and methodology . Section the statistical based on our empirical data . Finally , Section the . 
 . MEASUREMENT EQUIPMENT , AND 
 PROCEDURE 
 The channel sounding system employed of a continuous wave transmitter and a purpose built receiver coupled to a power meter . The receiver is . This is wide enough to capture any frequency dispersion that the transmission , induced for example by movement of vegetation . This receiver was connected to a rotating arm of . length and stepwise under computer control in a displacement of approximately . At each of the angular , consecutive power were made . This consistency and to remove residual temporal . These were found to be very small typically less than . . The resulting was used as the power sample at that range and angular position . At each placement of the rotating arm , the power in a rotation were used to calculate the spatially path loss at that range and to generate the statistics of small scale with respect to that average . The size of the region for the spatial average is well within the shadow fade correlation distance in previous work . In fact the in the literature , corresponding to indoor , are in the range of to , the for outdoor suburban being much , . 
 The operating frequency was . . The and used dipole and patch depending on the type of scenario tested . When the angular spread of in a environment is comparable to the antenna , the received power and thus the calculated will be affected by the antenna gain pattern . For this reason , in the we chose that may be considered representative for the type of service at the specific scenario , as will be detailed later . In the case of the nominally , we took into account in our the minor gain less than over the azimuth range through of the measured antenna pattern . This is in correspondence with the fact that our involve a rotation . In the case where we used patch , in our path loss we considered the measured gain of the antenna in the direction of the direct arrival path . All were measured in an chamber , and measurement were used to ensure repeatability . 
 Before at each environment , two calibration were . First , the transmitter and receiver system were connected back to back with a short , cable and a step attenuator , the , to verify the received power . Next , the transmitter and receiver were connected to the system and a free space calibration was in an open area with the extended . above ground and by . The were to the design of the equipment to confirm its proper operation and to verify the dynamic range for reliable . In all field the received power was at least above the noise floor . were carried out at residential type , representative of a suburban environment where the base station could be at the street curb , at a range of to from a one or two story house . This space was by with and no higher than . We tested suburban in and a Mar , Chile , and on the campus of 
 Mar ´ in ´ iso , which with garden like vegetation outside of ground level . At each of these , multiple were carried out for similar : line of sight : ; outdoor outdoor outdoor indoor . The construction is of the brick and mortar type , with non of size as below . 
 All were made by first the antenna that the wireless service base station at . and then at . at exactly the same spot . This the effect of reducing the antenna altitude . The choice of . as reference height was based on the fact that at such altitude , non links exhibit practically path at the measured , thus providing a convenient reference value . Further in height would have added complexity without providing novel information . 
 The measurement procedure at each type of setting was as : 
 outdoor outdoor : In this case , we emulate the terminal by the rotating arm the signal outside the home at from the residence , with link in the range . The in this case is designed to represent a customer terminal mounted on the wall of the residence , being by the . The was in close proximity to an outside wall at an altitude of about . . It was close to a window that would subsequently allow outdoor indoor through that same window without otherwise the setting . In this case , the used a patch antenna with a maximum gain of , typical for a wall mounted element . This was at three different along the wall for each link length tested to measure at nominally similar 
  
 Fig . . Measurement configuration for the outdoor outdoor case . 
  
 Fig . . Measurement configuration for the outdoor indoor case . 
 . The on the rotating arm , whose height was varied from . to . employed a dipole . The setup is in Fig . . 
 outdoor indoor : The setup is shown in Fig . . In this case , the terminal was by the rotating arm inside the construction across a window with respect to the outside the signal . Link ranged from to . The height of the antenna was . , at that would in practice correspond to the possible of indoor or of user . The indoor floor in Fig . was between . and . above ground . The antenna was outside of the residence at adequate for base station , again with of . and . . In this case , both used were . Window sizes varied in width in the range of . to . and in height from . to . . The minimum window area was m . 
 outdoor outdoor and outdoor indoor : These were under the same as in the outdoor outdoor and outdoor indoor respectively , with the only difference that the direct path was by the foliage of one or two of and small . These were usually low , densely foliated of the genus privet , while were typically horse chestnut . The thickness of the foliated obstruction was typically in the range of . to . The link tested ranged from to . 
 . MEASUREMENT AND ANALYSIS 
 This section the of the . We describe for the path loss in each of the previously and we include a thorough statistical description of the effect on the channel model that is a consequence of lowering the base station antenna . 
  
  
 a at . . 
  
 at . m 
 Fig . outdoor outdoor . 
 A . Path Loss , Contrast with Physical Model 
 outdoor outdoor : We here present the average over one receive antenna rotation . distance for all measured links of this type , at the two base station considered . This is shown in Fig . . In each case the plot as a reference the free space path loss equation and the path loss the two ray model . In the latter case , two for permittivity and conductivity , that cover the range of in the literature have been considered . Although these dielectric are frequency dependent , their change even over of or more will not place them outside the range of considered for our . As seen in Fig . the theoretical two ray path loss model is not very sensitive to the actual dielectric parameter and thus frequency dependent effects will be negligible . The fit of empirical data to the theoretical free space and is remarkably good . The dispersion of the data with respect to the free space model is comparable to the difference between the free space and the two ray model . It should also be that the path plotted here exclude small scale , which as before were out . However as will subsequently be shown , for these the small scale spatial were typically quite shallow high spatial factor . Consequently , the total path loss at any specific antenna position within the rotation is quite close to the average plotted here . It is also interesting to note that the follow the contour of the two ray model particularly at the ground level , even 
  
 a at . . 
  
 at . . outdoor indoor . 
 though were made in a variety of , some of which included height of the order of one . In this setting , lowering the base antenna no significant change in path loss , as will be in greater detail in subsection . It is worth that for both , the first Fresnel zone was not blocked . 
 outdoor indoor : As in Section , for in the outdoor indoor scenario , the movable arm antenna was inside the residence at a height of . , user . Several indoor were used , all of them possessing to the outside antenna through a single window . The antenna , outside of the residence , was again at . and at . height . The measured path at both are shown in Fig . , where we also include free space and two ray model path as a reference . As , outdoor indoor path are somewhat than those for the outdoor outdoor case and the dispersion of is also , which may be attributable to the variety of window sizes considered . There is however no evidence that lowering the base antenna will alter the path loss model in a significant way . 
 outdoor outdoor and outdoor indoor : Both considered here correspond to where the direct path was by one or two of typical garden vegetation , basically and small as previously . Fig . the for the outdoor indoor scenario . The scarcity of data between and is produced by the existence of a row of 
  
 a at . . 
  
 at . . outdoor indoor . 
 at this distance from the indoor antenna in some of the where were made . Additional vegetation at closer from the window . Therefore , at in excess of were in many by two vegetation . The show added attenuation and dispersion of than in the previous , a logical consequence of the greater variability of the environment . In this case , lowering the base antenna in somewhat greater , as well as greater uncertainty in their . 
 B . Path Loss , Statistical Modeling 
 The extensively used log normal model that the spatially path loss in may be as : 
  
 between the base station and subscriber unit , d is the free space path loss at a distance d in , is the path loss exponent and is a zero mean distributed random variable in with standard deviation . We applied this model to our data after out small scale over one rotation of the arm that the antenna . As conventionally done , we chose d . of the model , multiple or choosing the intercept point d different from the free space path loss did not provide a measurably better fit for the set of link tested , therefore we only used this very simple model . 
 TABLE I 
 SUMMARY OF PATH LOSS STANDARD VARIOUS OF AND ANTENNA 
 Type of 
 . . . . m 
 Outdoor Outdoor . . . . 
 Outdoor Indoor . . . . 
 Outdoor Outdoor . . . . 
 Outdoor Indoor . . . . 
 We the empirical Cumulative Distribution Function of the between the best fit linear regression and the measured average path . The of would be that of a zero mean random variable i . e under the assumption that the log normal model . We found that in fact the fit was very good , the corresponding of the not being included here to meet space . The indicate that in general lowering the base antenna will increase the standard deviation of , i . e ., the model path loss prediction becomes more uncertain . established that the log normal model is indeed accurate for our , we summarize in Table I the relevant of the model for all the studied . 
 Our so far have small scale spatial , which as stated were out over the rotation of the arm with the receive antenna . Our analysis of the between the average path loss and the path measured at the various arm revealed that the corresponding are very well by Rice probability . We thus use the factor as a metric to determine the small scale statistics , from the empirical data according to the method in . Fig . the of the for both and for the outdoor outdoor and outdoor indoor , which provided respectively the for the and small scale fade among all tested . As can be seen , no significant in the small scale fade statistics are associated with the in base station altitude . As , small scale spatial are much for the outdoor outdoor case than for the far more complex outdoor indoor environment . 
 C . Path Loss Variation with Base Altitude 
 We here present the statistics of the difference in path loss associated with lowering the base antenna from the . position to . . We define this difference as 
 . . , 
 where . and . are respectively the path in for base antenna of . and . . These are at exactly the same azimuth position of the rotating arm for both antenna , i . e ., without any small scale path loss . 
 Fig . a the of for the outdoor outdoor and outdoor indoor . As before , these correspond to the two extreme , the falling between the two shown . The ensemble 
  
 Fig . . of at . and . for outdoor outdoor and outdoor indoor . 
 size for our calculated between and depending on the amount of in each category . As can be seen , for the outdoor outdoor case , lowering the base antenna will result in a slight decrease in path loss when considering the median ensemble value . This is consistent with what can be in Fig . b , which that at some the path loss is with respect to the free space value by the constructive interference of the ground ray . There is a clear increase in the median path loss for the more complex outdoor indoor scenario . For the latter case , as already , there is a variability of path with respect to the mean , particularly at low altitude . The result of this can also be in Fig . a . This figure that a considerable fade margin may need to be added to the link budget if adequate availability is to be when lowering the base antenna . This added margin from about for the outdoor outdoor case to approximately . for the outdoor indoor case at the probability level . 
 In many it may be more informative to have statistical knowledge on the increase in path with respect to a well known and deterministic reference , rather than with respect to at . , which are random . Choosing as reference the free space at the same distance , we thus the statistics of the excess path loss . This is defined as the difference in between the path loss measured at any given azimuth position of the receive antenna and the corresponding from the equation according to 
 log . 
  
 Fig . b the of the for the outdoor outdoor and outdoor indoor , for the two antenna considered . Consistent with our previous , for the outdoor outdoor case the of the are quite low for both antenna . At . the is actually somewhat lower than at . . This is consistent with the previously result on the constructive interference from the near ground antenna 
  
 a of the difference in path loss for antenna of . and . . 
  
 of the Excess path loss over : . 
 Fig . . of power attenuation for the tested . 
 in Fig . b at some . In contrast , for the outdoor indoor scenario , the is always than at . . As can be seen , to cover of , a fade margin of needs to be added for a base at . , while for a base at . this margin to , i . e . an extra is to achieve the same degree of coverage . This value may appear at first sight to be inconsistent with the . increase in Fig . a . However we note that the statistics of path loss increase in Fig . a are the result of the difference between two random . The increase may occur under where the is high or is low . When percentage of wireless coverage or link , an increase in due to the lowering of the antenna may be of little consequence when the initial value of the is low . In contrast , Fig . b the difference in path at both antenna with respect to an absolute reference . This us to draw on the fade margin with respect to free space propagation at a specific coverage level for both antenna . 
 . 
 We have a large amount of statistical data for path loss statistics in suburban , representative of fixed wireless near ground base station . We that for unobstructed outdoor outdoor , with base antenna of . and . , the propagation channel is well by the two ray or even the free space . Our 
  
 also indicate that the increase in excess path loss over free space propagation , associated with lowering the base antenna is quite modest . Typical are to at the median level and for availability , for all tested , provided all links remain of the same type when lowering the base antenna . However , when we consider all links of all , the will vary over a considerable range . For availability , additional may have to be for an , near ground , outdoor indoor link , while only are if the link is of the outdoor outdoor type . For a system designer this the between providing through a wall mounted outdoor antenna , with additional indoor retransmission hardware considering the need for a significant fade margin to avoid such additional . Both also involve taking into account the associated interference . Extra transmission power should be through link adaptation to limit it to the site specific , while indoor retransmission may need to be to reduce interference between neighboring . 
  
  
 ﻿The performance of a noisy linear time-invariant (LTI) plant, controlled over a noiseless digital channel with transmission delay, is investigated in this paper. The ratelimited channel connects the single measurement output of the plant to its single control input through a causal, but otherwise arbitrary, coder-controller pair. An informationtheoretic approach is utilized to analyze the minimal average data rate required to attain the quadratic performance when the channel imposes a known constant delay on the transmitted data. This infimum average data rate is shown to be lower bounded by minimizing the directed information rate across a set of LTI filters and an additive white Gaussian noise (AWGN) channel. It is demonstrated that the presence of time delay in the channel increases the data rate needed to achieve a certain level of performance. The applicability of the results is verified through a numerical example.
 INTRODUCTION
 Rate-constrained NCSs are generally studied from two points of view; control theory and information theory. In the former case, classical nonlinear control methods are employed. In the latter case, the key idea is extending information-theoretic notions to the case of closed-loop control. In both frameworks, stability analysis of linear timeinvariant (LTI) systems is well-studied (see, e.g., [1], [2] as early results and [3], [4] as recent contributions).
 However, studies analyzing system performance from an information-theoretic viewpoint are less abundant in the literature. Fundamental results are presented in [5]. In this work, for a discrete-time LTI plant, the well-known Bode’s integral is extended to the case of causal rate-limited arbitrary feedback. Along the lines of [5], research reported in [6], [7] has investigated bounds on the minimum data rate which is needed to attain a quadratic performance level in NCSs with delay-free channels. For the lower bound, [7] shows that the rate-constrained optimization to find desired infimal data rates over causal but otherwise arbitrary coder-controller pairs, is reduced to a convex SNR-constrained optimization problem over an auxiliary LTI feedback loop closed through an AWGN channel. In [6], [7], it is furthermore shown that the directed information from the plant output and to the
 *This work has received funding from VILLUM FONDEN Young
 Investigator Programme, under grant agreement No. 19005.
 control input provides a lower bound on the coding rate for any coding policy, and that it suffices to use linear coding policies, when the initial state and external disturbances are jointly Gaussian and the plant is linear. These findings were used by [8], to establish a general SDP framework for the problem of LQG control for fully observable multiple-input multiple-output (MIMO) LTI plants.
 In this paper, we address output feedback control of an NCS comprised of a noisy LTI plant and a causal encoder-controller-decoder set connected through a noiseless digital channel with a constant transmission delay. More specifically, the problem is obtaining the bounds on minimal average data rate required to guarantee that the steady-state variance of an error signal does not become larger than a certain value. Motivated by its merits such as simplicity and practical appeal, we use the approach pursued in [6], [7] to gain outer bounds and build upon [7]. However, the main departure of this work from [7] is considering a channel which is not delay-free. So, as the first contribution, we rederive fundamental information inequalities of the system under the delay assumption. Secondly, we characterize the trade-off among performance, delay, and minimal desired average data rate. It is shown through a numerical example that greater transmission delay necessitates greater minimal average data rate needed to guarantee achieving the considered quadratic level of performance. Simulation indicates that by employing a simple scalar uniform quantizer in the LTI architecture that gives the lower bound, the quadratic performance is attained by operational average data rates at most 0.3 bits away from the lower bound.
 The outline of this work is as follows. Section II presents the notation and some preliminaries. Then the problem of interest is formalized in Section III. Section IV is dedicated to the lower bound characterization. An illustrative numerical simulation is provided in section V. Finally, Section VI concludes the paper.
 NOTATION AND PRELIMINARIES
 The set of real numbers is denoted by R with subset R+ as the set of strictly positive real numbers. N represents the set of natural numbers, based upon which N0 = N ? {0} is defined. Furthermore, k is the time index and for random processes considered in this paper, k ? N0 holds. Magnitude and H2-norm of a signal are symbolized by |.| and k.k2, respectively. The set U8 is defined as the set of all proper and real rational stable transfer functions with inverses that are
  
 	Fig. 1.	Considered NCS
 stable and proper as well. E denotes the expectation operator and log stands for the natural logarithm. The entry of matrix S on the i-th row and j-th column is denoted by [S]i,j. Moreover, ?min(S) and ?max(S) represent eigenvalues of S with the smallest and largest magnitude, respectively.
 All random variables and processes in this paper are assumed to be vector valued, unless otherwise stated. A random process ? is said to be asymptotically wide-sense stationary (AWSS) if it satisfies   and limk?8 E[(?(k + t) - E[?(k + t)])(?(k) - E[?(k)]) ] = R?(t) hold, where ?? is a constant. C? = R?(0) denotes the corresponding steady-state covariance matrix upon which the steady-state variance of ? is defined as s?2 , trace(C?). The covariance matrix for a scalar random sequence   is defined as  . Considering
 Pn,Qn ? Rn×n as two square matrices, the sequences   and  are asymptotically equivalent if and only if the following holds for finite %:
  
 PROBLEM STATEMENT
 The structure considered in this work can be found in Fig. 1 where G is an LTI plant with u ? R as control input and y ? R as sensor output. Moreover, there is a disturbance represented by w ? Rnw and the output signal z ? Rnz, with nw,nz ? N, upon which the desired performance is characterized. The plant has the following transfer-function matrix description:
 	 ,	(1)
 in which every Gij is proper and of suitable dimensions
 (nz × nw, nz × 1, 1 × nw and 1 × 1 for G11, G12, G21 and G22, respectively). The input alphabet of the channel is represented by A and is defined as a countable set of prefixfree binary words. Due to the delay, the output of the channel uq(k) follows uq(k) = yq(k - h) for k = h where yq(k) belongs to A. The average data rate across the channel is specified as follows:
 	 ,	(2)
 where R(i) denotes the expected length of the i-th binary word yq(i). The channel input is provided by the encoder E based on the following dynamics:
 	 ,	(3)
 in which ?e(k) is the side information at time k at the encoder with Ek representing an arbitrary (possibly nonlinear or time-varying) deterministic mapping. It should be noted that ßk is a shorthand for [ß(0),··· ,ß(k)]. On the decoder side, we have
 	 	(4)
 Dk is assumed to be an arbitrary deterministic mapping, like Ek, and ?d(k) signifies the side information available at the decoder at time k. It should be emphasized that E and D in Fig. 1 are possibly time-varying or nonlinear causal systems.
 Assumption 3.1: The plant G is LTI, proper and free of unstable hidden modes. Moreover, the open-loop transfer function from u to y is single-input single-output (SISO) and strictly proper. The disturbance signal, w, is a zero-mean white noise with identity covariance matrix Cw = I and jointly Gaussian with x0 = [x(-h),··· ,x(0)]T, the initial condition, having finite differential entropy.
 Assumption 3.2: Each of processes ?e and ?d is jointly independent of (x0,w). So regarding the dynamics of the system, I(u(k);yk-h | uk-1) = 0 holds for 0 = k < h. Moreover, upon knowledge of ui and ?di, the decoder is invertible. It means that there exists a deterministic mapping
 Qi such that .
 Now, suppose that Assumption 3.1 holds. Let Dinf(h) denote the infimum steady-state variance of the output z over all settings u(k) = Kk(?k) for 0 = k < h and u(k) = Kk(yk-h) for k = h with ?k independent of x0 and w.
 Then the problem of interest is finding
 	R(D) = inf R	(5)
 sz2=D
 for any D ? (Dinf(h),8), where the search is to be restricted to encoders with mapping Ek and decoders with mapping Dk which satisfy Assumption 3.2 and make the NCS of Fig. 1 strongly asymptotically wide-sense stationary (SAWSS) (this notion of stability is defined in [7]). Moreover,  denotes the steady-state variance of z. The optimization problem in (5) is feasible if D ? (Dinf(h),8) (see Appendix A for the proof).
 MAIN RESULTS
 Theorem 4.1: For the feedback loop depicted in Fig. 1 and satisfying Assumptions 3.1 and 3.2, the following holds:
  
 in which I(.;. | .) indicates conditional mutual information
 (see [9] for the definition). Moreover, as defined in [10],   denotes the directed information rate across the forward channel from y to u with delay h. The proof can be found in [11].
 Now, a lower bound can be derived on the directed information across the coding scheme of Fig. 1.
 	Lemma 4.1: For	the	NCS	of	Fig.	1,	assume	that
 (x(0),w,u,y) form a jointly second-order set of processes and that Assumptions 3.1 and 3.2 hold. Moreover, take yG and uG into account as the Gaussian counterparts of y and u where (x(0),w,uG,yG) are jointly Gaussian with the same first-and second-order (cross-) moments as (x(0),w,u,y).
 Then .
 Proof: The following inequalities and identities will justify the claim:
 Ski=0-1I(u(i);yi-h | ui-1) =(a) I(x(0),wk-1;uk-1)
 =(b) I(x(0),wk-1;ukG-1)
 	= S(c)	ki=0-1I(x(0),wi;uG(i) | uiG-1)	(7)
 =(d) Ski=0-1I(x(0),wi-1,yGi-h;uG(i) | uGi-1)
 	= S(e)	ki=0-1I(yGi-h;uG(i) | uiG-1),
 where (a) follows from a slight modification in [7, Lemma B.4], (b) follows from [7, Lemma B.1], (c) holds form the Markov chain   based upon (51b) in [7, Theorem B.3], (d) is a consequence of Assumption 3.1 and yGi being a deterministic function of uiG-1,x(0) and wi, and (e) stems from (51a) in [7, Theorem B.3] and Assumption 3.1. This completes the proof.  
 In what follows we will relate the directed information from yG to uG to their corresponding power spectral densities:
 Lemma 4.2: Consider y and u as jointly Gaussian AWSS processes. Moreover, suppose that u is SAWSS with   where µ > 0. Then the following
 can be obtained:
 	 	(8)
 in which ? is a Gaussian AWSS process with independent samples defined as:
 	?(k) , u(k) - u˜(k),u˜(k) , E[u(k) | yk-h,uk-1].	(9)
 Moreover, Su? represents the steady-state power spectral density of u.
 Proof: Having Gaussianity and joint AWSS-ness of (u,y) in mind and based on [12, Theorem 2.4] with a little modification, we can conclude that ? is Gaussian and AWSS as well. We start by the following equalities:
 I(u(i);yi-h | ui-1) =(f) h(u(i) | ui-1) - h(u(i) | yi-h,ui-1)
 =(g) h(u(i) | ui-1) - h(?(i) + u˜(i) | yi-h,ui-1)
 (10)
 =(h) h(u(i) | ui-1) - h(?(i) | yi-h,ui-1)
 =(i) h(u(i) | ui-1) - h(?(i)),
  
 	Fig. 2.	Auxiliary LTI NCS
 where (f) follows from the definition of mutual information and (g) from the definition of ?, (h) stems from [7, Property 2] and (9), and (i) holds from [7, Property 3]. So the directed information rate can therefore be rewritten as follows:
  (11)
 where (j) follows from the chain rule of the differential entropy and ?(k) being independent of ?k-1. Since the process u is SAWSS with   for some µ > 0, [7, Lemma B.5] will approve the validity of the leftmost term in (k). The rightmost term is self-explanatory because ? is Gaussian and AWSS.  
 It follows from Theorem 4.1, Lemma 4.1 and Lemma 4.2 that the rate-performance pair yielded by any encoderdecoder scheme which renders the NCS SAWSS, is attainable with a lower rate by a coding scheme comprised of LTI filters and an AWGN noise source. Such a scheme is depicted in Fig. 2.
 The NCS of Fig. 2 is defined under the same conditions (Assumption 3.1) as the main system in Fig. 1 except for one thing; the arbitrary mappings are replaced by proper LTI filters B and J. Moreover, the communication channel is a delayed AWGN channel with noiseless one-sample-delayed feedback. The dynamics of this auxiliary coding scheme can be summarized as follows:
  
 in which ? is the AWGN with zero mean and variance  . This noise is assumed to be independent of (x0,w). Additionally, we suppose that the initial state of B, J, and the delay are deterministic.
 Theorem 4.2: For the NCS depicted in Fig. 1 and satisfying Assumptions 3.1 and 3.2, R(D) is lower bounded as follows if D ? (Dinf(h),8):
  
  
 where the feasible set for the optimization problem defining ?0u(D) is all LTI filters B and the noise ? with s?2 ? R+ rendering the feedback loop of Fig. 2 internally stable and well-posed when J = 1. In these expressions,   and Su0 denote the steady-state variance of z0 and the steady-state power spectral density of u0 in Fig. 2, respectively.
 Proof: Since D > Dinf(h), there is at least one pair, say Eˆ and Dˆ, satisfying Assumption 3.2, rendering the NCS of Fig. 1 SAWSS and producing zˆ, yˆ and uˆ where  holds and
 	 	(14)
 can be concluded based on Theorem 4.1, if the conditions in Lemma 4.1 and Lemma 4.2 are met. It should be noted that Su? denotes the steady-state power spectral density of uˆG. A coding scheme comprised of linear filters with a unit-gain noisy channel and delay h can generate yˆG and uˆG which satisfy those conditions and keep  within (Dinf(h),8). Such a scheme is described as follows:
  
 where ?ˆG(k) represents a Gaussian noise with zero mean and independent of  . Regarding the causality and linearity of Lk, uˆkG can be written as follows:
 	 .	(16)
 According to the causality in (16), joint SAWSS-ness of (yˆG,uˆG) and transitivity of asymptotic equivalence for products and sum of the matrices noted in [13], the sequences {Qk} and {Pk} are asymptotically equivalent to sequences of lower triangular Toeplitz matrices. Moreover, Lk renders the NCS internally stable and well-posed. With all of this in mind, by setting J = 1 and B as a concatenation of linear filters with the steady-state behaviour of Lk in (15) and considering a variance for ? equal to s?2ˆG, the system of Fig. 2 will be rendered well-posed and internally stable where Su0 = Su? and  are resulted. Then according to Lemma 4.2, the following can be concluded:
 	 	(17)
 which completes the proof.	 
 Lemma 4.3: Consider the LTI loop of Fig. 2 with fixed s?2 ? R+. Define  as follows:
 	 ,	(18)
 in which Sr represents the steady-state power spectral density of r. Then for any ? > 0, upon the existence of the pair (B,J) = (B1,J1) making the system of Fig. 2 internally stable and well-posed, there exist another pair, comprised of
  
 	Fig. 3.	Equivalent archtiectural viewpoint of internal stability
 the biproper filter J2 and B2, which renders the feedback loop of Fig. 2 internally stable and well-posed, preserves the the steady-state power spectral density of z0 and satisfies the following:
 	 	(19)
 Proof: It is well-known that the system of Fig. 2 is well-posed and internally stable if and only if the transfer function T from [?,w,?1,?2]T to [z0,y0,r,u0]T in Fig. 3 belongs to RH8. By Ti and ri, we refer to the transferfunction matrix T and signal r when B and J are set such that (B,J) = (Bi,Ji),i ? {1,2}. Moreover, Byi and Bri represent elements of B (B = [Br By]) in the situation where B = Bi. Now, consider the following set of filters:
 	J2 = zd1J1V -1,	By2 = z-d1By1
 (20)
 - (1 - Br1z-1)V -1), Br2 = z(1
 in which d1 indicates the relative degree of J1 and V ? U8 is chosen in such a way that V (8) = 1. Consequently, J2 is biproper and T2 can be written as follows:
 T2 = diag{zd1I,zd1I,V,zd1I} × T1×
 diag -d1 -d1 -d1 }.(21) {I,z I,z I,z I
 So regarding the definition of d1 and properties of V , T2 ? RH8 if and only if T1 ? RH8. Moreover, based on the same argument, using (B2,J2) would give the same power spectral density for z0 as for the case where (B1,J1) is utilized. Let d1,...,dm represent the zeros of Gr1 lying on the unit circle. Now for ? ? (0,1) we define the following:
  
 (22)
 .
 Hence, V? ? U8 and V?(8) = 1 can be deduced for every ? ? (0,1). By following the same procedure as for the proof of [6, Theorem 5.2], the existence of ? ? (0,1) will be shown in such a way that for any ? > 0, setting V = V? will give a pair (B2,J2) that satisfies (19).  
  
 	Fig. 4.	Auxiliary system for ?0(D) minimization
 Corollary 4.1: If Assumptions 3.1 and 3.2 hold for the NCS of Fig. 1 and D ? (Dinf(h),8), then
 	 ,	(23)
 where the optimization is done over all LTI filter pairs (B,J) and the noise variance  making the system in Fig. 2 internally stable and well-posed.
 Proof: According to the feasibility of finding ?0u(D) (see [11] for the proof), there exist the triplet  , with B? a proper LTI filter and , that guarantees sz20 = D for the system of Fig. 2. Furthermore, based upon the definition of  and   in (13) and (18), the following can be derived for any ? > 0:
 	 .	(24)
 So regarding Lemma 4.3, since there exist a biproper filter J˜? and a proper one B˜? making the LTI feedback loop of Fig. 2 internally stable and well-posed and keeping   intact, the following can be concluded:
  
 Now the proof is completed by noting that (25) holds for
 any ?,? > 0	 
 To characterize ?0(D), we will mostly use properties of linear systems and some results on H2 optimization with input-delay. Consider the auxilary structure of Fig. 4, where except for shifting the delay block to the plant model, which leads to
 	,	(26)
 the same assumptions as Fig. 2 hold. The NCS of Fig. 4 is internally stable and well-posed if and only if the transfer function Ta from [?,w,?1,?2]T to [za,ya,ra,ua]T in Fig. 5 is a member of RH8. It can be easily shown that Ta = T. So the feedback loops of Fig. 4 and Fig. 2 are equivalent in the sense of internal stability and well-posed-ness. Moreover, the SNR and output variance of the NCS depicted in Fig. 2 can be stated in terms of H2-norms as follows:
 (27)
 ,
  
 	Fig. 5.	Stability analysis of the equivalent system
 where	N , JByz-h(1 - Brz-1)-1	and	M ,
 (1 - Brz-1 - G22Jz-hBy)-1. Likewise, the following holds for the structure of Fig. 4:
  
 in which Na = JBy(1 - Brz-1)-1 and Ma = M. As seen,
 comparing (27) and (28) signifies the equalities  
   and . So every triplet   that can infimize the SNR while making the system output satisfy   for the NCS of Fig. 4, can do the same for the the LTI system of interest, in Fig. 2, and vice versa. In other words, the NCSs in Fig. 2 and Fig. 4 are equivalent regarding the SNR-perfomance optimization problem in (23) as well. This problem is studied for such feedback systems as auxiliary system of Fig. 4 in [7]. Consequently, it can be concluded that the problem of finding ?0(D) is equivalent to an SNR-constrained optimal control problem which was proved to be convex. As another result, ?0(D) being a monotonically decreasing fuction of D can be deduced. All in all, the interplay between the desired performance, the average data rate and the time delay is characterized through
 (23), (27) and (28).
 SIMULATION EXAMPLE
 Consider the the following transfer function representation for the plant G in NCS of Fig. 1:
  
 where (x0,w) satisfies Assumption 3.1. Using the results of the previous section, we simulate the lower bound on R(D) obtained in (23) regarding five different values of delay (h = {0,1,2,3,4}) and for each h, over a range of D > Dinf(h). Fig. 6 demonstrates the behaviour of the lower bound with respect to D and h. Additionally, it shows the operational rates when using scalar uniform quantizers for h = 0 and h = 4. First, as expected, ?0(D) in (23) is a monotonically decreasing function of D. Secondly and more importantly, Dinf(h) increases when h grows (see [14]). So greater delay yields worse best performance. The most significant
  
 	Fig. 6.	Bounds on R(D) in (5) for different values of time delay h
 outcome is associated with the behaviour of R(D) in (5) with respect to delay. It can be observed from Fig. 6 that for a fixed D, ?0(D) is increasing in h. Therefore, a delay in the channel forces an increase in the required infimum data rate to achieve a quadratic level of performance. The greater delay, the higher rate to be spent in order to get a certain level of performance. Indeed, this finding extends the delay-free results of [7]. Another observation is the convergence of the obtained infimal data rates to the minimum rate required for stabilizability as D ? 8. As illustrated in Fig. 6, high rates are required to attain the ideal non-networked performance Dinf(h). Along the lines of [6], [7], we now simply replace the AWGN ? in the independent coding scheme depicted in Fig. 2, by a uniform scalar quantizer in order to assess the operational performance caused by a simple coding scheme. It is interesting to note that the obtained operational average data rate in Fig. 6 is at most around 0.3 bits away from the derived lower bound at all performance.
 CONCLUSIONS
 In this paper, rate-constrained networked control systems comprising noisy LTI plants, causal but otherwise arbitrary coding-control schemes and digital noiseless communication channels with time delay, have been studied. For such NCSs, a certain level of performance is attainable if and only if the average data rate does not fall below a minimal value. A lower bound on this infimum rate has been obtained. Through a numerical example, it has been illustrated that the channel’s time delay increases the infimum average data rate needed to achieve a prescribed level of performance. Moreover, by using a simple scalar quantizer, operational average data rates fairly close (around 0.3 bits) to the lower bound have been obtained.
 APPENDIX
 A. Feasibility proof of Dinf(h)
 Suppose that in the standard architecture depicted in Fig. 7,
 G, x0 and w satisfy Assumption 3.1 and K follows u(k) = Kk(yk-h). Regarding the Gaussianity of x0 and w and the fact that G is LTI, it can be implied from some results in
 [15] that:
 Dinf(h) = inf sz2,
 K??
  
 Fig. 7.	Standard feedback loop for proving feasibility of finding R(D)
 in which   denotes the variance of output z and ? is the set of all proper LTI filters which render the system of Fig. 7 internally stable and well-posed. The assumptions considered for G guarantee that finding Dinf(h) is feasible. The feasibility of finding ?0u(D) in (13) and  in (23) follows from the feasibility of Dinf(h) [11]. ﻿This work addresses the problem of universal density estimation under an operational data-rate constraint. We present a coding theorem that stipulates necessary and sufficient conditions to learn and transmit a memoryless source distribution with arbitrary precision (in total variations), under an asymptotic zero-rate regime, in bits per sample. In the process, we propose a concrete coding scheme to achieve this learning objective, adopting the Skeleton estimate developed by Y. Yatracos [1], [2].
 I. INTRODUCTION
 This work studies the problem of universal density estimation under an operational data-rate constraint. The basic setting consists of an agent (the sensor) observing i.i.d. samples from an unknown distribution µ with the objective of jointly learning and transmitting a finite description of µ to a second agent (the receiver), which decodes that information to construct an estimate µˆ. This density estimation and coding problem has taken the attention of the community because of its role in sensor networks, and because of its strong connection with
 universal lossy-source coding (ULSC) [3], [4].
 Making echo of the seminal work of Rissanen [3], it is well understood that the problem of universal lossless-source coding is connected with the problem of distribution estimation, as there exists a one-to-one correspondence between prefixfree codes and finite-entropy discrete distributions (models), in the finite-alphabet case [4]. This interplay, however, is less obvious when we move to the lossy-source coding scenario. Addressing this issue, Raginsky [5] has recently stipulated results that connect the problem of fixed-rate universal lossy source coding, with the problem of transmitting the source distribution with arbitrary precision, from one point to another, under an asymptotically zero-rate operational constraint [5]. This connection was made under the two-stage joint modeling and coding framework [5]. Taking ideas from statistical learning, the data was split in training and testing samples, where first, the training data is used to construct a finite description of the source distribution (first stage), and the second stage uses the first bits to pick a matched (with respect to the estimated distribution) lossy source code to encode the test data. Remarkably, in this joint modeling-coding framework, the existence of a zero-rate consistent estimate of the distribution (in total variations), is sufficient to show the existence of a universal fixed-rate source coding scheme, achieving the
 978-1-4577-0437-6/11/$26.00 ©2011 IEEE
 Shannon distortion-rate function [4], for any given rate, and for any distribution within a bounded parametric family with some needed regularity conditions [5, Th. 3.2]. This raises the question of whether there are broader families of measures (non-parametric) for which this result is also valid.
 In this work we study in deeper details the problem of universal density estimation under an asymptotically zerorate constraint. Our main result is a coding theorem that stipulates necessary and sufficient conditions to guarantee that zero-rate is achievable for this learning-coding problem. Interestingly, there is a tight connection with the rich nonparametric collection of L1-totally bounded densities [6]. Furthermore, we propose a concrete coding scheme, the Skeleton estimate developed by Yatracos [1], [2], [6], to achieve our coding objective, which is a concrete demonstration of its information theoretic attributes, something that was mentioned by Devroye and Lugosi [6, Ch. 7.1] and which, to the best of our knowledge, has not been presented before. In the parametric scenario considered in [5], the Skeleton scheme
  
 offers an optimal learning rate of O(p1/n) under the zerorate regime, where, furthermore, this rate is extended for general non-parametric families.
 II. DENSITY ESTIMATION UNDER A BIT RATE CONSTRAINT
 Let X ? B(Rd) be a separable and complete subset of Rd (i.e., X is a Polish subspace of Rd). Let P(X) be the collection of probability measures in (X,B(X)) and let AC(X) ? P(X) denote the set of probability measures absolutely continuous with respect to the Lebesgue measure ? [7]1. For any µ ? AC(X),  ?µ??(x) denotes the Radon-Nikodym (RN) derivative of µ with respect to ?.
 For the estimation problem the fidelity criterion adopted is the total variational distance. Let v and µ be two probability measures in P(X). The total variation of v and µ is given by
 	V (µ,v) =	sup |µ(A) - v(A)|,	(1)
 A?B(X)
 1A measure s is absolutely continuous with respect to a measure µ, denoted by  , if for any event A such that µ(A) = 0, then s(A) = 0.
 Consequently   is well defined, which is the Radon-Nicodym derivative or density, and furthermore, ?A ?B(X), s(A) = RA ?s?µ?µ.
 which is a bounded metric in P(X) and has been widely adopted in density estimation [6], [8]. For the case when, µ and v belong to AC(X), the Scheffe‘s identity´ [9] provides a connection between total variation and the L1-norm of the densities involved [6], [8], more precisely,
 	 .	(2)
 Alternatively, if we define the Scheffe set´	for the pair
 (µ,v) (with densities (f,g), respectively) by Aµ,v = {x ? X : f(x) > g(x)} ? B(X), then V (µ,v) = µ(Aµ,v) - v(Aµ,v) [6].
 A. The Main Learning Problem
 Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities of interest. T represents the index set of F, which can live, in general, in an infinite dimensional space (a nonparametric scenario).
 Definition 1: A (n,2nR) learning rule of length n and rate R is a pair of functions (f,f), with f : Xn ? S and f : S ? T, where S is a finite set and
  
 The composition of these two functions p = f ? f : Xn ? T defines the explicit learning rule taking values in the set {f(s) : s ? S} ? T, which is called the reproduction codebook of (f,f). For an arbitrary learning rule p, the operator R(p) returns log2(|S|)/n, which is the description complexity of the range of p, in bits per sample.
 Definition 2: A finite description learning scheme ? with rate sequence (Rn)n=1 is a collection of learning rules for all possible finite lengths, i.e., ? = {(fn,fn) : n = 1} such that R(pn) = Rn, for all n = 1.
 Let X1,X2 ... be independent and identically distributed (i.i.d.) realizations of a measure µ ? F. In this context,  denotes the product measure of the block  in (Xn,B(Xn)) and Pµ the entire process distribution of X1,X2 .... Hence, the problem is to study the existence of a universal learning scheme ? = {(fn,fn) : n = 1}, such that its finite description density estimate induced by the data µpn(X1n) ? F convergences to µ, in the following sense,
 	 .	(3)
 In other words, we are interested in characterizing a densityfree zero-distortion estimate for the family F, adopting a scheme ?, and consequently, in studying the necessary and sufficient conditions, if any, that this consistency assumption imposes on the intrinsic complexity of F.
 Definition 3: Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities in (X,B(X)). We say that the rate R = 0 is asymptotically achievable for F, if there exists a learning scheme ? = {(fn,fn) : n = 1}, with limsupn?8 R(pn) = R such that
  .
 In this case we say that ? is an R-rate uniformly consistent estimate for the class F.
 Definition 3 considers estimating the density in F with zero-distortion. We are interested in the zero distortion (i.e., lossless) operational point, as it is the standard objective in density estimation, and because it is fundamentally related with the problem of universal source coding [5]. The focus of the next section, containing the main results of this paper, is to characterize the class of distributions that permits a zerorate scheme for lossless density estimation.
 III. THE ZERO-RATE LOSSLESS DENSITY CODING THEOREM
 Definition 4: Let F ? AC(X) be a class of densities. We say that F is totally bounded if for every , there exists a finite covering   of elements in F such
 that
 	 ,	(4)
 where 
 is the L1 ball of radius  centered at µ. Let N denote the smallest positive integer that achieves (4). N is called the -covering number of F and   is the
 Kolmogorov‘s -entropy of the class [6]. Finally, an -covering   such that   , is called an -Skeleton of F
 [1].
 We can now state the following coding theorem.
 THEOREM 1: Let F = {µ? : ? ? T} be an indexed collection of densities in AC(X) with index set T (that could be an infinite dimensional set in general). Then, there exists a zero-rate uniformly consistent scheme ? for the class F, if and only if, F is totally bounded.
 Hence, the rate zero is achievable with zero distortion for the class F, if and only if, F is totally bounded. This offers a concrete correspondence between zero-rate density estimation and L1-totally bounded collections of densities. The proof has two parts which are presented in detail next.
 A. The Skeleton Estimate (Yatracos [1]) — Achievability
 For the achievability argument, we present a concrete learning scheme that achieves consistency in the sense of (3), under the desired zero asymptotic rate. The scheme is based on the Skeleton estimate [6, Chapter 7] proposed by Yatracos [1].
 For any arbitrary  , let us consider one of the -
 Skeletons . In what follows, we use  as a short-hand for the densities in G, and we define   to represent the index set of G. The idea of Yatracos was connecting this collection of probability measures with a set of measurable events in B(X). More precisely, let us define the Yatracos class of G [6] by
 	 ,	(5)
 where   is the
 Scheffe set of´   with respect to  [9], [6]. Then the idea is to partition T using as a codebook the Skeleton-index set T and to adopt a minimum distance principle [1], [6]. More precisely, given i.i.d. realizations X1,..,Xn with Xi ~ µ (µ ? F), the Skeleton estimator is given by [1],
 	 ) = arg min 	,	(6)
 where   is the standard empirical distribution [6]. Note that   is the minimum-distance approximation of µˆn with elements of G, adopting the similarity measure in (6), that is reminiscent of the total variational distance in (1). The following key theorem offers a performance bound with respect to the minimum-distance decision on the knowledge of the true distribution. THEOREM 2: (Yatracos [1]) For any µ ? F,
  .
 (7)
 This bound consists of an approximation error and an estimation error, the first and second term on the right-hand-side of (7), respectively The approximation error is bounded by the definition of G, as F is totally bounded. On the other hand for the estimation error, Yatracos proposed the use of the Hoeffding‘s inequality [6].
 THEOREM 3: (Devroye et al. [6, Th. 7.1]) ?µ ? P(X),
 	 ,	(8)
 where using this result in (7), we have that the skeleton
 estimate   satisfies that, ,
  
 Note that for any fixed  , as n tends to infinity one can make the estimation error component in (9) arbitrarily small. The beauty of Theorem 3 is that (8) is distributionfree (in particular valid for any µ ? F) and, furthermore, given its finite-length nature (non-asymptotic), it is valid even if the approximation fidelity  (attributed to  ) is chosen as a function of the amount of data n [6]. Consequently, for any sequence   of non-increasing positive numbers,
  
 for all n = 1. Here we can consider the sequence   that offers a balance between the estimation and the approximation error expressions, and by doing so, get a consistent estimate of µ in the sense of (3). A simpler idea was stated by Devroye and Lugosi [6, Chp. 7] . If the class F is totally bounded, we can consider the sequence  , which is well defined and clearly convergences to zero as	n	tends	to	infinity.	Consequently,	it	satisfies	that n	o
  .
 To conclude, for any skeleton learning rule in (6), let us characterize its encoder-decoder pair   by,
  ,
 for any sequence   and for any  , where µˆn is the empirical distribution induced from the argument xn. Then the learning scheme  satisfies the uniform consistency re-
 quirement in (3), and its rate is given by 
 v 
 , which by construction is O(1/ n). Consequently the skeleton estimate offers a zero-rate lossless learning scheme for the family F, which concludes the achievability part.
 B. Minimum Distance Oracle Solution — Converse
 Let us assume that there exists a learning scheme ? =
 {(fn,fn) : n = 1} such that limn?8 R(pn = fn ? fn) = 0
 and
 	 .	(11)
 Associated with the learning rule of length n, we have its reproduction codebook that we denote by Tn =
  . Let us define the minimum
 distance oracle solution in Tn by
 	?˜n(µ) = arg inf V (µ?,µ).	(12)
 ??Tn
 From (11), we have that limn?8 supµ?F V (µ?˜n(µ),µ) = 0.
 This means that   for all µ ? F. Rephrasing,  there exists  , such that for any arbitrary , where by construction |Tn¯| < 8. Then F is totally bounded, completing the proof of Theorem 1.
 IV. YATRACOS CLASSES WITH FINITE VC DIMENSION
 In this section we explore density collections with extra regularities (on top of the needed totally bounded assumption)
  
 to achieve the critical O(p1/n) asymptotic rate of convergence in total variation2. On this, we follow the ideas proposed by Yatracos [2] who, in the context of the minimum distance estimate [6, Ch. 8], explored families of distributions with finite Vapnik and Chervonenkis (VC) dimension, also called VC classes [10], [11].
 The following results focus exclusively on the skeleton based learning scheme presented in Section III-A for the achievability part of Theorem 1, i.e.,  ˆ   indexed by a sequence of nonnegative precision numbers  .
 2Note that from the structure of the estimation error bound in (9), the
  
 fastest rate of convergence that could be achieved is order O(p1/n).
 Definition 5: (Yatracos [2]) Let F = {µ? : ? ? T} ? AC(X) be an indexed collection of densities. The Yatracos class for such collection is given by,
 	 	(13)
 with A?,?¯ = {x ? X : f?(x) > f?¯(x)} ? B(X) denoting the
 Scheffe set of´	µ? with respect to µ?¯.
 THEOREM 4: Let F = {µ? : ? ? T} ? A(X) and let us assume that,
 i)	F is totally bounded,
 ii)	the Yatracos class AT has a finite VC dimension (see
 Definition 7 in Appendix VI-A) iii) and the Kolmogorov’s complexity of F associated with
 	the sequence	grows strictly sub-lineal, i.e.,
 log2( 1/vn ) is o(n).
 v 
 Then the skeleton learning scheme ?((1ˆ / n)n=1) has zero rate and
 	 ,	(14)
 where   is the skeleton density estimate in (6).
 Proof of Theorem 4: We consider the definitions and notations introduced in Section III-A. Let us first focus on the convergence rate of the skeleton estimate. First, from Theorem
 2, for any arbitrary sequence  
  
 with  the Yatracos class of the Skeleton  . It is clear that . Then by monotonocity
 EPnµ  
 for all  and for any distribution µ ? P(X). Here is where we use the assumption that AT has finite VC dimension V , which implies (see the details in Theorems 3.2 and 4.3 in [6]) that
 v 
 we can achieve the same rate O(1/ n) for the approximation error in (7) if condition iii) is satisfied.
 Remark 1: From Definition 4,   is inversely proportional to . In fact, depending of how rich is  can go from being , passing from being polynomial in 1/, to being   (see a number of examples in [6,
 Chp. 7] and references therein). In fact iii) provides a bound on how fast N should tend to infinity as  goes to zero, to guarantee zero-rate in the learning scheme. It is simple to show that N being   with q ? [0,2) is sufficient to get log2(N1/vn ) being o(n). This is a condition satisfied by a large collection of totally bounded classes in P(X) (see [6]).
 V. THE PARAMETRIC SCENARIO
 The results presented so far are of theoretical interest, because they relay on the Skeleton partition of F, which is typically unknown. Moving on the direction to make the Skeleton learning scheme of practical interest, we revisit the scenario studied in [5], in which F = {µ? : ? ? T} ? AC(X) is indexed by T, with T being a compact set leaving in a finite dimension Euclidean space Rk, or what people in learning theory call parametric families3. Interestingly, in this context we can achieve optimal learning rates adopting a practical L1covering of F, induced by a uniform partition of T. Let us first start with some preliminaries.
 Definition 6: (Raginsky [5]) Let F = {µ? : ? ? T} with T ? Rk. Let IF : T ? F be the index function of F, that maps ? ? T to µ? ? P(X). IF(·) is said to be locally uniformly Lipschitz, if there exists r > 0 and m > 0, such ?? ? T, ?f ? Br(?),
 	V (µ?,µf) = m||? - f||,	(18)
 ,where Br(?) ? T denotes the ball of radius r (with respect to the Euclidean norm in Rk) centered at ?.
 PROPOSITION 1: Let F = {µ? : ? ? T} ? P(X) with
 T ? Rk. If T is bounded (?L > 0 such that T ?
 The proof is presented in Appendix VI-B.
 Under the assumptions of Proposition 1,
 . (The proof is given in Appendix VI-C.)
 for some constant c > 0. Substituting this result in (15),	Under the assumptions of Proposition 1, let  
 denote the learning rule of length n associated to the minimum
 	.	(17)	distance principle in (6), where instead of using the oracle
 Skeleton covering G of F, it uses the uniform covering
 The argument concludes by replacing, a of T that induces a sub-optimal -covering of F, denoted solution which achieves the intended rate of convergence by(see details of this construction in Appendix VI-C). in (14). Finally by construction, the rate of the learning rule Then by definition,
 dlog2(N1/vn)e of length-n is	n	, which tends to zero by iii).this last part from Corollary 1. With this, let
 Interpreting this result, by imposing extra combinatorialdenote our practical learning scheme regularity assumptions on F (conditions ii) and iii)), we
 indexed by the precision numbers. We are
  ) and the mapping IF : T ? F is locally uniformly Lipschitz, then F is totally bounded.
  
 can achieve the optimal rate of convergence to estimate µ uniformly in F. More precisely, by imposing that the Yatracosv
 class is a VC class, we can achieve O(1/ n) rate of convergence for the estimation error in (7) and, on the other hand, in a position to integrate the results presented in Section IV
 3This parametric case is the setting considered by Raginsky [5] for the problem of fixed-rate lossy source coding and modeling.
 (Theorem 4, Proposition 1, Corollary 1 and Remark 1) to state the following implication.
 THEOREM 5: Under the assumptions of Proposition 1, let us in addition assume that the Yatracos collection AT =
 is a VC class. Then the learning
 v 
 scheme ?((1˜	/ n)n=1) satisfies that
 	 ,	(19)
  ,	where	 
  .
 Proof: Let be the -covering induced from the uniform partition of T presented in Appendix VI-C. From this we can construct the minimum-distance estimate in (6) adopting the Yatracos class of  (with index set T˜), i.e.,  from (5):
 	  arg min 	.	(20)
 Using the same arguments presented in the proof of Theorem 4, we can obtain an equivalent version of (17), i.e., ?,
 	 .	(21)
 From this point, the proof derives from the arguments of
 Theorem 4 and the fact that  
 (from Corollary 1). To conclude note that the same result extends to the Skeleton estimate of Section III-A as.
 v
 The behavior of the distortion overhead of ?((1˜ / n)n=1) in (19), has a faster asymptotic rate than its counterpart obtained by Raginsky [5] (O(plogn/n)), under the same parametric setting and with the same bits-per-sample overheadv of O(logn/n). In addition, ?((1˜ / n)n=1) offers an implementable scheme, as its minimum distance decision is carried out on a finite number of candidates.
 VI. APPENDIX
 A. Statistical Learning Notions
 Let C ? B(X) be a collection of measurable events, and   be a sequence of n points in Xn.
 Then we define by   the number of different sets in {{x1,x2,..,xn} n B : B ? C}, and the shatter coefficient of
 C by   is an indicator of the richness of C to dichotomize a finite sequence of points in the space, where by definition Sn(C) = 2n.
 Definition 7: The largest integer n where Sn(C) is strictly less than 2n is called the Vapnik and Chervonenkis (VC) dimension of C [12]. If Sn(C) = 2n for all n, then the class is said to have infinite VC-dimension.
 B. Proof of Proposition 1
 First note that T is contained in a compact set Nk [-L,L] ? Rk, then, , there exists a finite coveri=1
 ing	such that   . On the other hand, from the locally uniformly Lipschitz assumption on IF : T ? F, there exists r > 0 and m > 0 such that V (µ?,µf) = m||? - f||, ?? ? T, ?f ? Br(?). Then let us consider , then by construction
 of  
 	 	(22)
 where BdV (µ) = {v ? P(X) : V (v,µ) < d} is the ball induced from the total variational distances, and the last inequality is from the Lipschitz condition. Hence from (22),  there
 exists and 
 P(X), such that .	 
 C. Proof of Corollary 1
 Let (m,r) be the uniform parameters that characterize the Lipschitz condition of IF(·) (Definition 6). Without loss of generality, let us assume the critical regime where  , hence from (22) N is upper bounded by , which is the covering number of T (see Appendix VI-B). As T ?
  , we will work with a uniform partition of   to find a bound for . Let . Then inducing a product-type partition, where in each coordinate we have   uniform-length cells, we have the required ¯covering. The number of prototypes is , which as a function of .  
 VII. ACKNOWLEDGMENT
 The work of J. Silva is supported by funding from Fondecyt Grant 1110145, Conicyt-Chile.. The work of M. S. Derpich is supported by funding from post-doc Fondecyt project nr. 3100109, and the Conicyt Anillos project ACT-53.
 ﻿The objective of this note is to report some potentially useful mutual information inequalities. 
 Throughout this section, and unless otherwise stated, x, xi, i ? N0, y, z and n are continuous random variables taking values in appropriate subsets of Rn. We assume that they all have well defined probability density functions (PDFs), which we denote by fx, fxi, fy, fz and fn, respectively, and well defined joint PDFs denoted by fxy, fxz, etc.  We also use the notation fx|y to refer to the conditional PDF of x, given y. All definitions and results in this section are standard and can be found in [1]. 
 Definition 1 (Differential entropy) The differential entropy of x is defined via 
 If x and y are independent, then e2h(x+y) = e2h(x) + e2h(y) (e2h(x) is called entropy power of x. This property is called entropy power inequality.) 
 Definition 2 (Mutual information) The mutual information between x and y is defined via 
 Definition 3 (Markov chain) The random variables x,y and z are said to form a Markov chain (in that order) if and only if f(x,z|y) = f(x|y)f(z|y), i.e., if and only if x and z are conditionally independent given y. If that is the case, we write 
 Theorem 1 (Data processing inequality) If x ? y ? z, then I(x;y) = I(x;z). Equality holds if and only if, in addition, x ? z ? y.	¤¤¤ Definition 4 (Divergence between PDFs) The divergence of the distribution of x with respect to the distribution of y (in short, the divergence between x and y) is defined by 
 If xG is a second order Gaussian random variable and x is any other random variable with the same mean and covariance matrix, then 
 Remark 1 (Conditional divergence) It will prove useful to consider an extension of the definition of divergence. Given two joint distributions fxy and fwz, we define the conditional divergence between them5 via 
 If xG and yG are jointly Gaussian random variables having joint PDF fxGyG, and x and y are arbitrary random variables having a joint PDF fxy with the same first and second order moments as fxGyG, then 
 We end this section with an extension of the notion of differential entropy to random processes. 
 Definition 5 (Differential entropy rate) Consider an asymptotically stationary process x. The differential entropy rate of x is defined by 
 If x is stationary, then it is clear that h¯(x) = h(x(k)), with equality if and only if x is a sequence of independent random variables (recall Fact 1). 
 Theorem 2 (Differential entropy rate (see, e.g., [2,3])) If a stationary process xˆ is filtered by a stable filter having frequency response H(ej?), then the filter output x has an entropy rate given by 
 Lemma 1 Consider the situation depicted in Figure 1, where x and n are m-dimensional random variables that have arbitrary distributions. If x and n are independent, and xG and nG denote independent m-dimensional Gaussian random variables having the same mean and covariance matrix as x and n, respectively, then 
 Proof: Using Facts 2 and 1, the independence of x,n and xG,nG, and the definition of D(·||·), it is easy to see that 
 Lemma 2 Consider the situation depicted in Figure 1, where x and n are m-dimensional random variables, x is Gaussian and n has an arbitrary distribution. If nG denotes an m-dimensional Gaussian random variable, jointly Gaussian with x, having the same mean and covariance matrix as n, and such that the cross-covariance between n and x equals the cross-covariance between nG and x, then 
 with equality if the covariance matrix of x+n is non-singular, and n is Gaussian and jointly Gaussian with x. 
 Use of the facts in Remark 1 the first part of the result follows. Clearly, if n is Gaussian, then equality holds in (14). The proof of the converse can be found in [4]. ¤¤¤ 
 Lemma 3 Consider the situation depicted in Figure 1, where x and n are independent scalar random variables with arbitrary distributions. If xG and nG denote independent scalar Gaussian random variables having the same mean and covariance matrix as x and n, and D(x||xG) = D(n||nG), then 
 Proof: We will use the proof of Lemma 1. If the right hand side in equality (a) in (13) were positive, then the result would be true. Thus, we will start examining the difference D(n||nG)-D(x+ 
 where we have used Fact 3, the independence of xG,nG and Gaussianity. On the other hand, the entropy power inequality allows one to conclude that, since x,n are independent, 
 and, since the variance of the Gaussian and non-Gaussian random variables is the same, we have from 
 where (a) follows from Fact 1 and (b) from Facts 3 and 1, and the fact that the variance of the Gaussian and non-Gaussian random variables is the same. The result follows using (20) and (19) in equality (a) in (13).	¤¤¤ 
 Definition 6 Consider two random processes v and w. We define (if the defining limits exist) the mutual information rate between v and w as 
 Lemma 4 Consider the feedback system in Figure 2, where 1-F(z) is stable and strictly proper (i.e., limz?8 F(z) = 1), d is a random process, and q is an i.i.d. sequence that is independent of d and of the initial state of F(z). Then, 
 Proof: By definition of mutual information rate and the chain rule of mutual information we have that 
 Since w depends causally on d, it follows that I(w(i);dk-1|wi-1) = I(w(i);di|wi-1). Thus, 
 where (a) follows from Fact 2, (b) follows from the definition of n and Fact 1, (c) follows from the fact that, by definition of n, M ? (wi-1,di) ? (ni-1,di) for every random variable M, and (d) follows from the fact that, since d is independent of q and of the initial state of F(z), d is independent of n and, thus, n(i) ? ni-1 ? di holds. We also have that 
 where (a) follows from the definition of variables in Figure 2, (b) follows from Fact 1 and the fact that, by definition, M ? (wi-1,vi) ? (qi-1,di) for every random variable M, and (c) follows from the fact that both the initial state of F(z) and d being independent of q, q being i.i.d., and F(z) being strictly proper guarantees that q(i) ? qi-1 ? vi. 
 Use of Theorem 2, (26) and the Bode integral theorem (see, e.g., [5]) yields the result.	¤¤¤ 
 ﻿We improve the existing achievable rate regions for causal and for zero-delay source coding of stationary Gaussian sources under an average mean squared error distortion measure. To begin with, we find a closed-form expression for the information-theoretic causal rate-distortion function (RDF) under such distortion measure, denoted by , for first-order Gauss–Markov processes. is a lower bound to the optimal performance theoretically attainable (OPTA) by any causal source code, namely . We show that, for Gaussian sources, the latter can also be upper bounded as bits/sample. In order to 
 analyze for arbitrary zero-mean Gaussian stationary sources, we introduce , the information-theoretic causal RDF when the reconstruction error is jointly stationary with the source. Based upon , we derive three closed-form upper bounds to the additive rate loss defined as  , where   denotes Shannon’s RDF. Two of these bounds are strictly smaller than 0.5 bits/sample at all rates. These bounds differ from one another in their tightness and ease of evaluation; the tighter the bound, the more involved its evaluation. We then show that, for any source spectral density and any positive distortion  ,   can be realized by an additive white Gaussian noise channel surrounded by a unique set of causal pre-, post-, and feedback filters. We show that finding such filters constitutes a convex optimization problem. In order to solve the latter, we propose an iterative optimization procedure that yields the optimal filters and is guaranteed to converge to . Finally, by establishing a connection to feedback quantization, we design a causal and a zero-delay coding scheme which, for Gaussian sources, achieves an 
 bits/sample, respectively. This implies that the OPTA among all zero-delay source codes, denoted by , is upper bounded as bits/sample. 
 IndexTerms—Causality, convex optimization, differential pulsecode modulation, entropy coded dithered quantization (ECDQ), noise-shaping, rate-distortion theory, sequential coding. 
 N zero-delay source coding, the reconstruction of each input sample must take place at the same time instant the corresponding input sample has been encoded. Zero-delay source coding is desirable in many applications, e.g., in real-time applications where one cannot afford to have large delays [1], or in systems involving feedback, in which the current input depends on the previous outputs [2]–[4]. A weaker notion closely related to the principle behind zero-delay codes is that of causal source coding, wherein the reproduction of the present source sample depends only on the present and past source samples but not on the future source samples [5], [6]. This notion does not preclude the use of noncausal entropy coding, and thus, it does not guarantee zero-delay reconstruction. Nevertheless, any zero-delay source code must also be causal. 
 It is known that, in general, causal codes cannot achieve the rate-distortion function (RDF)   of the source, which is the optimal performance theoretically attainable (OPTA) in the absence of causality constraints [7]. However, it is in general not known how close to   one can get when restricting attention to the class of causal or zero-delay source codes, except, for causal codes, when dealing with memory-less sources [5], stationary sources at high resolution [6], or first-order Gauss–Markov sources under a per-sample mean squared error (MSE) distortion metric [3]. 
 For the case of memory-less sources, it was shown by Neuhoff and Gilbert that the optimum rate-distortion performance of causal source codes, say  , is achieved by time-sharing at most two memory-less scalar quantizers (followed by entropy coders) [5]. In this case, the rate loss due to causality was shown to be given by the space-filling loss of the quantizers, i.e., the loss is at most  ( ) bits/sample. For the case of Gaussian stationary sources with memory and MSE distortion, Gorbunov and Pinsker showed that the information-theoretic  causal RDF, here denoted by 
 (to be defined formally in Section II) and which satisfies , tends to Shannon’s RDF as the distortion goes to zero [8], [9]. The possible gap between the OPTA of causal source codes and this information-theoretic causal RDF was not assessed. Since operational data rates are lower bounded by the mutual information between the source and its reconstruction, we also have that  . 
 On the other hand, for arbitrary stationary sources with finite differential entropy and under high-resolution conditions, it was shown in [6] that the rate-loss of causal codes (i.e., the difference between their OPTA and Shannon’s RDF) is at most the space-filling loss of a uniform scalar quantizer. With the exception of memory-less sources and first-order Gauss–Markov sources, the “price” of causality at general rate regimes for other stationary sources remains an open problem. However, it is known that for any source, the mutual information rates across an additive white Gaussian noise (AWGN) channel and across a scalar entropy coded dithered quantization (ECDQ) channel do not exceed   by more than 0.5 and 0.754 bits/sample, respectively [10], [11]. This immediately yields the bounds  and	. 
 In causal source coding, it is generally difficult to provide a constructive proof of achievability since Shannon’s random codebook construction, which relies upon jointly encoding long sequences of source symbols, is not directly applicable even in the case of memory-less sources. Thus, even if one could obtain an outer bound for the achievable region based on an information-theoretic RDF, finding the inner bound, i.e., the OPTA, would still remain being a challenge. 
 There exist other results related to the information-theoretic causal RDF, in which achievability is not addressed. The minimum sum-rate necessary to sequentially block-encode and block-decode two scalar correlated random variables under a coupled fidelity criterion was studied in [12]. A closed-form expression for this minimum rate is given in [12, Th. 4] for the special case of a squared error distortion measure and a per-variable (as opposed to a sum or average) distortion constraint. In [2], the minimum rate for causally encoding and decoding source samples (under either a per-sample or average distortion constraints) was given the name sequential rate-distortion function (SRDF). Under a per-sample MSE distortion constraint  , it was also shown in [2, p. 187] that for a first-order Gauss–Markov source  , whereis a zero-mean white Gaussian process with variance , the information-theoretic SRDF   takes the form 
 for all .  No expressions are known for for higher order Gauss–Markov sources. Also, with the exception of memory-less Gaussian sources, , with its average MSE distortion constraint (weaker than a per-sample MSE constraint), has not been characterized. 
 In this paper, we improve the existing inner and outer rate-distortion bounds for causal and for zero-delay source coding of zero-mean Gaussian stationary sources and average MSE distortion. We start by showing that, for any zero-mean Gaussian source with bounded differential entropy rate, the causal OPTA exceeds   by less than approximately 0.254 bits/sample. Then, we revisit the SRDF problem for first-order Gauss–Markov sources under a per-sample distortion constraint schedule and find the explicit expression for the corresponding RDF by means of an alternative, constructive derivation. This expression, which turns out to differ from the one found in [2, bottom of p. 186], allows us to show that for first-order 
 Gauss–Markov sources, the information-theoretic causal RDF   for an average (as opposed to per-sample) distortion 
 measure coincides with (1). In order to upper bound  for general Gaussian stationary sources, we introduce the information-theoretic causal RDF when the distortion is jointly stationary with the source and denote it by  . We then derive three closed-form upper bounding functions to the rate-loss 
 , which can be applied to any stationary Gaussian random process. Two of these bounds are, at all rates, strictly tighter than the best previously known general bound of 0.5 bits/sample. Since, by definition,  , we have that 
 . As we shall see, equality would hold in  if could be realized by a test channel with distortion jointly stationary with the source, which seems a reasonable conjecture for stationary sources. 
 We do not provide a closed-form expression for   (except for first-order Gauss–Markov sources), and thus the upper bound on the right-hand side (RHS) of (2) (the tightest bound discussed in this paper) is not evaluated analytically for the general case. However, we propose an iterative procedure that can be implemented numerically and which allows one to evaluate , for any source power spectral density (PSD) and 
 , with any desired accuracy. This procedure is based upon the iterative optimization of causal pre-, post-, and feedback filters around an AWGN channel. A key result in this paper (and its second main contribution) is showing that such filter optimization problem is convex in the frequency responses of all the filters. This guarantees that the mutual information rate between source and reconstruction yielded by our iterative procedure converges monotonically to   as the number of iterations and the order of the filters tend to infinity. This equivalence between the solution to a convex filter design optimization problem and avoids the troublesome minimization over mutual informations, thus making it possible to actually compute in practice, for general Gaussian stationary sources. We then make the link between   and the OPTA of causal and zero-delay codes. More precisely, when the AWGN channel is replaced by a subtractively dithered uniform scalar quantizer (SDUSQ) followed by memory-less entropy coding, the filters obtained with the iterative procedure yield a causal source coding system whose operational rate is below   bits/sample. If the entropy coder in this system is restricted to encode quantized values individually (as opposed to long sequences of them), then this system achieves zero-delay operation with an operational rate belowbits/sample. This directly translates into an upper bound to the OPTA of zero-delay source codes, namely . To illustrate our results, we present an example for a zero-mean AR-1 and a zero-mean AR-2 Gaussian source, for which we evaluate the closed-form bounds and obtain an approximation of   numerically by applying the iterative procedure proposed herein. 
 This paper is organized as follows. In Section II, we review some preliminary notions. We prove in Section III that the OPTA for Gaussian sources does not exceed the information-theoretic RDF by more than approximately 0.254 bits/sample. Section IV contains the derivation of a closed-form expression for   for first-order Gauss–Markov sources. In Section V we formally introduce and derive the three closed-form upper bounding functions for the information-theoretic rate-loss of causality. Section VI presents the iterative procedure to calculate , after presenting the proof of convexity that guarantees its convergence. The two examples are provided in Section VII. Finally, Section VIII draws conclusions. (Most of the proofs of our results are given in Sections IX–XV.) 
 denote, respectively, the set of real numbers and the set of nonnegative real numbers. and denote, respectively, the sets of integers and positive integers. We use nonitalic lower case letters, such as  , to denote scalar random variables, and boldface lowercase and uppercase letters to denote vectors and matrices, respectively. We use , , and   to denote the Moore–Penrose pseudoinverse, the column span, and the null space of the matrix , respectively. The expectation operator is denoted by . The notation refers to the variance of . The notation describes a one-sided random process, which may also be written simply as . We write to refer to the sequence . The PSD of a wide-sense stationary process is denoted by 
 . Notice that	. For any two functions	,	, we write the standard squared norm and inner product as and	, 
 respectively, where denotes complex conjugation. For one-sided random processes and , the term denotes the 
 mutual information rate between and , provided the limit exists. Similarly, for a stationary random process 
 A source encoder–decoder (ED) pair encodes a source into binary symbols, from which a reconstruction is generated. The end-to-end effect 
 of any ED pair can be described by a series of reproduction functions , such that, for every 
 where we write as a short notation for . Following [5], we say that an ED pair is causal if and only if it satisfies the following definition [5]. 
 Definition 1 (Causal Source Coder): An ED pair is said to be causal if and only if its reproduction functions are such that 
 It also follows from Definition 1 that an ED pair is causal if and only if the following Markov chain holds for every possible random input process  : 
 It is worth noting that if the reproducing functions are random, then this equivalent causality constraint must require that (4) is satisfied for each realization of the reproducing functions 
 be the total number of bits that the decoder has received when it generates the output subsequence  . Define   as the random binary sequence that contains the bits that the decoder has received when   is generated. Notice that   is, in general, a function of all source samples, since the binary coding may be noncausal, i.e.,   may be generated only after the decoder has received enough bits to reproduce 
 , with . We highlight the fact that even though may contain bits which depend on samples with , the random sequences   may still satisfy (4), i.e., the ED pair can still be causal. Notice also thatis a random variable, which depends on  , the functions , and on the manner in which the source is encoded into the binary sequence sent to the decoder. 
 For further analysis, we define the average operational rate of an ED pair as [5] 
 In the sequel, we focus only on the MSE as the distortion measure. Accordingly, we define the average distortion associated with an ED pair as 
 The aforementioned notions allow us to define the operational causal RDF as follows: 
 We note that the operational causal RDF defined previously corresponds to the OPTA of all causal ED pairs. 
 where the last inequality turns into equality for a causal ED pair, since in that case (4) holds. Thus, combining (5), (8), and (9) 
 (10) This lower bound motivates the study of an information-theoretic causal RDF, as defined in the following. 
 Definition 3: The information-theoretic causal RDF for a source  , with respect to the average MSE distortion measure, is defined as 
 The aforementioned definition is a special case of the nonanticipative epsilon-entropy introduced by Pinsker and Gorbunov, which was shown to converge to Shannon’s RDF, for Gaussian stationary sources and in the limit as the rate goes to infinity [8], 
 In the noncausal case, it is known that for any source and for any single-letter distortion measure, the OPTA equals the information-theoretic RDF [13]. Unfortunately, such a strong equivalence between the OPTA and the information-theoretic 
 ]. (One exception is if one is to jointly and causally encode an asymptotically large number of parallel Gaussian sources,inwhichcase   canbeshowntocoincidewiththe OPTA of causal codes.) Nevertheless, as outlined in Section I, it is possible to obtain lower and upper bounds to the OPTA of causal codes from	. Indeed, and to begin with, since , it follows directly from (7) and (10) that 
 The previous inequality in (11) is strict, in general, and becomes equality when the source is white or when the rate tends to infinity. Also, as it will be shown in Section III, for Gaussian sources, does not exceed by more than approx- 
 For completeness, and for future reference, we recall that for any MSE distortion , the RDF for a stationary Gaussian source with PSD is equal to the associated informationtheoretic RDF, given by the “reverse water-filling” equations [7] (12a) 
 Although, in general, it is not known by how much  exceeds  , for Gaussian stationary sources one can readily 
 More generally, it is known that, for any source, the mutual information across an AWGN channel [which satisfies (4)] introducing noise with variance  , exceeds 
 Until now it has been an open question whether a bound tighter than (15) can be obtained for sources with memory and at general rate regimes [10]. In Sections IV–VI, we show that for Gaussian sources, this is indeed the case. But before focusing on upper bounds for  , its operational importance will be established by showing in the following section that, for Gaussian sources, the OPTA does not exceed   by more than approximately 0.254 bits/sample. 
 and, an upper bound to can be readily obtained from by adding (approximately) 0.254 bits per sample to . This result is first formally stated and proved for finite subsequences of any Gaussian source. Then, it is extended to Gaussian stationary processes. We start with two definitions. 
 Definition 4: The causal information-theoretic RDF for a zero-mean Gaussian random vector of length is defined as 
 where the infimum is taken over all output vectors satisfying the causality constraint 
 Definition 5: The operational causal RDF for a zero-mean Gaussian random vector of length   is defined as 
 Lemma 1: Let    be two random vectors with zero mean and the same covariance matrix, i.e., , and having the same cross-covariance matrix with respect to , that is, . If 
 If furthermore , then equality is achieved in (20) if and only if with and being jointly Gaussian. 
 Notice that if one applies Lemma 1 to a reconstruction error with which the output sequence satisfies the causality constraint (4), then the Gaussian version of the same reconstruction error will also produce an output causally related with the input. To see this, let 
 be the Cholesky factorization of the covariance matrix of the random vector , , where is a lower triangular matrix. This allows one to write as 
 where   satisfy (4). Then, there exists a set of reproduction functions satisfying the conditions of Definition 1 which generate each partial vector  . Specifically, for any given , there exists a function , such that . From (21) and given that 
 and thus , for some function . From this and the fact that is independent of , we have that 
 where denotes probabilistic independence. On the other hand, for each , , let be the minimum mean square error (MMSE) linear estimator of 
 given . Then, adopting the notation for the -bytop-left corner submatrix of a matrix , we have that 
 where   follows from (22) and all the subsequent equalities stem from (21) and from the fact that is lower triangular. Therefore, since the residual is uncorrelated to  , it holds that 
 The result stated in Lemma 2 for Gaussian random vector sources is extended to Gaussian stationary processes in the following theorem (the second main result of this section). 
 The fact that   for Gaussian sources allows one to find upper bounds to the OPTA of causal codes by explicitly finding or upper bounding  . This is accomplished in the following sections. 
 In this section, we will find   when the source is a firstorder Gauss–Markov process. More precisely, we will show that the information-theoretic causal RDF  , which is associated with an average distortion constraint, coincides with the expression for the SRDF on the RHS of (1) obtained in [2] for a per-sample distortion constraint. To do so, and to provide also a constructive method of realizing the SRDF as well as  , we will start by stating an alternative derivation of the SRDF for scalar source sequences of length  . In this case, from its definition in [2, Def. 5.3.5, p. 147], the SRDF takes the following form: 
 where the infimum is over all conditional distributions (of  given  ) satisfying the causality constraint (17) and the distortion schedule constraints 
 Before proceeding, it will be convenient to introduce some additionalnotation.Foranyprocess ,wewrite , , to denote the random column vector and adopt the shorter notation . For any two random vectors , 
 It was already stated in Lemma 1 that the reconstruction vector which realizes mutual information between a Gaussian source vector and for any given MSE distortion constraint, must be jointly Gaussian with the source. This holds, in particular, for a realization of the SRDF with distortion schedule  . In the next theorem, we will obtain an explicit expression for this RDF and prove that in its realization, the sample distortions 
 Fig. 1. Recursive Procedure 1 at its th iteration. Starting from known covariance matrices   , their next partial rows and columns are found. The numbers indicate the step in the algorithm which reveals the corresponding part of the matrix. 
 step   is responsible of revealing the partial rows and columns indicated by number   in the figure. 
 The aforementioned results are formally stated in the following theorem, which also gives an exact expression for the SRDF of first-order Gauss–Markov sources. 
 the constraints  . Under the latter interpretation, nothing precludes one from choosing an arbitrarily large value for, say  , yielding an arbitrarily large value for the second term in the summation on the RHS of (33), which is, of course, inadequate. 
 We are now in a position to find the expression for for first-order Gauss–Markov sources. This is done in the following theorem, whose proof is contained in Section XII. 
 The technique applied to prove Theorems 2 and 3 does not seem to be extensible to Gauss–Markov processes of order greater than 1. In the sequel, we will find upper bounds to   for arbitrary (any order) stationary Gaussian sources. 
 In order to upper bound the difference between and for arbitrary stationary Gaussian sources, we will start this section by defining an upper bounding function for , denoted by . Wewill then derive three closed-form upper bounding functions to the rate-loss , applicable to any Gaussian stationary process. Two of these bounds are 
 Definition 6 (Causal Stationary RDF): For a stationary , the information-theoretic causal SRDF 
 Next, we derive three closed-form upper bounding functions to that are applicable to arbitrary zero-mean stationary Gaussian sources with finite differential entropy rate. This result is stated in the following theorem, proved in Section XIII: 
 Theorem 4: Let be a zero-mean Gaussian stationary source with PSD with bounded differential entropy rate and variance . Let denote Shannon’s RDF for [given by (12)], and let denote the quadratic Gaussian RDF for source-uncorrelated distortions for the source defined in (13). Let denote the information-theoretic causal RDF (see Definition 3). Then, for all 
 Notice that is independent of , being therefore numerically simpler to evaluate than the other bounding functions introduced in Theorem 4. However, as is decreased away from and approaches , becomes very loose. In fact, it can be seen from (110a) that for , the gap between and is actually upper bounded by , which is of course tighter than , but requires one to evaluate . 
 It is easy to see that time-sharing between two causal realizationswithdistortions , andrates , yields an output process which satisfies causality with a rate-distortion pair corresponding to the linear combination of , 
 . Thus, in some cases, one could get a bound tighter than by considering the boundary of the convex hull of the region above and then subtracting . However, such bound would be much more involved to compute, since it requires to evaluate not only  , but also the already mentioned convex hull. 
 It is also worth noting that the first term within the erator on the RHS of (39) becomes smaller when is reduced. This difference, which from Jensen’s inequality is 
 Fig. 2. AWGN channel within a “perfect reconstruction” system followed by the causal denoising filter	. 
 always nonnegative, could be taken as a measure of the “nonflatness” of the PSD of (especially when ). Indeed, as   approaches a white process, tends to zero. 
 It can be seen from (36) that provides the tightest upper bound for the information-theoretic RDF among all bounds presented so far. Although it does not seem to be feasible to obtain a closed-form expression for , we show in the next section how to get arbitrarily close to it. 
 In this section, we present an iterative procedure that allows one to calculate   with arbitrary accuracy, for any . 
 In addition, we will see that this procedure yields a characterization of the filters in a dithered feedback quantizer [15] that achievean operationalrate which isupper boundedby   [bits/sample]. 
 To derive the results mentioned previously, we will work on a scheme consisting of an AWGN channel and a set of causal filters, as depicted in Fig. 2. In this scheme, the source   is Gaussian and stationary, with PSD  , and is assumed to 
 is a zero-mean Gaussian process with i.i.d. samples, independent of . Thus, between and lies the AWGN channel . The filter is stable and strictly causal, i.e., it has at least a one sample delay. The filters   are causal and stable. The idea, to be developed in the remainder of this section, is to first show that with the filters that minimize the variance of the reconstruction error for a fixed ratio  , the system of Fig. 2 attains a mutual information rate between source and reconstruction equal to  , with a reconstruction MSE equal to  . We will then show that finding such filters is a convex optimization problem, which naturally suggests an iterative procedure to solve it. 
 In order to analyze the system in Fig. 2, and for notational convenience, we define 
 The perfect reconstruction condition (41) induces a division ofrolesinthesystem, whichwilllatertranslateintoa convenient parametrization of the optimization problem associated with it. On the one hand, because of (41), the net effect of the AWGN channel and the filters , , and is to introduce 
 (colored) Gaussian stationary additive noise, namely , independent of the source. The PSD of this noise, , is given by 
 The diagram in Fig. 3 shows how the signal transfer function and the noise transfer function act upon and to yield the output process. 
 On the other hand, by looking at Fig. 2, one can see that plays also the role of a denoising filter, which can be utilized to reduce additive noise at the expense of introducing linear disacts upon the Gaussian stationary corrupted by additive Gaussian stationary noise 
 On the RHS of (44), the first term is the variance of the additive, source independent, Gaussian noise. The second term corresponds to the error due to linear distortion, that is, from the deviation of   from a unit gain. 
 Since we will be interested in minimizing , for any given and , the filters and in Fig. 2 are chosen so as to minimize in (44), while still satisfying (41). From the viewpoint of the subsystem comprised of the filters , , and   and the AWGN channel, acts as an error frequency weighting filter, see (43). Thus, for any and , the filters   that minimize are those characterized in [15, Prop. 1], by setting in [15, eq. (20b)] equal to  . With the minimizer filters in [15], the variance of the source-independent error term is given by 
 On the other hand, the filter   needs to be strictly causal and stable. As a consequence, it holds that 
 which follows from Jensen’s formula [16] (see also the Bode Integral Theorem in [17]). 
 Thus, from (44) and (45), if one wishes to minimize the reconstruction MSE by choosing appropriate causal filters in the 
 where   denotes the space of all frequency responses that can be realized with causal filters. 
 From the aforementioned lemma, whose proof can be found in Section XIV, one can find   either by solving the minimization in Definition 6 or by solving Optimization Problem 1. In the following, we will pursue the latter approach. As we shall see, our formulation of Optimization Problem 1 provides a convenient parametrization of its decision variables. In fact, it makes itpossibletoestablishthe convexityofthecostfunctional defined in (46a) with respect to the set of all causal frequency responses involved. That result can be obtained directly from the following key lemma, proved in Section XV: 
 Proof: With the change of variables  in (47), we obtain  , see (44). With this, Optimization Problem 1 amounts to finding the functions and that 
 Clearly, the space of frequency responses associated with causal transfer functions   is a convex set. This implies that   is a convex set. In addition, is also a convex set, and from Lemma 4, is a convex functional. Therefore, the optimization problem stated in (48), and thus 
 Lemma 5 and the parametrization in Optimization Problem 1 allow one to define an iterative algorithm that, as will be shown later, yields the information-theoretic causal RDF. Such algorithm is embodied in iterative Procedure 2. 
 Notice that after solving Step 3 in the first iteration of Procedure 2, the MSE is comprised of only additive noise independent of the source.4 Step 4 then reduces the MSE by attenuating source-independent noise at the expense of introducing linear distortion. Each step reduces the MSE until a local (or global) minimum of the MSE is obtained. Based upon the convexityofOptimization Problem 1, the following theorem, which is the main technical result in this section, guarantees convergence to the global minimum of the MSE, say  , for a given end-to-end mutual information. Since all the filters in Optimization Problem 1 are causal, the mutual information achieved at this global minimum is equal to  . 
 Theorem 5 (Convergence of Iterative Procedure 2): Iterative Procedure 2 converges monotonically to the unique and that realize  . More precisely, letting denote the 
 4Indeed, after solving Step 3 for the first time, the resulting rate is the quadratic Gaussian RDF for source uncorrelated distortionsintroduced in [14] [see also (14)]. 
 Fig. 4. Uniform scalar quantizer and dither signals , , forming an SDUSQ, replacing the AWGN channel of the system from Fig. 2. 
 Fig. 5. (in bits/sample) and several upper bounding functions for   for zero-mean unit variance white Gaussian noise filtered through . The resulting source variance is 5.26. 
 MSE obtained after the  th iteration of Iterative Procedure 2 aimed at a target rate  , we have that 
 Proof: The result follows directly from the fact that Optimization Problem 1 is strictly convex in and , which was shown in Lemma 4, and from Lemma 3. 
 The aforementioned theorem states that the stationary information-theoretic causal RDF can be obtained by using Iterative Procedure 2. In practice, this means that an approximation arbitrarily closeto   for a given   can be obtained if sufficient iterations of the procedure are carried out. 
 The feasibility of running Iterative Procedure 2 depends on being able to solve each of the minimization sub-problems involved in steps 3 and 4. We next show how these subproblems can be solved. 
 If   is given, the minimization problem in Step 3 of Iterative Procedure 2 is equivalent to solving a feedback quantizer design problem with the constraint and with error weighting filter . Therefore, the solution to Step 3 is given in closed form by [15, eqs. (20), (29), and (31b)], where   in [15, eq. (20b)] is replaced by  . The latter 
 Fig. 6. (in bits/sample) and several upper bounding functions for for zero-mean unit variance white Gaussian noise filtered through . The resulting source variance is 6.37. 
 equations in [15] characterize the frequency response magnitudes of the optimal , and given . The existence of rational transfer functions , , and arbitrarily close (in an sense) to such frequency response magnitudes is also shown in [15]. 
 Finding the causal frequency response that minimizes   for a given is equivalent to solving 
 for a given  , where   is as defined in (49). Since  are convex, (50) is a convex optimization problem. As such, its global solution can always be found iteratively. In particular, if is constrained to be an th order finite impulse response (FIR) filter with impulse response  , such that   denotes the discrete-time 
 is a convex functional. The latter follows directly from the convexity of and the linearity of . As a consequence, one can solve the minimization problem in Step 4, to any degree of accuracy, by minimizing over the values of the impulse response of , using standard convex optimization methods (see, e.g., [18]). This approach also has the benefit of being amenable to numerical computation. 
 It is interesting to note that if the order of the denoising filter   were not a priori restricted, then, after Iterative Procedure 
 Wiener filter (i.e., the MMSE causal estimator) for the noisy signal that comes out of the perfect reconstruction system that precedes  . Notice also that one can get the system in Fig. 7 to yield a realization of Shannon’s using Iterative Procedure 1 by simply allowing to be noncausal. This would yield a system equivalent to the one that was obtained analytically in [10]. An important observation is that one could not obtain a realization offrom such a system in one step by simply replacing (a noncausal Wiener filter) by the MMSE causal estimator (that is, a causal Wiener filter). To see viously matched filters , and would no longer be optimal for . One would then have to change  , and then again, and so on, thus having to carry out infinitely many recursive optimization steps. However, a causally truncated version of the non causal Wiener filter   that realizes Shannon’s RDF could be used as an alternative starting guess in Step 2 of the iterative procedure. 
 If the AWGN channel in the system of Fig. 2 is replaced by an SDUSQ, as shown in Fig. 4, then instead of the noise 
 , we will have an i.i.d. process independent of  , whose samples are uniformly distributed over the quantization interval [19]. The dither signal, denoted by  , is an i.i.d. sequence of uniformly distributed random variables, independent of the source. Let be the quantized output of the SDUSQ. Denote the resulting input and the output to the quantizer, before adding and after subtracting the dither, respectively, as and , and let be the quantization noise introduced by the SDUSQ. Notice that the elements of are independent, both mutually and from the source . However, unlike and , the processes and are not Gaussian, since they contain samples of the uniformly distributed process . We then have the following. 
 Theorem 6: If the scheme shown in Fig. 4 uses the filters yielded by Iterative Procedure 2, and if long sequences of the quantized output of this system are entropy coded conditioned to the dither values in a memoryless fashion, then an operational rate   satisfying 
 InviewofTheorem6,andsinceanyEDpairusinganSDUSQ and linear time-invariant filters yields a reconstruction error jointly stationary with the source, it follows that the operational rate-distortion performance of the feedback quantizer thus obtained is within 
 Remark 2: When the rate goes to infinity, so does . In that limiting case, the transfer function tends to unity, and it follows from [15] that the optimal filters asymptotically satisfy , , 
 Moreover, when , the system of Fig. 4 achieves which, in this asymptotic regime, coincides with 
 If the requirement of zero-delay, which is stronger than that of causality, was to be satisfied, then it would not be possible to apply entropy coding to long sequences of quantized samples. This would entail an excess bit-rate not greater than 1 bit per sample, see, e.g., [13, Sec. 5.4]. Consequently, we have the following result. 
 Theorem 7: The OPTA of zero-delay codes, say  , can be upper bounded by the operational rate of the scheme of Fig. 4 when each quantized output value is entropy-coded independently, conditioned to the current dither value. Thus 
 The 0.254 bits/sample in (53), commonly referred to as the “space-filling loss” of scalar quantization, can be reduced by using vector quantization [11], [20]. Vector quantization could be applied while preserving causality (and without introducing delay) if the samples of the source were  -dimensional vectors. This would also allow for the use of entropy coding over 
 -dimensional vectors of quantized samples, which reduces the extra 1 bit/sample at the end of (53) to   bits/sample (see [13, Th. 5.4.2]). 
 It is worth noting that Lemma 3and the aboveanalysis reveals an interesting fact: the rate loss due to causality for Gaussian sources with memory, i.e., the difference between the OPTA of causal codes and  , is upper bounded by the sum of two terms. The first term is 0.254 bits/sample, and results from the space filling loss associated with scalar quantization, as was also pointed out in [6] for the high resolution situation. This term is associated only with the encoder. For a scalar Gaussian stationary source, such excess rate can only be avoided by jointly quantizing blocks of consecutive source samples (vector quantization), i.e., by allowing for noncausal encoding (or by encoding several parallel sources). The second term can be attributed to the reduced denoising capabilities of causal filters, compared to those of noncausal (or smoothing) filters. The contribution of the causal filtering aspect to the total rate-loss is indeed  . This latter gap can also be associated with the performance loss of causal decoding. 
 As a final remark, we note that the architecture of Fig. 2, which allowed us to pose the search of   as a convex optimization problem, is by no means the only scheme capable of achieving the upper bounds (52) and (53). For instance, it can be shown that the same performance can be attained removing either  or in the system of Fig. 2, provided an entropy coder with infinite memory is used. Indeed, the theoretical optimality (among causal codes) of the differential pulse-code modulationarchitecture,with predictivefeedbackand causalMMSE estimation at the decoding end, has been shown in a different setting [21]. 
 To illustrate the upper bounds presented in the previous sections, we here evaluate , , and , and calculate an approximation of via Iterative Procedure 2, for two Gaussian zero-mean AR-1 and AR-2 sources. These sources were generated by the recursion 
 where the elements of the process   are i.i.d. zero-mean unit-variance Gaussian random variables. 
 Iterative Procedure 2 was carried out by restricting   to be an eight-tap FIR filter. For each of the target rates considered, the procedure was stopped after four complete iterations. 
 The first-order source (Source 1) was chosen by setting the values of the coefficients in (54) to be  . This amounts to zero-mean, unit variance white Gaussian noise filtered through the coloring transfer function  . The second-order source (Source 2) consisted of zero-mean, unit variance white Gaussian noise filtered through the coloring transfer function . The resulting upper bounds for Source 1 and Source 2 are shown in Figs. 5 and 6, respectively. As predicted by (103) and (39), all the upper bounds for derived in Section V converge to 
 in the limit of both large and small distortions (i.e., when and	, respectively). 
 For both sources, the gap between and is significantly smaller than 0.5 bits/sample, for all rates at which was evaluated. Indeed, this gap is smaller than 0.22 
 For the first-order source, the magnitude of the coefficients of the FIR filter obtained decays rapidly with coefficient index. For example, when running five cycles of Iterative Procedure 2, using a tenth-order FIR filter for , for Source 1 at bits/sample, the obtained was 
 Such fast decay of the impulse response of suggests that, at least for AR-1 sources, there is little to be gained by letting   be an FIR filter of larger order. (It is worth noting that, in the iterative procedure, the initial guess for is a unit scalar gain.) The frequency response magnitude of is plotted in Fig. 7, together with and the resulting frequency responsemagnitude after fouriterations onSource1 for a target rate of	. 
 Notice that for Source 1, after four iterations of Iterative Procedure 1, the obtained values for are almost identical to 
 , evaluated according to (35). This suggests that Iterative Procedure 2 has fast convergence. For example, when applying four iterations of Iterative Procedure 2 to Source 1 with a target rate of 0.2601 bits/sample, the distortions obtained after each iteration were 1.6565, 1.6026, 1.6023, and 1.6023, respectively. For the same source with a target rate of 0.0441 bits/sample, the distortion took the values 4.0152, 3.9783, 3.9783, and 3.9782 as the iterations proceeded. A similar behavior is observed for other target rates, and for other choices of   in (54) as well. Thus, at least for AR-1 sources, one gets close to the global optimum   after just three iterations. 
 In this paper, we have obtained expressions and upper bounds to the causal and zero-delay rate distortion function for Gaussian stationary sources and MSE as the distortion measure. We first showed that for Gaussian sources with bounded differential entropy rate, the causal OPTA does not exceed the information-theoretic RDF by more than approximately 0.254 bits/sample. After that, we derived an explicit expression for the information-theoretic RDF under per-sample MSE distortion constraints using a constructive method. This result was then utilized for obtaining a closed-form formula for the causal information-theoretic RDF   of first-order Gauss–Markov sources under an average MSE distortion constraint. 
 We then derived three closed-form upper bounding functions to the difference between   and Shannon’s RDF. Two of these bounding functions are tighter than the previously best known bound of 0.5 bits/sample, at all rates. We also provided a tighter fourth upper bound to  , named  , that is constructive. More precisely, we provide a practical scheme that attains this bound, based on a noise-shaped predictive coder consisting of an AWGN channel surrounded by pre-, post-, and feedback filters. For a given source spectral density and desired distortion, the design of the filters is convex in their frequency responses. We proposed an iterative algorithm, which is guaranteed to converge to the optimal set of unique filters. Moreover, the mutual information obtained across the AWGN channel, converges monotonically to  . Thus, one avoids having to solve the more complicated minimization of the mutual information over all possible conditional distributions satisfying the distortion constraint. To achieve the upper bounds on the operational coding rates, one may simply replace the AWGN channel by a subtractively dithered scalar quantizer and using memoryless entropy coding conditioned to the dither values. 
 We will first show that   can be realized by a vector AWGN channel between two square matrices. It was already established in Lemma 1 that an output   corresponds to a realization of   only if it is jointly Gaussian with the source 
 where the inverse of   exists from the fact that   has bounded differential entropy. It is clear from (55) and the joint Gaussianity between and that the causality condition is satisfied if and only if the matrix 
 We will now describe a simple scheme which is capable of reproducing the joint statistics between and any given jointly Gaussian with   satisfying (56)–(58). 
 Suppose is first multiplied by a matrix yielding the random vector . Then a vector with Gaussian i.i.d. entries with unit variance, independent from , say , is added to  , to yield the random vector . Finally, this result is multiplied by a matrix to yield the output 
 On the other hand, the joint second-order statistics between  and   are fully characterized by the matrices 
 It can be seen from these equations that all that is needed for the system described previously to reproduce any given pair of covariance matrices   is that the matrices and 
 Thus,   can be chosen, for example, as the lower triangular matrix in a Cholesky factorization of  . With this, a tentative solution for could be obtained as , which would satisfy (62) if and only if. The latter holds if and only if (recall that is nonsingular since has bounded differential entropy). We will now show that this condition actually holds by using a contradiction argument. Suppose 
 . Since , the former supposition is equivalent to . If this were the case, then there would exist such that and . The latter, combined with (63), would imply 
 . One could then construct the scalar random variable	, which would have nonzero variance. The MSE of predicting	from	is given by 
 From this, and in view of the fact that is Gaussian with nonzero variance, we conclude that would be unbounded. However, by construction, the Markov chain holds, and therefore by the Data Processing Inequality we would have that 
 , implying that is unbounded too. This contradicts the assumption that is a realization of , leading to the conclusion that . There- 
 is guaranteed to satisfy (62), and thus for every  , there exist matrices and which yield an output vector satisfying (56)–(58). 
 The first equality follows from the data-processing inequality and the fact that   is obtained deterministically from  . To prove that the second equality in (65) holds, we will prove that 
 . We first have, from (64), that   , which combined with the identity  reveals immediately that (64) implies 
 where denotes the orthogonal projection operator onto a given subspace . Since and is orthogonal to the other two terms on the RHS of (67), we have that 
 where the last equality follows from the fact that   is Gaussian i.i.d., which implies that   is independent of the other two terms in the expression. On the other hand, from (67) 
 Thus, we have (69)–(72), shown at the bottom of the page, where   comes from the data-processing inequality, and 
 follows from the fact that and from (66). To complete the proof of the second equality in (65), we note that the data-processing inequality also yields . 
 Finally,ifwekeepthe	and	satisfyingtheaforementioned conditions and replace the noise	by the vector of noise samples	with unit variance introduced by	independently operating SDUQS [11], with their outputs being jointly entropycoded conditioned to the dither, then the operational data rate would be upper bounded by [11] 
 where is the output of the ECDQ channel. Since the distortion yielded by the SDUQs is the same as that obtained with the original Gaussian channel, we conclude that 
 First, following exactly the same proof as in Lemma 6 in the Appendix, it is straightforward to show that 
 Now, consider the following family of encoding/decoding schemes. For some positive integer , the entire source sequence is encoded in blocks of contiguous samples. Encoding and decoding of each block is independent of the encoding and decoding of any other block. As in the scheme described in the second part of the proof of Lemma 2, each source block is multiplied by the optimal   pre-processing matrix, the resulting block being encoded and decoded utilizing   parallel and independent SDUSQs, with their outputs jointly entropy coded conditioned to the dither values. When decoded, the result is then multiplied by the optimal post-processing matrix described in the proof of Lemma 2. 
 For such an ED pair, and from (5), the operational rate after samples have been reconstructed is 
 where denotes rounding to the nearest larger integer (since the th sample is reconstructed only after blocks of length are decoded). On the other hand, since the variance of each reconstruction error sample cannot be larger than the variance ofthe source, we havethatthe averagedistortionassociated with the first samples is upper bounded as 
 where denotes rounding to the nearest smaller integer. Therefore, for any finite , the average distortion of this scheme equals when (i.e., when we consider the entire source process). Also, from (75) and (5), letting  , we conclude that 
 for every finite . Our aim is to use this result to show that . Since (77) is valid only for 
 finite values of , we must resort to analyzing the convergence of as . First of all, since   is bounded, 
 From Lemma 1, for any given reconstruction-error covariance matrix, the mutual information is minimized if and only if the output is jointly Gaussian with the source. In addition, for any givenmutual information between   and a jointly Gaussian 
 , the variance of every reconstruction error sample is minimized if and only if is the estimation error resulting from estimating from , that is, if and only if 
 Thus, hereafter we restrict the analysis to output processes jointly Gaussian with and causally related towhich also satisfy (80). For any such output process, say, , the following holds: 
 and thus equality holds in (83) if and only if the following Markov chain is satisfied: 
 is lower bounded by the RHS of (87), which in turn depends only on the error variances associated with . We shall now see that this lower bound is minimized by a unique set of error variances, and then show that the resulting bound is achievable while having these error variances.  Revisiting (84)–(86), we have that 
 With this, and since the RHS of (87) decreases when any error variance   increases, the minimum value of the RHS of (87) subject to the constraints 
 [see (29)]. Therefore, for all outputs causally related to and jointly Gaussian with satisfying the distortion con- 
 Now, we will show that for any distortion schedule , the output   yielded by the recursive algorithm of Procedure 1 is such that	equals the lower bound (91), thus being a realization of	. 
 and the conditions (80) (MMSE), and (88) (Source’s Past Independence) which are necessary and sufficient to attain equality in (91). 
 pose   satisfies causality. Then, since , it follows from (56) that the top-left square submatrix   is lower triangular, being 
 This means that the top entries in the th column of   depend only on the entries of above its th row. Recalling that , we conclude that is also lower triangular, and thus also satisfies causality. Notice that for any given andsatisfying causality up to sample , the vectoryielded by Step 2 is the only vector consistent with satisfying causality up to the  th sample. 
 MMSECondition(80): Step1guaranteesthat(80) issatisfied for . Steps 3, 4 and 5 mean thatfor all. Therefore, the reconstruction vector yielded by the above algorithm satisfies (80) for all . 
 Source’s Past Independence (88): Since all variables are jointly Gaussian, condition (88) is equivalent to 
 Since the above algorithm yields an output which satisfies (92), (80), and (88), for all , this output attains equality in (91), thus being a realization of . Notice that once the distortions are given, each step in the recursive algorithm yields the only variances and covariances that satisfy (92), (80) and (88). Therefore, for any given distortion schedule , the latter algorithm yields the unique output that realizes . This completes the proof. 
 Consider the first samples of input and output. The average distortion constraint here takes the form 
 Then, we have (98), shown at the bottom of the page, where the last inequality follows from Jensen’s inequality and the fact that is a convex function of  . Equality is achieved if and 
 . Given that the RHS of (98) is minimized when constraint (97) is active (i.e., by making ), we can attain equality in (98) and minimize its RHS by picking 
 The first inequality in (36) follows directly from definitions 3 and 6. For a plain AWGN channel with noise variance  , the mutual information between source and reconstruction is 
 In both cases the end-to-end distortion can be reduced by placing a scalar gain after the test channel. The optimal (minimum MSE) gain is  . The mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it. However, now the resulting end-to-end distortion is . Therefore, for a given end-to-end distortion , the distortion between the source and the signal before the optimal scalar gain is 
 where (106) follows from (12), (104), and (105) and by noting that  , (108) stems from (104), and (109) follows from Jensen’s inequality. Notice that the RHS of (109) equals the first term on the RHS of (39). 
 The middle term on the RHS of (39) follows directly from (15). Finally, for distortions close to  , a bound tighter than (109) can be obtained from (103a) as follows: 
 which is precisely the third term on the RHS of (39). In the above, (110a) holds trivially since  , and 
 (110b) follows from Jensen’s inequality. Therefore, equality holds in (110b) if and only if   is white. The validity of the chain of inequalities in (36) follows directly from (103) and (110). This completes the proof. 
 Immediately afterward we prove that, despite the distortion and causality constraints, the scheme in Fig. 2 has enough degrees of freedom to turn all the above inequalities into equalities. That means that if we are able to globally infimize  over the filters of the system while satisfying the distortion and causality constraints, then that infimum, say  , must satisfy 
 We now proceed to demonstrate the validity of (111) and to state the conditions under which equalities are achieved. The first equality in (111) follows from the fact that is a Gaussian i.i.d. process. Inequality stems from the following: 
 where   is the signal at the output of , see Fig. 2. In the above, (112) follows from the fact that and are independent and from the fact that is strictly causal. As a consequence,   is independent of , for all . Inequality (113) holds from the property , with equality if and only if and are independent, i.e., if and only if is white. Similarly, (114) holds since the samples of are independent. By noting that is a linear combination of and , it follows immediately that is independent from upon knowledge of , which leads to (115). On the other hand, (116) stems from the fact that 
 . Equality in (117) holds from the fact that, if is known, then can be obtained deterministically from 
 , and vice-versa, see Fig. 2. Equality (118) follows from the fact that there exists no feedback from to , and thus the Markov chain holds. On the other hand,	, with equality if and only if is invertible for all frequencies for which . Finally, (119) follows directly from the 
 Since is by definition an infimum, it follows that, for every , there exists an output process jointly Gaussian with , satisfying the causality and distortion constraints and such that	. 
 Such output can be characterized by its noise PSD, say , and its signal transfer function, say , by using the model in Fig. 3. 
 chosen   in (121a) for simplicity and because, as we shall see next, we have enough degrees of freedom to do so without compromising rate/distortion performance. Solving the system of equations formed by (121a), (121c), and (121b), we obtain 
 that their squared magnitudes equal their RHSs in (122). To do so, we will make use of the Paley-Wiener theorem (Theorem 8 in the Appendix). 
 Substitution of the RHS of the second equation of (121c) into the above, together with the Paley–Wiener theorem, yields that there exists a causal, stable and minimum phase transfer func tion 
 Since   can be chosen to be arbitrarily small, it can always be chosen so that  , which contradicts (147). Therefore 
 from Theorem 2 that if we construct an output process by using the recursive algorithm of that theorem, with the choice , for all , then this output process is such that equals . 
 Proposition 1 (MMSE Column Correspondence): Let    be a Gaussian random vector source with covariance matrix  . A reconstruction Gaussian random vector   satisfies 
 , with , be a Gaussian random source vector with covariance matrix . A reconstruction Gaussian random vector satisfies 
 Proof: Let us first introduce the notation , denoting the top-left submatrix of any given square matrix 
 Lemma 8 implies that, if the reconstruction is the output of a causal Wiener filter applied to the noisy source for some noise vector   [a condition equivalent to (157)], then and   have identical entries on and above their main diagonals. 
 Theorem 8 (see [22, p. 229]): Let   be a nonnegative function defined on  . There exists a unique stable, causal and minimum phase transfer function   such that   if and only if5 
 5In [22, p. 229], it is stated that (160) is a sufficient condition for such a to exist. However, from [22, Note 2, p. 228] and the discrete-continuous equivalence in [22, p. 229], it follows that (160) is also necessary. 
 ﻿ We derive closed form for the statistics of the spectral power gain of wide band microwave indoor . We obtain our within a framework general enough to be compatible with several popular channel , such as those by the task group , as well as the channel model . As all these , our channel description is based upon and with and random . Our consist of closed form for the statistics of the channel power frequency response , where statistical involve over ray and arrival times . We first express the auto covariance of this frequency response in closed form . We then use this result to obtain an analytical expression for the variance and moment of the channel power within any given interval of . This us to express the channel spectral diversity as a function of model and . From this function , we determine the range within which diversity scales approximately linearly with and its upper limit . 
 Stochastic wireless channel allow one to predict the statistics of the radio propagation over an ensemble of with similar . This is specially useful in complex , heterogeneous and time , such as office , residential and industrial indoor . The the . . a task group accepted a channel model for ultra wide band indoor , based on the channel model , and similar have been in that led to the . . c standard . Like , the . . a channel model is a discrete time description of the impulse response of a wireless channel , in which path are grouped into . The arrival times of and within them follow . One exponential ray gain decay profile is defined among and another one for the in each cluster . Unlike , the fading statistics of path in the . . a channel model are log normal , not . 
 Some of the useful temporal statistics of channel like this are the power delay profile , the average delay and the delay , which allow one to determine spectral statistics such as coherence and average power gain under suitable , been extensively in the literature , . In contrast , the second order statistics of the channel power frequency response have received less 
 attention . In , it been shown that the number of significant of the covariance matrix of the random channel impulse response , and hence the diversity order of the channel , scales approximately linearly with . Such an increase in diversity , which to the reduction of the relative channel power variance as the , been to reach a saturation point , . In a recent paper , analytical for the of the channel frequency response squared magnitude , here by , as well as the variance of the power over any frequency band , have been derived for the . . a channel model , conditioned to fixed and given ray and cluster arrival times . The corresponding statistics over random arrival times were via . To the best of the knowledge , no closed form are available in the literature for second order un conditional statistics of the spectral power gain of wireless channel such as the . . a and the . 
 In this paper , we derive closed form for the second order statistics of for greater than some minimum value min rad , whose precise definition will be given later . Statistical randomness of ray an arrival times , for a generalized version of the ultra wide band microwave indoor channel model accepted by the . . a task group . We obtain these within a framework general enough to be compatible with other related channel , such the . The flexibility of our formulation it to include some of the in the . . a standard channel model for high as well . As in all these , our channel description is based upon and with , with different average power decay among and within . We first derive a closed form expression for the of . We then use this result to obtain an analytical expression for the variance and raw second order moment of the channel power gain within any given interval min . This us to estimate fade depth as a function of channel explicit . Our also predict a positive lower bound to the ratio between the variance of and its squared mean . This bound only as the arrival rate of in the model to infinity . 
 In this section we formulate the wireless channel model framework which will be throughout this work . As in the . . a channel model , we describe the radio propagation of indoor by a random impulse response of the form 
 is the random arrival time of the th path in the i th cluster , Ti is the random arrival time of the i th cluster , and ti , is the random delay of the th path in the i th cluster relative to Ti . Each real valued random coefficient ai , the amplitude of the th path in cluster i . These random are formed as 
 where Ai is a random variable the amplitude of the i th cluster , pi , are un binary random taking from , , and each real valued random variable ai , the amplitude or gain of the th path or ray within the i th cluster relative to that cluster amplitude . For simplicity , we do not consider here a large scale fading factor before the sum in , but its effects can be applied to our with ease . As in , , , all the random in the super set Ti ti , Ai pi , ai , are independent , except Ai from Ti , and ai , from ti ,, for all i ,. For future use , we describe the dependency of the second and of Ai and ai , on Ti and ti ,, respectively , by of the following : 
 In our formulation , the number of Ti and the number of ti , that take within any unit length time interval follow with mean and , respectively . This is also part of the channel of and . 
 By choosing the ai as distributed , for fixed and given , and deterministic Ai with the form Ai b Ti , , for some ,, , the above description the widely used model . A similar choice with log normally distributed Ai and ai the model by the . . a task group excluding its large scale fading term . The decay of the by can also be chosen to match those that the . a standard for high in some scenario such as office and industrial . Mixed , another model feature present in , can be handled within the current framework with little additional effort . 
 In , the number of and the number of within each cluster are finite . Such choice to the fact that , in practice , choosing and large enough will be sufficient to include all path component with significant power . This is equivalent to saying that for every e , there exist finite and such that 
 with probability close to . Clearly , this equivalence that the decay fast enough with increasing delay . We will make this requirement precise by assuming that the conditional of the satisfy 
 We will also assume that the conditional moment , b , and g decay slow enough so that , for some minimum angular frequency min rad , we can approximate 
 where the non negative integer ni satisfy n n , n n . Thus , our will be valid for and where the band of interest above min . In general , the for which is a good approximation will be determined by whichever of the within the integral of . Therefore , min will depend mostly on the decay time of the , the reciprocal of which should be several times smaller than min . 
 Since and arrival times are random , so is . In the following we will derive analytical for the second order statistics of as well of the channel power over a given frequency interval . 
 Here we will obtain closed form for the mean and correlation coefficient of over any interval of angular , min , . To derive these , we will make use of the following technical lemma . 
 Lemma : Suppose the sequence of non negative random a process with finite arrival rate be any two such that exist and are bounded . Then 
 where E expectation with respect to all the random that determine the expression within the square . N 
 Proof : For each of i . i .. random uniformly distributed over ,. We have that 
 and the independence between stated in the previous section , we obtain 
 Thus , the value of at any min is the product of the average energy of the cluster Ai and the ray relative ai ,. Notice also that E ai ti the average power profile of in a cluster with amplitude Ai . For this reason , it will be to as the the cluster . Similarly , E Ai Ti the average power density of cluster . Accordingly , we will call the inter cluster power delay profile . 
 For notational simplicity , we will temporarily adopt a single indexing nomenclature for the path arrival times ti ,. More precisely , and with a slight abuse of notation , we define the infinite random set For our , it will not be necessary to define a between the i , and the index . Indeed , it will be sufficient to note that the double index matching condition 
 the single index , , i is equivalent . 
 single index notation , the first term on the right hand side of is from , which 
 ray arrival times ti ,, which imply that the in the sum of are zero for the ,. ,,, ,, and ,, r 
 this from , , and the independence between Ai , ai ,, Ti and ti , in the previous section , we obtain 
 It can already be seen from that the the sum of a frequency independent term and a term which only on the frequency difference . From , the latter term as this frequency difference to infinity , so because of the first term , the between two infinitely distant frequency is greater than zero . At first sight this may seem . Nevertheless , such behaviour also been found in , and is consistent with the saturation of the channel diversity order as in , . As will be in Remark below , such behaviour is due to the fact that the arrival for and its path are finite . Indeed , as the path environment becomes , i . e ., the number of path component , the constant term to zero . 
 Before the last step in our derivation of a simpler closed form expression for , , we define 
 With these , and Lemma , we obtain that the of is given by 
 and , respectively . Notice in this expression that only the K and K depend on the conditional fourth order of the . Therefore , for given inter and cluster , the of ray and cluster will modify the frequency independent K and K . Nevertheless , such change would have no effect on the frequency dependency of the spectral power . 
 Remark : Dense impulse If we keep the average energy of the the cluster and impulse constant while increasing the cluster arrival rate to infinity , then the frequency independent in and vanish . To see this , notice that the cluster impulse response energy is given by 
 where Lemma been used . Thus , if we keep this average energy constant and increase the density by a factor , then the have to be divided by . By uniformly scaling the Ai to yield such reduction , the resulting function b is divided by . Therefore , the integral of b in the form b , which clearly to zero as . 
 From this analysis it that the of when the impulse response becomes infinitely dense , say , the simple form 
 That is , in this case is the product of the squared magnitude of the inter cluster and the cluster at . 
 In this section we will derive closed form for the mean and variance of the channel power over any band 
 Value of the Channel Power Over a Frequency Band : Denote the channel power over the frequency . Given the fact that E is constant for all min see , it from that 
 The variance of the channel power over a band , , min , can be directly from the of as : 
 The fluctuation of the received power over a wireless where ,, as defined in . 
 channel its fade statistics . The fact that fade depth with channel been in the literature , both from empirical data see , e .., as well as . However , a closed form formula fade depth and the classical channel model used here , to the best of our knowledge , not been to date . The derived above allow us to easily deal with this issue , as seen in the example of the next section . 
 In this section we will illustrate the application of the previous , our framework to the classical model . In this model , the ai , are distributed and the Ai are deterministic exponentially . The decay profile of the ray is given by 
 where is the average power gain of a ray at the beginning of a cluster at Ti . Since the fourth moment E x of a distributed random related to its second order moment as E E , we have that 
 In order to find the of for this example , we substitute into , 
 The variance of is directly by , which the previous expression for 
 Thus , again it can be that , as in Remark , the lower asymptote of the spectral power correlation only when the cluster arrival rate as the mean channel power is kept constant . 
 B . Second Order Statistics Over a Band The variance of the channel power over a band 
 min , rad is by substituting into , and then the in . This 
 On the other hand , from , E . Thus , the channel power variance for this example is 
 Equation several interesting about the channel power variance , which are below : 
 The channel power variance is the sum of three . The first one to K , which on all the except the . It is inversely proportional to the product , which is in turn proportional to the average number of with significant in the channel impulse response . This term is the lower asymptote of E when , and the latter limit to total channel power variance divided by its squared mean . Again , in agreement with what was shown in Remark , this term only when the impulse response becomes infinitely dense with . From the viewpoint of , this is by the fact that , with a finite number of with random , the law of large does not apply to the total impulse response energy , and hence its variance to squared mean ratio is not zero . 
 The second term on the of all the other , except , which it does 
 Although also the right hand side of vanish , grow unbounded would violate the under which the above are valid see and . 
 Figure . coefficient for with from the . . a . are over . channel . p . The analytical plot was and . 
 not change with the arrival rate of within . This term is monotonically decreasing in and , approaching when , and when . It also when the cluster arrival rate to infinity . 
 The third term only the and inter cluster power decay . In particular , it is independent of cluster and ray arrival . It is monotonically decreasing with the , tending to zero when to infinity , and to when . 
 It can be seen that the channel power variance over a band unbounded as . This is due to the fact that , when , the channel power is close to zero with increasing probability . 
 To see how this tendency can make the ratio E go to infinity , consider a simpler exam 
 . Let the discrete random the with with probability . Then E , which clearly goes to infinity as . 
 The narrow band channel power variance , P E P is from by . The ratio between the of power of a and a system , i . e ., P E P E can be as a measure of frequency diversity order . From , such diversity order would range from when to , when . For » , the latter limit is approximately , which is roughly twice the average number of that fall within . It can also be seen from that before reaching this limit , and for » , the diversity order approximately linearly with , consistent with , . 
 The coefficient for with the of the . . a is plotted in Figure , both from and from our . The of the are : . , . , , and . . It can be seen from this figure that from channel agree well with . In particular , w , the . threshold for p between and p , as by , and close to its lower asymptotic value when p . 
 We derived general that characterize the statistics of the frequency response power gain in microwave indoor . Our are applicable to several well established channel in the literature . In particular , the closed form formula here for the of the squared frequency response magnitude of the channel one to predict the variance of the channel power over any frequency band . This an approximation to the diversity order of wide band over in such . Our also allow one to obtain an upper bound for this measure of diversity , and show how and why this limit from a finite number of in the channel impulse response . 
  
 ﻿ The problem of lower on data for closed loop stability been in a variety of . However , the available lead to which are very complex and , thus , of limited practical interest . In this paper , we show how simple only and memoryless entropy scalar can be used to stabilize strongly plant over error free bit rate limited feedback . Despite the simplicity of the building employed , we prove that the data do not exceed absolute lower by more than . per sample . 
 I . INTRODUCTION 
 The study of control , i . e ., control with communication , as an active research field during the past see , e .., the special issue . Key within this framework are related to the way in which network affect the stability and performance of control that employ non transparent communication . Typical channel include data rate , random and data . A framework for the treatment of the general analysis or design problem does not exist . Nevertheless , there been significant progress in the study of . For example , data rate have been studied in , e .., . The issue of data been studied in , e .., and random time have been considered in , e .., . 
 In this paper we focus on feedback closed over delay and error free bit rate limited . Within this context , a key result necessary and sufficient on the channel data rate that one to achieve closed loop stability in an appropriate sense ; see , e .., , , and the many therein . This result is given in of a lower bound on the channel which on the unstable plant only over which can be so as to achieve stability . These are quite complex and are not attractive from a practical point of view . On the other hand , showing that the rate at which a stable control system is data is always greater than the bound i . e ., necessity is fairly simple see also , , whereas an actual scheme that stability at any rate above the absolute limit i . e ., the proof of sufficiency is much more involved see , . Clearly , the need to develop the study of simple that achieve close to the in , , , and of that allow one to exploit the when the necessity of the in the construction of . 
 by the discussion above , this paper how simple only and entropy see , e .., can be used to stabilize strongly plant in data which exceed the minimal established in , , by no more than . per sample . The excess rate of our simple scheme is given by the sum of two : a first term due to the divergence of the quantization noise distribution from , and a second term that in the inefficiency of the loss less coder that the binary . 
 In our , the coder and architecture an essential role , which , given the in , e .., , is by no surprising . Our work also light into the why the in are not always consistent with the lower bound on data for stability studied in . 
 The remainder of this paper is organized as : Section the notation used in the paper . and present the considered setup and scheme , respectively . a simplified setting where are . Section the in derive upper on the data that allow one to achieve with the scheme . Section . Due to space , all have been and can be found in or via e mail from the . 
 . NOTATION 
 We define N , , , and . We both the argument of the transform and as the forward shift operator , where the meaning is clear from the context . Given any scalar , magnitude . 
 The set of all discrete time strictly proper real rational transfer is by . 
 Unless otherwise stated , random are always scalar and defined for N , we abbreviate N , , by , and define , . 
 E the expectation operator . The variance , at time instant , of a via ; similarly , a random variable , then its variance . We define , , provided the limit . 
  
 channel 
  
 Fig . . Considered control system generic scheme . 
 a wide sense stationary asymptotically process , then its stationary power spectral density and any spectral factor of , a second order one if and only if it finite mean and finite i . e . We say that a random variable is 
 and non zero variance . 
 If ,, are continuous resp . discrete random , then resp . the differential resp . discrete entropy of ; resp . the conditional differential discrete entropy of , given ; I ; the mutual information ; I ; the conditional mutual information , given . We recall that , then 
 . In this paper we use in base e . Thus , information is measured in nat . For and basic of the above we refer the reader to . 
 . PROBLEM SETUP 
 In this paper we are concerned about the stability of a closed loop system built around a discrete time plant model , which a delay and error free data rate limited channel in the feedback path , as shown in Fig . . In that figure , is the plant model , is an controller , is the plant output , u is the plant input , is a reference signal , . In order to make use of the bit rate limited channel , the feedback path an E which binary based on information regarding the plant output . These , which we will denote by , are then sent delay and through the channel . At the end , received to generate the is fed back into the controller . 
 We will make the following : 
 Assumption Plant model : is unstable , no or on the unit circle , is strongly , and its initial state is an independent second order random variable . N 
 Assumption Reference and disturbance : second order zero mean that admit rational , and that are mutually uncorrelated . Moreover , 
 . N 
 Assuming that the plant is strictly proper that the feedback loop in Fig . is well for all causal , and . The assumption regarding 
  
 Fig . . scheme . 
 the plant initial state for most of interest . The on are not essential , but we have made them to maintain a straightforward presentation see for the general case . Assumption is standard except for the fact that we be non zero . We avoid the case for brevity . 
 For future reference , we define p as the set of unstable plant , and 
 . 
 Throughout this paper we adopt the following notion of stability see also , e .., : 
 Definition Mean square stability : Consider a system by ,,, where N ,: , is the system state at time instant , , where is a second order random variable , and the a second order process independent of . We say that the system is mean square stable if and only if there exist finite and finite , , such that 
  
 regardless of the initial state . N 
