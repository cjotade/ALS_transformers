{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_TYPE = \"gpt2\" \n",
    "OUTPUT_DIR = f\"../../weights/es-gpt2/\"\n",
    "#TRAIN_PATH = f\"../../data/spanish-corpora/preprocessed_DOGC_lower.txt\"\n",
    "#TEST_PATH = f\"../../data/spanish-corpora/preprocessed_DGT_lower.txt\"\n",
    "#VAL_PATH = f\"../../data/spanish-corpora/preprocessed_DOGC_lower.txt\"\n",
    "TRAIN_PATH = f\"../../data/minicorpus/train_1.txt\"\n",
    "TEST_PATH = f\"../../data/minicorpus/test_1.txt\"\n",
    "VAL_PATH = f\"../../data/minicorpus/val_1.txt\"\n",
    "\n",
    "#TRAIN_PATH = f\"/home/camilojd/Universidad/ALS/data/minicorpus/train_1.txt\"\n",
    "#TEST_PATH = f\"/home/camilojd/Universidad/ALS/data/minicorpus/test_1.txt\"\n",
    "#VAL_PATH = f\"./home/camilojd/Universidad/ALS/data/minicorpus/val_1.txt\"\n",
    "\n",
    "\n",
    "# Model paths\n",
    "#MODEL_TYPE = \"gpt2\" \n",
    "#OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/papers_milan/\"\n",
    "#TRAIN_PATH = f\"../../data/papers_milan/train_papers.txt\"\n",
    "#TEST_PATH = f\"../../data/papers_milan/test_papers.txt\"\n",
    "#VAL_PATH = f\"../../data/papers_milan/val_papers.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/camilojd/Universidad/ALS/src/finetuning\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "#paths = [str(x) for x in Path(TRAIN_PATH).glob(\"**/*.txt\")]\n",
    "paths = [TRAIN_PATH]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir OUTPUT_DIR\n",
    "tokenizer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "\n",
    "config = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(OUTPUT_DIR)#, max_len=512)\n",
    "\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(52004, 768)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "model = GPT2Model(config=config)\n",
    "model.num_parameters()\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from filelock import FileLock\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from nlp import load_dataset\n",
    "from nlp.builder import FORCE_REDOWNLOAD\n",
    "\n",
    "class LineByLineNLPTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int, overwrite_cache=False):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "        \n",
    "        if overwrite_cache:\n",
    "            ### FIX\n",
    "            dataset = load_dataset(path='text', \n",
    "                       name=\"dataset\",\n",
    "                       data_files={\n",
    "                           'train': [file_path]\n",
    "                       },\n",
    "                       download_mode=FORCE_REDOWNLOAD,\n",
    "                       version=\"0.0.0\"\n",
    "                  )    \n",
    "        else:\n",
    "            dataset = load_dataset(path='text', \n",
    "                       name=\"dataset\",\n",
    "                       data_files={\n",
    "                           'train': [file_path]\n",
    "                       }\n",
    "                  )\n",
    "\n",
    "        dataset = dataset[\"train\"].map(lambda line: tokenizer(\n",
    "                                            line[\"text\"], \n",
    "                                            add_special_tokens=True, \n",
    "                                            truncation=True, \n",
    "                                            max_length=block_size),\n",
    "                                  batched=True)\n",
    "        #dataset.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "        dataset.set_format(type='numpy', columns=['input_ids'])\n",
    "        \n",
    "        self.examples = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i][\"input_ids\"], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = []\n",
    "            for line in f.read().splitlines():\n",
    "                if (len(line) > 0 and not line.isspace()):\n",
    "                    print(line)\n",
    "                    lines.append(line)\n",
    "                    break\n",
    "            #lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/camilojd/.cache/huggingface/datasets/3e34209a2741375a1db1ff03bf1abba1a9bd0e6016912d3ead0114b9d1ca2685.557eb955d95e9c9f3882df6ec794d4f05c1bff683ea7c7e207ab764c7fc9eac3.py for additional imports.\n",
      "Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text\n",
      "Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b\n",
      "Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py to /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b/text.py\n",
      "Couldn't find dataset infos file at https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/dataset_infos.json\n",
      "Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b/text.json\n",
      "Using custom data configuration dataset\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /home/camilojd/.cache/huggingface/datasets/text/dataset-d8cb74a71389f7f3/0.0.0/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b\n",
      "Reusing dataset text (/home/camilojd/.cache/huggingface/datasets/text/dataset-d8cb74a71389f7f3/0.0.0/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b)\n",
      "Constructing Dataset for split train, from /home/camilojd/.cache/huggingface/datasets/text/dataset-d8cb74a71389f7f3/0.0.0/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b\n",
      "All the checksums matched successfully for post processing resources\n",
      "Loading cached processed dataset at /home/camilojd/.cache/huggingface/datasets/text/dataset-d8cb74a71389f7f3/0.0.0/3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b/cache-64e3be7147d7747c.arrow\n",
      "Set __getitem__(key) output type to numpy for ['input_ids'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aprobada en la 81a. sesión plenaria, celebrada el 4 de diciembre de 2000, por recomendación de la comisión (a/55/602/add.2, párr. 94),el proyecto de resolución recomendado en el informe fue patrocinado en la comisión por los países siguientes: bolivia, cuba, el salvador, ghana y honduras. en votación registrada de 106 votos contra uno y 67 abstenciones, como sigue: \n"
     ]
    }
   ],
   "source": [
    "#from transformers import LineByLineTextDataset, TextDataset\n",
    "from nlp.builder import FORCE_REDOWNLOAD\n",
    "\n",
    "dataset = LineByLineNLPTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=TRAIN_PATH,\n",
    "    block_size=64,\n",
    "    overwrite_cache=False\n",
    ")\n",
    "\n",
    "dataset1 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=TRAIN_PATH,\n",
    "    block_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18904, 46048,   286,   283, 11485,    69,    18,  5029,  6214,    16,\n",
       "         3671,   289,   613,   262,  1121,   262,  1376,    16,   342,  2685,\n",
       "          262,   283,   839,   358,    69,    19,  5873,    19, 45082,    19,\n",
       "         7202,    18,    22,    16,  6329,    18,  8411,  1637,   473,  1566,\n",
       "          262,  2158,  7293,   286,   289,  1290,  1009,  8092,   286,   283,\n",
       "          839,   342,   300,   762,  1634,    30,  7400,    16,  5349,    16,\n",
       "          289,  7270,    16,  7676])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18904, 46048,   286,   283, 11485,    69,    18,  5029,  6214,    16,\n",
       "         3671,   289,   613,   262,  1121,   262,  1376,    16,   342,  2685,\n",
       "          262,   283,   839,   358,    69,    19,  5873,    19, 45082,    19,\n",
       "         7202,    18,    22,    16,  6329,    18,  8411,  1637,   473,  1566,\n",
       "          262,  2158,  7293,   286,   289,  1290,  1009,  8092,   286,   283,\n",
       "          839,   342,   300,   762,  1634,    30,  7400,    16,  5349,    16,\n",
       "          289,  7270,    16,  7676])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=10_000,\n",
    "    max_steps=900_000,\n",
    "    learning_rate=0.0001,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dbf3f992a649e394b1655027bc8b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=900001.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51343c852b6b4a3b929eaa167ae5191d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, model, inputs, optimizer)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# model outputs are always tuple in transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
