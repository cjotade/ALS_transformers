{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "PyTorch version 1.6.0 available.\n",
      "Apache Beam available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from LineByLineNLPTextDataset import LineByLineNLPTextDataset\n",
    "from LineByLineClassificationTextDataset import LineByLineClassificationTextDataset\n",
    "from nlp.builder import FORCE_REDOWNLOAD\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f\"../../weights/es-lstm/\"\n",
    "TRAIN_PATH = f\"../../data/minicorpus1/train_1.txt\"\n",
    "TEST_PATH = f\"../../data/minicorpus1/test_1.txt\"\n",
    "VAL_PATH = f\"../../data/minicorpus1/val_1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter()\n",
    "data_path = \"../../data/spanish-corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"../../data/spanish-corpora\"\n",
    "TRAIN_PATHS = [os.path.join(data_path, filename) for filename in filter(lambda x: x.endswith((\".txt\")), os.listdir(data_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/spanish-corpora/preprocessed_JRC_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_OpenSubtitles2018_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_allwikis_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_EMEA_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_GlobalVoices_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_DGT_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_ParaCrawl_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_NewsCommentary11_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_EUBookShop_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_DOGC_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_Europarl_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_multiUN_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_TED_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_UN_lower.txt',\n",
       " '../../data/spanish-corpora/preprocessed_ECB_lower.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1.txt  train_1.txt  val_1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/minicorpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"../../weights/beto/\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/camilojd/.cache/huggingface/datasets/3e34209a2741375a1db1ff03bf1abba1a9bd0e6016912d3ead0114b9d1ca2685.964a0948391d87479f79716750233e9a593a71bb70d468a93367cd00247e6451.py for additional imports.\n",
      "Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text\n",
      "Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28\n",
      "Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py to /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28/text.py\n",
      "Couldn't find dataset infos file at https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/dataset_infos.json\n",
      "Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/text/text.py at /home/camilojd/Universidad/ALS/nlp/src/nlp/datasets/text/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28/text.json\n",
      "Using custom data configuration dataset\n",
      "Generating dataset text (/home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/dataset-1343f33c3d71378a (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Unable to verify checksums.\n",
      "Generating split train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 300904000 examples in 42477164603 bytes /home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28.incomplete/text-train.arrow.\n",
      "Unable to verify splits sizes.\n",
      "Constructing Dataset for split train, from /home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify checksums.\n",
      "Caching processed dataset at /home/camilojd/.cache/huggingface/datasets/text/dataset-1343f33c3d71378a/0.0.0/c3b177069f0fad4da737a020bb39bbdb7aa16992e1f401e4347568618c906e28/cache-e84d071e5f4f445f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3601c1dff68e44eab2117d36969b0177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300904.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-448fbfa0755c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineByLineNLPTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_PATHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Universidad/ALS/src/finetuning/LineByLineNLPTextDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                             max_length=block_size),\n\u001b[0;32m---> 53\u001b[0;31m                                   batched=True)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m#dataset.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Universidad/ALS/nlp/src/nlp/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Universidad/ALS/nlp/src/nlp/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint)\u001b[0m\n\u001b[1;32m   1193\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m                         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Universidad/ALS/nlp/src/nlp/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Universidad/ALS/nlp/src/nlp/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_stream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/pyarrow/ipc.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "dataset = LineByLineNLPTextDataset(tokenizer, TRAIN_PATHS, block_size, overwrite_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GenerativeLSTM(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(GenerativeLSTM, self).__init__(config)\n",
    "        \n",
    "        # Params\n",
    "        self.input_size = 128\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 3 \n",
    "        self.output_size = 31002\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.output_size,\n",
    "            embedding_dim=self.input_size\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size, \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, \n",
    "            #bidirectional=True, \n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.prev_state = None\n",
    "        #self.tokenizer = BertTokenizerFast.from_pretrained(\"../../weights/beto/\", do_lower_case=False)\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size = len(input_ids)\n",
    "        \n",
    "        embed = self.embedding(input_ids)\n",
    "        lstm_output, actual_state = self.lstm(embed, self.prev_state)\n",
    "        self.prev_state = (actual_state[0].detach(), actual_state[1].detach())\n",
    "        lm_logits = self.fc(lstm_output)\n",
    "        \n",
    "        outputs = (lm_logits,) + (lstm_output,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Reshape the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            #loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(shift_logits.size(0), shift_logits.size(-1), shift_logits.size(-2)), \n",
    "                shift_labels\n",
    "            )\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedConfig()\n",
    "model = GenerativeLSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=10_000,\n",
    "    max_steps=90_000,\n",
    "    learning_rate=0.0001,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=1_00,\n",
    "    save_total_limit=2,\n",
    "    max_steps=1_000,\n",
    "    learning_rate=0.0001,\n",
    "    #fp16=True\n",
    ")\"\"\"\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  optimizer.pt  pytorch_model.bin  scheduler.pt  training_args.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../weights/es-lstm/checkpoint-60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeLSTM(\n",
       "  (embedding): Embedding(31002, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=256, out_features=31002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PretrainedConfig()\n",
    "model_pred = GenerativeLSTM(config)\n",
    "model_pred.load_state_dict(torch.load(\"../../weights/es-lstm/checkpoint-60000/pytorch_model.bin\"))\n",
    "model_pred.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 31002])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = tokenizer.encode(\"los signos más probables de\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_pred(sentence)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##mart', 'Vigo', 'Escuelas', 'proporcionados', 'Front']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = tokenizer.encode(\"Buenos dias a todos, desean salir a\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "predictions = model_pred(sentence)[0]\n",
    "idxs = torch.argsort(predictions[0, -1], descending=True)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(sen) for sen in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.argsort(predictions[0, -1], descending=True)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
    "predicted_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
