{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_model.py  run_language_modeling.py  run_model.ipynb  runs  wandb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_TYPE = \"gpt2\" \n",
    "OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/spanish/\"\n",
    "TRAIN_PATH = f\"../../data/spanish-corpora/preprocessed_DOGC_lower.txt\"\n",
    "#TEST_PATH = f\"../../data/spanish-corpora/preprocessed_DGT_lower.txt\"\n",
    "#VAL_PATH = f\"../../data/spanish-corpora/preprocessed_DOGC_lower.txt\"\n",
    "\n",
    "\n",
    "# Model paths\n",
    "#MODEL_TYPE = \"gpt2\" \n",
    "#OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/papers_milan/\"\n",
    "#TRAIN_PATH = f\"../../data/papers_milan/train_papers.txt\"\n",
    "#TEST_PATH = f\"../../data/papers_milan/test_papers.txt\"\n",
    "#VAL_PATH = f\"../../data/papers_milan/val_papers.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_modeling(output_dir, model_type=\"gpt2\", model_name_or_path=None, train_path=None, eval_path=None, \n",
    "                             do_train=False, do_eval=False, evaluate_during_training=False, line_by_line=False, block_size=-1):\n",
    "    return {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"do_train\": \"--do_train\" if do_train else \"\",\n",
    "    \"train_data_file\": train_path if do_train else None,\n",
    "    \"do_eval\": \"--do_eval\" if do_eval else \"\",\n",
    "    \"eval_data_file\": eval_path if do_eval else None,\n",
    "    \"evaluate_during_training\": \"--evaluate_during_training\" if evaluate_during_training else \"\",\n",
    "    \"block_size\": block_size,\n",
    "    \"line_by_line\": \"--line_by_line\" if line_by_line else \"\",\n",
    "    \"fp16\": \"--fp16\",\n",
    "    \"fp16_opt_level\": \"O1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_finetuning = \"\"\"./run_language_modeling.py \\\n",
    "    --output_dir={output_dir} \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    {do_train} \\\n",
    "    --train_data_file={train_data_file} \\\n",
    "    {do_eval} \\\n",
    "    --eval_data_file={eval_data_file} \\\n",
    "    {evaluate_during_training} \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --block_size={block_size}\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 5000 \\\n",
    "    --save_total_limit 3 \\\n",
    "    {line_by_line} \\\n",
    "    {fp16} \\\n",
    "    --fp16_opt_level={fp16_opt_level} \\\n",
    "    --logging_steps 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
    "#   line_by_line, should_continue, and model_name_or_path.\n",
    "train_params = create_params_modeling(output_dir=OUTPUT_DIR, \n",
    "                                        model_type=MODEL_TYPE,\n",
    "                                        model_name_or_path=MODEL_TYPE,\n",
    "                                        train_path=TRAIN_PATH, \n",
    "                                        eval_path=None, \n",
    "                                        do_train=True, \n",
    "                                        do_eval=False, \n",
    "                                        evaluate_during_training=False,\n",
    "                                        line_by_line=True,\n",
    "                                        block_size=-1\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "08/13/2020 12:31:19 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "08/13/2020 12:31:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "08/13/2020 12:31:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/spanish/', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Aug13_12-31-19_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "08/13/2020 12:31:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "08/13/2020 12:31:20 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "08/13/2020 12:31:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "08/13/2020 12:31:21 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "08/13/2020 12:31:22 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/camilojd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "08/13/2020 12:31:22 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/camilojd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "/home/camilojd/Environments/als-env/lib/python3.7/site-packages/transformers/modeling_auto.py:792: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "08/13/2020 12:31:25 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /home/camilojd/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "08/13/2020 12:31:34 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "08/13/2020 12:31:34 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/13/2020 12:31:34 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/spanish-corpora/preprocessed_DOGC_lower.txt\n",
      "08/13/2020 12:31:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "08/13/2020 12:44:39 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/home/camilojd/Environments/als-env/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIfEEEPKNS_6detail12TypeMetaDataEv')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:40 - INFO - transformers.trainer -   ***** Running training *****\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Num examples = 10458162\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Instantaneous batch size per device = 1\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "08/13/2020 12:44:40 - INFO - transformers.trainer -     Total optimization steps = 31374486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ccded2037b4f14aa9223e980fb658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b6f33a5838421086a9f990930ebada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=10458162.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilojd/Environments/als-env/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "08/13/2020 12:44:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999999681269679e-05, 'epoch': 1.9123819271493405e-07, 'step': 2}\n",
      "08/13/2020 12:44:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999999362539358e-05, 'epoch': 3.824763854298681e-07, 'step': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999999043809037e-05, 'epoch': 5.737145781448021e-07, 'step': 6}\n",
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999998725078716e-05, 'epoch': 7.649527708597362e-07, 'step': 8}\n",
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999984063483946e-05, 'epoch': 9.561909635746701e-07, 'step': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999998087618073e-05, 'epoch': 1.1474291562896043e-06, 'step': 12}\n",
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999977688877516e-05, 'epoch': 1.3386673490045382e-06, 'step': 14}\n",
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999997450157431e-05, 'epoch': 1.5299055417194724e-06, 'step': 16}\n",
      "08/13/2020 12:44:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999971314271094e-05, 'epoch': 1.7211437344344063e-06, 'step': 18}\n",
      "08/13/2020 12:44:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999996812696788e-05, 'epoch': 1.9123819271493403e-06, 'step': 20}\n",
      "08/13/2020 12:44:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999996493966467e-05, 'epoch': 2.1036201198642746e-06, 'step': 22}\n",
      "08/13/2020 12:44:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999996175236146e-05, 'epoch': 2.2948583125792086e-06, 'step': 24}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999995856505825e-05, 'epoch': 2.4860965052941425e-06, 'step': 26}\n",
      "08/13/2020 12:44:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999995537775504e-05, 'epoch': 2.6773346980090764e-06, 'step': 28}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999952190451825e-05, 'epoch': 2.8685728907240104e-06, 'step': 30}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999994900314861e-05, 'epoch': 3.0598110834389447e-06, 'step': 32}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99999458158454e-05, 'epoch': 3.2510492761538787e-06, 'step': 34}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999994262854219e-05, 'epoch': 3.4422874688688126e-06, 'step': 36}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999993944123897e-05, 'epoch': 3.6335256615837466e-06, 'step': 38}\n",
      "08/13/2020 12:44:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999993625393576e-05, 'epoch': 3.8247638542986805e-06, 'step': 40}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999993306663255e-05, 'epoch': 4.016002047013615e-06, 'step': 42}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999992987932934e-05, 'epoch': 4.207240239728549e-06, 'step': 44}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999992669202613e-05, 'epoch': 4.398478432443483e-06, 'step': 46}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999923504722917e-05, 'epoch': 4.589716625158417e-06, 'step': 48}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999920317419705e-05, 'epoch': 4.780954817873351e-06, 'step': 50}\n",
      "08/13/2020 12:44:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999917130116494e-05, 'epoch': 4.972193010588285e-06, 'step': 52}\n",
      "08/13/2020 12:44:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999991394281328e-05, 'epoch': 5.163431203303219e-06, 'step': 54}\n",
      "08/13/2020 12:44:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999991075551007e-05, 'epoch': 5.354669396018153e-06, 'step': 56}\n",
      "08/13/2020 12:44:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999990756820685e-05, 'epoch': 5.545907588733087e-06, 'step': 58}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999990438090364e-05, 'epoch': 5.737145781448021e-06, 'step': 60}\n",
      "08/13/2020 12:44:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999990119360044e-05, 'epoch': 5.928383974162955e-06, 'step': 62}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999989800629722e-05, 'epoch': 6.1196221668778895e-06, 'step': 64}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999989481899401e-05, 'epoch': 6.310860359592823e-06, 'step': 66}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999891631690796e-05, 'epoch': 6.502098552307757e-06, 'step': 68}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999888444387585e-05, 'epoch': 6.693336745022692e-06, 'step': 70}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999885257084374e-05, 'epoch': 6.884574937737625e-06, 'step': 72}\n",
      "08/13/2020 12:44:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999988206978116e-05, 'epoch': 7.07581313045256e-06, 'step': 74}\n",
      "08/13/2020 12:44:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999987888247795e-05, 'epoch': 7.267051323167493e-06, 'step': 76}\n",
      "08/13/2020 12:44:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999987569517473e-05, 'epoch': 7.4582895158824275e-06, 'step': 78}\n",
      "08/13/2020 12:44:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999987250787153e-05, 'epoch': 7.649527708597361e-06, 'step': 80}\n",
      "08/13/2020 12:44:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999986932056832e-05, 'epoch': 7.840765901312296e-06, 'step': 82}\n",
      "08/13/2020 12:44:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99998661332651e-05, 'epoch': 8.03200409402723e-06, 'step': 84}\n",
      "08/13/2020 12:44:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999986294596189e-05, 'epoch': 8.223242286742163e-06, 'step': 86}\n",
      "08/13/2020 12:44:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999859758658676e-05, 'epoch': 8.414480479457098e-06, 'step': 88}\n",
      "08/13/2020 12:44:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999985657135547e-05, 'epoch': 8.605718672172032e-06, 'step': 90}\n",
      "08/13/2020 12:44:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999853384052253e-05, 'epoch': 8.796956864886965e-06, 'step': 92}\n",
      "08/13/2020 12:44:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999985019674904e-05, 'epoch': 8.988195057601899e-06, 'step': 94}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999984700944583e-05, 'epoch': 9.179433250316834e-06, 'step': 96}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999984382214262e-05, 'epoch': 9.370671443031768e-06, 'step': 98}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999984063483941e-05, 'epoch': 9.561909635746701e-06, 'step': 100}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99998374475362e-05, 'epoch': 9.753147828461636e-06, 'step': 102}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999983426023298e-05, 'epoch': 9.94438602117657e-06, 'step': 104}\n",
      "08/13/2020 12:44:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999983107292977e-05, 'epoch': 1.0135624213891504e-05, 'step': 106}\n",
      "08/13/2020 12:44:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999982788562656e-05, 'epoch': 1.0326862406606439e-05, 'step': 108}\n",
      "08/13/2020 12:44:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999982469832335e-05, 'epoch': 1.0518100599321372e-05, 'step': 110}\n",
      "08/13/2020 12:44:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999982151102013e-05, 'epoch': 1.0709338792036306e-05, 'step': 112}\n",
      "08/13/2020 12:44:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999981832371692e-05, 'epoch': 1.0900576984751241e-05, 'step': 114}\n",
      "08/13/2020 12:44:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999981513641371e-05, 'epoch': 1.1091815177466174e-05, 'step': 116}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99998119491105e-05, 'epoch': 1.1283053370181108e-05, 'step': 118}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999980876180729e-05, 'epoch': 1.1474291562896042e-05, 'step': 120}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999805574504076e-05, 'epoch': 1.1665529755610977e-05, 'step': 122}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999980238720086e-05, 'epoch': 1.185676794832591e-05, 'step': 124}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999799199897654e-05, 'epoch': 1.2048006141040844e-05, 'step': 126}\n",
      "08/13/2020 12:44:54 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999979601259444e-05, 'epoch': 1.2239244333755779e-05, 'step': 128}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999979282529123e-05, 'epoch': 1.2430482526470712e-05, 'step': 130}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999978963798801e-05, 'epoch': 1.2621720719185646e-05, 'step': 132}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99997864506848e-05, 'epoch': 1.2812958911900581e-05, 'step': 134}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99997832633816e-05, 'epoch': 1.3004197104615515e-05, 'step': 136}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999978007607838e-05, 'epoch': 1.3195435297330448e-05, 'step': 138}\n",
      "08/13/2020 12:44:55 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999977688877517e-05, 'epoch': 1.3386673490045383e-05, 'step': 140}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999773701471956e-05, 'epoch': 1.3577911682760317e-05, 'step': 142}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999770514168745e-05, 'epoch': 1.376914987547525e-05, 'step': 144}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999767326865534e-05, 'epoch': 1.3960388068190186e-05, 'step': 146}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999976413956232e-05, 'epoch': 1.415162626090512e-05, 'step': 148}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999976095225911e-05, 'epoch': 1.4342864453620053e-05, 'step': 150}\n",
      "08/13/2020 12:44:56 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999975776495589e-05, 'epoch': 1.4534102646334986e-05, 'step': 152}\n",
      "08/13/2020 12:44:57 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999975457765269e-05, 'epoch': 1.4725340839049921e-05, 'step': 154}\n",
      "08/13/2020 12:44:57 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999975139034948e-05, 'epoch': 1.4916579031764855e-05, 'step': 156}\n",
      "08/13/2020 12:44:57 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999974820304626e-05, 'epoch': 1.5107817224479788e-05, 'step': 158}\n",
      "08/13/2020 12:44:57 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999974501574305e-05, 'epoch': 1.5299055417194722e-05, 'step': 160}\n",
      "08/13/2020 12:44:57 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999741828439836e-05, 'epoch': 1.5490293609909656e-05, 'step': 162}\n",
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999738641136625e-05, 'epoch': 1.5681531802624592e-05, 'step': 164}\n",
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999973545383341e-05, 'epoch': 1.5872769995339526e-05, 'step': 166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99997322665302e-05, 'epoch': 1.606400818805446e-05, 'step': 168}\n",
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999972907922699e-05, 'epoch': 1.6255246380769393e-05, 'step': 170}\n",
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999972589192378e-05, 'epoch': 1.6446484573484326e-05, 'step': 172}\n",
      "08/13/2020 12:44:58 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999972270462057e-05, 'epoch': 1.663772276619926e-05, 'step': 174}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999719517317357e-05, 'epoch': 1.6828960958914197e-05, 'step': 176}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999971633001414e-05, 'epoch': 1.702019915162913e-05, 'step': 178}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999971314271093e-05, 'epoch': 1.7211437344344064e-05, 'step': 180}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999970995540772e-05, 'epoch': 1.7402675537058997e-05, 'step': 182}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999706768104504e-05, 'epoch': 1.759391372977393e-05, 'step': 184}\n",
      "08/13/2020 12:44:59 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999970358080129e-05, 'epoch': 1.7785151922488865e-05, 'step': 186}\n",
      "08/13/2020 12:45:00 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999970039349808e-05, 'epoch': 1.7976390115203798e-05, 'step': 188}\n",
      "08/13/2020 12:45:00 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999969720619487e-05, 'epoch': 1.8167628307918735e-05, 'step': 190}\n",
      "08/13/2020 12:45:00 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999969401889166e-05, 'epoch': 1.835886650063367e-05, 'step': 192}\n",
      "08/13/2020 12:45:00 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999969083158845e-05, 'epoch': 1.8550104693348602e-05, 'step': 194}\n",
      "08/13/2020 12:45:00 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999687644285236e-05, 'epoch': 1.8741342886063535e-05, 'step': 196}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999968445698202e-05, 'epoch': 1.893258107877847e-05, 'step': 198}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999681269678814e-05, 'epoch': 1.9123819271493403e-05, 'step': 200}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99996780823756e-05, 'epoch': 1.931505746420834e-05, 'step': 202}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999674895072384e-05, 'epoch': 1.9506295656923273e-05, 'step': 204}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999967170776917e-05, 'epoch': 1.9697533849638206e-05, 'step': 206}\n",
      "08/13/2020 12:45:01 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999966852046596e-05, 'epoch': 1.988877204235314e-05, 'step': 208}\n",
      "08/13/2020 12:45:02 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999966533316276e-05, 'epoch': 2.0080010235068073e-05, 'step': 210}\n",
      "08/13/2020 12:45:02 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999966214585954e-05, 'epoch': 2.0271248427783007e-05, 'step': 212}\n",
      "08/13/2020 12:45:02 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999965895855633e-05, 'epoch': 2.046248662049794e-05, 'step': 214}\n",
      "08/13/2020 12:45:02 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999655771253116e-05, 'epoch': 2.0653724813212877e-05, 'step': 216}\n",
      "08/13/2020 12:45:02 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999652583949905e-05, 'epoch': 2.084496300592781e-05, 'step': 218}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999649396646693e-05, 'epoch': 2.1036201198642744e-05, 'step': 220}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999964620934348e-05, 'epoch': 2.1227439391357678e-05, 'step': 222}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999643022040264e-05, 'epoch': 2.141867758407261e-05, 'step': 224}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999963983473705e-05, 'epoch': 2.1609915776787545e-05, 'step': 226}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999963664743385e-05, 'epoch': 2.1801153969502482e-05, 'step': 228}\n",
      "08/13/2020 12:45:03 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999963346013064e-05, 'epoch': 2.1992392162217415e-05, 'step': 230}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999963027282742e-05, 'epoch': 2.218363035493235e-05, 'step': 232}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999962708552421e-05, 'epoch': 2.2374868547647282e-05, 'step': 234}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999623898220996e-05, 'epoch': 2.2566106740362216e-05, 'step': 236}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999620710917785e-05, 'epoch': 2.275734493307715e-05, 'step': 238}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999961752361457e-05, 'epoch': 2.2948583125792083e-05, 'step': 240}\n",
      "08/13/2020 12:45:04 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999961433631136e-05, 'epoch': 2.313982131850702e-05, 'step': 242}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999611149008144e-05, 'epoch': 2.3331059511221953e-05, 'step': 244}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999960796170494e-05, 'epoch': 2.3522297703936887e-05, 'step': 246}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999960477440173e-05, 'epoch': 2.371353589665182e-05, 'step': 248}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999601587098516e-05, 'epoch': 2.3904774089366754e-05, 'step': 250}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99995983997953e-05, 'epoch': 2.4096012282081688e-05, 'step': 252}\n",
      "08/13/2020 12:45:05 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999959521249209e-05, 'epoch': 2.4287250474796624e-05, 'step': 254}\n",
      "08/13/2020 12:45:06 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999959202518888e-05, 'epoch': 2.4478488667511558e-05, 'step': 256}\n",
      "08/13/2020 12:45:06 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999588837885664e-05, 'epoch': 2.466972686022649e-05, 'step': 258}\n",
      "08/13/2020 12:45:06 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999958565058245e-05, 'epoch': 2.4860965052941425e-05, 'step': 260}\n",
      "08/13/2020 12:45:06 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999958246327924e-05, 'epoch': 2.505220324565636e-05, 'step': 262}\n",
      "08/13/2020 12:45:06 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999957927597603e-05, 'epoch': 2.5243441438371292e-05, 'step': 264}\n",
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999957608867282e-05, 'epoch': 2.5434679631086226e-05, 'step': 266}\n",
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999957290136961e-05, 'epoch': 2.5625917823801162e-05, 'step': 268}\n",
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999569714066396e-05, 'epoch': 2.5817156016516096e-05, 'step': 270}\n",
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999956652676318e-05, 'epoch': 2.600839420923103e-05, 'step': 272}\n",
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999563339459974e-05, 'epoch': 2.6199632401945963e-05, 'step': 274}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:45:07 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999956015215676e-05, 'epoch': 2.6390870594660896e-05, 'step': 276}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999556964853544e-05, 'epoch': 2.658210878737583e-05, 'step': 278}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999955377755033e-05, 'epoch': 2.6773346980090767e-05, 'step': 280}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999955059024712e-05, 'epoch': 2.69645851728057e-05, 'step': 282}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999954740294391e-05, 'epoch': 2.7155823365520634e-05, 'step': 284}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99995442156407e-05, 'epoch': 2.7347061558235567e-05, 'step': 286}\n",
      "08/13/2020 12:45:08 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999954102833749e-05, 'epoch': 2.75382997509505e-05, 'step': 288}\n",
      "08/13/2020 12:45:09 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999537841034276e-05, 'epoch': 2.7729537943665434e-05, 'step': 290}\n",
      "08/13/2020 12:45:09 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999534653731065e-05, 'epoch': 2.792077613638037e-05, 'step': 292}\n",
      "08/13/2020 12:45:09 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999953146642785e-05, 'epoch': 2.8112014329095305e-05, 'step': 294}\n",
      "08/13/2020 12:45:09 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999952827912464e-05, 'epoch': 2.830325252181024e-05, 'step': 296}\n",
      "08/13/2020 12:45:09 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999525091821424e-05, 'epoch': 2.8494490714525172e-05, 'step': 298}\n",
      "08/13/2020 12:45:10 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999952190451821e-05, 'epoch': 2.8685728907240105e-05, 'step': 300}\n",
      "08/13/2020 12:45:10 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999951871721501e-05, 'epoch': 2.887696709995504e-05, 'step': 302}\n",
      "08/13/2020 12:45:10 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999951552991179e-05, 'epoch': 2.9068205292669972e-05, 'step': 304}\n",
      "08/13/2020 12:45:10 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999951234260858e-05, 'epoch': 2.925944348538491e-05, 'step': 306}\n",
      "08/13/2020 12:45:10 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999950915530537e-05, 'epoch': 2.9450681678099843e-05, 'step': 308}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999505968002156e-05, 'epoch': 2.9641919870814776e-05, 'step': 310}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999502780698944e-05, 'epoch': 2.983315806352971e-05, 'step': 312}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999949959339573e-05, 'epoch': 3.0024396256244643e-05, 'step': 314}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999949640609252e-05, 'epoch': 3.0215634448959577e-05, 'step': 316}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999493218789304e-05, 'epoch': 3.0406872641674514e-05, 'step': 318}\n",
      "08/13/2020 12:45:11 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99994900314861e-05, 'epoch': 3.0598110834389444e-05, 'step': 320}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999948684418289e-05, 'epoch': 3.078934902710438e-05, 'step': 322}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999948365687967e-05, 'epoch': 3.098058721981931e-05, 'step': 324}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999948046957646e-05, 'epoch': 3.117182541253425e-05, 'step': 326}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999947728227325e-05, 'epoch': 3.1363063605249185e-05, 'step': 328}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999947409497004e-05, 'epoch': 3.155430179796412e-05, 'step': 330}\n",
      "08/13/2020 12:45:12 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999470907666824e-05, 'epoch': 3.174553999067905e-05, 'step': 332}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999946772036361e-05, 'epoch': 3.1936778183393985e-05, 'step': 334}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99994645330604e-05, 'epoch': 3.212801637610892e-05, 'step': 336}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999946134575719e-05, 'epoch': 3.231925456882385e-05, 'step': 338}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999945815845398e-05, 'epoch': 3.2510492761538786e-05, 'step': 340}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999945497115077e-05, 'epoch': 3.270173095425372e-05, 'step': 342}\n",
      "08/13/2020 12:45:13 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999945178384755e-05, 'epoch': 3.289296914696865e-05, 'step': 344}\n",
      "08/13/2020 12:45:14 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999944859654434e-05, 'epoch': 3.3084207339683587e-05, 'step': 346}\n",
      "08/13/2020 12:45:14 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999445409241133e-05, 'epoch': 3.327544553239852e-05, 'step': 348}\n",
      "08/13/2020 12:45:14 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999944222193792e-05, 'epoch': 3.3466683725113454e-05, 'step': 350}\n",
      "08/13/2020 12:45:14 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999439034634704e-05, 'epoch': 3.3657921917828394e-05, 'step': 352}\n",
      "08/13/2020 12:45:14 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999943584733149e-05, 'epoch': 3.384916011054333e-05, 'step': 354}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999943266002828e-05, 'epoch': 3.404039830325826e-05, 'step': 356}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999942947272507e-05, 'epoch': 3.4231636495973194e-05, 'step': 358}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999942628542186e-05, 'epoch': 3.442287468868813e-05, 'step': 360}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999942309811865e-05, 'epoch': 3.461411288140306e-05, 'step': 362}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999941991081543e-05, 'epoch': 3.4805351074117995e-05, 'step': 364}\n",
      "08/13/2020 12:45:15 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999416723512225e-05, 'epoch': 3.499658926683293e-05, 'step': 366}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999941353620901e-05, 'epoch': 3.518782745954786e-05, 'step': 368}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99994103489058e-05, 'epoch': 3.5379065652262795e-05, 'step': 370}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999407161602584e-05, 'epoch': 3.557030384497773e-05, 'step': 372}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999940397429937e-05, 'epoch': 3.576154203769266e-05, 'step': 374}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999940078699617e-05, 'epoch': 3.5952780230407596e-05, 'step': 376}\n",
      "08/13/2020 12:45:16 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999939759969295e-05, 'epoch': 3.6144018423122536e-05, 'step': 378}\n",
      "08/13/2020 12:45:17 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999939441238974e-05, 'epoch': 3.633525661583747e-05, 'step': 380}\n",
      "08/13/2020 12:45:17 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999939122508653e-05, 'epoch': 3.65264948085524e-05, 'step': 382}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:45:17 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999388037783316e-05, 'epoch': 3.671773300126734e-05, 'step': 384}\n",
      "08/13/2020 12:45:17 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999384850480104e-05, 'epoch': 3.690897119398227e-05, 'step': 386}\n",
      "08/13/2020 12:45:17 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999938166317689e-05, 'epoch': 3.7100209386697204e-05, 'step': 388}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999937847587368e-05, 'epoch': 3.729144757941214e-05, 'step': 390}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999375288570464e-05, 'epoch': 3.748268577212707e-05, 'step': 392}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999937210126726e-05, 'epoch': 3.7673923964842004e-05, 'step': 394}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999936891396405e-05, 'epoch': 3.786516215755694e-05, 'step': 396}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999936572666083e-05, 'epoch': 3.805640035027187e-05, 'step': 398}\n",
      "08/13/2020 12:45:18 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999936253935762e-05, 'epoch': 3.8247638542986805e-05, 'step': 400}\n",
      "08/13/2020 12:45:19 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999935935205441e-05, 'epoch': 3.843887673570174e-05, 'step': 402}\n",
      "08/13/2020 12:45:19 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999356164751195e-05, 'epoch': 3.863011492841668e-05, 'step': 404}\n",
      "08/13/2020 12:45:19 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999352977447984e-05, 'epoch': 3.882135312113161e-05, 'step': 406}\n",
      "08/13/2020 12:45:19 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999934979014477e-05, 'epoch': 3.9012591313846546e-05, 'step': 408}\n",
      "08/13/2020 12:45:19 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999934660284156e-05, 'epoch': 3.920382950656148e-05, 'step': 410}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999934341553834e-05, 'epoch': 3.939506769927641e-05, 'step': 412}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999934022823514e-05, 'epoch': 3.9586305891991346e-05, 'step': 414}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999933704093193e-05, 'epoch': 3.977754408470628e-05, 'step': 416}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999933385362871e-05, 'epoch': 3.9968782277421213e-05, 'step': 418}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99993306663255e-05, 'epoch': 4.016002047013615e-05, 'step': 420}\n",
      "08/13/2020 12:45:20 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999327479022287e-05, 'epoch': 4.035125866285108e-05, 'step': 422}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999324291719075e-05, 'epoch': 4.0542496855566014e-05, 'step': 424}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999321104415864e-05, 'epoch': 4.073373504828095e-05, 'step': 426}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999931791711265e-05, 'epoch': 4.092497324099588e-05, 'step': 428}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999931472980944e-05, 'epoch': 4.111621143371082e-05, 'step': 430}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999931154250623e-05, 'epoch': 4.1307449626425755e-05, 'step': 432}\n",
      "08/13/2020 12:45:21 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999930835520302e-05, 'epoch': 4.149868781914069e-05, 'step': 434}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999930516789981e-05, 'epoch': 4.168992601185562e-05, 'step': 436}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999930198059659e-05, 'epoch': 4.1881164204570555e-05, 'step': 438}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999929879329338e-05, 'epoch': 4.207240239728549e-05, 'step': 440}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999929560599017e-05, 'epoch': 4.226364059000042e-05, 'step': 442}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999292418686955e-05, 'epoch': 4.2454878782715356e-05, 'step': 444}\n",
      "08/13/2020 12:45:22 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999289231383744e-05, 'epoch': 4.264611697543029e-05, 'step': 446}\n",
      "08/13/2020 12:45:23 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999928604408053e-05, 'epoch': 4.283735516814522e-05, 'step': 448}\n",
      "08/13/2020 12:45:23 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999928285677732e-05, 'epoch': 4.3028593360860156e-05, 'step': 450}\n",
      "08/13/2020 12:45:23 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999927966947411e-05, 'epoch': 4.321983155357509e-05, 'step': 452}\n",
      "08/13/2020 12:45:23 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99992764821709e-05, 'epoch': 4.3411069746290024e-05, 'step': 454}\n",
      "08/13/2020 12:45:23 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999927329486769e-05, 'epoch': 4.3602307939004964e-05, 'step': 456}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999927010756447e-05, 'epoch': 4.37935461317199e-05, 'step': 458}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999266920261264e-05, 'epoch': 4.398478432443483e-05, 'step': 460}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999926373295805e-05, 'epoch': 4.4176022517149764e-05, 'step': 462}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999260545654835e-05, 'epoch': 4.43672607098647e-05, 'step': 464}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999257358351623e-05, 'epoch': 4.455849890257963e-05, 'step': 466}\n",
      "08/13/2020 12:45:24 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999925417104841e-05, 'epoch': 4.4749737095294565e-05, 'step': 468}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999925098374521e-05, 'epoch': 4.49409752880095e-05, 'step': 470}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999924779644199e-05, 'epoch': 4.513221348072443e-05, 'step': 472}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999924460913878e-05, 'epoch': 4.5323451673439365e-05, 'step': 474}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999924142183557e-05, 'epoch': 4.55146898661543e-05, 'step': 476}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999238234532355e-05, 'epoch': 4.570592805886923e-05, 'step': 478}\n",
      "08/13/2020 12:45:25 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999235047229144e-05, 'epoch': 4.5897166251584166e-05, 'step': 480}\n",
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999923185992593e-05, 'epoch': 4.6088404444299106e-05, 'step': 482}\n",
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999228672622715e-05, 'epoch': 4.627964263701404e-05, 'step': 484}\n",
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99992254853195e-05, 'epoch': 4.647088082972897e-05, 'step': 486}\n",
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99992222980163e-05, 'epoch': 4.666211902244391e-05, 'step': 488}\n",
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999921911071309e-05, 'epoch': 4.685335721515884e-05, 'step': 490}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:45:26 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999921592340987e-05, 'epoch': 4.7044595407873774e-05, 'step': 492}\n",
      "08/13/2020 12:45:27 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999921273610666e-05, 'epoch': 4.723583360058871e-05, 'step': 494}\n",
      "08/13/2020 12:45:27 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999209548803446e-05, 'epoch': 4.742707179330364e-05, 'step': 496}\n",
      "08/13/2020 12:45:27 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999206361500235e-05, 'epoch': 4.7618309986018574e-05, 'step': 498}\n",
      "08/13/2020 12:45:27 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999203174197024e-05, 'epoch': 4.780954817873351e-05, 'step': 500}\n",
      "08/13/2020 12:45:27 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999919998689381e-05, 'epoch': 4.800078637144844e-05, 'step': 502}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999196799590594e-05, 'epoch': 4.8192024564163375e-05, 'step': 504}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999919361228739e-05, 'epoch': 4.838326275687831e-05, 'step': 506}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999919042498418e-05, 'epoch': 4.857450094959325e-05, 'step': 508}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999918723768097e-05, 'epoch': 4.876573914230818e-05, 'step': 510}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999918405037775e-05, 'epoch': 4.8956977335023116e-05, 'step': 512}\n",
      "08/13/2020 12:45:28 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999918086307454e-05, 'epoch': 4.914821552773805e-05, 'step': 514}\n",
      "08/13/2020 12:45:29 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999917767577133e-05, 'epoch': 4.933945372045298e-05, 'step': 516}\n",
      "08/13/2020 12:45:29 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999174488468115e-05, 'epoch': 4.9530691913167916e-05, 'step': 518}\n",
      "08/13/2020 12:45:29 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999171301164904e-05, 'epoch': 4.972193010588285e-05, 'step': 520}\n",
      "08/13/2020 12:45:29 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999916811386169e-05, 'epoch': 4.9913168298597783e-05, 'step': 522}\n",
      "08/13/2020 12:45:29 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999916492655848e-05, 'epoch': 5.010440649131272e-05, 'step': 524}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999916173925527e-05, 'epoch': 5.029564468402765e-05, 'step': 526}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999915855195206e-05, 'epoch': 5.0486882876742584e-05, 'step': 528}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999915536464885e-05, 'epoch': 5.067812106945752e-05, 'step': 530}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999915217734563e-05, 'epoch': 5.086935926217245e-05, 'step': 532}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999148990042424e-05, 'epoch': 5.106059745488739e-05, 'step': 534}\n",
      "08/13/2020 12:45:30 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999914580273921e-05, 'epoch': 5.1251835647602325e-05, 'step': 536}\n",
      "08/13/2020 12:45:31 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999142615435995e-05, 'epoch': 5.144307384031726e-05, 'step': 538}\n",
      "08/13/2020 12:45:31 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999913942813278e-05, 'epoch': 5.163431203303219e-05, 'step': 540}\n",
      "08/13/2020 12:45:31 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999913624082957e-05, 'epoch': 5.1825550225747125e-05, 'step': 542}\n",
      "08/13/2020 12:45:31 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999913305352636e-05, 'epoch': 5.201678841846206e-05, 'step': 544}\n",
      "08/13/2020 12:45:31 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999912986622315e-05, 'epoch': 5.220802661117699e-05, 'step': 546}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999912667891994e-05, 'epoch': 5.2399264803891926e-05, 'step': 548}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999123491616727e-05, 'epoch': 5.259050299660686e-05, 'step': 550}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999120304313515e-05, 'epoch': 5.278174118932179e-05, 'step': 552}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999117117010304e-05, 'epoch': 5.2972979382036726e-05, 'step': 554}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999911392970709e-05, 'epoch': 5.316421757475166e-05, 'step': 556}\n",
      "08/13/2020 12:45:32 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999110742403874e-05, 'epoch': 5.33554557674666e-05, 'step': 558}\n",
      "08/13/2020 12:45:33 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999910755510066e-05, 'epoch': 5.3546693960181534e-05, 'step': 560}\n",
      "08/13/2020 12:45:33 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999910436779746e-05, 'epoch': 5.373793215289647e-05, 'step': 562}\n",
      "08/13/2020 12:45:33 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999910118049424e-05, 'epoch': 5.39291703456114e-05, 'step': 564}\n",
      "08/13/2020 12:45:33 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999909799319103e-05, 'epoch': 5.4120408538326334e-05, 'step': 566}\n",
      "08/13/2020 12:45:33 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999909480588782e-05, 'epoch': 5.431164673104127e-05, 'step': 568}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999091618584606e-05, 'epoch': 5.45028849237562e-05, 'step': 570}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999088431281395e-05, 'epoch': 5.4694123116471135e-05, 'step': 572}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999085243978184e-05, 'epoch': 5.488536130918607e-05, 'step': 574}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999908205667497e-05, 'epoch': 5.5076599501901e-05, 'step': 576}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999078869371754e-05, 'epoch': 5.5267837694615935e-05, 'step': 578}\n",
      "08/13/2020 12:45:34 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999907568206855e-05, 'epoch': 5.545907588733087e-05, 'step': 580}\n",
      "08/13/2020 12:45:35 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999907249476534e-05, 'epoch': 5.56503140800458e-05, 'step': 582}\n",
      "08/13/2020 12:45:35 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999906930746212e-05, 'epoch': 5.584155227276074e-05, 'step': 584}\n",
      "08/13/2020 12:45:35 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999906612015891e-05, 'epoch': 5.6032790465475676e-05, 'step': 586}\n",
      "08/13/2020 12:45:35 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99990629328557e-05, 'epoch': 5.622402865819061e-05, 'step': 588}\n",
      "08/13/2020 12:45:35 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999905974555249e-05, 'epoch': 5.641526685090554e-05, 'step': 590}\n",
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999056558249275e-05, 'epoch': 5.660650504362048e-05, 'step': 592}\n",
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999053370946063e-05, 'epoch': 5.679774323633541e-05, 'step': 594}\n",
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999905018364285e-05, 'epoch': 5.6988981429050344e-05, 'step': 596}\n",
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999904699633964e-05, 'epoch': 5.718021962176528e-05, 'step': 598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999904380903643e-05, 'epoch': 5.737145781448021e-05, 'step': 600}\n",
      "08/13/2020 12:45:36 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999904062173322e-05, 'epoch': 5.7562696007195144e-05, 'step': 602}\n",
      "08/13/2020 12:45:37 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999903743443e-05, 'epoch': 5.775393419991008e-05, 'step': 604}\n",
      "08/13/2020 12:45:37 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999903424712679e-05, 'epoch': 5.794517239262501e-05, 'step': 606}\n",
      "08/13/2020 12:45:37 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999031059823584e-05, 'epoch': 5.8136410585339945e-05, 'step': 608}\n",
      "08/13/2020 12:45:37 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999902787252037e-05, 'epoch': 5.8327648778054885e-05, 'step': 610}\n",
      "08/13/2020 12:45:37 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999024685217155e-05, 'epoch': 5.851888697076982e-05, 'step': 612}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999902149791394e-05, 'epoch': 5.871012516348475e-05, 'step': 614}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999901831061073e-05, 'epoch': 5.8901363356199686e-05, 'step': 616}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999901512330752e-05, 'epoch': 5.909260154891462e-05, 'step': 618}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999901193600431e-05, 'epoch': 5.928383974162955e-05, 'step': 620}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99990087487011e-05, 'epoch': 5.9475077934344486e-05, 'step': 622}\n",
      "08/13/2020 12:45:38 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999900556139788e-05, 'epoch': 5.966631612705942e-05, 'step': 624}\n",
      "08/13/2020 12:45:39 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9999002374094675e-05, 'epoch': 5.9857554319774353e-05, 'step': 626}\n",
      "08/13/2020 12:45:39 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998999186791464e-05, 'epoch': 6.004879251248929e-05, 'step': 628}\n",
      "08/13/2020 12:45:39 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999899599948825e-05, 'epoch': 6.024003070520422e-05, 'step': 630}\n",
      "08/13/2020 12:45:39 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998992812185034e-05, 'epoch': 6.0431268897919154e-05, 'step': 632}\n",
      "08/13/2020 12:45:39 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999898962488182e-05, 'epoch': 6.062250709063409e-05, 'step': 634}\n",
      "08/13/2020 12:45:40 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999898643757862e-05, 'epoch': 6.081374528334903e-05, 'step': 636}\n",
      "08/13/2020 12:45:40 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99989832502754e-05, 'epoch': 6.100498347606396e-05, 'step': 638}\n",
      "08/13/2020 12:45:40 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999898006297219e-05, 'epoch': 6.119622166877889e-05, 'step': 640}\n",
      "08/13/2020 12:45:40 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999897687566898e-05, 'epoch': 6.138745986149383e-05, 'step': 642}\n",
      "08/13/2020 12:45:40 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998973688365766e-05, 'epoch': 6.157869805420876e-05, 'step': 644}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998970501062555e-05, 'epoch': 6.17699362469237e-05, 'step': 646}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998967313759344e-05, 'epoch': 6.196117443963862e-05, 'step': 648}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999896412645613e-05, 'epoch': 6.215241263235356e-05, 'step': 650}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998960939152914e-05, 'epoch': 6.23436508250685e-05, 'step': 652}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999895775184971e-05, 'epoch': 6.253488901778343e-05, 'step': 654}\n",
      "08/13/2020 12:45:41 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99989545645465e-05, 'epoch': 6.272612721049837e-05, 'step': 656}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999895137724328e-05, 'epoch': 6.29173654032133e-05, 'step': 658}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999894818994007e-05, 'epoch': 6.310860359592824e-05, 'step': 660}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999894500263686e-05, 'epoch': 6.329984178864316e-05, 'step': 662}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998941815333646e-05, 'epoch': 6.34910799813581e-05, 'step': 664}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998938628030435e-05, 'epoch': 6.368231817407303e-05, 'step': 666}\n",
      "08/13/2020 12:45:42 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999893544072722e-05, 'epoch': 6.387355636678797e-05, 'step': 668}\n",
      "08/13/2020 12:45:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999893225342401e-05, 'epoch': 6.40647945595029e-05, 'step': 670}\n",
      "08/13/2020 12:45:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99989290661208e-05, 'epoch': 6.425603275221784e-05, 'step': 672}\n",
      "08/13/2020 12:45:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999892587881759e-05, 'epoch': 6.444727094493276e-05, 'step': 674}\n",
      "08/13/2020 12:45:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999892269151438e-05, 'epoch': 6.46385091376477e-05, 'step': 676}\n",
      "08/13/2020 12:45:43 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999891950421116e-05, 'epoch': 6.482974733036265e-05, 'step': 678}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999891631690795e-05, 'epoch': 6.502098552307757e-05, 'step': 680}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998913129604744e-05, 'epoch': 6.521222371579251e-05, 'step': 682}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998909942301526e-05, 'epoch': 6.540346190850744e-05, 'step': 684}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998906754998314e-05, 'epoch': 6.559470010122238e-05, 'step': 686}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99989035676951e-05, 'epoch': 6.57859382939373e-05, 'step': 688}\n",
      "08/13/2020 12:45:44 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999890038039189e-05, 'epoch': 6.597717648665225e-05, 'step': 690}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999889719308868e-05, 'epoch': 6.616841467936717e-05, 'step': 692}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999889400578547e-05, 'epoch': 6.635965287208211e-05, 'step': 694}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999889081848226e-05, 'epoch': 6.655089106479704e-05, 'step': 696}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999888763117904e-05, 'epoch': 6.674212925751198e-05, 'step': 698}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998884443875835e-05, 'epoch': 6.693336745022691e-05, 'step': 700}\n",
      "08/13/2020 12:45:45 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998881256572624e-05, 'epoch': 6.712460564294185e-05, 'step': 702}\n",
      "08/13/2020 12:45:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998878069269406e-05, 'epoch': 6.731584383565679e-05, 'step': 704}\n",
      "08/13/2020 12:45:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998874881966194e-05, 'epoch': 6.750708202837171e-05, 'step': 706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/13/2020 12:45:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999887169466298e-05, 'epoch': 6.769832022108665e-05, 'step': 708}\n",
      "08/13/2020 12:45:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999886850735978e-05, 'epoch': 6.788955841380158e-05, 'step': 710}\n",
      "08/13/2020 12:45:46 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999886532005656e-05, 'epoch': 6.808079660651652e-05, 'step': 712}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999886213275335e-05, 'epoch': 6.827203479923145e-05, 'step': 714}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999885894545014e-05, 'epoch': 6.846327299194639e-05, 'step': 716}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998855758146926e-05, 'epoch': 6.865451118466132e-05, 'step': 718}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998852570843715e-05, 'epoch': 6.884574937737626e-05, 'step': 720}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998849383540503e-05, 'epoch': 6.903698757009118e-05, 'step': 722}\n",
      "08/13/2020 12:45:47 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998846196237285e-05, 'epoch': 6.922822576280612e-05, 'step': 724}\n",
      "08/13/2020 12:45:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998843008934074e-05, 'epoch': 6.941946395552105e-05, 'step': 726}\n",
      "08/13/2020 12:45:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999883982163087e-05, 'epoch': 6.961070214823599e-05, 'step': 728}\n",
      "08/13/2020 12:45:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999883663432766e-05, 'epoch': 6.980194034095093e-05, 'step': 730}\n",
      "08/13/2020 12:45:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999883344702444e-05, 'epoch': 6.999317853366586e-05, 'step': 732}\n",
      "08/13/2020 12:45:48 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999883025972123e-05, 'epoch': 7.01844167263808e-05, 'step': 734}\n",
      "08/13/2020 12:45:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999882707241802e-05, 'epoch': 7.037565491909572e-05, 'step': 736}\n",
      "08/13/2020 12:45:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998823885114806e-05, 'epoch': 7.056689311181066e-05, 'step': 738}\n",
      "08/13/2020 12:45:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998820697811595e-05, 'epoch': 7.075813130452559e-05, 'step': 740}\n",
      "08/13/2020 12:45:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999881751050838e-05, 'epoch': 7.094936949724053e-05, 'step': 742}\n",
      "08/13/2020 12:45:49 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998814323205165e-05, 'epoch': 7.114060768995546e-05, 'step': 744}\n",
      "08/13/2020 12:45:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999881113590196e-05, 'epoch': 7.13318458826704e-05, 'step': 746}\n",
      "08/13/2020 12:45:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999880794859875e-05, 'epoch': 7.152308407538533e-05, 'step': 748}\n",
      "08/13/2020 12:45:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999880476129554e-05, 'epoch': 7.171432226810027e-05, 'step': 750}\n",
      "08/13/2020 12:45:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999880157399232e-05, 'epoch': 7.190556046081519e-05, 'step': 752}\n",
      "08/13/2020 12:45:50 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999879838668911e-05, 'epoch': 7.209679865353013e-05, 'step': 754}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998795199385904e-05, 'epoch': 7.228803684624507e-05, 'step': 756}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998792012082686e-05, 'epoch': 7.247927503896e-05, 'step': 758}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998788824779474e-05, 'epoch': 7.267051323167494e-05, 'step': 760}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999878563747626e-05, 'epoch': 7.286175142438987e-05, 'step': 762}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999878245017305e-05, 'epoch': 7.30529896171048e-05, 'step': 764}\n",
      "08/13/2020 12:45:51 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999877926286984e-05, 'epoch': 7.324422780981973e-05, 'step': 766}\n",
      "08/13/2020 12:45:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999877607556663e-05, 'epoch': 7.343546600253467e-05, 'step': 768}\n",
      "08/13/2020 12:45:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.999877288826342e-05, 'epoch': 7.36267041952496e-05, 'step': 770}\n",
      "08/13/2020 12:45:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.99987697009602e-05, 'epoch': 7.381794238796454e-05, 'step': 772}\n",
      "08/13/2020 12:45:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998766513656995e-05, 'epoch': 7.400918058067947e-05, 'step': 774}\n",
      "08/13/2020 12:45:52 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998763326353784e-05, 'epoch': 7.420041877339441e-05, 'step': 776}\n",
      "08/13/2020 12:45:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998760139050565e-05, 'epoch': 7.439165696610933e-05, 'step': 778}\n",
      "08/13/2020 12:45:53 - INFO - transformers.trainer -   {'loss': nan, 'learning_rate': 4.9998756951747354e-05, 'epoch': 7.458289515882427e-05, 'step': 780}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Universidad/ALS/src/finetuning/run_language_modeling.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Universidad/ALS/src/finetuning/run_language_modeling.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         )\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# For convenience, we also re-save the tokenizer to the same directory,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    469\u001b[0m                 ):\n\u001b[1;32m    470\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(arg0, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcast_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcasted_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_func_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/apex/amp/utils.py\u001b[0m in \u001b[0;36mcasted_args\u001b[0;34m(cast_fn, args, kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_fp_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/als-env/lib/python3.7/site-packages/apex/amp/utils.py\u001b[0m in \u001b[0;36mmaybe_float\u001b[0;34m(x, name, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaybe_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FloatTensor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**train_params)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
