{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(sentence, tokenizer, T=20):\n",
    "    #Step 1: Tokenize\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    #Step 2: Add [CLS] and [SEP]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    #Step 3: Pad tokens\n",
    "    padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))]\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    #Step 4: Segment ids\n",
    "    seg_ids = [0 for _ in range(len(padded_tokens))] #Optional!\n",
    "    #Step 5: Get BERT vocabulary index for each token\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "    #Converting everything to torch tensors before feeding them to bert_model\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    attn_mask = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "    seg_ids   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    \n",
    "    return token_ids, attn_mask, seg_ids, padded_tokens\n",
    "\n",
    "def predict_masks(padded_tokens, hidden_reps, tokenizer):\n",
    "    predicted_tokens = []\n",
    "    for i, midx in enumerate(np.where(np.array(padded_tokens) == '[MASK]')[0]):\n",
    "        idxs = torch.argsort(hidden_reps[0,midx], descending=True)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
    "        print(f'MASK {i}:', predicted_token)\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    return predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "class GPT2:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2').to('cuda')\n",
    "        #self.model = GPT2Model.from_pretrained('gpt2')\n",
    "        self.model.eval()\n",
    "\n",
    "    def create_model_inputs(self, sentence):\n",
    "        token_sentence = self.tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        input_ids = torch.tensor(token_sentence).unsqueeze(0).to('cuda')  # Batch size 1\n",
    "        return input_ids.to('cuda')\n",
    "\n",
    "    def predict_tokens(self, sentence, width=5):\n",
    "        input_ids = self.create_model_inputs(sentence).to('cuda')\n",
    "        hidden_reps = self.model(input_ids)[0].to('cuda')\n",
    "        idxs = torch.argsort(hidden_reps[0,-1], descending=True).to('cuda')\n",
    "        predicted_token = self.tokenizer.convert_ids_to_tokens(idxs[:width])\n",
    "        predicted_token = [pred[1:]  for pred in predicted_token]\n",
    "        return predicted_token\n",
    "\n",
    "    def expand_predictions(self, sentence, sentences_tree=[], width=3, length=3):\n",
    "        predicted_tokens = self.predict_tokens(sentence, width=width)\n",
    "        sentences = [sentence + pred_token + \" \" for pred_token in predicted_tokens]\n",
    "        for sent in sentences:\n",
    "            if length > 0:\n",
    "                self.expand_predictions(sent, sentences_tree=sentences_tree, width=width, length=length-1)\n",
    "            else:\n",
    "                sentences_tree.append(sent)\n",
    "                \n",
    "        return sentences_tree.copy()\n",
    "\n",
    "    def run(self, sentence):\n",
    "        predicted_token = self.predict(sentence, width=5)\n",
    "        return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Before boarding your rocket to Mars, remember to pack these items \"\n",
    "#sentence = \"My dear \"\n",
    "model.expand_predictions(sentence, [], width=2, length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "HhAqZLs3lwhW",
    "outputId": "753ba408-b42d-4529-d36b-edd11a4aed07"
   },
   "outputs": [],
   "source": [
    "# Fist install the library and download the models from github\n",
    "\n",
    "#!pip install transformers\n",
    "#!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "#!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "#!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "#!tar -xzvf pytorch_weights.tar.gz\n",
    "#!mv config.json pytorch/.\n",
    "#!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_1ucCCUnXqa"
   },
   "outputs": [],
   "source": [
    "# import the necessary\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gCpOxoXkoGFC",
    "outputId": "c6bbb381-ce5c-4875-d3d7-fc02a98365a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../weights/pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ../weights/pytorch/ and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create the tokenizer and the model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../weights/pytorch/\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"../weights/pytorch/\")\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "9KXo6-ahoJoM",
    "outputId": "4e906e00-62df-4ce2-dcdd-7683cfd91e51"
   },
   "outputs": [],
   "source": [
    "#Specifying the max length\n",
    "T = 12\n",
    "\n",
    "#sentence = \"[CLS] Para [MASK] los [MASK] de Chile, el ministro debe [MASK] de inmediato. [SEP]\"\n",
    "sentence = \"Tengo sed, dame un [MASK] de [MASK]\"\n",
    "\n",
    "# Inputs\n",
    "token_ids, attn_mask, seg_ids, padded_tokens = create_model_inputs(sentence, tokenizer, T)\n",
    "\n",
    "#Feed them to bert\n",
    "hidden_reps = model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.1508, -7.5639, -5.4450,  ...,  1.7980, -5.7680, -0.5667],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0][0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2190,  1847,  2903,  ..., 30838, 27206, 28800])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(\n",
    "    model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0][0, 1], \n",
    "    descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK 0: ['poco', 'vaso', 'trago', 'poquito', 'beso']\n",
      "MASK 1: ['agua', 'vino', 'leche', 'amor', 'lluvia']\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens, idxs = predict_masks(padded_tokens, hidden_reps, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCIBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertForMaskedLM.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Specifying the max length\n",
    "T = 12\n",
    "\n",
    "# Sentence\n",
    "#sentence = 'Character-level modeling of [MASK] language text is [MASK], for several [MASK].'\n",
    "#sentence = 'Prognosis may be essentially understood as the [MASK]' #the [MASK] of long-[MASK] predictions for a [MASK] indicator, made with the purpose'\n",
    "sentence = 'The evaluation of these integrals, though, may be difficult and/or may require significant [MASK]'\n",
    "\n",
    "# Inputs\n",
    "token_ids, attn_mask, seg_ids, padded_tokens = create_model_inputs(sentence, tokenizer, T)\n",
    "\n",
    "#Feed them to bert\n",
    "hidden_reps = model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK 0: ['computational', '.', 'numerical', 'mathematical', 'computation']\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = predict_masks(padded_tokens, hidden_reps, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation of these integrals, though, may be difficult and/or may require significant computational [MASK]\n",
      "The evaluation of these integrals, though, may be difficult and/or may require significant computational effort [MASK]\n",
      "The evaluation of these integrals, though, may be difficult and/or may require significant computational effort . [MASK]\n",
      "The evaluation of these integrals, though, may be difficult and/or may require significant computational effort . ( [MASK]\n",
      "The evaluation of these integrals, though, may be difficult and/or may require significant computational effort . ( ) [MASK]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Specifying the max length\n",
    "T = 100\n",
    "\n",
    "# Sentence\n",
    "#sentence = 'Character-level modeling of [MASK] language text is [MASK], for several [MASK].'\n",
    "sentence = 'The evaluation of these integrals, though, may be difficult and/or may require significant [MASK]'\n",
    "\n",
    "for _ in range(5):\n",
    "    # Inputs\n",
    "    token_ids, attn_mask, seg_ids, padded_tokens = create_model_inputs(sentence, tokenizer, T)\n",
    "\n",
    "    #Feed them to bert\n",
    "    hidden_reps = model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0]\n",
    "\n",
    "    for i, midx in enumerate(np.where(np.array(padded_tokens) == '[MASK]')[0]):\n",
    "        idxs = torch.argsort(hidden_reps[0,midx], descending=True)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
    "        #print(f'MASK {i}:', predicted_token)\n",
    "        sentence = sentence.split(\"[MASK]\")[0] + predicted_token[0] + \" [MASK]\"\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "#Creating instance of BertModel\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#Creating intance of tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the max length\n",
    "T = 20\n",
    "\n",
    "# Sentence\n",
    "sentence = 'Character-level modeling of [MASK] language text is [MASK], for several [MASK].'\n",
    "\n",
    "# Inputs\n",
    "token_ids, attn_mask, seg_ids, padded_tokens = create_model_inputs(sentence, tokenizer, T)\n",
    "\n",
    "#Feed them to bert\n",
    "hidden_reps = bert_model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK 0: ['regurg', 'quasi', '##wn', 'here', 'marx']\n",
      "MASK 1: ['konnten', 'norm', 'lighting', 'leb', '##ÑŽ']\n",
      "MASK 2: ['lighting', '##match', 'konnten', 'ses', 'leb']\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = predict_masks(padded_tokens, hidden_reps, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv(filename, delimiter = '\\t')\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and validation set\n",
    "train_set = SSTDataset(filename = 'DATA/trainDevTestTrees_PTB/trees/train.txt', maxlen = 30)\n",
    "val_set = SSTDataset(filename = 'DATA/trainDevTestTrees_PTB/trees/dev.txt', maxlen = 30)\n",
    "\n",
    "#Creating intsances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
    "val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = True):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #Classification layer\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentClassifier(freeze_bert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, train_loader, val_loader, args):\n",
    "\n",
    "    for ep in range(args.max_eps):\n",
    "        \n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(args.gpu), attn_masks.cuda(args.gpu), labels.cuda(args.gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % args.print_every == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, gpu=0, freeze_bert=\"store_true\", maxlen=25, batch_size=32, lr=2e-5, print_every=100, max_eps=5):\n",
    "        self.gpu = gpu\n",
    "        self.freeze_bert = freeze_bert\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.print_every = print_every\n",
    "        self.max_eps = max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, criterion, opti, train_loader, val_loader, args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BETO examples",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
