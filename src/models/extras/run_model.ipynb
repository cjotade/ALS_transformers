{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_TYPE = \"gpt2\" \n",
    "OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/papers_milan/\"\n",
    "TRAIN_PATH = f\"../../data/papers_milan/train_papers.txt\"\n",
    "TEST_PATH = f\"../../data/papers_milan/test_papers.txt\"\n",
    "VAL_PATH = f\"../../data/papers_milan/val_papers.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_modeling(output_dir, model_type=\"gpt2\", model_name_or_path=None, train_path=None, eval_path=None, \n",
    "                             do_train=False, do_eval=False, evaluate_during_training=False, line_by_line=False, block_size=-1):\n",
    "    return {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"do_train\": \"--do_train\" if do_train else \"\",\n",
    "    \"train_data_file\": train_path if do_train else None,\n",
    "    \"do_eval\": \"--do_eval\" if do_eval else \"\",\n",
    "    \"eval_data_file\": eval_path if do_eval else None,\n",
    "    \"evaluate_during_training\": \"--evaluate_during_training\" if evaluate_during_training else \"\",\n",
    "    \"block_size\": block_size,\n",
    "    \"line_by_line\": \"--line_by_line\" if line_by_line else \"\",\n",
    "    \"fp16\": \"--fp16\",\n",
    "    \"fp16_opt_level\": \"O1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_finetuning = \"\"\"../../transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "    --output_dir={output_dir} \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    {do_train} \\\n",
    "    --train_data_file={train_data_file} \\\n",
    "    {do_eval} \\\n",
    "    --eval_data_file={eval_data_file} \\\n",
    "    {evaluate_during_training} \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --block_size={block_size}\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 5000 \\\n",
    "    --save_total_limit 5 \\\n",
    "    {line_by_line} \\\n",
    "    {fp16} \\\n",
    "    --fp16_opt_level={fp16_opt_level} \\\n",
    "    --logging_steps 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
    "#   line_by_line, should_continue, and model_name_or_path.\n",
    "train_params = create_params_modeling(output_dir=OUTPUT_DIR, \n",
    "                                        model_type=MODEL_TYPE,\n",
    "                                        model_name_or_path=MODEL_TYPE,\n",
    "                                        train_path=TRAIN_PATH, \n",
    "                                        eval_path=TEST_PATH, \n",
    "                                        do_train=True, \n",
    "                                        do_eval=True, \n",
    "                                        evaluate_during_training=False,\n",
    "                                        line_by_line=True\n",
    "                                        )\n",
    "\n",
    "val_finetuning_params = create_params_modeling(output_dir=OUTPUT_DIR,\n",
    "                                    model_name_or_path=OUTPUT_DIR,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,                                      \n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                    )\n",
    "\n",
    "val_params = create_params_modeling(output_dir=OUTPUT_DIR,\n",
    "                                    model_name_or_path=MODEL_TYPE,\n",
    "                                    model_type=MODEL_TYPE,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,\n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_finetuning.format(**train_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:32:10 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/01/2020 18:32:10 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "07/01/2020 18:32:10 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/papers_milan/', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul01_18-32-10_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=5000, save_total_limit=5, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "07/01/2020 18:32:10 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "07/01/2020 18:32:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:32:10 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "07/01/2020 18:32:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   Model name '../../weights/gpt2/papers_milan/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '../../weights/gpt2/papers_milan/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   Didn't find file ../../weights/gpt2/papers_milan/added_tokens.json. We won't load it.\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/vocab.json\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/merges.txt\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/special_tokens_map.json\n",
      "07/01/2020 18:32:10 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/tokenizer_config.json\n",
      "/home/camilojd/Environments/als-env/lib/python3.7/site-packages/transformers/modeling_auto.py:792: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "07/01/2020 18:32:14 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/val_papers.txt\n",
      "07/01/2020 18:32:14 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "07/01/2020 18:32:14 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/01/2020 18:32:14 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2020 18:32:14 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/01/2020 18:32:14 - INFO - transformers.trainer -     Num examples = 447\n",
      "07/01/2020 18:32:14 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2a1f5c8cee448192c006b4ca9d4c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=447.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:32:18 - INFO - transformers.trainer -   {'eval_loss': 3.5828039736822412, 'step': 0}\n",
      "07/01/2020 18:32:18 - INFO - __main__ -   ***** Eval results *****\n",
      "07/01/2020 18:32:18 - INFO - __main__ -     perplexity = 35.974270467091586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**val_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:32:18 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/01/2020 18:32:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "07/01/2020 18:32:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/papers_milan/', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul01_18-32-18_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=5000, save_total_limit=5, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "07/01/2020 18:32:19 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/01/2020 18:32:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:32:19 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/01/2020 18:32:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:32:21 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/camilojd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "07/01/2020 18:32:21 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/camilojd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "07/01/2020 18:32:26 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/val_papers.txt\n",
      "07/01/2020 18:32:26 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "07/01/2020 18:32:26 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/01/2020 18:32:26 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2020 18:32:26 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/01/2020 18:32:26 - INFO - transformers.trainer -     Num examples = 447\n",
      "07/01/2020 18:32:26 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b96c02224d42b09472d5cb314b7ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=447.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:32:30 - INFO - transformers.trainer -   {'eval_loss': 5.188957473042294, 'step': 0}\n",
      "07/01/2020 18:32:30 - INFO - __main__ -   ***** Eval results *****\n",
      "07/01/2020 18:32:30 - INFO - __main__ -     perplexity = 179.28154962227336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**val_params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_generation(MODEL_TYPE, MODEL_NAME_OR_PATH, NUM_RETURN_SEQUENCES=1, LENGTH=20, TRANSLATE_TO=\"\"):\n",
    "    return {\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"model_name_or_path\": MODEL_NAME_OR_PATH,\n",
    "        \"num_return_sequences\": NUM_RETURN_SEQUENCES,\n",
    "        \"length\": LENGTH,\n",
    "        \"translate_to\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"0\": \"mensaje a continuación para predicción\"\n",
    "\"1\": \"mensaje a continuación para configuración\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type -> \"estructura del modelo\"\n",
    "model_name_or_path -> \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTP2 (frases) -> ingles -> translator -> español\n",
    "bert (siguiente palabra) -> ingles\n",
    "Scibert papers (siguiente palabra) -> ingles \n",
    "beto (siguiente palabra) -> español\n",
    "marian (translator) -> idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_generation = \"\"\"python run_generation_server.py \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --num_return_sequences={num_return_sequences} \\\n",
    "    --length={length} \\\n",
    "    {translate_to}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   Model name '../../weights/gpt2/papers_milan/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '../../weights/gpt2/papers_milan/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   Didn't find file ../../weights/gpt2/papers_milan/added_tokens.json. We won't load it.\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/vocab.json\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/merges.txt\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/special_tokens_map.json\n",
      "08/05/2020 19:46:34 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/tokenizer_config.json\n",
      "08/05/2020 19:46:34 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "08/05/2020 19:46:34 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"run_generation_server.py\", line 468, in <module>\n",
      "    model = GenerativeModel(args)\n",
      "  File \"run_generation_server.py\", line 232, in __init__\n",
      "    self.set_tokenizer_and_model(args)\n",
      "  File \"run_generation_server.py\", line 238, in set_tokenizer_and_model\n",
      "    self.tokenizer, self.model = get_tokenizer_and_model(args)\n",
      "  File \"run_generation_server.py\", line 220, in get_tokenizer_and_model\n",
      "    model = model_class.from_pretrained(args.model_name_or_path)\n",
      "  File \"/home/camilojd/Environments/als-env/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 674, in from_pretrained\n",
      "    state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "  File \"/home/camilojd/Environments/als-env/lib/python3.7/site-packages/torch/serialization.py\", line 585, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/camilojd/Environments/als-env/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "{cmd_generation.format(**generation_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_generation = \"\"\"../../transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --num_return_sequences={num_return_sequences} \\\n",
    "    --length={length} \\\n",
    "    --translate_to={translate_to}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_finetuning_params = create_params_generation(MODEL_TYPE, OUTPUT_DIR, NUM_RETURN_SEQUENCES=5, LENGTH=10)\n",
    "generation_params = create_params_generation(MODEL_TYPE, MODEL_TYPE, NUM_RETURN_SEQUENCES=5, LENGTH=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'gpt2',\n",
       " 'model_name_or_path': '../../weights/gpt2/papers_milan/',\n",
       " 'num_return_sequences': 5,\n",
       " 'length': 10,\n",
       " 'translate_to': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_finetuning_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   Model name '../../weights/gpt2/papers_milan/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '../../weights/gpt2/papers_milan/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   Didn't find file ../../weights/gpt2/papers_milan/added_tokens.json. We won't load it.\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/vocab.json\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/merges.txt\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/special_tokens_map.json\n",
      "07/01/2020 18:35:16 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/tokenizer_config.json\n",
      "07/01/2020 18:35:16 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "07/01/2020 18:35:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:35:19 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=100, model_name_or_path='../../weights/gpt2/papers_milan/', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prompt >>> WTF\n",
      "=== GENERATED SEQUENCE 1 ===\n",
      "WTF are compressible let us consider a choice that to and with that is known as compressible norm. We now conclude that the latter is indeed in the works. However, as before, the definition of the in the definition of and the in Appendix A indicate that we are limited by and only consider the rate distortion function. Indeed, this definition is standard and relevant for all, except compressible rate distortion. Of course, a real universal compressible norm for a given rate distortion function is not made explicit\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "WTF: A general case showing that E is zero for all possible, in a compact set. Also, this technique can be as a mathematical method for finite dimensional AR. In this paper, we review, in a compact set, the complete set of and why for some, it can be used for some robust AR. For the remainder of this paper, we present an alternative simple and elegant approach to compress the. We consider the following : If the source is not zero, then the lossless entropy of\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "WTF is the density of a random vector with respect to the th cluster i. e., an arbitrary uniformly distributed over,.., a, i. e., the set of. We define the distribution function as, and define the initial state, and. By the same we define the steady state power spectral density of, in,. It should be noted that, since we are assuming scalar uniform, can only be induced by a finite memoryless and, accordingly, can be assumed to hold\n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "WTFEET LOSS STANDARD THE AND AND AND EXEMPTION OF THE AND THE LOSS STANDARD THE AND EXEMPTION OF THE CATEGOREM THE AND THE EXEMPTION OF THE LOSS STANDARD THE AND AND EXEMPTION OF THE LOSS STANDARD AND THE EXEMPTION OF THE LOSS STANDARD that the. In these, let, denote the unit vector of the and the complex conjugation of the, respectively. For a\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "WTF is not valid for which the are fixed. From the above, the same on the variance of,, i. e.,, can be by, and the upper bound in,, if E, is fixed. Thus, the lower bound in for, a function of the distribution of. From Lemma, it is clear from that is not a constant function. The of, that is, are a function of the distribution of. Therefore, if,, then. This the proof. On\n"
     ]
    }
   ],
   "source": [
    "run {cmd_generation.format(**generation_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 18:35:24 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/camilojd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "07/01/2020 18:35:24 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/camilojd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "07/01/2020 18:35:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/01/2020 18:35:25 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 18:35:30 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=100, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prompt >>> WTF\n",
      "=== GENERATED SEQUENCE 1 ===\n",
      "WTF from Cueball's room.\n",
      "\n",
      "When The Flash came in, he kept mentioning the possibility of him having Flash's storybook, which was in fact not called \"Flash\", but \"Flash's place\" in this incident.\n",
      "\n",
      "Grant denied saying anything about the deaths of Barry and Iris, when there were only five policemen left and over ten minutes between them.\n",
      "\n",
      "Grant's Batman version of himself is very, very angry over this comic.\n",
      "\n",
      "On one of the many episodes in\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "WTF!\n",
      "\n",
      "I DON'T SEE ANY MOCKING FOR A\n",
      "\n",
      "BANK WITH NO ASSETS AROUND!\n",
      "\n",
      "I DON'T SEE ANY MOCKING FOR A BANK WITH\n",
      "\n",
      "NO ASSETS AROUND!\n",
      "\n",
      "I DON'T SEE ANY MOCKING FOR A BANK WITH NO\n",
      "\n",
      "UNDER THOSE COMPOSITE MATTERS!\n",
      "\n",
      "I DON'T SEE ANY MOCKING FOR A BANK WITH\n",
      "\n",
      "UNDER TH\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "WTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTFWTF\n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "WTF' – some but not all (of which are English-speaking).\n",
      "\n",
      "Then I just tried it out at Raoult's house; it worked! I got to know them a little bit before I started reading about them online. They are very conservative about what they do in the club and how much attention they give to their tech and gym work.\n",
      "\n",
      "Gift X\n",
      "\n",
      "Hope your gift meets our tone on these.\n",
      "\n",
      "There was also a month of performance music in one of\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "WTF-DD 7.7-1 F.D.D.A.C. Investigation on Mysterious Youth Activities of Scheuermanstein Games: The Economic Manuva Negrião http://www.etsteja.com/2010/10/07/0801-of-uk.htm - [ON 1.37 F.D.D.] Historical Thesaurus of Britain | Online, by Adi Apekar -- Bibliographies, Bookseller, by Anthology of\n"
     ]
    }
   ],
   "source": [
    "run {cmd_generation.format(**generation_params)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
