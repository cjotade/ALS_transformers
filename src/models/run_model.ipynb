{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_TYPE = \"gpt2\" \n",
    "OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/papers_milan/\"\n",
    "TRAIN_PATH = f\"../../data/papers_milan/train_papers.txt\"\n",
    "TEST_PATH = f\"../../data/papers_milan/test_papers.txt\"\n",
    "VAL_PATH = f\"../../data/papers_milan/val_papers.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_modeling(output_dir, model_type=\"gpt2\", model_name_or_path=None, train_path=None, eval_path=None, \n",
    "                             do_train=False, do_eval=False, evaluate_during_training=False, line_by_line=False, block_size=-1):\n",
    "    return {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"do_train\": \"--do_train\" if do_train else \"\",\n",
    "    \"train_data_file\": train_path if do_train else None,\n",
    "    \"do_eval\": \"--do_eval\" if do_eval else \"\",\n",
    "    \"eval_data_file\": eval_path if do_eval else None,\n",
    "    \"evaluate_during_training\": \"--evaluate_during_training\" if evaluate_during_training else \"\",\n",
    "    \"block_size\": block_size,\n",
    "    \"line_by_line\": \"--line_by_line\" if line_by_line else \"\",\n",
    "    \"fp16\": \"--fp16\",\n",
    "    \"fp16_opt_level\": \"O1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_finetuning = \"\"\"../../transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "    --output_dir={output_dir} \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    {do_train} \\\n",
    "    --train_data_file={train_data_file} \\\n",
    "    {do_eval} \\\n",
    "    --eval_data_file={eval_data_file} \\\n",
    "    {evaluate_during_training} \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --block_size={block_size}\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 5000 \\\n",
    "    --save_total_limit 5 \\\n",
    "    {line_by_line} \\\n",
    "    {fp16} \\\n",
    "    --fp16_opt_level={fp16_opt_level} \\\n",
    "    --logging_steps 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
    "#   line_by_line, should_continue, and model_name_or_path.\n",
    "train_params = create_params_modeling(output_dir=OUTPUT_DIR, \n",
    "                                        model_type=MODEL_TYPE,\n",
    "                                        model_name_or_path=MODEL_TYPE,\n",
    "                                        train_path=TRAIN_PATH, \n",
    "                                        eval_path=TEST_PATH, \n",
    "                                        do_train=True, \n",
    "                                        do_eval=True, \n",
    "                                        evaluate_during_training=False,\n",
    "                                        line_by_line=True\n",
    "                                        )\n",
    "\n",
    "val_finetuning_params = create_params_modeling(output_dir=OUTPUT_DIR,\n",
    "                                    model_name_or_path=OUTPUT_DIR,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,                                      \n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                    )\n",
    "\n",
    "val_params = create_params_modeling(output_dir=OUTPUT_DIR,\n",
    "                                    model_name_or_path=MODEL_TYPE,\n",
    "                                    model_type=MODEL_TYPE,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,\n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_finetuning.format(**train_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_finetuning.format(**val_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_finetuning.format(**val_params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_generation(MODEL_TYPE, MODEL_NAME_OR_PATH, NUM_RETURN_SEQUENCES=1, LENGTH=20):\n",
    "    return {\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"model_name_or_path\": MODEL_NAME_OR_PATH,\n",
    "        \"num_return_sequences\": NUM_RETURN_SEQUENCES,\n",
    "        \"length\": LENGTH\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_generation = \"\"\"../../transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --num_return_sequences={num_return_sequences} \\\n",
    "    --length={length}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_finetuning_params = create_params_generation(MODEL_TYPE, OUTPUT_DIR, NUM_RETURN_SEQUENCES=5, LENGTH=100)\n",
    "generation_params = create_params_generation(MODEL_TYPE, MODEL_TYPE, NUM_RETURN_SEQUENCES=5, LENGTH=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   Model name '../../weights/gpt2/papers_milan/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '../../weights/gpt2/papers_milan/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   Didn't find file ../../weights/gpt2/papers_milan/added_tokens.json. We won't load it.\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/vocab.json\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/merges.txt\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/special_tokens_map.json\n",
      "07/01/2020 15:30:07 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/tokenizer_config.json\n",
      "07/01/2020 15:30:07 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "07/01/2020 15:30:07 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/01/2020 15:30:10 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=100, model_name_or_path='../../weights/gpt2/papers_milan/', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prompt >>> Hello my dear\n",
      "=== GENERATED SEQUENCE 1 ===\n",
      "Hello my dear, please consider for the case of, if and only if is not identically zero as, is not identically zero and. In view of the above, we will consider for this case only the case of and then assume that A and consequently,,. However, if one, then one is able to verify that, irrespective of whether or not A, is, and therefore, and vice. We will need to do this. Firstly, define A and. Then, and. This is easy\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "Hello my dear, and I will derive the minimum average data rate that is feasible in a feasible and practical setting. To begin with, the best achievable data rate must be strictly higher than the bound given by. Consider a well known algorithm that the performance of an exogenous. For this purpose, it is convenient to recall that an exogenous source such as an i. i.. process can be chosen so that, the exogenous, is able to achieve a given average data rate, which the intended performance level.\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "Hello my dear, we now make a choice as to the function of the distortion redundancy in a proper sense when used as a basis. We now that the optimal choice the redundancy, as in Fig.. With this choice, we obtain the covering in. We derive a fundamental result in the generality of this choice and conclude that the optimal value for is. Furthermore, by the fundamental distortion redundancy of, our solution will give upper on the achievable rate of distortion. All the other applicable and before the discussion are below\n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "Hello my dear and we have the following : We have two main : the measurement campaign and the measurement campaign. We firstly observe that, for any given street canyon, the channel power gain can be within ¦ of the corresponding neighboring, namely,.. This that, for all of us in this region, channel gains can not only be given by path at the of the two main, but also their propagation within ¦ of them. The first two are quite remarkable and we will give them the following le\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "Hello my dear that if there a memoryless u then the memoryless u at a i th time instant when x. Furthermore if there a memoryless, it is known that the matrix, the successive compute and forward channel is independent. Therefore we have that we have a uniform in which there a memoryless such that x u is k k k while only the data of x does not have a memory. Since if there a single such that one k k, it that x of k, it is known that we\n"
     ]
    }
   ],
   "source": [
    "run {cmd_generation.format(**generation_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_generation.format(**generation_params)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
