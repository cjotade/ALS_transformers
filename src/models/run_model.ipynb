{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_TYPE = \"gpt2\" \n",
    "OUTPUT_DIR = f\"../../weights/{MODEL_TYPE}/papers_milan/\"\n",
    "OUTPUT_DIR1 = f\"../../weights/{MODEL_TYPE}/papers_milan1/\"\n",
    "TRAIN_PATH = f\"../../data/papers_milan/train_papers.txt\"\n",
    "TEST_PATH = f\"../../data/papers_milan/test_papers.txt\"\n",
    "VAL_PATH = f\"../../data/papers_milan/val_papers.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_modeling(output_dir, model_type=\"gpt2\", model_name_or_path=None, train_path=None, eval_path=None, \n",
    "                             do_train=False, do_eval=False, evaluate_during_training=False, line_by_line=False, block_size=-1):\n",
    "    return {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"do_train\": \"--do_train\" if do_train else \"\",\n",
    "    \"train_data_file\": train_path if do_train else None,\n",
    "    \"do_eval\": \"--do_eval\" if do_eval else \"\",\n",
    "    \"eval_data_file\": eval_path if do_eval else None,\n",
    "    \"evaluate_during_training\": \"--evaluate_during_training\" if evaluate_during_training else \"\",\n",
    "    \"block_size\": block_size,\n",
    "    \"line_by_line\": \"--line_by_line\" if line_by_line else \"\",\n",
    "    \"fp16\": \"--fp16\",\n",
    "    \"fp16_opt_level\": \"O1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_finetuning = \"\"\"../../transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "    --output_dir={output_dir} \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    {do_train} \\\n",
    "    --train_data_file={train_data_file} \\\n",
    "    {do_eval} \\\n",
    "    --eval_data_file={eval_data_file} \\\n",
    "    {evaluate_during_training} \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --block_size={block_size}\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps 5000 \\\n",
    "    --save_total_limit 5 \\\n",
    "    {line_by_line} \\\n",
    "    {fp16} \\\n",
    "    --fp16_opt_level={fp16_opt_level} \\\n",
    "    --logging_steps 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
    "#   line_by_line, should_continue, and model_name_or_path.\n",
    "train_params = create_params_modeling(output_dir=OUTPUT_DIR, \n",
    "                                        model_type=MODEL_TYPE,\n",
    "                                        train_path=TRAIN_PATH, \n",
    "                                        eval_path=TEST_PATH, \n",
    "                                        do_train=True, \n",
    "                                        do_eval=True, \n",
    "                                        evaluate_during_training=True,\n",
    "                                        line_by_line=True\n",
    "                                        )\n",
    "\n",
    "val_finetuning_params = create_params_modeling(output_dir=OUTPUT_DIR,\n",
    "                                    model_name_or_path=OUTPUT_DIR,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,                                      \n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                    )\n",
    "\n",
    "val_params = create_params_modeling(output_dir=OUTPUT_DIR1,\n",
    "                                    model_name_or_path=MODEL_TYPE,\n",
    "                                    model_type=MODEL_TYPE,\n",
    "                                    train_path=None, \n",
    "                                    eval_path=VAL_PATH,\n",
    "                                    do_train=False, \n",
    "                                    do_eval=True,\n",
    "                                    line_by_line=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:33 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "06/29/2020 22:30:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "06/29/2020 22:30:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/papers_milan/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jun29_22-30-33_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=500, save_total_limit=5, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "06/29/2020 22:30:34 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "06/29/2020 22:30:34 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:30:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "06/29/2020 22:30:35 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:30:36 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/camilojd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "06/29/2020 22:30:36 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/camilojd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "06/29/2020 22:30:46 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /home/camilojd/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "06/29/2020 22:30:49 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "06/29/2020 22:30:49 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/29/2020 22:30:49 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/train_papers.txt\n",
      "06/29/2020 22:30:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "06/29/2020 22:30:50 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/test_papers.txt\n",
      "06/29/2020 22:30:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   ***** Running training *****\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Num examples = 1\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Instantaneous batch size per device = 1\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f428e57dbd44b378a7394853dd1bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d95b3c8efc4383b28b276d9e9d6931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilojd/Environments/als-env/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca9d2fcf49d43d99f9425123bc83fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   {'loss': 4.027278184890747, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0, 'step': 2}\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Num examples = 1\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d26ca62f6ce4808b3579193d1e3f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   {'eval_loss': 4.249687671661377, 'epoch': 2.0, 'step': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9ccc2163294d83ad8f899984bf7535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "06/29/2020 22:30:50 - INFO - transformers.trainer -   Saving model checkpoint to ../../weights/gpt2/papers_milan/\n",
      "06/29/2020 22:30:50 - INFO - transformers.configuration_utils -   Configuration saved in ../../weights/gpt2/papers_milan/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:51 - INFO - transformers.modeling_utils -   Model weights saved in ../../weights/gpt2/papers_milan/pytorch_model.bin\n",
      "06/29/2020 22:30:51 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2020 22:30:51 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "06/29/2020 22:30:51 - INFO - transformers.trainer -     Num examples = 1\n",
      "06/29/2020 22:30:51 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7a8997cf1d42f0b5d94caabc605347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:30:51 - INFO - transformers.trainer -   {'eval_loss': 4.249687671661377, 'epoch': 3.0, 'step': 3}\n",
      "06/29/2020 22:30:51 - INFO - __main__ -   ***** Eval results *****\n",
      "06/29/2020 22:30:51 - INFO - __main__ -     perplexity = 70.0835198587213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**train_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:50:49 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "06/29/2020 22:50:49 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "06/29/2020 22:50:49 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/papers_milan/', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jun29_22-50-49_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=500, save_total_limit=5, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "06/29/2020 22:50:49 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "06/29/2020 22:50:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:50:49 - INFO - transformers.configuration_utils -   loading configuration file ../../weights/gpt2/papers_milan/config.json\n",
      "06/29/2020 22:50:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   Model name '../../weights/gpt2/papers_milan/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '../../weights/gpt2/papers_milan/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   Didn't find file ../../weights/gpt2/papers_milan/added_tokens.json. We won't load it.\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/vocab.json\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/merges.txt\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/special_tokens_map.json\n",
      "06/29/2020 22:50:49 - INFO - transformers.tokenization_utils_base -   loading file ../../weights/gpt2/papers_milan/tokenizer_config.json\n",
      "/home/camilojd/Environments/als-env/lib/python3.7/site-packages/transformers/modeling_auto.py:792: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "06/29/2020 22:50:50 - INFO - transformers.modeling_utils -   loading weights file ../../weights/gpt2/papers_milan/pytorch_model.bin\n",
      "06/29/2020 22:50:52 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "06/29/2020 22:50:52 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ../../weights/gpt2/papers_milan/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "06/29/2020 22:50:52 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/val_papers.txt\n",
      "06/29/2020 22:50:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "06/29/2020 22:50:53 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "06/29/2020 22:50:53 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2020 22:50:53 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "06/29/2020 22:50:53 - INFO - transformers.trainer -     Num examples = 1\n",
      "06/29/2020 22:50:53 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c5f204c5b64ef7b36e431bc6d6f6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:50:53 - INFO - transformers.trainer -   {'eval_loss': 4.179441452026367, 'step': 0}\n",
      "06/29/2020 22:50:53 - INFO - __main__ -   ***** Eval results *****\n",
      "06/29/2020 22:50:53 - INFO - __main__ -     perplexity = 65.3293534435378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**val_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:53:01 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "06/29/2020 22:53:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "06/29/2020 22:53:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../weights/gpt2/papers_milan1/', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jun29_22-53-01_Camilo-UbuntuPC', logging_first_step=False, logging_steps=2, save_steps=500, save_total_limit=5, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "06/29/2020 22:53:01 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "06/29/2020 22:53:01 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:53:02 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/camilojd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "06/29/2020 22:53:02 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "06/29/2020 22:53:03 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/camilojd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "06/29/2020 22:53:03 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/camilojd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "06/29/2020 22:53:13 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /home/camilojd/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "06/29/2020 22:53:16 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "06/29/2020 22:53:16 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/29/2020 22:53:16 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../../data/papers_milan/val_papers.txt\n",
      "06/29/2020 22:53:16 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "06/29/2020 22:53:16 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "06/29/2020 22:53:16 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2020 22:53:16 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "06/29/2020 22:53:16 - INFO - transformers.trainer -     Num examples = 1\n",
      "06/29/2020 22:53:16 - INFO - transformers.trainer -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f17d562a2448ac8ce691fdf8cc3104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2020 22:53:16 - INFO - transformers.trainer -   {'eval_loss': 4.179441452026367, 'step': 0}\n",
      "06/29/2020 22:53:16 - INFO - __main__ -   ***** Eval results *****\n",
      "06/29/2020 22:53:16 - INFO - __main__ -     perplexity = 65.3293534435378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run {cmd_finetuning.format(**val_params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params_generation(MODEL_TYPE, MODEL_NAME_OR_PATH, NUM_RETURN_SEQUENCES=1, LENGTH=20):\n",
    "    return {\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"model_name_or_path\": MODEL_NAME_OR_PATH,\n",
    "        \"num_return_sequences\": NUM_RETURN_SEQUENCES,\n",
    "        \"length\": LENGTH\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_generation = \"\"\"../../transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type={model_type} \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --num_return_sequences={num_return_sequences} \\\n",
    "    --length={length}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_finetuning_params = create_params_generation(MODEL_TYPE, OUTPUT_DIR, NUM_RETURN_SEQUENCES=5, LENGTH=100)\n",
    "generation_params = create_params_generation(MODEL_TYPE, MODEL_TYPE, NUM_RETURN_SEQUENCES=5, LENGTH=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_generation.format(**generation_finetuning_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run {cmd_generation.format(**generation_params)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
